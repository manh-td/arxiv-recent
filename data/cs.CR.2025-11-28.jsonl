{"id": "http://arxiv.org/abs/2511.11784v2", "title": "NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks", "summary": "Jailbreak attacks designed to bypass safety mechanisms pose a serious threat by prompting LLMs to generate harmful or inappropriate content, despite alignment with ethical guidelines. Crafting universal filtering rules remains difficult due to their inherent dependence on specific contexts. To address these challenges without relying on threshold calibration or model fine-tuning, this work introduces a semantic consistency analysis between successful and unsuccessful responses, demonstrating that a negation-aware scoring approach captures meaningful patterns. Building on this insight, a novel detection framework called NegBLEURT Forest is proposed to evaluate the degree of alignment between outputs elicited by adversarial prompts and expected safe behaviors. It identifies anomalous responses using the Isolation Forest algorithm, enabling reliable jailbreak detection. Experimental results show that the proposed method consistently achieves top-tier performance, ranking first or second in accuracy across diverse models using the crafted dataset, while competing approaches exhibit notable sensitivity to model and data variations.", "published": "2025-11-14T14:43:54Z", "updated": "2025-11-28T18:49:16Z", "authors": ["Lama Sleem", "Jerome Francois", "Lujun Li", "Nathan Foucher", "Niccolo Gentile", "Radu State"], "pdf_url": "https://arxiv.org/pdf/2511.11784v2"}
{"id": "http://arxiv.org/abs/2511.23408v1", "title": "Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities", "summary": "Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.", "published": "2025-11-28T18:03:47Z", "updated": "2025-11-28T18:03:47Z", "authors": ["Aayush Garg", "Zanis Ali Khan", "Renzo Degiovanni", "Qiang Tang"], "pdf_url": "https://arxiv.org/pdf/2511.23408v1"}
{"id": "http://arxiv.org/abs/2511.23406v1", "title": "Quantum Private Distributed Matrix Multiplication With Degree Tables", "summary": "In this paper, we explore how quantum resources can be used to increase the rate of private distributed matrix multiplication (PDMM). In PDMM, a user who has two high-dimensional matrices, $A$ and $B$, and lacks the computational capabilities to apply matrix multiplication locally, divides the matrices $A$ and $B$ into $K$ and $L$ sub-blocks, respectively. Then, the user sends them to $N$ servers to apply the required multiplication privately from any $T$ servers. The goal is to reduce the number of servers needed to perform the required matrix multiplication. In the quantum setting, we allow the servers to share an entangled state and respond over quantum channels. Upon receiving the qudits, the user applies measurements to obtain the required multiplication. There are two main regimes in the PDMM literature: The high-privacy regime and the low-privacy regime where $T$ is less than $K$ and $L$.\n  First, in the high-privacy regime, the state-of-the-art classical code is called the gap additive secure polynomial (GASP) code. We define a feasibility requirement in the quantum setting for the GASP code such that the highest performance is achieved when it is satisfied. When it is not satisfied, we address two main concerns. The first is to find a relation between the minimum privacy requirement and the dimensions of the two matrices needed for the feasibility condition to be satisfied. Second, we develop a new family of codes that can work in the quantum setting.\n  Second, since GASP does not work efficiently in the low-privacy regimes compared to cyclic-addition degree tables (CAT) and discretely optimized GASP (DOG), we show that the feasibility condition developed for GASP can be adopted for both CAT and DOG codes as well. In addition, we propose another set of codes that can be used in the low privacy regime in the quantum setting when the feasibility requirement is not satisfied.", "published": "2025-11-28T18:02:11Z", "updated": "2025-11-28T18:02:11Z", "authors": ["Mohamed Nomeir", "Alptug Aytekin", "Lei Hu", "Sennur Ulukus"], "pdf_url": "https://arxiv.org/pdf/2511.23406v1"}
{"id": "http://arxiv.org/abs/2511.23393v1", "title": "FedSGT: Exact Federated Unlearning via Sequential Group-based Training", "summary": "Federated Learning (FL) enables collaborative, privacy-preserving model training, but supporting the \"Right to be Forgotten\" is especially challenging because data influences the model through distributed and interleaved client updates. Existing exact unlearning methods typically require frequent retraining from scratch, resulting in high communication cost and long service downtime. To address this, we propose Federated Sequential Group-based Training (FedSGT), an exact unlearning framework for FL. FedSGT partitions the data into uniform groups, and each client may participate in multiple groups. To control communication overhead, each client can limit the number of groups it contributes to. FedSGT then trains multiple sequences of Parameter-Efficient Fine-Tuning (PEFT) modules, each corresponding to a different group permutation. Since the PEFT modules are lightweight and maintained server-side, FedSGT isolates the influence of different data groups into independent modules without incurring significant storage overhead and communication cost. Exact unlearning is thus achieved instantly by deactivating the modules corresponding to the group containing the unlearned data. Furthermore, using multiple training sequences helps maintain high model utility as deletion requests accumulate. We provide a rigorous theoretical analysis of both the deletion rate -- expected number of deletions before retraining is needed -- and the expected model performance. Experiments on various tasks demonstrate that FedSGT achieves a significantly longer service maintenance under multiple unlearning requests while maintaining comparable learning performance and training efficiency to other exact unlearning baselines. Extensive ablation studies validate the robustness of our method across a wide range of parameter settings.", "published": "2025-11-28T17:36:03Z", "updated": "2025-11-28T17:36:03Z", "authors": ["Bokang Zhang", "Hong Guan", "Hong kyu Lee", "Ruixuan Liu", "Jia Zou", "Li Xiong"], "pdf_url": "https://arxiv.org/pdf/2511.23393v1"}
{"id": "http://arxiv.org/abs/2510.22726v2", "title": "SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking", "summary": "SpoofTrackBench is a reproducible, modular benchmark for evaluating adversarial robustness in real-time localization and tracking (RTLS) systems under radar spoofing. Leveraging the Hampton University Skyler Radar Sensor dataset, we simulate drift, ghost, and mirror-type spoofing attacks and evaluate tracker performance using both Joint Probabilistic Data Association (JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates clean and spoofed detection streams, visualizes spoof-induced trajectory divergence, and quantifies assignment errors via direct drift-from-truth metrics. Clustering overlays, injection-aware timelines, and scenario-adaptive visualizations enable interpretability across spoof types and configurations. Evaluation figures and logs are auto-exported for reproducible comparison. SpoofTrackBench sets a new standard for open, ethical benchmarking of spoof-aware tracking pipelines, enabling rigorous cross-architecture analysis and community validation.", "published": "2025-10-26T15:54:16Z", "updated": "2025-11-28T17:14:16Z", "authors": ["Van Le", "Tan Le"], "pdf_url": "https://arxiv.org/pdf/2510.22726v2"}
{"id": "http://arxiv.org/abs/2511.23278v1", "title": "RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications", "summary": "Modern cloud applications are built on independent, diverse microservices, offering scalability, flexibility, and usage-based billing. However, the structural design of these varied services, along with their reliance on auto-scalers for dynamic internet traffic, introduces significant coordination challenges. As we demonstrate in this paper, common default retry patterns used between misaligned services can turn into retry storms which drive up resource usage and costs, leading to self-inflicted Denial-of-Wallet (DoW) scenarios. To overcome these problems we introduce RetryGuard, a distributed framework for productive control of retry patterns across interdependent microservices. By managing retry policy on a per-service basis and making parallel decisions, RetryGuard prevents retry storms, curbs resource contention, and mitigates escalating operational costs. RetryGuard makes its decisions based on an analytic model that captures the relationships among retries, throughput (rejections), delays, and costs. Experimental results show that RetryGuard significantly reduces resource usage and costs compared to AWS standard and advanced retry policies. We further demonstrate its scalability and superior performance in a more complex Kubernetes deployment with the Istio service mesh, where it achieves substantial improvements.", "published": "2025-11-28T15:31:25Z", "updated": "2025-11-28T15:31:25Z", "authors": ["Jhonatan Tavori", "Anat Bremler-Barr", "Hanoch Levy", "Ofek Lavi"], "pdf_url": "https://arxiv.org/pdf/2511.23278v1"}
{"id": "http://arxiv.org/abs/2511.23252v1", "title": "One-Shot Secure Aggregation: A Hybrid Cryptographic Protocol for Private Federated Learning in IoT", "summary": "Federated Learning (FL) offers a promising approach to collaboratively train machine learning models without centralizing raw data, yet its scalability is often throttled by excessive communication overhead. This challenge is magnified in Internet of Things (IoT) environments, where devices face stringent bandwidth, latency, and energy constraints. Conventional secure aggregation protocols, while essential for protecting model updates, frequently require multiple interaction rounds, large payload sizes, and per-client costs rendering them impractical for many edge deployments.\n  In this work, we present Hyb-Agg, a lightweight and communication-efficient secure aggregation protocol that integrates Multi-Key CKKS (MK-CKKS) homomorphic encryption with Elliptic Curve Diffie-Hellman (ECDH)-based additive masking. Hyb-Agg reduces the secure aggregation process to a single, non-interactive client-to-server transmission per round, ensuring that per-client communication remains constant regardless of the number of participants. This design eliminates partial decryption exchanges, preserves strong privacy under the RLWE, CDH, and random oracle assumptions, and maintains robustness against collusion by the server and up to $N-2$ clients.\n  We implement and evaluate Hyb-Agg on both high-performance and resource-constrained devices, including a Raspberry Pi 4, demonstrating that it delivers sub-second execution times while achieving a constant communication expansion factor of approximately 12x over plaintext size. By directly addressing the communication bottleneck, Hyb-Agg enables scalable, privacy-preserving federated learning that is practical for real-world IoT deployments.", "published": "2025-11-28T15:01:26Z", "updated": "2025-11-28T15:01:26Z", "authors": ["Imraul Emmaka", "Tran Viet Xuan Phuong"], "pdf_url": "https://arxiv.org/pdf/2511.23252v1"}
{"id": "http://arxiv.org/abs/2511.23200v1", "title": "Quantifying the Privacy-Utility Trade-off in GPS-based Daily Stress Recognition using Semantic Features", "summary": "Psychological stress is a widespread issue that significantly impacts student well-being and academic performance. Effective remote stress recognition is crucial, yet existing methods often rely on wearable devices or GPS-based clustering techniques that pose privacy risks. In this study, we introduce a novel, end-to-end privacy-enhanced framework for semantic location encoding using a self-hosted OSM engine and an LLM-bootstrapped static map. We rigorously quantify the privacy-utility trade-off and demonstrate (via LOSO validation) that our Privacy-Aware (PA) model achieves performance statistically indistinguishable from a non-private model, proving that utility does not require sacrificing privacy. Feature importance analysis highlights that recreational activity time, working time, and travel time play a significant role in stress recognition.", "published": "2025-11-28T14:04:00Z", "updated": "2025-11-28T14:04:00Z", "authors": ["Hoang Khang Phan", "Nhat Tan Le"], "pdf_url": "https://arxiv.org/pdf/2511.23200v1"}
{"id": "http://arxiv.org/abs/2511.23198v1", "title": "Clustering Malware at Scale: A First Full-Benchmark Study", "summary": "Recent years have shown that malware attacks still happen with high frequency. Malware experts seek to categorize and classify incoming samples to confirm their trustworthiness or prove their maliciousness. One of the ways in which groups of malware samples can be identified is through malware clustering. Despite the efforts of the community, malware clustering which incorporates benign samples has been under-explored. Moreover, despite the availability of larger public benchmark malware datasets, malware clustering studies have avoided fully utilizing these datasets in their experiments, often resorting to small datasets with only a few families. Additionally, the current state-of-the-art solutions for malware clustering remain unclear. In our study, we evaluate malware clustering quality and establish the state-of-the-art on Bodmas and Ember - two large public benchmark malware datasets. Ours is the first study of malware clustering performed on whole malware benchmark datasets. Additionally, we extend the malware clustering task by incorporating benign samples. Our results indicate that incorporating benign samples does not significantly degrade clustering quality. We find that there are significant differences in the quality of the created clusters between Ember and Bodmas, as well as a private industry dataset. Contrary to popular opinion, our top clustering performers are K-Means and BIRCH, with DBSCAN and HAC falling behind.", "published": "2025-11-28T14:02:17Z", "updated": "2025-11-28T14:02:17Z", "authors": ["Martin Mocko", "Jakub Ševcech", "Daniela Chudá"], "pdf_url": "https://arxiv.org/pdf/2511.23198v1"}
{"id": "http://arxiv.org/abs/2511.08905v2", "title": "iSeal: Encrypted Fingerprinting for Reliable LLM Ownership Verification", "summary": "Given the high cost of large language model (LLM) training from scratch, safeguarding LLM intellectual property (IP) has become increasingly crucial. As the standard paradigm for IP ownership verification, LLM fingerprinting thus plays a vital role in addressing this challenge. Existing LLM fingerprinting methods verify ownership by extracting or injecting model-specific features. However, they overlook potential attacks during the verification process, leaving them ineffective when the model thief fully controls the LLM's inference process. In such settings, attackers may share prompt-response pairs to enable fingerprint unlearning or manipulate outputs to evade exact-match verification. We propose iSeal, the first fingerprinting method designed for reliable verification when the model thief controls the suspected LLM in an end-to-end manner. It injects unique features into both the model and an external module, reinforced by an error-correction mechanism and a similarity-based verification strategy. These components are resistant to verification-time attacks, including collusion-based fingerprint unlearning and response manipulation, backed by both theoretical analysis and empirical results. iSeal achieves 100 percent Fingerprint Success Rate (FSR) on 12 LLMs against more than 10 attacks, while baselines fail under unlearning and response manipulations.", "published": "2025-11-12T02:30:19Z", "updated": "2025-11-28T13:58:50Z", "authors": ["Zixun Xiong", "Gaoyi Wu", "Qingyang Yu", "Mingyu Derek Ma", "Lingfeng Yao", "Miao Pan", "Xiaojiang Du", "Hao Wang"], "pdf_url": "https://arxiv.org/pdf/2511.08905v2"}
{"id": "http://arxiv.org/abs/2511.23183v1", "title": "Identification of Malicious Posts on the Dark Web Using Supervised Machine Learning", "summary": "Given the constant growth and increasing sophistication of cyberattacks, cybersecurity can no longer rely solely on traditional defense techniques and tools. Proactive detection of cyber threats has become essential to help security teams identify potential risks and implement effective mitigation measures. Cyber Threat Intelligence (CTI) plays a key role by providing security analysts with evidence-based knowledge about cyber threats. CTI information can be extracted using various techniques and data sources; however, machine learning has proven promising. As for data sources, social networks and online discussion forums are commonly explored. In this study, we apply text mining techniques and machine learning to data collected from Dark Web forums in Brazilian Portuguese to identify malicious posts. Our contributions include the creation of three original datasets, a novel multi-stage labeling process combining indicators of compromise (IoCs), contextual keywords, and manual analysis, and a comprehensive evaluation of text representations and classifiers. To our knowledge, this is the first study to focus specifically on Brazilian Portuguese content in this domain. The best-performing model, using LightGBM and TF-IDF, was able to detect relevant posts with high accuracy. We also applied topic modeling to validate the model's outputs on unlabeled data, confirming its robustness in real-world scenarios.", "published": "2025-11-28T13:51:18Z", "updated": "2025-11-28T13:51:18Z", "authors": ["Sebastião Alves de Jesus Filho", "Gustavo Di Giovanni Bernardo", "Paulo Henrique Ribeiro Gabriel", "Bruno Bogaz Zarpelão", "Rodrigo Sanches Miani"], "pdf_url": "https://arxiv.org/pdf/2511.23183v1"}
{"id": "http://arxiv.org/abs/2511.18531v2", "title": "LockForge: Automating Paper-to-Code for Logic Locking with Multi-Agent Reasoning LLMs", "summary": "Despite rapid progress in logic locking (LL), reproducibility remains a challenge as codes are rarely made public. We present LockForge, a first-of-its-kind, multi-agent large language model (LLM) framework that turns LL descriptions in papers into executable and tested code. LockForge provides a carefully crafted pipeline realizing forethought, implementation, iterative refinement, and a multi-stage validation, all to systematically bridge the gap between prose and practice for complex LL schemes. For validation, we devise (i) an LLM-as-Judge stage with a scoring system considering behavioral checks, conceptual mechanisms, structural elements, and reproducibility on benchmarks, and (ii) an independent LLM-as-Examiner stage for ground-truth assessment. We apply LockForge to 10 seminal LL schemes, many of which lack reference implementations. Our evaluation on multiple SOTA LLMs, including ablation studies, reveals the significant complexity of the task. We show that an advanced reasoning model and a sophisticated, multi-stage framework like LockForge are required. We release all implementations and benchmarks, providing a reproducible and fair foundation for evaluation of further LL research.", "published": "2025-11-23T16:51:32Z", "updated": "2025-11-28T13:42:32Z", "authors": ["Akashdeep Saha", "Zeng Wang", "Prithwish Basu Roy", "Johann Knechtel", "Ozgur Sinanoglu", "Ramesh Karri"], "pdf_url": "https://arxiv.org/pdf/2511.18531v2"}
{"id": "http://arxiv.org/abs/2505.04195v2", "title": "AutoPatch: Multi-Agent Framework for Patching Real-World CVE Vulnerabilities", "summary": "Large Language Models (LLMs) have emerged as promising tools in software development, enabling automated code generation and analysis. However, their knowledge is limited to a fixed cutoff date, making them prone to generating code vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets is costly, and existing LLM-based approaches focus on oversimplified CWE examples and require providing explicit bug locations to LLMs, limiting their ability to patch complex real-world vulnerabilities. To address these limitations, we propose AutoPatch, a multi-agent framework designed to patch vulnerable LLM-generated code, particularly those introduced after the LLMs' knowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG) with a structured database of recently disclosed vulnerabilities, comprising 525 code snippets derived from 75 high-severity CVEs across real-world systems such as the Linux kernel and Chrome. AutoPatch combines semantic and taint analysis to identify the most relevant CVE and leverages enhanced Chain-of-Thought (CoT) reasoning to construct enriched prompts for verification and patching. Our unified similarity model, which selects the most relevant vulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch attains 89.5 percent F1-score for vulnerability verification and 95.0 percent accuracy in patching, while being over 50x more cost-efficient than traditional fine-tuning approaches.", "published": "2025-05-07T07:49:05Z", "updated": "2025-11-28T11:15:57Z", "authors": ["Minjae Seo", "Wonwoo Choi", "Myoungsung You", "Seungwon Shin"], "pdf_url": "https://arxiv.org/pdf/2505.04195v2"}
{"id": "http://arxiv.org/abs/2511.23026v1", "title": "A Game-Theoretic Approach for Adversarial Information Fusion in Distributed Sensor Networks", "summary": "Every day we share our personal information through digital systems which are constantly exposed to threats. For this reason, security-oriented disciplines of signal processing have received increasing attention in the last decades: multimedia forensics, digital watermarking, biometrics, network monitoring, steganography and steganalysis are just a few examples. Even though each of these fields has its own peculiarities, they all have to deal with a common problem: the presence of one or more adversaries aiming at making the system fail. Adversarial Signal Processing lays the basis of a general theory that takes into account the impact that the presence of an adversary has on the design of effective signal processing tools. By focusing on the application side of Adversarial Signal Processing, namely adversarial information fusion in distributed sensor networks, and adopting a game-theoretic approach, this thesis contributes to the above mission by addressing four issues. First, we address decision fusion in distributed sensor networks by developing a novel soft isolation defense scheme that protect the network from adversaries, specifically, Byzantines. Second, we develop an optimum decision fusion strategy in the presence of Byzantines. In the next step, we propose a technique to reduce the complexity of the optimum fusion by relying on a novel near-optimum message passing algorithm based on factor graphs. Finally, we introduce a defense mechanism to protect decentralized networks running consensus algorithm against data falsification attacks.", "published": "2025-11-28T09:47:26Z", "updated": "2025-11-28T09:47:26Z", "authors": ["Kassem Kallas"], "pdf_url": "https://arxiv.org/pdf/2511.23026v1"}
{"id": "http://arxiv.org/abs/2511.22924v1", "title": "AgentShield: Make MAS more secure and efficient", "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) offer powerful cooperative reasoning but remain vulnerable to adversarial attacks, where compromised agents can undermine the system's overall performance. Existing defenses either depend on single trusted auditors, creating single points of failure, or sacrifice efficiency for robustness. To resolve this tension, we propose \\textbf{AgentShield}, a distributed framework for efficient, decentralized auditing. AgentShield introduces a novel three-layer defense: \\textbf{(i) Critical Node Auditing} prioritizes high-influence agents via topological analysis; \\textbf{(ii) Light Token Auditing} implements a cascade protocol using lightweight sentry models for rapid discriminative verification; and \\textbf{(iii) Two-Round Consensus Auditing} triggers heavyweight arbiters only upon uncertainty to ensure global agreement. This principled design optimizes the robustness-efficiency trade-off. Experiments demonstrate that AgentShield achieves a 92.5\\% recovery rate and reduces auditing overhead by over 70\\% compared to existing methods, maintaining high collaborative accuracy across diverse MAS topologies and adversarial scenarios.", "published": "2025-11-28T06:55:50Z", "updated": "2025-11-28T06:55:50Z", "authors": ["Kaixiang Wang", "Zhaojiacheng Zhou", "Bunyod Suvonov", "Jiong Lou", "Jie LI"], "pdf_url": "https://arxiv.org/pdf/2511.22924v1"}
{"id": "http://arxiv.org/abs/2412.01784v2", "title": "Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models", "summary": "Capability evaluations play a crucial role in assessing and regulating frontier AI systems. The effectiveness of these evaluations faces a significant challenge: strategic underperformance, or ``sandbagging'', where models deliberately underperform during evaluation. Sandbagging can manifest either through explicit developer intervention or through unintended model behavior, presenting a fundamental obstacle to accurate capability assessment. We introduce a novel sandbagging detection method based on injecting noise of varying magnitudes into model weights. While non-sandbagging models show predictable performance degradation with increasing noise, we demonstrate that sandbagging models exhibit anomalous performance improvements, likely due to disruption of underperformance mechanisms while core capabilities remain partially intact. Through experiments across various model architectures, sizes, and sandbagging techniques, we establish this distinctive response pattern as a reliable, model-agnostic signal for detecting sandbagging behavior. Importantly, we find noise-injection is capable of eliciting the full performance of Mistral Large 120B in a setting where the model underperforms without being instructed to do so. Our findings provide a practical tool for AI evaluation and oversight, addressing a challenge in ensuring accurate capability assessment of frontier AI systems.", "published": "2024-12-02T18:34:51Z", "updated": "2025-11-28T05:08:15Z", "authors": ["Cameron Tice", "Philipp Alexander Kreer", "Nathan Helm-Burger", "Prithviraj Singh Shahani", "Fedor Ryzhenkov", "Jacob Haimes", "Felix Hofstätter", "Teun van der Weij"], "pdf_url": "https://arxiv.org/pdf/2412.01784v2"}
{"id": "http://arxiv.org/abs/2511.22859v1", "title": "TokCom-UEP: Semantic Importance-Matched Unequal Error Protection for Resilient Image Transmission", "summary": "Based on the provided LaTeX code, here is the metadata for the submission form: Title: TokCom-UEP: Semantic Importance-Matched Unequal Error Protection for Resilient Image Transmission Author(s): Kaizheng Zhang, Zuolin Jin, Zhihang Cheng, Ming Zeng, Li Qiao, Zesong Fei Abstract: Token communication (TokCom), an emerging semantic communication framework powered by Large Multimodal Model (LMM), has become a key paradigm for resilient data transmission in 6G networks. A key limitation of existing TokCom designs lies in the assumption of uniform token importance, which leads to the adoption of equal error protection (EEP). However, compressed one-dimensional (1D) token sequences inherently exhibit heterogeneous semantic importance hierarchies, rendering EEP schemes suboptimal. To address this, this paper proposes TokCom-UEP, a novel semantic importance-matched unequal error protection (UEP) framework designed for resilient image transmission. TokCom-UEP integrates rateless UEP coding with the non-uniform semantic importance of tokens by partitioning source tokens into nested expanding windows, assigning higher selection probabilities to windows containing critical tokens to ensure their prioritized recovery. Simulation results demonstrate that TokCom-UEP outperforms EEP schemes in terms of three core semantic restoration metrics and spectral efficiency under low-overhead conditions.", "published": "2025-11-28T03:29:06Z", "updated": "2025-11-28T03:29:06Z", "authors": ["Kaizheng Zhang", "Zuolin Jin", "Zhihang Cheng", "Ming Zeng", "Li Qiao", "Zesong Fei"], "pdf_url": "https://arxiv.org/pdf/2511.22859v1"}
