{"id": "http://arxiv.org/abs/2601.20755v1", "title": "ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler", "summary": "As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.", "published": "2026-01-28T16:39:38Z", "updated": "2026-01-28T16:39:38Z", "authors": ["Bohua Zou", "Debayan Roy", "Dhimankumar Yogesh Airao", "Weihao Xu", "Binqi Sun", "Yutao Liu", "Haibo Chen"], "pdf_url": "https://arxiv.org/pdf/2601.20755v1"}
{"id": "http://arxiv.org/abs/2601.20662v1", "title": "Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model", "summary": "Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.\n  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.\n  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.", "published": "2026-01-28T14:44:23Z", "updated": "2026-01-28T14:44:23Z", "authors": ["Julien Malka", "Arnout Engelen"], "pdf_url": "https://arxiv.org/pdf/2601.20662v1"}
{"id": "http://arxiv.org/abs/2502.05739v2", "title": "Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning", "summary": "Large Language Models for Code (LLMs4Code) have achieved strong performance in code generation, but recent studies reveal that they may memorize and leak sensitive information contained in training data, posing serious privacy risks. To address this gap, this work presents the first comprehensive empirical study on applying machine unlearning to mitigate sensitive information leakage in LLMs4Code. We first construct a dedicated benchmark that includes: (i) a synthetic forget set containing diverse forms of personal information, and (ii) a retain set designed to evaluate whether code-generation capability is preserved after unlearning. Using this benchmark, we systematically assess three representative unlearning algorithms (GA, GA+GD, GA+KL) across three widely used open-source LLMs4Code models (AIXCoder-7B, CodeLlama-7B, CodeQwen-7B). Experimental results demonstrate that machine unlearning can substantially reduce direct memorization-based leakage: on average, the direct leak rate drops by more than 50% while retaining about over 91% of the original code-generation performance. Moreover, by analyzing post-unlearning outputs, we uncover a consistent shift from direct to indirect leakage, revealing an underexplored vulnerability that persists even when the target data has been successfully forgotten. Our findings show that machine unlearning is a feasible and effective solution for enhancing privacy protection in LLMs4Code, while also highlighting the need for future techniques capable of mitigating both direct and indirect leakage simultaneously.", "published": "2025-02-09T01:50:34Z", "updated": "2026-01-28T14:43:46Z", "authors": ["Shanzhi Gu", "Zhaoyang Qu", "Ruotong Geng", "Mingyang Geng", "Shangwen Wang", "Chuanfu Xu", "Haotian Wang", "Zhipeng Lin", "Dezun Dong"], "pdf_url": "https://arxiv.org/pdf/2502.05739v2"}
{"id": "http://arxiv.org/abs/2509.20353v2", "title": "Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study", "summary": "This study investigates the real-world impact of the generative AI (GenAI) tool GitHub Copilot on developer activity and perceived productivity. We conducted a mixed-methods case study in NAV IT, a large public sector agile organization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's GitHub repositories over a two-year period, focusing on commit-based activity metrics from 25 Copilot users and 14 non-users. The analysis was complemented by survey responses on their roles and perceived productivity, as well as 13 interviews. Our analysis of activity metrics revealed that individuals who used Copilot were consistently more active than non-users, even prior to Copilot's introduction. We did not find any statistically significant changes in commit-based activity for Copilot users after they adopted the tool, although minor increases were observed. This suggests a discrepancy between changes in commit-based metrics and the subjective experience of productivity.", "published": "2025-09-24T17:55:56Z", "updated": "2026-01-28T14:17:12Z", "authors": ["Viktoria Stray", "Elias Goldmann Brandtzæg", "Viggo Tellefsen Wivestad", "Astri Barbala", "Nils Brede Moe"], "pdf_url": "https://arxiv.org/pdf/2509.20353v2"}
{"id": "http://arxiv.org/abs/2601.20615v1", "title": "DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.", "published": "2026-01-28T13:51:00Z", "updated": "2026-01-28T13:51:00Z", "authors": ["Yanlin Wang", "Jiadong Wu", "Tianyue Jiang", "Mingwei Liu", "Jiachi Chen", "Chong Wang", "Ensheng Shi", "Xilin Liu", "Yuchi Ma", "Zibin Zheng"], "pdf_url": "https://arxiv.org/pdf/2601.20615v1"}
{"id": "http://arxiv.org/abs/2509.22202v2", "title": "Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries", "summary": "Large language models (LLMs) are increasingly used to generate code, yet they continue to hallucinate, often inventing non-existent libraries. Such library hallucinations are not just benign errors: they can mislead developers, break builds, and expose systems to supply chain threats such as slopsquatting. Despite increasing awareness of these risks, little is known about how real-world prompt variations affect hallucination rates. Therefore, we present the first systematic study of how user-level prompt variations impact library hallucinations in LLM-generated code. We evaluate seven diverse LLMs across two hallucination types: library name hallucinations (invalid imports) and library member hallucinations (invalid calls from valid libraries). We investigate how realistic user language extracted from developer forums and how user errors of varying degrees (one- or multi-character misspellings and completely fake names/members) affect LLM hallucination rates. Our findings reveal systemic vulnerabilities: one-character misspellings in library names trigger hallucinations in up to 26% of tasks, fake library names are accepted in up to 99% of tasks, and time-related prompts lead to hallucinations in up to 84% of tasks. Prompt engineering shows promise for mitigating hallucinations, but remains inconsistent and LLM-dependent. Our results underscore the fragility of LLMs to natural prompt variation and highlight the urgent need for safeguards against library-related hallucinations and their potential exploitation.", "published": "2025-09-26T11:14:38Z", "updated": "2026-01-28T11:43:56Z", "authors": ["Lukas Twist", "Jie M. Zhang", "Mark Harman", "Helen Yannakoudakis"], "pdf_url": "https://arxiv.org/pdf/2509.22202v2"}
{"id": "http://arxiv.org/abs/2601.20459v1", "title": "Challenges in Android Data Disclosure: An Empirical Study", "summary": "Current legal frameworks enforce that Android developers accurately report the data their apps collect. However, large codebases can make this reporting challenging. This paper employs an empirical approach to understand developers' experience with Google Play Store's Data Safety Section (DSS) form.\n  We first survey 41 Android developers to understand how they categorize privacy-related data into DSS categories and how confident they feel when completing the DSS form. To gain a broader and more detailed view of the challenges developers encounter during the process, we complement the survey with an analysis of 172 online developer discussions, capturing the perspectives of 642 additional developers. Together, these two data sources represent insights from 683 developers.\n  Our findings reveal that developers often manually classify the privacy-related data their apps collect into the data categories defined by Google-or, in some cases, omit classification entirely-and rely heavily on existing online resources when completing the form. Moreover, developers are generally confident in recognizing the data their apps collect, yet they lack confidence in translating this knowledge into DSS-compliant disclosures. Key challenges include issues in identifying privacy-relevant data to complete the form, limited understanding of the form, and concerns about app rejection due to discrepancies with Google's privacy requirements.\n  These results underscore the need for clearer guidance and more accessible tooling to support developers in meeting privacy-aware reporting obligations.", "published": "2026-01-28T10:33:38Z", "updated": "2026-01-28T10:33:38Z", "authors": ["Mugdha Khedkar", "Michael Schlichtig", "Mohamed Soliman", "Eric Bodden"], "pdf_url": "https://arxiv.org/pdf/2601.20459v1"}
{"id": "http://arxiv.org/abs/2601.20415v1", "title": "An Empirical Evaluation of Modern MLOps Frameworks", "summary": "Given the increasing adoption of AI solutions in professional environments, it is necessary for developers to be able to make informed decisions about the current tool landscape. This work empirically evaluates various MLOps (Machine Learning Operations) tools to facilitate the management of the ML model lifecycle: MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines. The tools are evaluated by assessing the criteria of Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, result interpretability, and Documentation when implementing two common ML scenarios: Digit classifier with MNIST and Sentiment classifier with IMDB and BERT. The evaluation is completed by providing weighted results that lead to practical conclusions on which tools are best suited for different scenarios.", "published": "2026-01-28T09:22:22Z", "updated": "2026-01-28T09:22:22Z", "authors": ["Jon Marcos-Mercadé", "Unai Lopez-Novoa", "Mikel Egaña Aranguren"], "pdf_url": "https://arxiv.org/pdf/2601.20415v1"}
{"id": "http://arxiv.org/abs/2509.23130v3", "title": "SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems", "summary": "Formal models are essential to specifying large, complex computer systems and verifying their correctness, but are notoriously expensive to write and maintain. Recent advances in generative AI show promise in generating certain forms of specifications. However, existing work mostly targets small code, not complete systems. It is unclear whether AI can deal with realistic system artifacts, as this requires abstracting their complex behavioral properties into formal models. We present SysMoBench, a benchmark that evaluates AI's ability to formally model large, complex systems. We focus on concurrent and distributed systems, which are keystones of today's critical computing infrastructures, encompassing operating systems and cloud infrastructure. We use TLA+, the de facto specification language for concurrent and distributed systems, though the benchmark can be extended to other specification languages. We address the primary challenge of evaluating AI-generated models by automating metrics like syntactic and runtime correctness, conformance to system code, and invariant correctness. SysMoBench currently includes eleven diverse system artifacts: the Raft implementation of Etcd and Redis, the leader election of ZooKeeper, the Spinlock, Mutex, and Ringbuffer in Asterinas OS, etc., with more being added. SysMoBench enables us to understand the capabilities and limitations of today's LLMs and agents, putting tools in this area on a firm footing and opening up promising new research directions.", "published": "2025-09-27T05:24:54Z", "updated": "2026-01-28T09:19:42Z", "authors": ["Qian Cheng", "Ruize Tang", "Emilie Ma", "Finn Hackett", "Peiyang He", "Yiming Su", "Ivan Beschastnikh", "Yu Huang", "Xiaoxing Ma", "Tianyin Xu"], "pdf_url": "https://arxiv.org/pdf/2509.23130v3"}
{"id": "http://arxiv.org/abs/2601.20412v1", "title": "Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents", "summary": "The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.", "published": "2026-01-28T09:17:51Z", "updated": "2026-01-28T09:17:51Z", "authors": ["Qihao Wang", "Yue Hu", "Mingzhe Lu", "Jiayue Wu", "Yanbing Liu", "Yuanmin Tang"], "pdf_url": "https://arxiv.org/pdf/2601.20412v1"}
{"id": "http://arxiv.org/abs/2601.14861v2", "title": "Reclaiming Software Engineering as the Enabling Technology for the Digital Age", "summary": "Software engineering is the invisible infrastructure of the digital age. Every breakthrough in artificial intelligence, quantum computing, photonics, and cybersecurity relies on advances in software engineering, yet the field is too often treated as a supportive digital component rather than as a strategic, enabling discipline. In policy frameworks, including major European programmes, software appears primarily as a building block within other technologies, while the scientific discipline of software engineering remains largely absent. This position paper argues that the long-term sustainability, dependability, and sovereignty of digital technologies depend on investment in software engineering research. It is a call to reclaim the identity of software engineering.", "published": "2026-01-21T10:44:35Z", "updated": "2026-01-28T09:13:34Z", "authors": ["Tanja E. J. Vos", "Tijs van der Storm", "Alexander Serebrenik", "Lionel Briand", "Roberto Di Cosmo", "J. -M Bruel", "Benoît Combemale"], "pdf_url": "https://arxiv.org/pdf/2601.14861v2"}
{"id": "http://arxiv.org/abs/2601.20404v1", "title": "On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents", "summary": "AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS.md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS.md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS.md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.", "published": "2026-01-28T09:09:30Z", "updated": "2026-01-28T09:09:30Z", "authors": ["Jai Lal Lulla", "Seyedmoein Mohsenimofidi", "Matthias Galster", "Jie M. Zhang", "Sebastian Baltes", "Christoph Treude"], "pdf_url": "https://arxiv.org/pdf/2601.20404v1"}
{"id": "http://arxiv.org/abs/2601.20394v1", "title": "Comprehension vs. Adoption: Evaluating a Language Workbench Through a Family of Experiments", "summary": "Language workbenches are tools that enable the definition, reuse, and composition of programming languages and their ecosystems, aiming to streamline language development. To facilitate their adoption by language designers, the comprehensibility of the language used to define other languages is an important aspect to evaluate. Moreover, considering that language workbenches are relatively new tools, user acceptance emerges as a crucial factor to be accounted for during their assessment. Current literature often neglects user-centred aspects like comprehensibility and acceptance in the assessment of this breed of tools. This paper addresses this gap through a family of experiments assessing Neverlang, a modular language workbench. The study adopts a tailored version of the Method Evaluation Model (MEM) to evaluate the comprehensibility of Neverlang's meta-language and programs, as well as user acceptance in terms of perceived ease of use, perceived usefulness, and intention to use. It also investigates the relationships among these dimensions. The experiments were conducted in three iterations involving participants from academia. The results reveal that users demonstrate sufficient comprehension of Neverlang's meta-language, particularly concerning its syntax, express a favourable perception of its usefulness, and indicate their intention to use it. However, the results also indicate that Neverlang's ease of use remains a challenge. Additionally, variations in the perceived ease of use and perceived usefulness, whether low or high, influence the users' intention to use the tool. Surprisingly, no significant correlation is found between comprehensibility and user acceptance. Notably, higher comprehensibility of the meta-language does not necessarily translate into greater acceptance, underscoring the complex interplay between comprehension and adoption.", "published": "2026-01-28T09:00:59Z", "updated": "2026-01-28T09:00:59Z", "authors": ["Giovanna Broccia", "Maurice H. ter Beek", "Walter Cazzola", "Luca Favalli", "Francesco Bertolotti", "Alessio Ferrari"], "pdf_url": "https://arxiv.org/pdf/2601.20394v1"}
{"id": "http://arxiv.org/abs/2601.20382v1", "title": "How Software Engineering Research Overlooks Local Industry: A Smaller Economy Perspective", "summary": "The software engineering researchers from countries with smaller economies, particularly non-English speaking ones, represent valuable minorities within the software engineering community. As researchers from Poland, we represent such a country. We analyzed the ICSE FOSE (Future of Software Engineering) community survey through reflexive thematic analysis to show our viewpoint on key software community issues. We believe that the main problem is the growing research-industry gap, which particularly impacts smaller communities and small local companies. Based on this analysis and our experiences, we present a set of recommendations for improvements that would enhance software engineering research and industrial collaborations in smaller economies.", "published": "2026-01-28T08:47:22Z", "updated": "2026-01-28T08:47:22Z", "authors": ["Klara Borowa", "Andrzej Zalewski", "Lech Madeyski"], "pdf_url": "https://arxiv.org/pdf/2601.20382v1"}
{"id": "http://arxiv.org/abs/2511.05097v2", "title": "Did You Forkget It? Detecting One-Day Vulnerabilities in Open-source ForksWith Global History Analysis", "summary": "Tracking vulnerabilities inherited from third-party open-source software is a well-known challenge, often addressed by tracing the threads of dependency information. However, vulnerabilities can also propagate through forking: a code repository forked after the introduction of a vulnerability, but before it is patched, may remain vulnerable long after the vulnerability has been fixed in the initial repository. History analysis approaches are used to track vulnerable software versions at scale. However, such approaches fail to track vulnerabilities in forks, leaving fork maintainers to identify them manually. This paper presents a global history analysis approach to help software developers identify one-day (known but unpatched) vulnerabilities in forked repositories. Leveraging the global graph of public code, as captured by the Software Heritage archive, our approach propagates vulnerability information at the commit level and performs automated impact analysis. Starting from 7162 repositories with vulnerable commits listed in OSV, we propagate vulnerability information to 2.2 million forks. We evaluate our approach by filtering forks with significant user bases whose latest commit is still potentially vulnerable, manually auditing the code, and contacting maintainers for confirmation and responsible disclosure. This process identified 135 high-severity one-day vulnerabilities, achieving a precision of 0.69, with 9 confirmed by maintainers.", "published": "2025-11-07T09:25:47Z", "updated": "2026-01-28T08:44:45Z", "authors": ["Romain Lefeuvre", "Charly Reux", "Stefano Zacchiroli", "Olivier Barais", "Benoit Combemale"], "pdf_url": "https://arxiv.org/pdf/2511.05097v2"}
{"id": "http://arxiv.org/abs/2402.15769v3", "title": "GenCode: A Generic Data Augmentation Framework for Boosting Deep Learning-Based Code Understanding", "summary": "Pre-trained code models lead the era of code intelligence, with multiple models designed with impressive performance. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in this field. In this paper, we introduce a generic data augmentation framework, GenCode, to enhance the training of code understanding models. Simply speaking, GenCode follows a generation-and-selection paradigm to prepare useful training code data. Specifically, it employs code augmentation techniques to generate new code candidates first and then identifies important ones as the training data by influence scores. To evaluate the effectiveness of GenCode, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5) and two recent released code-specific Large Language Models (LLMs) (e.g., Qwen2.5-Coder). Compared to the state-of-the-art (SOTA) code augmentation method MixCode, GenCode produces pre-trained code models with 2.92% higher accuracy and 4.90% adversarial robustness on average. For code-specific LLMs, GenCode achieves an average improvement of 0.93% in accuracy and 0.98% in natural robustness.", "published": "2024-02-24T08:57:12Z", "updated": "2026-01-28T08:05:32Z", "authors": ["Zeming Dong", "Qiang Hu", "Xiaofei Xie", "Maxime Cordy", "Mike Papadakis", "Yves Le Traon", "Jianjun Zhao"], "pdf_url": "https://arxiv.org/pdf/2402.15769v3"}
{"id": "http://arxiv.org/abs/2511.05878v2", "title": "FusionLog: Cross-System Log-based Anomaly Detection via Fusion of General and Proprietary Knowledge", "summary": "Log-based anomaly detection is critical for ensuring the stability and reliability of web systems. One of the key problems in this task is the lack of sufficient labeled logs, which limits the rapid deployment in new systems. Existing works usually leverage large-scale labeled logs from a mature web system and a small amount of labeled logs from a new system, using transfer learning to extract and generalize general knowledge across both domains. However, these methods focus solely on the transfer of general knowledge and neglect the disparity and potential mismatch between such knowledge and the proprietary knowledge of target system, thus constraining performance. To address this limitation, we propose FusionLog, a novel zero-label cross-system log-based anomaly detection method that effectively achieves the fusion of general and proprietary knowledge, enabling cross-system generalization without any labeled target logs. Specifically, we first design a training-free router based on semantic similarity that dynamically partitions unlabeled target logs into 'general logs' and 'proprietary logs.' For general logs, FusionLog employs a small model based on system-agnostic representation meta-learning for direct training and inference, inheriting the general anomaly patterns shared between the source and target systems. For proprietary logs, we iteratively generate pseudo-labels and fine-tune the small model using multi-round collaborative knowledge distillation and fusion based on large language model (LLM) and small model (SM) to enhance its capability to recognize anomaly patterns specific to the target system. Experimental results on three public log datasets from different systems show that FusionLog achieves over 90% F1-score under a fully zero-label setting, significantly outperforming state-of-the-art cross-system log-based anomaly detection methods.", "published": "2025-11-08T06:30:50Z", "updated": "2026-01-28T05:23:53Z", "authors": ["Xinlong Zhao", "Tong Jia", "Minghua He", "Xixuan Yang", "Ying Li"], "pdf_url": "https://arxiv.org/pdf/2511.05878v2"}
{"id": "http://arxiv.org/abs/2601.20255v1", "title": "HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH", "summary": "SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the \"Long-Context Tax\" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders (\"reasonable hesitation\"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.", "published": "2026-01-28T05:03:24Z", "updated": "2026-01-28T05:03:24Z", "authors": ["Yueyang Wang", "Jiawei Fu", "Baolong Bi", "Xili Wang", "Xiaoqing Liu"], "pdf_url": "https://arxiv.org/pdf/2601.20255v1"}
{"id": "http://arxiv.org/abs/2601.20240v1", "title": "Understanding npm Developers' Practices, Challenges, and Recommendations for Secure Package Development", "summary": "Background: The Node Package Manager (npm) ecosystem plays a vital role in modern software development by providing a vast repository of packages and tools that developers can use to implement their software systems. However, recent vulnerabilities in third-party packages have led to serious security breaches, compromising the integrity of applications that depend on them. Objective: This study investigates how npm package developers perceive and handle security in their work. We examined developers' understanding of security risks, the practices and tools they use, the barriers to stronger security measures, and their suggestions for improving the npm ecosystem's security. Method: We conducted an online survey with 75 npm package developers and undertook a mixed-methods approach to analyzing their responses. Results: While developers prioritize security, they perceive their packages as only moderately secure, with concerns about supply chain attacks, dependency vulnerabilities, and malicious code. Only 40% are satisfied with the current npm security tools due to issues such as alert fatigue. Automated methods such as two-factor authentication and npm audit are favored over code reviews. Many drop dependencies due to abandonment or vulnerabilities, and typically respond to vulnerabilities in their packages by quickly releasing patches. Key barriers include time constraints and high false-positive rates. To improve npm security, developers seek better detection tools, clearer documentation, stronger account protections, and more education initiatives. Conclusion: Our findings will benefit npm package contributors and maintainers by highlighting prevalent security challenges and promoting discussions on best practices to strengthen security and trustworthiness within the npm landscape.", "published": "2026-01-28T04:26:16Z", "updated": "2026-01-28T04:26:16Z", "authors": ["Anthony Peruma", "Truman Choy", "Gerald Lee", "Italo De Oliveira Santos"], "pdf_url": "https://arxiv.org/pdf/2601.20240v1"}
{"id": "http://arxiv.org/abs/2601.20223v1", "title": "Control Models for In-IDE Code Completion", "summary": "We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.", "published": "2026-01-28T03:47:03Z", "updated": "2026-01-28T03:47:03Z", "authors": ["Aral de Moor", "Yana Hrynevich", "Hleb Badzeika", "Vladyslav Furda", "Marko Kojic", "Artem Savelev", "Kostadin Cvejoski", "Darya Rovdo", "Ekaterina Garanina"], "pdf_url": "https://arxiv.org/pdf/2601.20223v1"}
{"id": "http://arxiv.org/abs/2601.20171v1", "title": "Who Writes the Docs in SE 3.0? Agent vs. Human Documentation Pull Requests", "summary": "As software engineering moves toward SE3.0, AI agents are increasingly used to carry out development tasks and contribute changes to software projects. It is therefore important to understand the extent of these contributions and how human developers review and intervene, since these factors shape the risks of delegating work to AI agents. While recent studies have examined how AI agents support software development tasks (e.g., code generation, issue resolution, and PR automation), their role in documentation tasks remains underexplored-even though documentation is widely consumed and shapes how developers understand and use software.\n  Using the AIDev, we analyze 1,997 documentation-related pull requests (PRs) authored by AI agents and human developers, where documentation PRs are those that create or modify project documentation artifacts. We find that AI agents submit substantially more documentation-related PRs than humans in the studied repositories. We further observe that agent-authored documentation edits are typically integrated with little follow-up modification from humans, raising concerns about review practices and the reliability of agent-generated documentation. Overall, while AI agents already contribute substantially to documentation workflows, our results suggest concerns for emerging challenges for documentation quality assurance and human-AI collaboration in SE3.0.", "published": "2026-01-28T02:11:34Z", "updated": "2026-01-28T02:11:34Z", "authors": ["Kazuma Yamasaki", "Joseph Ayobami Joshua", "Tasha Settewong", "Mahmoud Alfadel", "Kazumasa Shimari", "Kenichi Matsumoto"], "pdf_url": "https://arxiv.org/pdf/2601.20171v1"}
{"id": "http://arxiv.org/abs/2601.20160v1", "title": "How do Agents Refactor: An Empirical Study", "summary": "Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, there is little work investigating how these agents perform Java refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first analysis of agentic refactoring pull requests in Java, comparing them to developer refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 3.0, we identify refactoring types and detect code smells before and after refactoring commits. Our results show that agent refactorings are dominated by annotation changes (the 5 most common refactoring types done by agents are annotation related), in contrast to the diverse structural improvements typical of developers. Despite these differences in refactoring types, we find Cursor to be the only model to show a statistically significant increase in refactoring smells.", "published": "2026-01-28T01:34:15Z", "updated": "2026-01-28T01:34:15Z", "authors": ["Lukas Ottenhof", "Daniel Penner", "Abram Hindle", "Thibaud Lutellier"], "pdf_url": "https://arxiv.org/pdf/2601.20160v1"}
{"id": "http://arxiv.org/abs/2601.20158v1", "title": "Cascaded Vulnerability Attacks in Software Supply Chains", "summary": "Most of the current software security analysis tools assess vulnerabilities in isolation. However, sophisticated software supply chain security threats often stem from cascaded vulnerability and security weakness chains that span dependent components. Moreover, although the adoption of Software Bills of Materials (SBOMs) has been accelerating, downstream vulnerability findings vary substantially across SBOM generators and analysis tools. We propose a novel approach to SBOM-driven security analysis methods and tools. We model vulnerability relationships over dependency structure rather than treating scanner outputs as independent records. We represent enriched SBOMs as heterogeneous graphs with nodes being the SBOM components and dependencies, the known software vulnerabilities, and the known software security weaknesses. We then train a Heterogeneous Graph Attention Network (HGAT) to predict whether a component is associated with at least one known vulnerability. Since documented multi-vulnerability chains are scarce, we model cascade discovery as a link prediction problem over CVE pairs using a multi-layer perceptron neural network. This way, we produce ranked candidate links that can be composed into multi-step paths. The HGAT component classifier achieves an Accuracy of 91.03% and an F1-score of 74.02%.", "published": "2026-01-28T01:31:09Z", "updated": "2026-01-28T01:31:09Z", "authors": ["Laura Baird", "Armin Moin"], "pdf_url": "https://arxiv.org/pdf/2601.20158v1"}
{"id": "http://arxiv.org/abs/2601.20148v1", "title": "LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis", "summary": "Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.\n  We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.\n  Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation.", "published": "2026-01-28T00:49:50Z", "updated": "2026-01-28T00:49:50Z", "authors": ["Marcus Emmanuel Barnes", "Taher A. Ghaleb", "Safwat Hassan"], "pdf_url": "https://arxiv.org/pdf/2601.20148v1"}
{"id": "http://arxiv.org/abs/2601.20147v1", "title": "Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization", "summary": "Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.", "published": "2026-01-28T00:45:28Z", "updated": "2026-01-28T00:45:28Z", "authors": ["Saima Afrin", "Zaiyu Cheng", "Tushar Sharma", "Alexander Serebrenik", "Massimiliano Di Penta", "Antonio Mastropaolo"], "pdf_url": "https://arxiv.org/pdf/2601.20147v1"}
