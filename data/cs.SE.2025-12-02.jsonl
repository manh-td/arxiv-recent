{"id": "http://arxiv.org/abs/2512.02953v1", "title": "The Evolutionary Ecology of Software: Constraints, Innovation, and the AI Disruption", "summary": "This chapter investigates the evolutionary ecology of software, focusing on the symbiotic relationship between software and innovation. An interplay between constraints, tinkering, and frequency-dependent selection drives the complex evolutionary trajectories of these socio-technological systems. Our approach integrates agent-based modeling and case studies, drawing on complex network analysis and evolutionary theory to explore how software evolves under the competing forces of novelty generation and imitation. By examining the evolution of programming languages and their impact on developer practices, we illustrate how technological artifacts co-evolve with and shape societal norms, cultural dynamics, and human interactions. This ecological perspective also informs our analysis of the emerging role of AI-driven development tools in software evolution. While large language models (LLMs) provide unprecedented access to information, their widespread adoption introduces new evolutionary pressures that may contribute to cultural stagnation, much like the decline of diversity in past software ecosystems. Understanding the evolutionary pressures introduced by AI-mediated software production is critical for anticipating broader patterns of cultural change, technological adaptation, and the future of software innovation.", "published": "2025-12-02T17:29:57Z", "updated": "2025-12-02T17:29:57Z", "authors": ["Sergi Valverde", "Blai Vidiella", "Salva Duran-Nebreda"], "pdf_url": "https://arxiv.org/pdf/2512.02953v1"}
{"id": "http://arxiv.org/abs/2511.18538v3", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "published": "2025-11-23T17:09:34Z", "updated": "2025-12-02T17:14:33Z", "authors": ["Jian Yang", "Xianglong Liu", "Weifeng Lv", "Ken Deng", "Shawn Guo", "Lin Jing", "Yizhi Li", "Shark Liu", "Xianzhen Luo", "Yuyu Luo", "Changzai Pan", "Ensheng Shi", "Yingshui Tan", "Renshuai Tao", "Jiajun Wu", "Xianjie Wu", "Zhenhe Wu", "Daoguang Zan", "Chenchen Zhang", "Wei Zhang", "He Zhu", "Terry Yue Zhuo", "Kerui Cao", "Xianfu Cheng", "Jun Dong", "Shengjie Fang", "Zhiwei Fei", "Xiangyuan Guan", "Qipeng Guo", "Zhiguang Han", "Joseph James", "Tianqi Luo", "Renyuan Li", "Yuhang Li", "Yiming Liang", "Congnan Liu", "Jiaheng Liu", "Qian Liu", "Ruitong Liu", "Tyler Loakman", "Xiangxin Meng", "Chuang Peng", "Tianhao Peng", "Jiajun Shi", "Mingjie Tang", "Boyang Wang", "Haowen Wang", "Yunli Wang", "Fanglin Xu", "Zihan Xu", "Fei Yuan", "Ge Zhang", "Jiayi Zhang", "Xinhao Zhang", "Wangchunshu Zhou", "Hualei Zhu", "King Zhu", "Bryan Dai", "Aishan Liu", "Zhoujun Li", "Chenghua Lin", "Tianyu Liu", "Chao Peng", "Kai Shen", "Libo Qin", "Shuangyong Song", "Zizheng Zhan", "Jiajun Zhang", "Jie Zhang", "Zhaoxiang Zhang", "Bo Zheng"], "pdf_url": "https://arxiv.org/pdf/2511.18538v3"}
{"id": "http://arxiv.org/abs/2510.11059v2", "title": "Defects4C: Benchmarking Large Language Model Repair Capability with C/C++ Bugs", "summary": "Automated Program Repair (APR) plays a critical role in enhancing the quality and reliability of software systems. While substantial progress has been made in Java-based APR, largely facilitated by benchmarks like Defects4J, there remains a significant gap in research on C/C++ program repair, despite the widespread use of C/C++ and the prevalence of associated vulnerabilities. This gap is primarily due to the lack of high-quality, open-source benchmarks tailored for C/C++.\n  To address this issue, we introduce Defects4C, a comprehensive and executable benchmark specifically designed for C/C++ program repair. Our dataset is constructed from real-world C/C++ repositories and includes a large collection of bug-relevant commits (9M in total), 248 high-quality buggy functions, and 102 vulnerable functions, all paired with test cases for reproduction. These resources enable rigorous evaluation of repair techniques and support the retraining of learning-based approaches for enhanced performance.\n  Using Defects4C, we conduct a comprehensive empirical study evaluating the effectiveness of 24 state-of-the-art large language models (LLMs) in repairing C/C++ faults. Our findings offer valuable insights into the strengths and limitations of current LLM-based APR techniques in this domain, highlighting both the need for more robust methods and the critical role of Defects4C in advancing future research", "published": "2025-10-13T06:49:28Z", "updated": "2025-12-02T16:36:21Z", "authors": ["Jian Wang", "Xiaofei Xie", "Qiang Hu", "Shangqing Liu", "Jiongchi Yu", "Jiaolong Kong", "Yi Li"], "pdf_url": "https://arxiv.org/pdf/2510.11059v2"}
{"id": "http://arxiv.org/abs/2512.02918v1", "title": "Belobog: Move Language Fuzzing Framework For Real-World Smart Contracts", "summary": "Move is a research-oriented programming language design for secure and verifiable smart contract development and has been widely used in managing billions of digital assets in blockchains, such as Sui and Aptos. Move features a strong static type system and explicit resource semantics to enforce safety properties such as the prevention of data races, invalid asset transfers, and entry vulnerabilities. However, smart contracts written in Move may still contain certain vulnerabilities that are beyond the reach of its type system. It is thus essential to validate Move smart contracts. Unfortunately, due to its strong type system, existing smart contract fuzzers are ineffective in producing syntactically or semantically valid transactions to test Move smart contracts. This paper introduces the first fuzzing framework, Belobog, for Move smart contracts. Belobog is type-aware and ensures that all generated and mutated transactions are well-typed. More specifically, for a target Move smart contract, Belobog first constructs a type graph based on Move's type system, and then generates or mutates a transaction based on the graph trace derived from the type graph. In order to overcome the complex checks in Move smart contracts, we further design and implement a concolic executor in Belobog. We evaluated Belobog on 109 real-world Move smart contract projects. The experimental results show that Belobog is able to detect 100\\% critical and 79\\% major vulnerabilities manually audited by human experts. We further selected two recent notorious incidents in Move smart contracts, i.e., Cetus and Nemo. Belobog successfully reproduced full exploits for both of them, without any prior knowledge.", "published": "2025-12-02T16:36:13Z", "updated": "2025-12-02T16:36:13Z", "authors": ["Wanxu Xia", "Ziqiao Kong", "Zhengwei Li", "Yi Lu", "Pan Li", "Liqun Yang", "Yang Liu", "Xiapu Luo", "Shaohua Li"], "pdf_url": "https://arxiv.org/pdf/2512.02918v1"}
{"id": "http://arxiv.org/abs/2507.09481v2", "title": "Evaluating LLMs on Sequential API Call Through Automated Test Generation", "summary": "By integrating tools from external APIs, Large Language Models (LLMs) have expanded their promising capabilities in a diverse spectrum of complex real-world tasks. However, testing, evaluation, and analysis of LLM tool use remain in their early stages. Most existing benchmarks rely on manually collected test cases, many of which cannot be automatically checked for semantic correctness and instead depend on static methods such as string matching. Additionally, these benchmarks often overlook the complex interactions that occur between sequential API calls, which are common in real-world applications. To fill the gap, in this paper, we introduce StateGen, an automated framework designed to generate diverse coding tasks involving sequential API interactions. StateGen combines state-machine-based API constraint solving and validation, energy-based sampling, and control-flow injection to generate executable programs. These programs are then translated into human-like natural language task descriptions through a collaboration of two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark encompassing 120 verified test cases spanning across three representative scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental results confirm that StateGen can effectively generate challenging and realistic API-oriented tasks, highlighting areas for improvement in current LLMs incorporating APIs.We make our framework and benchmark publicly available to support future research.", "published": "2025-07-13T03:52:51Z", "updated": "2025-12-02T16:25:02Z", "authors": ["Yuheng Huang", "Jiayang Song", "Da Song", "Zhenlan Ji", "Wenhan Wang", "Shuai Wang", "Lei Ma"], "pdf_url": "https://arxiv.org/pdf/2507.09481v2"}
{"id": "http://arxiv.org/abs/2512.02898v1", "title": "Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits", "summary": "Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults.\n  This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).", "published": "2025-12-02T16:04:51Z", "updated": "2025-12-02T16:04:51Z", "authors": ["Pedro Orvalho", "Marta Kwiatkowska", "Mikoláš Janota", "Vasco Manquinho"], "pdf_url": "https://arxiv.org/pdf/2512.02898v1"}
{"id": "http://arxiv.org/abs/2512.02795v1", "title": "Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior", "summary": "Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse", "published": "2025-12-02T14:12:36Z", "updated": "2025-12-02T14:12:36Z", "authors": ["Marcus Kessel"], "pdf_url": "https://arxiv.org/pdf/2512.02795v1"}
{"id": "http://arxiv.org/abs/2512.02750v1", "title": "\"Can you feel the vibes?\": An exploration of novice programmer engagement with vibe coding", "summary": "Emerging alongside generative AI and the broader trend of AI-assisted coding, the term \"vibe coding\" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.", "published": "2025-12-02T13:32:23Z", "updated": "2025-12-02T13:32:23Z", "authors": ["Kiev Gama", "Filipe Calegario", "Victoria Jackson", "Alexander Nolte", "Luiz Augusto Morais", "Vinicius Garcia"], "pdf_url": "https://arxiv.org/pdf/2512.02750v1"}
{"id": "http://arxiv.org/abs/2512.02728v1", "title": "Integrative Analysis of Risk Management Methodologies in Data Science Projects", "summary": "Data science initiatives frequently exhibit high failure rates, driven by technical constraints, organizational limitations and insufficient risk management practices. Challenges such as low data maturity, lack of governance, misalignment between technical and business teams, and the absence of structured mechanisms to address ethical and sociotechnical risks have been widely identified in the literature. In this context, the purpose of this study is to conduct a comparative analysis of the main risk management methodologies applied to data science projects, aiming to identify, classify, and synthesize their similarities, differences and existing gaps. An integrative literature review was performed using indexed databases and a structured protocol for selection and content analysis. The study examines widely adopted risk management standards ISO 31000, PMBOK Risk Management and NIST RMF, as well as frameworks specific to data science workflows, such as CRISP DM and the recently proposed DS EthiCo RMF, which incorporates ethical and sociotechnical dimensions into the project life cycle. The findings reveal that traditional approaches provide limited coverage of emerging risks, whereas contemporary models propose multidimensional structures capable of integrating ethical oversight, governance and continuous monitoring. As a contribution, this work offers theoretical support for the development of hybrid frameworks that balance technical efficiency, organizational alignment and responsible data practices, while highlighting research gaps that can guide future investigations.", "published": "2025-12-02T13:06:52Z", "updated": "2025-12-02T13:06:52Z", "authors": ["Sabrina Delmondes da Costa Feitosa"], "pdf_url": "https://arxiv.org/pdf/2512.02728v1"}
{"id": "http://arxiv.org/abs/2512.02707v1", "title": "Empirical Assessment of the Perception of Software Product Line Engineering by an SME before Migrating its Code Base", "summary": "Migrating a set of software variants into a software product line (SPL) is an expensive and potentially challenging endeavor. Indeed, SPL engineering can significantly impact a company's development process and often requires changes to established developer practices. The work presented in this paper stems from a collaboration with a Small and Medium-sized Enterprise (SME) that decided to migrate its existing code base into an SPL. In this study, we conducted an in-depth evaluation of the company's current development processes and practices, as well as the anticipated benefits and risks associated with the migration. Key stakeholders involved in software development participated in this evaluation to provide insight into their perceptions of the migration and their potential resistance to change. This paper describes the design of the interviews conducted with these stakeholders and presents an analysis of the results. Among the qualitative findings, we observed that all participants, regardless of their role in the development process, identified benefits of the migration relevant to their own activities. Furthermore, our results suggest that an effective risk mitigation strategy involves keeping stakeholders informed and engaged throughout the process, preserving as many good practices as possible, and actively involving them in the migration to ensure a smooth transition and minimize potential challenges.", "published": "2025-12-02T12:39:05Z", "updated": "2025-12-02T12:39:05Z", "authors": ["Thomas Georges", "Marianne Huchard", "Mélanie König", "Clémentine Nebut", "Chouki Tibermacine"], "pdf_url": "https://arxiv.org/pdf/2512.02707v1"}
{"id": "http://arxiv.org/abs/2512.02651v1", "title": "Real-Time Multimodal Data Collection Using Smartwatches and Its Visualization in Education", "summary": "Wearable sensors, such as smartwatches, have become increasingly prevalent across domains like healthcare, sports, and education, enabling continuous monitoring of physiological and behavioral data. In the context of education, these technologies offer new opportunities to study cognitive and affective processes such as engagement, attention, and performance. However, the lack of scalable, synchronized, and high-resolution tools for multimodal data acquisition continues to be a significant barrier to the widespread adoption of Multimodal Learning Analytics in real-world educational settings. This paper presents two complementary tools developed to address these challenges: Watch-DMLT, a data acquisition application for Fitbit Sense 2 smartwatches that enables real-time, multi-user monitoring of physiological and motion signals; and ViSeDOPS, a dashboard-based visualization system for analyzing synchronized multimodal data collected during oral presentations. We report on a classroom deployment involving 65 students and up to 16 smartwatches, where data streams including heart rate, motion, gaze, video, and contextual annotations were captured and analyzed. Results demonstrate the feasibility and utility of the proposed system for supporting fine-grained, scalable, and interpretable Multimodal Learning Analytics in real learning environments.", "published": "2025-12-02T11:12:46Z", "updated": "2025-12-02T11:12:46Z", "authors": ["Alvaro Becerra", "Pablo Villegas", "Ruth Cobos"], "pdf_url": "https://arxiv.org/pdf/2512.02651v1"}
{"id": "http://arxiv.org/abs/2512.01155v2", "title": "Beyond Greenfield: The D3 Framework for AI-Driven Productivity in Brownfield Engineering", "summary": "Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and 83% of participants spent less time fixing or rewriting code due to better initial planning with AI. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.", "published": "2025-12-01T00:26:41Z", "updated": "2025-12-02T10:47:38Z", "authors": ["Krishna Kumaar Sharma"], "pdf_url": "https://arxiv.org/pdf/2512.01155v2"}
{"id": "http://arxiv.org/abs/2512.02605v1", "title": "IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai", "summary": "This technical white paper introduces the Interactive Agents Call Tree (IACT), a computational model designed to address the limitations of static, hard-coded agent workflows. Unlike traditional systems that require pre-defined graphs or specialized programming, IACT operates as a general-purpose autonomous system driven purely by user dialogue. Given a high-level objective, the system autonomously grows a dynamic, recursive agent topology incrementally tailored to the problem's structure. This allows it to scale its organizational complexity to match open-ended tasks. To mitigate the error propagation inherent in unidirectional function calls, IACT introduces interactional redundancy by replacing rigid invocations with bidirectional, stateful dialogues. This mechanism enables runtime error correction and ambiguity resolution. We describe the architecture, design principles, and practical lessons behind the production deployment of this model in the kragent.ai system, presenting qualitative evidence from real-world workflows rather than exhaustive benchmark results.", "published": "2025-12-02T10:10:56Z", "updated": "2025-12-02T10:10:56Z", "authors": ["Pengju Lu"], "pdf_url": "https://arxiv.org/pdf/2512.02605v1"}
{"id": "http://arxiv.org/abs/2512.02589v1", "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing", "summary": "Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.", "published": "2025-12-02T10:00:37Z", "updated": "2025-12-02T10:00:37Z", "authors": ["Junyi Hou", "Andre Lin Huikai", "Nuo Chen", "Yiwei Gong", "Bingsheng He"], "pdf_url": "https://arxiv.org/pdf/2512.02589v1"}
{"id": "http://arxiv.org/abs/2512.02567v1", "title": "Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System", "summary": "The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.\n  We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.\n  Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.", "published": "2025-12-02T09:38:20Z", "updated": "2025-12-02T09:38:20Z", "authors": ["Martin Weiss", "Jesko Hecking-Harbusch", "Jochen Quante", "Matthias Woehrle"], "pdf_url": "https://arxiv.org/pdf/2512.02567v1"}
{"id": "http://arxiv.org/abs/2508.04295v3", "title": "EvoC2Rust: A Skeleton-guided Framework for Project-Level C-to-Rust Translation", "summary": "Translating legacy C codebases to Rust is increasingly demanded for building safety-critical systems. While various approaches have emerged for this task, they face inherent trade-offs: rule-based methods often struggle to satisfy code safety and idiomaticity requirements, while LLM-based methods frequently fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting complete C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros, and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates functions, replacing the corresponding stub placeholders; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates the superior performance of EvoC2Rust in project-level C-to-Rust translation. The results show that our approach outperforms the strongest LLM-based baseline by 17.24% in syntax accuracy and 14.32% in semantic accuracy, while also achieving a 43.59% higher code safety rate than the best rule-based tool.", "published": "2025-08-06T10:31:23Z", "updated": "2025-12-02T08:42:25Z", "authors": ["Chaofan Wang", "Tingrui Yu", "Chen Xie", "Jie Wang", "Dong Chen", "Wenrui Zhang", "Yuling Shi", "Xiaodong Gu", "Beijun Shen"], "pdf_url": "https://arxiv.org/pdf/2508.04295v3"}
{"id": "http://arxiv.org/abs/2511.16708v2", "title": "Multi-Agent Code Verification via Information Theory", "summary": "LLMs generate buggy code: 29.6% of SWE-bench solved patches fail, 62% of BaxBench solutions have vulnerabilities, and existing\n  tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized\n  agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns\n  finds more bugs than any single agent when the agents look for different problems, using submodularity of mutual information\n  under conditional independence. Measuring agent correlation of rho = 0.05 to 0.25 confirms they detect different bugs. Testing\n  on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method (Meta\n  Prompt Testing: 75%) while running faster and without test execution. We tested all 15 agent combinations and found that using\n  multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with diminishing\n  returns of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4, validating our theoretical model. The best two-agent\n  combination (Correctness + Performance) reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in\n  under 200ms per sample, making this practical for production use.", "published": "2025-11-20T03:40:27Z", "updated": "2025-12-02T05:26:03Z", "authors": ["Shreshth Rajan"], "pdf_url": "https://arxiv.org/pdf/2511.16708v2"}
{"id": "http://arxiv.org/abs/2512.02393v1", "title": "Process-Centric Analysis of Agentic Software Systems", "summary": "Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.\n  Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.", "published": "2025-12-02T04:12:29Z", "updated": "2025-12-02T04:12:29Z", "authors": ["Shuyang Liu", "Yang Chen", "Rahul Krishna", "Saurabh Sinha", "Jatin Ganhotra", "Reyhan Jabbarvand"], "pdf_url": "https://arxiv.org/pdf/2512.02393v1"}
{"id": "http://arxiv.org/abs/2512.02329v1", "title": "Towards autonomous normative multi-agent systems for Human-AI software engineering teams", "summary": "This paper envisions a transformative paradigm in software engineering, where Artificial Intelligence, embodied in fully autonomous agents, becomes the primary driver of the core software development activities. We introduce a new class of software engineering agents, empowered by Large Language Models and equipped with beliefs, desires, intentions, and memory to enable human-like reasoning. These agents collaborate with humans and other agents to design, implement, test, and deploy software systems with a level of speed, reliability, and adaptability far beyond the current software development processes. Their coordination and collaboration are governed by norms expressed as deontic modalities - commitments, obligations, prohibitions and permissions - that regulate interactions and ensure regulatory compliance. These innovations establish a scalable, transparent and trustworthy framework for future Human-AI software engineering teams.", "published": "2025-12-02T01:57:17Z", "updated": "2025-12-02T01:57:17Z", "authors": ["Hoa Khanh Dam", "Geeta Mahala", "Rashina Hoda", "Xi Zheng", "Cristina Conati"], "pdf_url": "https://arxiv.org/pdf/2512.02329v1"}
