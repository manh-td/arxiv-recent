{"id": "http://arxiv.org/abs/2601.21761v1", "title": "Towards A Sustainable Future for Peer Review in Software Engineering", "summary": "Peer review is the main mechanism by which the software engineering community assesses the quality of scientific results. However, the rapid growth of paper submissions in software engineering venues has outpaced the availability of qualified reviewers, creating a growing imbalance that risks constraining and negatively impacting the long-term growth of the Software Engineering (SE) research community. Our vision of the Future of the SE research landscape involves a more scalable, inclusive, and resilient peer review process that incorporates additional mechanisms for: 1) attracting and training newcomers to serve as high-quality reviewers, 2) incentivizing more community members to serve as peer reviewers, and 3) cautiously integrating AI tools to support a high-quality review process.", "published": "2026-01-29T14:14:44Z", "updated": "2026-01-29T14:14:44Z", "authors": ["Esteban Parra", "Sonia Haiduc", "Preetha Chatterjee", "Ramtin Ehsani", "Polina Iaremchuk"], "pdf_url": "https://arxiv.org/pdf/2601.21761v1"}
{"id": "http://arxiv.org/abs/2601.21755v1", "title": "Migrating Esope to Fortran 2008 using model transformations", "summary": "Legacy programming languages such as FORTRAN 77 still play a vital role in many industrial applications. Maintaining and modernizing these languages is challenging, especially when migrating to newer standards such as Fortran 2008. This is exacerbated in the presence of legacy proprietary extensions on such legacy languages, because their semantics are often based on old context (limits of legacy language, domain logic,...). This paper presents an approach for automatically migrating FORTRAN 77 with a proprietary extension, named Esope, to Fortran 2008. We introduce a tool that converts Esope source code to Fortran 2008. While supporting readability of the generated code, we want to maintain the level of abstraction provided by Esope. Our method uses model-driven engineering techniques, with transformations to generate a target model from which we export easy-to-read Fortran 2008 source code. We discuss the advantages, limitations, and maintainability considerations of our approach and provide insights into its scalability and adaptability to evolving requirements.", "published": "2026-01-29T14:11:18Z", "updated": "2026-01-29T14:11:18Z", "authors": ["Younoussa Sow", "Nicolas Anquetil", "Léandre Brault", "Stéphane Ducasse"], "pdf_url": "https://arxiv.org/pdf/2601.21755v1"}
{"id": "http://arxiv.org/abs/2601.21724v1", "title": "A costing framework for fusion power plants", "summary": "This paper summarizes and consolidates fusion power-plant costing work performed in support of ARPA-E from 2017 through 2024, and documents the evolution of the associated analysis framework from early capital-cost-focused studies to a standards-aligned, auditable costing capability. Early efforts applied ARIES-style cost-scaling relations to generate Nth-of-a-kind (NOAK) estimates and were calibrated through a pilot study with Bechtel and Decysive Systems to benchmark balance-of-plant (BOP) costs and validate plant-level reasonableness from an engineering, procurement, and construction (EPC) perspective. Subsequent work, informed by Lucid Catalyst studies of nuclear cost drivers, expanded the methodology to treat indirect costs explicitly and to evaluate cost-reduction pathways for non-fusion-island systems through design-for-cost practices, modularization, centralized manufacturing, and learning. As ARPA-E's fusion portfolio expanded, these methods were applied across BETHE and GAMOW concepts (and select ALPHA revisits), including enhanced treatment of tritium handling and plant integration supported by Princeton/PPPL expertise. In 2023 the capability was refactored to align with the IAEA-GEN-IV EMWG-EPRI code-of-accounts lineage, while key ARIES-derived scaling relations were replaced by bottom-up subsystem models for dominant fusion cost drivers (e.g., magnets, lasers, power supplies, and power-core components) coupled to physics-informed power balances and engineering-constrained radial builds. These developments were implemented in the spreadsheet-based Fusion Economics code (FECONs) and released as an open-source Python framework (pyFECONs), providing a transparent mapping from subsystem estimates to standardized accounts and a consistent computation of LCOE.", "published": "2026-01-29T13:47:02Z", "updated": "2026-01-29T13:47:02Z", "authors": ["Simon Woodruff"], "pdf_url": "https://arxiv.org/pdf/2601.21724v1"}
{"id": "http://arxiv.org/abs/2601.21695v1", "title": "AtPatch: Debugging Transformers via Hot-Fixing Over-Attention", "summary": "Transformer-based deep neural networks (DNNs) affected by backdoor attacks and unfairness typically exhibit anomalous attention patterns, leading to over-attend to backdoor triggers or protected attributes. Existing neuron-editing mitigation strategies often struggle to handle such situation and most of them lack flexibility and tend to distort feature representations. Motivated by such over-attention phenomenon and software engineering paradigms such as delta debugging and hot patching, we propose AtPatch, a hot-fix method that dynamically redistributes attention maps during model inference. Specifically, for a given input, AtPatch first extracts the attention map from the model's inference process. Then, it uses a pre-trained detector to identify anomalous columns and replace them with unified benign attention. Then, AtPatch rescales other columns to mitigate the impact of over-attention. Finally, AtPatch returns the redistributed attention map to the model for continued inference. Notably, if the detector does not report any anomalous columns, AtPatch directly returns the original attention map to the model. Unlike existing techniques, AtPatch selectively redistributes the attention map, making it better at preserving the model's original functionality. Furthermore, AtPatch's on-the-fly nature allows it to work without modifying model parameters or retraining, making it better suited for deployed models. We conducted extensive experiments to validate AtPatch. Experimental results show that, compared to existing methods, AtPatch can more effectively mitigate backdoor attacks and unfairness while better preserving the model's original functionality.", "published": "2026-01-29T13:29:35Z", "updated": "2026-01-29T13:29:35Z", "authors": ["Shihao Weng", "Yang Feng", "Jincheng Li", "Yining Yin", "Xiaofei Xie", "Jia Liu"], "pdf_url": "https://arxiv.org/pdf/2601.21695v1"}
{"id": "http://arxiv.org/abs/2601.21649v1", "title": "SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning", "summary": "The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the \"physics\" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.", "published": "2026-01-29T12:49:25Z", "updated": "2026-01-29T12:49:25Z", "authors": ["Jinjun Peng", "Magnus Saebo", "Tianjun Zhong", "Yi-Jie Cheng", "Junfeng Yang", "Baishakhi Ray", "Simin Chen", "Yangruibo Ding"], "pdf_url": "https://arxiv.org/pdf/2601.21649v1"}
{"id": "http://arxiv.org/abs/2601.21605v1", "title": "Age Matters: Analyzing Age-Related Discussions in App Reviews", "summary": "In recent years, mobile applications have become indispensable tools for managing various aspects of life. From enhancing productivity to providing personalized entertainment, mobile apps have revolutionized people's daily routines. Despite this rapid growth and popularity, gaps remain in how these apps address the needs of users from different age groups. Users of varying ages face distinct challenges when interacting with mobile apps, from younger users dealing with inappropriate content to older users having difficulty with usability due to age-related vision and cognition impairments. Although there have been initiatives to create age-inclusive apps, a limited understanding of user perspectives on age-related issues may hinder developers from recognizing specific challenges and implementing effective solutions. In this study, we explore age discussions in app reviews to gain insights into how mobile apps should cater to users across different age groups.We manually curated a dataset of 4,163 app reviews from the Google Play Store and identified 1,429 age-related reviews and 2,734 non-age-related reviews. We employed eight machine learning, deep learning, and large language models to automatically detect age discussions, with RoBERTa performing the best, achieving a precision of 92.46%. Additionally, a qualitative analysis of the 1,429 age-related reviews uncovers six dominant themes reflecting user concerns.", "published": "2026-01-29T12:11:58Z", "updated": "2026-01-29T12:11:58Z", "authors": ["Shashiwadana Nirmania", "Garima Sharma", "Hourieh Khalajzadeh", "Mojtaba Shahin"], "pdf_url": "https://arxiv.org/pdf/2601.21605v1"}
{"id": "http://arxiv.org/abs/2601.21593v1", "title": "Is My RPC Response Reliable? Detecting RPC Bugs in Ethereum Blockchain Client under Context", "summary": "Blockchain clients are fundamental software for running blockchain nodes. They provide users with various RPC (Remote Procedure Call) interfaces to interact with the blockchain. These RPC methods are expected to follow the same specification across different blockchain nodes, providing users with seamless interaction. However, there have been continuous reports on various RPC bugs that can cause unexpected responses or even Denial of Service weakness. Existing studies on blockchain RPC bug detection mainly focus on generating the RPC method calls for testing blockchain clients. However, a wide range of the reported RPC bugs are triggered in various blockchain contexts. To the best of our knowledge, little attention is paid to generating proper contexts that can trigger these context-dependent RPC bugs.\n  In this work, we propose EthCRAFT, a Context-aware RPC Analysis and Fuzzing Tool for client RPC bug detection. EthCRAFT first proposes to explore the state transition program space of blockchain clients and generate various transactions to construct the context. EthCRAFT then designs a context-aware RPC method call generation method to send RPC calls to the blockchain clients. The responses of 5 different client implementations are used as cross-referring oracles to detect the RPC bugs. We evaluate EthCRAFT on real-world RPC bugs collected from the GitHub issues of Ethereum client implementations. Experiment results show that EthCRAFT outperforms existing client RPC detectors by detecting more RPC bugs. Moreover, EthCRAFT has found six new bugs in major Ethereum clients and reported them to the developers. One of the bug fixes has been written into breaking changes in the client's updates. Three of our bug reports have been offered a vulnerability bounty by the Ethereum Foundation.", "published": "2026-01-29T12:03:05Z", "updated": "2026-01-29T12:03:05Z", "authors": ["Zhijie Zhong", "Yuhong Nan", "Mingxi Ye", "Qing Xue", "Jiashui Wang", "Xinlei Ying", "Long Liu", "Zibin Zheng"], "pdf_url": "https://arxiv.org/pdf/2601.21593v1"}
{"id": "http://arxiv.org/abs/2601.21565v1", "title": "Multi-objective Integer Linear Programming approach for Automatic Software Cognitive Complexity Reduction", "summary": "Clear and concise code is necessary to ensure maintainability, so it is crucial that the software is as simple as possible to understand, to avoid bugs and, above all, vulnerabilities. There are many ways to enhance software without changing its functionality, considering the extract method refactoring the primary process to reduce the effort required for code comprehension. The cognitive complexity measure employed in this work is the one defined by SonarSource, which is a company that develops well-known applications for static code analysis. This extraction problem can be modeled as a combinatorial optimization problem. The main difficulty arises from the existence of different criteria for evaluating the solutions obtained, requiring the formulation of the code extraction problem as a multi-objective optimization problem using alternative methods. We propose a multi-objective integer linear programming model to obtain a set of solutions that reduce the cognitive complexity of a given piece of code, such as balancing the number of lines of code and its cognitive complexity. In addition, several algorithms have been developed to validate the model. These algorithms have been integrated into a tool that enables the parameterised resolution of the problem of reducing software cognitive complexity.", "published": "2026-01-29T11:28:54Z", "updated": "2026-01-29T11:28:54Z", "authors": ["Adriana Novoa-Hurtado", "Rubén Saborido", "Francisco Chicano", "Manuel Giménez-Medina"], "pdf_url": "https://arxiv.org/pdf/2601.21565v1"}
{"id": "http://arxiv.org/abs/2601.21552v1", "title": "Chasing Elusive Memory Bugs in GPU Programs", "summary": "Memory safety bugs, such as out-of-bound accesses (OOB) in GPU programs, can compromise the security and reliability of GPU-accelerated software. We report the existence of input-dependent OOBs in the wild that manifest only under specific inputs. All existing tools to detect OOBs in GPU programs rely on runtime techniques that require an OOB to manifest for detection. Thus, input-dependent OOBs elude them. We also discover intra-allocation OOBs that arise in the presence of logical partitioning of a memory allocation into multiple data structures. Existing techniques are oblivious to the possibility of such OOBs.\n  We make a key observation that the presence (or absence) of semantic relations among program variables, which determines the size of allocations (CPU code) and those calculating offsets into memory allocations (GPU code), helps identify the absence (or presence) of OOBs. We build SCuBA, a first-of-its-kind compile-time technique that analyzes CPU and GPU code to capture such semantic relations (if present). It uses a SAT solver to check if an OOB access is possible under any input, given the captured relations expressed as constraints. It further analyzes GPU code to track logical partitioning of memory allocations for detecting intra-allocation OOB. Compared to NVIDIA's Compute Sanitizer that misses 45 elusive memory bugs across 20 programs, SCuBA misses none with no false alarms.", "published": "2026-01-29T11:13:36Z", "updated": "2026-01-29T11:13:36Z", "authors": ["Anubhab Ghosh", "Ajay Nayak", "Dhananjay Rao Thallikar Shyam", "Arkaprava Basu"], "pdf_url": "https://arxiv.org/pdf/2601.21552v1"}
{"id": "http://arxiv.org/abs/2601.20755v2", "title": "ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler", "summary": "As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama-cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.", "published": "2026-01-28T16:39:38Z", "updated": "2026-01-29T10:43:56Z", "authors": ["Bohua Zou", "Debayan Roy", "Dhimankumar Yogesh Airao", "Weihao Xu", "Binqi Sun", "Yutao Liu", "Haibo Chen"], "pdf_url": "https://arxiv.org/pdf/2601.20755v2"}
{"id": "http://arxiv.org/abs/2506.13821v4", "title": "Software is infrastructure: failures, successes, costs, and the case for formal verification", "summary": "In this chapter we outline the role that software has in modern society, along with the staggering costs of poor software quality. To lay this bare, we recall the costs of some of the major software failures that happened during the last 40 years. We argue that these costs justify researching, studying and applying formal software verification and in particular program analysis. This position is supported by successful industrial experiences.", "published": "2025-06-15T07:40:22Z", "updated": "2026-01-29T10:43:02Z", "authors": ["Giovanni Bernardi", "Adrian Francalanza", "Marco Peressotti", "Mohammad Reza Mousavi"], "pdf_url": "https://arxiv.org/pdf/2506.13821v4"}
{"id": "http://arxiv.org/abs/2601.21526v1", "title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization", "summary": "We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.\n  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.\n  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.\n  Code Available at: https://github.com/Leeroo-AI/kapso", "published": "2026-01-29T10:40:54Z", "updated": "2026-01-29T10:40:54Z", "authors": ["Alireza Nadaf", "Alireza Mohammadshahi", "Majid Yazdani"], "pdf_url": "https://arxiv.org/pdf/2601.21526v1"}
{"id": "http://arxiv.org/abs/2601.21511v1", "title": "LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI", "summary": "Large language models have enabled automated algorithm design (AAD) by generating optimization algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused. We propose a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions. Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity.\n  We propose LLaMEA-SAGE, which integrates this feature-driven guidance into LLaMEA, and evaluate it across several benchmarks. We show that the proposed structured guidance achieves the same performance faster than vanilla LLaMEA in a small controlled experiment. In a larger-scale experiment using the MA-BBOB suite from the GECCO-MA-BBOB competition, our guided approach achieves superior performance compared to state-of-the-art AAD methods. These results demonstrate that signals derived from code can effectively bias LLM-driven algorithm evolution, bridging the gap between code structure and human-understandable performance feedback in automated algorithm design.", "published": "2026-01-29T10:27:29Z", "updated": "2026-01-29T10:27:29Z", "authors": ["Niki van Stein", "Anna V. Kononova", "Lars Kotthoff", "Thomas Bäck"], "pdf_url": "https://arxiv.org/pdf/2601.21511v1"}
{"id": "http://arxiv.org/abs/2601.14743v3", "title": "ARISE -- Adaptive Refinement and Iterative Scenario Engineering", "summary": "The effectiveness of collision-free trajectory planners depends on the quality and diversity of training data, especially for rare scenarios. A widely used approach to improve dataset diversity involves generating realistic synthetic traffic scenarios. However, producing such scenarios remains difficult due to the precision required when scripting them manually or generating them in a single pass. Natural language offers a flexible way to describe scenarios, but existing text-to-simulation pipelines often rely on static snippet retrieval, limited grammar, single-pass decoding, or lack robust executability checks. Moreover, they depend heavily on constrained LLM prompting with minimal post-processing. To address these limitations, we introduce ARISE - Adaptive Refinement and Iterative Scenario Engineering, a multi-stage tool that converts natural language prompts into executable Scenic scripts through iterative LLM-guided refinement. After each generation, ARISE tests script executability in simulation software, feeding structured diagnostics back to the LLM until both syntactic and functional requirements are met. This process significantly reduces the need for manual intervention. Through extensive evaluation, ARISE outperforms the baseline in generating semantically accurate and executable traffic scenarios with greater reliability and robustness.", "published": "2026-01-21T07:57:24Z", "updated": "2026-01-29T10:16:08Z", "authors": ["Konstantin Poddubnyy", "Igor Vozniak", "Ivan Burmistrov", "Nils Lipp", "Davit Hovhannisyan", "Christian Mueller", "Philipp Slusallek"], "pdf_url": "https://arxiv.org/pdf/2601.14743v3"}
{"id": "http://arxiv.org/abs/2601.21469v1", "title": "Adaptive Confidence Gating in Multi-Agent Collaboration for Efficient and Optimized Code Generation", "summary": "While Large Language Models (LLMs) have catalyzed breakthroughs in automated code generation, Small Language Models (SLMs) often encounter reasoning bottlenecks and failure loops when addressing complex logical requirements. To overcome these challenges, we propose DebateCoder, a multi-agent collaborative framework designed to improve the reasoning ability of SLMs (e.g., Pangu-1B) in resource-constrained environments. DebateCoder uses a structured role-playing protocol with three agents: User Agent (A_UA), Technical Agent (A_TA), and Quality Assurance Agent (A_QA). It also includes an Adaptive Confidence Gating mechanism with a 95% threshold to balance accuracy and inference efficiency. In addition, we introduce a multi-turn deliberation module and a reviewer-guided analytical debugging loop for orthogonal pre-generation debate and post-generation refinement. Experiments on HumanEval and MBPP show that DebateCoder achieves 70.12% Pass@1 on HumanEval, outperforming MapCoder while reducing API overhead by about 35%. These results indicate that collaborative protocols can mitigate limitations of small-parameter models and provide a scalable, efficient approach to high-quality automated software engineering.", "published": "2026-01-29T09:48:15Z", "updated": "2026-01-29T09:48:15Z", "authors": ["Haoji Zhang", "Yuzhe Li", "Zhenqiang Liu", "Chenyang Liu", "Shenyang Zhang", "Yi Zhou"], "pdf_url": "https://arxiv.org/pdf/2601.21469v1"}
{"id": "http://arxiv.org/abs/2601.19494v2", "title": "AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context", "summary": "High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizability of evaluation results; second, the reliance on noisy, incomplete ground truth derived from raw Pull Request (PR) comments, which constrains the scope of issue detection. To address these challenges, we introduce AACR-Bench a comprehensive benchmark that provides full cross-file context across multiple programming languages. Unlike traditional datasets, AACR-Bench employs an \"AI-assisted, Expert-verified\" annotation pipeline to uncover latent defects often overlooked in original PRs, resulting in a 285% increase in defect coverage. Extensive evaluations of mainstream LLMs on AACR-Bench reveal that previous assessments may have either misjudged or only partially captured model capabilities due to data limitations. Our work establishes a more rigorous standard for ACR evaluation and offers new insights on LLM based ACR, i.e., the granularity/level of context and the choice of retrieval methods significantly impact ACR performance, and this influence varies depending on the LLM, programming language, and the LLM usage paradigm e.g., whether an Agent architecture is employed. The code, data, and other artifacts of our evaluation set are available at https://github.com/alibaba/aacr-bench .", "published": "2026-01-27T11:28:44Z", "updated": "2026-01-29T09:05:14Z", "authors": ["Lei Zhang", "Yongda Yu", "Minghui Yu", "Xinxin Guo", "Zhengqi Zhuang", "Guoping Rong", "Dong Shao", "Haifeng Shen", "Hongyu Kuang", "Zhengfeng Li", "Boge Wang", "Guoan Zhang", "Bangyu Xiang", "Xiaobin Xu"], "pdf_url": "https://arxiv.org/pdf/2601.19494v2"}
{"id": "http://arxiv.org/abs/2601.21379v1", "title": "Predicting Developer Acceptance of AI-Generated Code Suggestions", "summary": "AI-assisted programming tools are widely adopted, yet their practical utility is often undermined by undesired suggestions that interrupt developer workflows and cause frustration. While existing research has explored developer-AI interactions when programming qualitatively, a significant gap remains in quantitative analysis of developers' acceptance of AI-generated code suggestions, partly because the necessary fine-grained interaction data is often proprietary. To bridge this gap, this paper conducts an empirical study using 66,329 industrial developer-AI interactions from a large technology company. We analyze features that are significantly different between accepted code suggestions and rejected ones. We find that accepted suggestions are characterized by significantly higher historical acceptance counts and ratios for both developers and projects, longer generation intervals, shorter preceding code context in the project, and older IDE versions. Based on these findings, we introduce CSAP (Code Suggestion Acceptance Prediction) to predict whether a developer will accept the code suggestion before it is displayed. Our evaluation of CSAP shows that it achieves the accuracy of 0.973 and 0.922 on imbalanced and balanced dataset respectively. Compared to a large language model baseline and an in-production industrial filter, CSAP relatively improves the accuracy by 12.6\\% and 69.5\\% on imbalanced dataset, and improves the accuracy by 87.0\\% and 140.1\\% on balanced dataset. Our results demonstrate that targeted personalization is a powerful approach for filtering out code suggestions with predicted rejection and reduce developer interruption. To the best of our knowledge, it is the first quantitative study of code suggestion acceptance on large-scale industrial data, and this work also sheds light on an important research direction of AI-assisted programming.", "published": "2026-01-29T08:14:24Z", "updated": "2026-01-29T08:14:24Z", "authors": ["Jing Jiang", "Liehao Li", "Jinyun Hou", "Xin Tan", "Li Zhang"], "pdf_url": "https://arxiv.org/pdf/2601.21379v1"}
{"id": "http://arxiv.org/abs/2601.21360v1", "title": "The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation", "summary": "The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread \"False Certification\" of functionally broken code. Our findings suggest that current alignment paradigms create a \"Trojan\" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.", "published": "2026-01-29T07:40:58Z", "updated": "2026-01-29T07:40:58Z", "authors": ["Devanshu Sahoo", "Manish Prasad", "Vasudev Majhi", "Arjun Neekhra", "Yash Sinha", "Murari Mandal", "Vinay Chamola", "Dhruv Kumar"], "pdf_url": "https://arxiv.org/pdf/2601.21360v1"}
{"id": "http://arxiv.org/abs/2601.21359v1", "title": "Graph-Free Root Cause Analysis", "summary": "Failures in complex systems demand rapid Root Cause Analysis (RCA) to prevent cascading damage. Existing RCA methods that operate without dependency graph typically assume that the root cause having the highest anomaly score. This assumption fails when faults propagate, as a small delay at the root cause can accumulate into a much larger anomaly downstream. In this paper, we propose PRISM, a simple and efficient framework for RCA when the dependency graph is absent. We formulate a class of component-based systems under which PRISM performs RCA with theoretical guarantees. On 735 failures across 9 real-world datasets, PRISM achieves 68% Top-1 accuracy, a 258% improvement over the best baseline, while requiring only 8ms per diagnosis.", "published": "2026-01-29T07:38:20Z", "updated": "2026-01-29T07:38:20Z", "authors": ["Luan Pham"], "pdf_url": "https://arxiv.org/pdf/2601.21359v1"}
{"id": "http://arxiv.org/abs/2601.21344v1", "title": "Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms", "summary": "This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring diverse and inclusive educational experiences. Key innovations include robust feedback mechanisms that refine AI moderation, promote reflective learning, and balance participation among users. The system's modular architecture featuring ReactJS for the frontend, Flask for backend operations, and efficient question retrieval supports personalized and engaging interactions through dynamic adjustments to prompts and discussion flows. Testing demonstrates that the framework significantly improves student collaboration, fosters deeper comprehension, and scales effectively across various subjects and user groups. By addressing limitations in static moderation and personalization in existing systems, this work establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes.", "published": "2026-01-29T07:14:43Z", "updated": "2026-01-29T07:14:43Z", "authors": ["Hassam Tahir", "Faizan Faisal", "Fady Alnajjar", "Muhammad Imran Taj", "Lucia Gordon", "Aila Khan", "Michael Lwin", "Omar Mubin"], "pdf_url": "https://arxiv.org/pdf/2601.21344v1"}
{"id": "http://arxiv.org/abs/2601.21305v1", "title": "Developers in the Age of AI: Adoption, Policy, and Diffusion of AI Software Engineering Tools", "summary": "The rapid advance of Generative AI into software development prompts this empirical investigation of perceptual effects on practice. We study the usage patterns of 147 professional developers, examining perceived correlates of AI tools use, the resulting productivity and quality outcomes, and developer readiness for emerging AI-enhanced development. We describe a virtuous adoption cycle where frequent and broad AI tools use are the strongest correlates of both Perceived Productivity (PP) and quality, with frequency strongest. The study finds no perceptual support for the Quality Paradox and shows that PP is positively correlated with Perceived Code Quality (PQ) improvement. Developers thus report both productivity and quality gains. High current usage, breadth of application, frequent use of AI tools for testing, and ease of use correlate strongly with future intended adoption, though security concerns remain a moderate and statistically significant barrier to adoption. Moreover, AI testing tools' adoption lags that of coding tools, opening a Testing Gap. We identify three developer archetypes (Enthusiasts, Pragmatists, Cautious) that align with an innovation diffusion process wherein the virtuous adoption cycle serves as the individual engine of progression. Our findings reveal that organizational adoption of AI tools follows such a process: Enthusiasts push ahead with tools, creating organizational success that converts Pragmatists. The Cautious are held in organizational stasis: without early adopter examples, they don't enter the virtuous adoption cycle, never accumulate the usage frequency that drives intent, and never attain high efficacy. Policy itself does not predict individuals' intent to increase usage but functions as a marker of maturity, formalizing the successful diffusion of adoption by Enthusiasts while acting as a gateway that the Cautious group has yet to reach.", "published": "2026-01-29T05:56:35Z", "updated": "2026-01-29T05:56:35Z", "authors": ["Mark Looi", "Julianne Quinn"], "pdf_url": "https://arxiv.org/pdf/2601.21305v1"}
{"id": "http://arxiv.org/abs/2601.21298v1", "title": "Detecting Multiple Semantic Concerns in Tangled Code Commits", "summary": "Code commits in a version control system (e.g., Git) should be atomic, i.e., focused on a single goal, such as adding a feature or fixing a bug. In practice, however, developers often bundle multiple concerns into tangled commits, obscuring intent and complicating maintenance. Recent studies have used Conventional Commits Specification (CCS) and Language Models (LMs) to capture commit intent, demonstrating that Small Language Models (SLMs) can approach the performance of Large Language Models (LLMs) while maintaining efficiency and privacy. However, they do not address tangled commits involving multiple concerns, leaving the feasibility of using LMs for multi-concern detection unresolved. In this paper, we frame multi-concern detection in tangled commits as a multi-label classification problem and construct a controlled dataset of artificially tangled commits based on real-world data. We then present an empirical study using SLMs to detect multiple semantic concerns in tangled commits, examining the effects of fine-tuning, concern count, commit-message inclusion, and header-preserving truncation under practical token-budget limits. Our results show that a fine-tuned 14B-parameter SLM is competitive with a state-of-the-art LLM for single-concern commits and remains usable for up to three concerns. In particular, including commit messages improves detection accuracy by up to 44% (in terms of Hamming Loss) with negligible latency overhead, establishing them as important semantic cues.", "published": "2026-01-29T05:50:16Z", "updated": "2026-01-29T05:50:16Z", "authors": ["Beomsu Koh", "Neil Walkinshaw", "Donghwan Shin"], "pdf_url": "https://arxiv.org/pdf/2601.21298v1"}
{"id": "http://arxiv.org/abs/2601.21276v1", "title": "More Code, Less Reuse: Investigating Code Quality and Reviewer Sentiment towards AI-generated Pull Requests", "summary": "Large Language Model (LLM) Agents are advancing quickly, with the increasing leveraging of LLM Agents to assist in development tasks such as code generation. While LLM Agents accelerate code generation, studies indicate they may introduce adverse effects on development. However, existing metrics solely measure pass rates, failing to reflect impacts on long-term maintainability and readability, and failing to capture human intuitive evaluations of PR. To increase the comprehensiveness of this problem, we investigate and evaluate the characteristics of LLM to know the pull requests' characteristics beyond the pass rate. We observe the code quality and maintainability within PRs based on code metrics to evaluate objective characteristics and developers' reactions to the pull requests from both humans and LLM's generation. Evaluation results indicate that LLM Agents frequently disregard code reuse opportunities, resulting in higher levels of redundancy compared to human developers. In contrast to the quality issues, our emotions analysis reveals that reviewers tend to express more neutral or positive emotions towards AI-generated contributions than human ones. This disconnect suggests that the surface-level plausibility of AI code masks redundancy, leading to the silent accumulation of technical debt in real-world development environments. Our research provides insights for improving human-AI collaboration.", "published": "2026-01-29T05:13:21Z", "updated": "2026-01-29T05:13:21Z", "authors": ["Haoming Huang", "Pongchai Jaisri", "Shota Shimizu", "Lingfeng Chen", "Sota Nakashima", "Gema Rodríguez-Pérez"], "pdf_url": "https://arxiv.org/pdf/2601.21276v1"}
{"id": "http://arxiv.org/abs/2601.21259v1", "title": "The Role of Social Identity in Shaping Biases Against Minorities in Software Organizations", "summary": "While systemic workplace bias is well-documented in non-computing fields, its specific impact on software engineers remains poorly understood. This study addresses that gap by applying Social Identity Theory (SIT) to investigate four distinct forms of bias: lack of career development, stereotyped task selection, unwelcoming environments, and identity attacks. Using a vignette-based survey, we quantified the prevalence of these biases, identified the demographics most affected, assessed their consequences, and explored the motivations behind biased actions. Our results show that career development and task selection biases are the most prevalent forms, with over two-thirds of victims experiencing them multiple times. Women were more than three times as likely as men to face career development bias, task selection bias, and an unwelcoming environment. In parallel, individuals from marginalized ethnic backgrounds were disproportionately targeted by identity attacks. Our analysis also confirms that, beyond gender and race, factors such as age, years of experience, organization size, and geographic location are significant predictors of bias victimization.", "published": "2026-01-29T04:39:32Z", "updated": "2026-01-29T04:39:32Z", "authors": ["Sayma Sultana", "London Cavaletto", "Bianca Trinkenreich", "Amiangshu Bosu"], "pdf_url": "https://arxiv.org/pdf/2601.21259v1"}
{"id": "http://arxiv.org/abs/2601.21258v1", "title": "Virtualization-based Penetration Testing Study for Detecting Accessibility Abuse Vulnerabilities in Banking Apps in East and Southeast Asia", "summary": "Android banking applications have revolutionized financial management by allowing users to perform various financial activities through mobile devices. However, this convenience has attracted cybercriminals who exploit security vulnerabilities to access sensitive financial data. FjordPhantom, a malware identified by our industry collaborator, uses virtualization and hooking to bypass the detection of malicious accessibility services, allowing it to conduct keylogging, screen scraping, and unauthorized data access. This malware primarily affects banking and finance apps across East and Southeast Asia region where our industry partner's clients are primarily based in. It requires users to be deceived into installing a secondary malicious component and activating a malicious accessibility service. In our study, we conducted an empirical study on the susceptibility of banking apps in the region to FjordPhantom, analyzed the effectiveness of protective measures currently implemented in those apps, and discussed ways to detect and prevent such attacks by identifying and mitigating the vulnerabilities exploited by this malware.", "published": "2026-01-29T04:37:53Z", "updated": "2026-01-29T04:37:53Z", "authors": ["Wei Minn", "Phong Phan", "Vikas K. Malviya", "Benjamin Adolphi", "Yan Naing Tun", "Henning Benzon Treichl", "Albert Ching", "Lwin Khin Shar", "David Lo"], "pdf_url": "https://arxiv.org/pdf/2601.21258v1"}
{"id": "http://arxiv.org/abs/2601.21253v1", "title": "CovAgent: Overcoming the 30% Curse of Mobile Application Coverage with Agentic AI and Dynamic Instrumentation", "summary": "Automated GUI testing is crucial for ensuring the quality and reliability of Android apps. However, the efficacy of existing UI testing techniques is often limited, especially in terms of coverage. Recent studies, including the state-of-the-art, struggle to achieve more than 30% activity coverage in real-world apps. This limited coverage can be attributed to a combination of factors such as failing to generate complex user inputs, unsatisfied activation conditions regarding device configurations and external resources, and hard-to-reach code paths that are not easily accessible through the GUI. To overcome these limitations, we propose CovAgent, a novel agentic AI-powered approach to enhance Android app UI testing. Our fuzzer-agnostic framework comprises an AI agent that inspects the app's decompiled Smali code and component transition graph, and reasons about unsatisfied activation conditions within the app code logic that prevent access to the activities that are unreachable by standard and widely adopted GUI fuzzers. Then, another agent generates dynamic instrumentation scripts that satisfy activation conditions required for successful transitions to those activities. We found that augmenting existing fuzzing approaches with our framework achieves a significant improvement in test coverage over the state-of-the-art, LLMDroid, and other baselines such as Fastbot and APE (e.g., 101.1%, 116.3% and 179.7% higher activity coverage, respectively). CovAgent also outperforms all the baselines in other metrics such as class, method, and line coverage. We also conduct investigations into components within CovAgent to reveal further insights regarding the efficacy of Agentic AI in the field of automated app testing such as the agentic activation condition inference accuracy, and agentic activity-launching success rate.", "published": "2026-01-29T04:21:11Z", "updated": "2026-01-29T04:21:11Z", "authors": ["Wei Minn", "Biniam Fisseha Demissie", "Yan Naing Tun", "Jiakun Liu", "Mariano Ceccato", "Lwin Khin Shar", "David Lo"], "pdf_url": "https://arxiv.org/pdf/2601.21253v1"}
{"id": "http://arxiv.org/abs/2601.02066v3", "title": "The State of Open Science in Software Engineering Research: A Case Study of ICSE Artifacts", "summary": "Replication packages are crucial for enabling transparency, validation, and reuse in software engineering (SE) research. While artifact sharing is now a standard practice and even expected at premier SE venues such as ICSE, the practical usability of these replication packages remains underexplored. In particular, there is a marked lack of studies that comprehensively examine the executability and reproducibility of replication packages in SE research. In this paper, we aim to fill this gap by evaluating 100 replication packages published as part of ICSE proceedings over the past decade (2015--2024). We assess the (1) executability of the replication packages, (2) efforts and modifications required to execute them, (3) challenges that prevent executability, and (4) reproducibility of the original findings. We spent approximately 650 person-hours in total executing the artifacts and reproducing the study findings. Our findings reveal that only 40\\% of the 100 evaluated artifacts were executable, of which 32.5\\% (13 out of 40) ran without any modification. Regarding effort levels, 17.5\\% (7 out of 40) required low effort, while 82.5\\% (33 out of 40) required moderate to high effort to execute successfully. We identified five common types of modifications and 13 challenges leading to execution failure, spanning environmental, documentation, and structural issues. Among the executable artifacts, only 35\\% (14 out of 40) reproduced the original results. These findings highlight a notable gap between artifact availability, executability, and reproducibility. Our study proposes three actionable guidelines to improve the preparation, documentation, and review of research artifacts, thereby strengthening the rigor and sustainability of open science practices in SE research.", "published": "2026-01-05T12:47:43Z", "updated": "2026-01-29T03:26:31Z", "authors": ["Al Muttakin", "Saikat Mondal", "Chanchal Roy"], "pdf_url": "https://arxiv.org/pdf/2601.02066v3"}
{"id": "http://arxiv.org/abs/2601.12448v2", "title": "Evaluating Large Language Models for Time Series Anomaly Detection in Aerospace Software", "summary": "Time series anomaly detection (TSAD) is essential for ensuring the safety and reliability of aerospace software systems. Although large language models (LLMs) provide a promising training-free alternative to unsupervised approaches, their effectiveness in aerospace settings remains under-examined because of complex telemetry, misaligned evaluation metrics, and the absence of domain knowledge. To address this gap, we introduce ATSADBench, the first benchmark for aerospace TSAD. ATSADBench comprises nine tasks that combine three pattern-wise anomaly types, univariate and multivariate signals, and both in-loop and out-of-loop feedback scenarios, yielding 108,000 data points. Using this benchmark, we systematically evaluate state-of-the-art open-source LLMs under two paradigms: Direct, which labels anomalies within sliding windows, and Prediction-Based, which detects anomalies from prediction errors. To reflect operational needs, we reformulate evaluation at the window level and propose three user-oriented metrics: Alarm Accuracy (AA), Alarm Latency (AL), and Alarm Contiguity (AC), which quantify alarm correctness, timeliness, and credibility. We further examine two enhancement strategies, few-shot learning and retrieval-augmented generation (RAG), to inject domain knowledge. The evaluation results show that (1) LLMs perform well on univariate tasks but struggle with multivariate telemetry, (2) their AA and AC on multivariate tasks approach random guessing, (3) few-shot learning provides modest gains whereas RAG offers no significant improvement, and (4) in practice LLMs can detect true anomaly onsets yet sometimes raise false alarms, which few-shot prompting mitigates but RAG exacerbates. These findings offer guidance for future LLM-based TSAD in aerospace software.", "published": "2026-01-18T15:07:16Z", "updated": "2026-01-29T03:08:58Z", "authors": ["Yang Liu", "Yixing Luo", "Xiaofeng Li", "Xiaogang Dong", "Bin Gu", "Zhi Jin"], "pdf_url": "https://arxiv.org/pdf/2601.12448v2"}
{"id": "http://arxiv.org/abs/2601.20615v2", "title": "DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.", "published": "2026-01-28T13:51:00Z", "updated": "2026-01-29T02:59:26Z", "authors": ["Yanlin Wang", "Jiadong Wu", "Tianyue Jiang", "Mingwei Liu", "Jiachi Chen", "Chong Wang", "Ensheng Shi", "Xilin Liu", "Yuchi Ma", "Zibin Zheng"], "pdf_url": "https://arxiv.org/pdf/2601.20615v2"}
{"id": "http://arxiv.org/abs/2601.21194v1", "title": "Human-Agent versus Human Pull Requests: A Testing-Focused Characterization and Comparison", "summary": "AI-based coding agents are increasingly integrated into software development workflows, collaborating with developers to create pull requests (PRs). Despite their growing adoption, the role of human-agent collaboration in software testing remains poorly understood. This paper presents an empirical study of 6,582 human-agent PRs (HAPRs) and 3,122 human PRs (HPRs) from the AIDev dataset. We compare HAPRs and HPRs along three dimensions: (i) testing frequency and extent, (ii) types of testing-related changes (code-and-test co-evolution vs. test-focused), and (iii) testing quality, measured by test smells. Our findings reveal that, although the likelihood of including tests is comparable (42.9% for HAPRs vs. 40.0% for HPRs), HAPRs exhibit a larger extent of testing, nearly doubling the test-to-source line ratio found in HPRs. While test-focused task distributions are comparable, HAPRs are more likely to add new tests during co-evolution (OR=1.79), whereas HPRs prioritize modifying existing tests. Finally, although some test smell categories differ statistically, negligible effect sizes suggest no meaningful differences in quality. These insights provide the first characterization of how human-agent collaboration shapes testing practices.", "published": "2026-01-29T02:50:02Z", "updated": "2026-01-29T02:50:02Z", "authors": ["Roberto Milanese", "Francesco Salzano", "Angelica Spina", "Antonio Vitale", "Remo Pareschi", "Fausto Fasano", "Mattia Fazzini"], "pdf_url": "https://arxiv.org/pdf/2601.21194v1"}
{"id": "http://arxiv.org/abs/2601.21186v1", "title": "From Logic to Toolchains: An Empirical Study of Bugs in the TypeScript Ecosystem", "summary": "TypeScript has rapidly become a popular language for modern web development, yet its effect on software faults remains poorly understood. This paper presents the first large-scale empirical study of bugs in real-world TypeScript projects. We analyze 633 bug reports from 16 popular open-source repositories to construct a taxonomy of fault types, quantify their prevalence, and relate them to project characteristics such as size, domain, and dependency composition. Our results reveal a fault landscape dominated not by logic or syntax errors but by tooling and configuration faults, API misuses, and asynchronous error-handling issues. We show that these categories correlate strongly with build complexity and dependency heterogeneity, indicating that modern failures often arise at integration and orchestration boundaries rather than within algorithmic logic. A longitudinal comparison with JavaScript studies shows that while static typing in TypeScript has reduced traditional runtime and type errors, it has shifted fragility toward build systems and toolchains. These findings offer new insight into how language design and ecosystem evolution reshape the fault profiles of large-scale software systems.", "published": "2026-01-29T02:36:04Z", "updated": "2026-01-29T02:36:04Z", "authors": ["TianYi Tang", "Saba Alimadadi", "Nick Sumner"], "pdf_url": "https://arxiv.org/pdf/2601.21186v1"}
{"id": "http://arxiv.org/abs/2510.03217v2", "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair", "summary": "Agentic Automated Program Repair (APR) is increasingly tackling complex, repository-level bugs in industry, but ultimately these patches still need to be reviewed by a human before committing them to ensure they address the bug. Showing patches unlikely to be accepted can lead to substantial noise, wasting valuable developer time and eroding trust in automated code changes. We introduce two complementary LLM-based policies to reduce such noise: bug abstention and patch validation policies. Bug abstention excludes bugs that the agentic APR system is unlikely to fix. Patch validation rejects patches that are unlikely to be a good fix for the given bug. We evaluate both policies on three sets of bugs from Google's codebase, and their candidate patches generated by an internal agentic APR system. On a set of 174 human-reported bugs, removing bugs and patches rejected by our policies can raise success rates by up to 13 percentage points and 15 percentage points, respectively, and by up to 39 percentage points in combination. On null pointer exceptions and sanitizer-reported bugs with machine-generated bug reports, patch validation also improves average single-sample success rates. This two-policy approach provides a practical path to the reliable, industrial-scale deployment of agentic APR systems.", "published": "2025-10-03T17:53:28Z", "updated": "2026-01-29T00:09:30Z", "authors": ["José Cambronero", "Michele Tufano", "Sherry Shi", "Renyao Wei", "Grant Uy", "Runxiang Cheng", "Chin-Jung Liu", "Shiying Pan", "Satish Chandra", "Pat Rondon"], "pdf_url": "https://arxiv.org/pdf/2510.03217v2"}
