{"id": "http://arxiv.org/abs/2512.07824v1", "title": "Studying the Role of Reusing Crowdsourcing Knowledge in Software Development", "summary": "Crowdsourcing platforms, such as Stack Overflow, have changed and impacted the software development practice. In these platforms, developers share and reuse their software development and programming experience. Therefore, a plethora of research work focused on crowdsourcing in software engineering and showed that, among other things, crowdsourced development tends to increase developers' productivity and reduce time-to-market. However, in crowdsourcing, the empirical studies of software quality are lacking, and simple questions, such as what developers use the crowdsourcing knowledge for, are unanswered.\n  Therefore, our research focused on studying the impact of reusing crowdsourcing knowledge on software projects. To do so, we conduct several large-scale empirical studies on some of the well-known crowdsourcing platforms, including Stack Overflow and npm. Our results showed that reusing knowledge from these crowdsourcing platforms has the potential to assist software development practice, specifically in the form of reusing crowdsourced code. However, using such knowledge affects the quality of the software in several aspects, such as making the software projects suffer from dependency overhead and increasing the maintenance effort. Based on these findings, we use the gained knowledge to make sound data-driven decisions where we examine software quality assurance methods to mitigate the risk of relying on crowd sourcing knowledge in software development. We examine the use of continuous integration (CI). Our analysis showed how CI can be improved to increase developers' productivity and save their resources.", "published": "2025-12-08T18:54:47Z", "updated": "2025-12-08T18:54:47Z", "authors": ["Rabe Abdalkareem"], "pdf_url": "https://arxiv.org/pdf/2512.07824v1"}
{"id": "http://arxiv.org/abs/2512.07814v1", "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach", "summary": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.", "published": "2025-12-08T18:47:40Z", "updated": "2025-12-08T18:47:40Z", "authors": ["Hua Yang", "Alejandro Velasco", "Sen Fang", "Bowen Xu", "Denys Poshyvanyk"], "pdf_url": "https://arxiv.org/pdf/2512.07814v1"}
{"id": "http://arxiv.org/abs/2509.23045v3", "title": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents", "summary": "Large Language Models (LLMs) are increasingly applied to software engineering (SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent frameworks with multi-turn interactions and workflow-based Agentless methods with single-turn verifiable steps. We argue these paradigms are not mutually exclusive: reasoning-intensive Agentless training induces skill priors, including localization, code edit, and self-reflection that enable efficient and effective SWE-Agent adaptation. In this work, we first curate the Agentless training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\% on SWE-bench Verified, the best among workflow approaches. With additional SFT adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to 48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These results show that structured skill priors from Agentless training can bridge workflow and agentic frameworks for transferable coding agents.", "published": "2025-09-27T01:49:13Z", "updated": "2025-12-08T17:33:03Z", "authors": ["Zonghan Yang", "Shengjie Wang", "Kelin Fu", "Wenyang He", "Weimin Xiong", "Yibo Liu", "Yibo Miao", "Bofei Gao", "Yejie Wang", "Yingwei Ma", "Yanhao Li", "Yue Liu", "Zhenxing Hu", "Kaitai Zhang", "Shuyi Wang", "Huarong Chen", "Flood Sung", "Yang Liu", "Yang Gao", "Zhilin Yang", "Tianyu Liu"], "pdf_url": "https://arxiv.org/pdf/2509.23045v3"}
{"id": "http://arxiv.org/abs/2506.19653v3", "title": "Waterfall Model Simulation: A Systematic Mapping Study", "summary": "This paper systematically maps peer-reviewed research and graduate theses/dissertations that explicitly simulate the waterfall model. Following Petersen's mapping guidelines and Kitchenham's systematic literature review practices, major databases (ACM Digital Library, IEEE Xplore, Scopus, Springer, Google Scholar, and Web of Science) were searched for studies published between 2000-2024 using the title query (\"simulation\" OR \"simulating\") AND \"waterfall\". A PRISMA workflow guided the screening process, and approximately 9% of retrieved records met the inclusion criteria. A repeated extraction process captured methods, tools, venues, geography, publication years, comparative scope, and fidelity to Royce's original model; findings were synthesized thematically. Discrete-event simulation dominates (80%) compared to system dynamics (20%). Reported tools center on Simphony.NET (40%) and SimPy (20%), while 40% of studies omit tool details, limiting reproducibility. Research is distributed across Italy, Lebanon, India, Japan, and the United States; publication venues include 60% journals and 40% conferences. Sixty percent of studies are comparative, while 40% model only the waterfall approach. No study reproduces Royce's original model; all employ adaptations. The paper concludes by presenting a consolidated view of waterfall simulation research and recommending clearer model reporting, fuller tool disclosure, and wider adoption of open-source platforms.", "published": "2025-06-24T14:15:34Z", "updated": "2025-12-08T16:02:50Z", "authors": ["Antonios Saravanos"], "pdf_url": "https://arxiv.org/pdf/2506.19653v3"}
{"id": "http://arxiv.org/abs/2510.03894v2", "title": "A Brief History of the Waterfall Model: Past, Present, and Future", "summary": "The waterfall model, one of the earliest software development methodologies, has played a foundational role in shaping contemporary software engineering practices. This paper provides a historical and critical overview of the model, tracing its conceptual origins in software engineering, its formalization by Royce, and its evolution through decades of industry adoption and critique. Although often criticized for its rigidity, shortcomings, and high failure rates, the waterfall model persists in specific domains. Its principles continue to influence contemporary hybrid development frameworks that combine traditional and agile methods. Drawing on a range of scholarly sources, this study synthesizes key developments in the perception and application of the waterfall model. The analysis highlights how the model has shifted from a standalone framework to a component within modern hybrid methodologies. By revisiting its origins, assessing its present utility, and examining its role in contemporary development practices, this paper argues that the waterfall model remains relevant, not as a relic of the past but as part of context-aware development strategies. The paper contends that the model's enduring relevance lies in its adaptability. By recognizing both its limitations and its strengths, and by understanding its integration within hybrid approaches, practitioners can make more informed decisions about methodology selection and process design in diverse development environments.", "published": "2025-10-04T18:22:01Z", "updated": "2025-12-08T16:02:07Z", "authors": ["Antonios Saravanos"], "pdf_url": "https://arxiv.org/pdf/2510.03894v2"}
{"id": "http://arxiv.org/abs/2512.07666v1", "title": "Bridging Code Graphs and Large Language Models for Better Code Understanding", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.", "published": "2025-12-08T16:00:29Z", "updated": "2025-12-08T16:00:29Z", "authors": ["Zeqi Chen", "Zhaoyang Chu", "Yi Gui", "Feng Guo", "Yao Wan", "Chuan Shi"], "pdf_url": "https://arxiv.org/pdf/2512.07666v1"}
{"id": "http://arxiv.org/abs/2512.07665v1", "title": "Reliable agent engineering should integrate machine-compatible organizational principles", "summary": "As AI agents built on large language models (LLMs) become increasingly embedded in society, issues of coordination, control, delegation, and accountability are entangled with concerns over their reliability. To design and implement LLM agents around reliable operations, we should consider the task complexity in the application settings and reduce their limitations while striving to minimize agent failures and optimize resource efficiency. High-functioning human organizations have faced similar balancing issues, which led to evidence-based theories that seek to understand their functioning strategies. We examine the parallels between LLM agents and the compatible frameworks in organization science, focusing on what the design, scaling, and management of organizations can inform agentic systems towards improving reliability. We offer three preliminary accounts of organizational principles for AI agent engineering to attain reliability and effectiveness, through balancing agency and capabilities in agent design, resource constraints and performance benefits in agent scaling, and internal and external mechanisms in agent management. Our work extends the growing exchanges between the operational and governance principles of AI systems and social systems to facilitate system integration.", "published": "2025-12-08T15:58:55Z", "updated": "2025-12-08T15:58:55Z", "authors": ["R. Patrick Xian", "Garry A. Gabison", "Ahmed Alaa", "Christoph Riedl", "Grigorios G. Chrysos"], "pdf_url": "https://arxiv.org/pdf/2512.07665v1"}
{"id": "http://arxiv.org/abs/2508.20902v2", "title": "Automated Test Validators for Flaky Cyber-Physical System Simulators: Approach and Evaluation", "summary": "Simulation-based testing of cyber-physical systems (CPS) is costly due to the time-consuming execution of CPS simulators. In addition, CPS simulators may be flaky, leading to inconsistent test outcomes and requiring repeated test re-execution for reliable test verdicts. Many test inputs within the input space of CPS may not effectively exercise the behaviour of the system under test (SUT) -- for instance, those that violate system preconditions, exceed operational design domain (ODD) limits, or represent inherently safe scenarios. In this article, we propose to use test validators to filter out such test inputs before execution. We describe two methods for generating test validators: one using genetic programming (GP) that employs well-known spectrum-based fault localization (SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness functions; and the other using decision trees (DT) and decision rules (DR). We evaluate our test validators through case studies in the domains of aerospace, networking and autonomous driving. We show that test validators generated using GP with Ochiai are significantly more accurate than those generated using GP with Tarantula and Naish or using DT or DR. Moreover, this accuracy advantage remains even when accounting for the flakiness of the simulator. We further show that our test validators generated by GP with Ochiai are robust against flakiness with only 4% average variation in their accuracy results across four different network and autonomous-driving systems with flaky behaviours. Finally, we show that, on average, 88.7% of the assertions inferred by our approach align or overlap with requirements precondition violations, ODD-limit violations, and nominal safe conditions extracted from technical standards and empirical results in the literature.", "published": "2025-08-28T15:33:42Z", "updated": "2025-12-08T15:56:55Z", "authors": ["Baharin A. Jodat", "Khouloud Gaaloul", "Mehrdad Sabetzadeh", "Shiva Nejati"], "pdf_url": "https://arxiv.org/pdf/2508.20902v2"}
{"id": "http://arxiv.org/abs/2507.20109v2", "title": "Learning to Align Human Code Preferences", "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in automating software development tasks. While recent advances leverage Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align models with human preferences, the optimal training strategy remains unclear across diverse code preference scenarios. This paper systematically investigates the roles of SFT and DPO in aligning LLMs with different code preferences. Through both theoretical analysis and empirical observation, we hypothesize that SFT excels in scenarios with objectively verifiable optimal solutions, while applying SFT followed by DPO (S&D) enables models to explore superior solutions in scenarios without objectively verifiable optimal solutions. Based on the analysis and experimental evidence, we propose Adaptive Preference Optimization (APO), a dynamic integration approach that adaptively amplifies preferred responses, suppresses dispreferred ones, and encourages exploration of potentially superior solutions during training. Extensive experiments across six representative code preference tasks validate our theoretical hypotheses and demonstrate that APO consistently matches or surpasses the performance of existing SFT and S&D strategies. Our work provides both theoretical foundations and practical guidance for selecting appropriate training strategies in different code preference alignment scenarios.", "published": "2025-07-27T02:48:26Z", "updated": "2025-12-08T12:57:25Z", "authors": ["Xin Yin", "Chao Ni", "Xiaohu Yang"], "pdf_url": "https://arxiv.org/pdf/2507.20109v2"}
{"id": "http://arxiv.org/abs/2512.07507v1", "title": "VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform", "summary": "The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.", "published": "2025-12-08T12:43:33Z", "updated": "2025-12-08T12:43:33Z", "authors": ["Yiming Cui", "Shiyu Fang", "Jiarui Zhang", "Yan Huang", "Chengkai Xu", "Bing Zhu", "Hao Zhang", "Peng Hang", "Jian Sun"], "pdf_url": "https://arxiv.org/pdf/2512.07507v1"}
{"id": "http://arxiv.org/abs/2512.07501v1", "title": "AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution", "summary": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\\% verification success rate, significantly surpassing the $65$\\% success rate of the SOTA approach.", "published": "2025-12-08T12:35:10Z", "updated": "2025-12-08T12:35:10Z", "authors": ["Weilin Luo", "Xueyi Liang", "Haotian Deng", "Yanan Liu", "Hai Wan"], "pdf_url": "https://arxiv.org/pdf/2512.07501v1"}
{"id": "http://arxiv.org/abs/2512.07497v1", "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations", "summary": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.", "published": "2025-12-08T12:27:15Z", "updated": "2025-12-08T12:27:15Z", "authors": ["JV Roig"], "pdf_url": "https://arxiv.org/pdf/2512.07497v1"}
{"id": "http://arxiv.org/abs/2405.08788v5", "title": "Using weakest application conditions to rank graph transformations for graph repair", "summary": "When using graphs and graph transformations to model systems, consistency is an important concern. While consistency has primarily been viewed as a binary property, i.e., a graph is consistent or inconsistent with respect to a set of constraints, recent work has presented an approach to consistency as a graduated property. This allows living with inconsistencies for a while and repairing them when necessary. For repairing inconsistencies in a graph, we use graph transformation rules with so-called {\\em impairment-indicating and repair-indicating application conditions} to understand how much repair gain certain rule applications would bring. Both types of conditions can be derived from given graph constraints. Our main theorem shows that the difference between the number of actual constraint violations before and after a graph transformation step can be characterised by the difference between the numbers of violated impairment-indicating and repair-indicating application conditions. This theory forms the basis for algorithms with look-ahead that rank graph transformations according to their potential for graph repair. An evaluation shows that graph repair can be well-supported by rules with these new types of application conditions in terms of effectiveness and scalability.", "published": "2024-05-14T17:37:01Z", "updated": "2025-12-08T11:53:28Z", "authors": ["Lars Fritsche", "Alexander Lauer", "Maximilian Kratz", "Andy Schürr", "Gabriele Taentzer"], "pdf_url": "https://arxiv.org/pdf/2405.08788v5"}
{"id": "http://arxiv.org/abs/2512.07434v1", "title": "Systematic Evaluation of Black-Box Checking for Fast Bug Detection", "summary": "Combinations of active automata learning, model-based testing and model checking have been successfully used in numerous applications, e.g., for spotting bugs in implementations of major network protocols and to support refactoring of embedded controllers. However, in the large majority of these applications, model checking is only used at the very end, when no counterexample can be found anymore for the latest hypothesis model. This contrasts with the original proposal of black-box checking (BBC) by Peled, Vardi & Yannakakis, which applies model checking for all hypotheses, also the intermediate ones. In this article, we present the first systematic evaluation of the ability of BBC to find bugs quickly, based on 77 benchmarks models from real protocol implementations and controllers for which specifications of safety properties are available. Our main finding are: (a) In cases where the full model can be learned, BBC detects violations of the specifications with just 3.4% of the queries needed by an approach in which model checking is only used for the full model. (b) Even when the full model cannot be learned, BBC is still able to detect many violations of the specification. In particular, BBC manages to detect 94% of the safety properties violations in the challenging RERS 2019 industrial LTL benchmarks. (c) Our results also confirm that BBC is way more effective than existing MBT algorithms in finding deep bugs in implementations.", "published": "2025-12-08T11:10:05Z", "updated": "2025-12-08T11:10:05Z", "authors": ["Bram Pellen", "María Belén Rodríguez", "Frits Vaandrager", "Petra van den Bos"], "pdf_url": "https://arxiv.org/pdf/2512.07434v1"}
{"id": "http://arxiv.org/abs/2512.07404v1", "title": "Do LLMs Trust the Code They Write?", "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.", "published": "2025-12-08T10:38:03Z", "updated": "2025-12-08T10:38:03Z", "authors": ["Francisco Ribeiro", "Claudio Spiess", "Prem Devanbu", "Sarah Nadi"], "pdf_url": "https://arxiv.org/pdf/2512.07404v1"}
{"id": "http://arxiv.org/abs/2512.07368v1", "title": "Challenges in Developing Secure Software -- Results of an Interview Study in the German Software Industry", "summary": "The damage caused by cybercrime makes the development of secure software inevitable. Although many tools and frameworks exist to support the development of secure software, statistics on cybercrime show no improvement in recent years. To understand the challenges software companies face in developing secure software, we conducted an interview study with 19 industry experts from 12 cross-industry companies. The results of our study show that the challenges are mainly due to high complexity, a lack of security awareness, and unsuitable processes, which are further exacerbated by an immediate lack of skilled personnel. This article presents our study and the challenges we identified, and derives potential research directions from them.", "published": "2025-12-08T10:05:08Z", "updated": "2025-12-08T10:05:08Z", "authors": ["Alex R. Mattukat", "Timo Langstrof", "Horst Lichter"], "pdf_url": "https://arxiv.org/pdf/2512.07368v1"}
{"id": "http://arxiv.org/abs/2511.08059v2", "title": "\"I need to learn better searching tactics for privacy policy laws.\" Investigating Software Developers' Behavior When Using Sources on Privacy Issues", "summary": "Since the introduction of the European General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), software developers increasingly have to make privacy-related decisions during system design and implementation. However, past research showed that they often lack legal expertise and struggle with privacy-compliant development. To shed light on how effective current information sources are in supporting them with privacy-sensitive implementation, we conducted a qualitative study with 30 developers. Participants were presented with a privacy-sensitive scenario and asked to identify privacy issues and suggest measures using their knowledge, online resources, and an AI assistant. We observed developers' decision-making in think-aloud sessions and discussed it in follow-up interviews. We found that participants struggled with all three sources: personal knowledge was insufficient, web content was often too complex, and while AI assistants provided clear and user-tailored responses, they lacked contextual relevance and failed to identify scenario-specific issues. Our study highlights major shortcomings in existing support for privacy-related development tasks. Based on our findings, we discuss the need for more accessible, understandable, and actionable privacy resources for developers.", "published": "2025-11-11T09:58:06Z", "updated": "2025-12-08T09:55:32Z", "authors": ["Stefan Albert Horstmann", "Sandy Hong", "Maziar Niazian", "Cristiana Santos", "Alena Naiakshina"], "pdf_url": "https://arxiv.org/pdf/2511.08059v2"}
{"id": "http://arxiv.org/abs/2510.03879v2", "title": "Adversarial Agent Collaboration for C to Rust Translation", "summary": "Translating C to memory-safe languages, like Rust, prevents critical memory safety vulnerabilities that are prevalent in legacy C software. Existing approaches for C to safe Rust translation, including LLM-assisted ones, do not generalize on larger (> 500 LoC) C codebases because they depend on complex program analyses that frequently break. In this work, we present ACToR (Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired by GANs, ACToR pits a generator agent against a discriminator agent, which collaborate to iteratively generate a Rust translation. On each iteration, the translator agent synthesizes and refines a Rust translation to pass an existing suite of tests, and then the discriminator agent finds new failing tests. We demonstrate that ACToR translates all of the 63 real-world command-line utilities considered in our benchmarks, which have an average size of 473 lines of code, and it achieves over 90% test pass rate with zero human intervention during translation. To our knowledge, it is the first work to show evidence that an agent-centric approach can reliably and automatically convert standalone command-line C programs at this scale. Furthermore, ACToR improves translation correctness by up to 25.1% compared to baseline, non-adversarial approaches.", "published": "2025-10-04T17:08:36Z", "updated": "2025-12-08T09:32:51Z", "authors": ["Tianyu Li", "Ruishi Li", "Bo Wang", "Brandon Paulsen", "Umang Mathur", "Prateek Saxena"], "pdf_url": "https://arxiv.org/pdf/2510.03879v2"}
{"id": "http://arxiv.org/abs/2506.14683v2", "title": "Unified Software Engineering Agent as AI Software Engineer", "summary": "The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future.", "published": "2025-06-17T16:19:13Z", "updated": "2025-12-08T09:06:06Z", "authors": ["Leonhard Applis", "Yuntong Zhang", "Shanchao Liang", "Nan Jiang", "Lin Tan", "Abhik Roychoudhury"], "pdf_url": "https://arxiv.org/pdf/2506.14683v2"}
{"id": "http://arxiv.org/abs/2512.07293v1", "title": "The Human Need for Storytelling: Reflections on Qualitative Software Engineering Research With a Focus Group of Experts", "summary": "From its first adoption in the late 80s, qualitative research has slowly but steadily made a name for itself in what was, and perhaps still is, the predominantly quantitative software engineering (SE) research landscape. As part of our regular column on empirical software engineering (ACM SIGSOFT SEN-ESE), we reflect on the state of qualitative SE research with a focus group of experts. Among other things, we discuss why qualitative SE research is important, how it evolved over time, common impediments faced while practicing it today, and what the future of qualitative SE research might look like. Joining the conversation are Rashina Hoda (Monash University, Australia), Carolyn Seaman (University of Maryland, United States), and Klaas Stol (University College Cork, Ireland). The content of this paper is a faithful account of our conversation from October 25, 2025, which we moderated and edited for our column.", "published": "2025-12-08T08:32:31Z", "updated": "2025-12-08T08:32:31Z", "authors": ["Roberto Verdecchia", "Justus Bogner"], "pdf_url": "https://arxiv.org/pdf/2512.07293v1"}
{"id": "http://arxiv.org/abs/2510.02007v3", "title": "ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column", "summary": "From its early foundations in the 1970s, empirical software engineering (ESE) has evolved into a mature research discipline that embraces a plethora of different topics, methodologies, and industrial practices. Despite its remarkable progress, the ESE research field still needs to keep evolving, as new impediments, shortcoming, and technologies emerge. Research reproducibility, limited external validity, subjectivity of reviews, and porting research results to industrial practices are just some examples of the drivers for improvements to ESE research. Additionally, several facets of ESE research are not documented very explicitly, which makes it difficult for newcomers to pick them up. With this new regular ACM SIGSOFT SEN column (SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research, ranging from general topics such as the nature and best practices for replication packages, to more nuanced themes such as statistical methods, interview transcription tools, and publishing interdisciplinary research. Our aim for the column is to be a place where we can regularly spark conversations on ESE topics that might not often be touched upon or are left implicit. Contributions to this column will be grounded in expert interviews, focus groups, surveys, and position pieces, with the goal of encouraging reflection and improvement in how we conduct, communicate, teach, and ultimately improve ESE research. Finally, we invite feedback from the ESE community on challenging, controversial, or underexplored topics, as well as suggestions for voices you would like to hear from. While we cannot promise to act on every idea, we aim to shape this column around the community interests and are grateful for all contributions.", "published": "2025-10-02T13:28:54Z", "updated": "2025-12-08T08:25:43Z", "authors": ["Justus Bogner", "Roberto Verdecchia"], "pdf_url": "https://arxiv.org/pdf/2510.02007v3"}
{"id": "http://arxiv.org/abs/2512.07261v1", "title": "Automatic Syntax Error Repair for Discrete Controller Synthesis using Large Language Model", "summary": "Discrete Controller Synthesis (DCS) is a powerful formal method for automatically generating specifications of discrete event systems. However, its practical adoption is often hindered by the highly specialized nature of formal models written in languages such as FSP and FLTL. In practice, syntax errors in modeling frequently become an important bottleneck for developers-not only disrupting the workflow and reducing productivity, but also diverting attention from higher-level semantic design. To this end, this paper presents an automated approach that leverages Large Language Models (LLMs) to repair syntax errors in DCS models using a well-designed, knowledge-informed prompting strategy. Specifically, the prompting is derived from a systematic empirical study of common error patterns, identified through expert interviews and student workshops. It equips the LLM with DCS-specific domain knowledge, including formal grammar rules and illustrative examples, to guide accurate corrections. To evaluate our method, we constructed a new benchmark by systematically injecting realistic syntax errors into validated DCS models. The quantitative evaluation demonstrates the high effectiveness of the proposed approach in terms of repair accuracy and its practical utility regarding time, achieving a speedup of 3.46 times compared to human developers. The experimental replication suite, including the benchmark and prompts, is available at https://github.com/Uuusay1432/DCSModelRepair.git", "published": "2025-12-08T07:57:15Z", "updated": "2025-12-08T07:57:15Z", "authors": ["Yusei Ishimizu", "Takuto Yamauchi", "Sinan Chen", "Jinyu Cai", "Jialong Li", "Kenji Tei"], "pdf_url": "https://arxiv.org/pdf/2512.07261v1"}
{"id": "http://arxiv.org/abs/2512.07193v1", "title": "Towards Benchmarking Design Pattern Detection Under Obfuscation: Reproducing and Evaluating Attention-Based Detection Method", "summary": "This paper investigates the semantic robustness of attention-based classifiers for design pattern detection, particularly focusing on their reliance on structural and behavioral semantics. We reproduce the DPDAtt, an attention-based design pattern detection approach using learning-based classifiers, and evaluate its performance under obfuscation. To this end, we curate an obfuscated version of the DPDAtt Corpus, where the name identifiers in code such as class names, method names, etc., and string literals like print statements and comment blocks are replaced while preserving control flow, inheritance, and logic. Our findings reveal that these trained classifiers in DPDAtt depend significantly on superficial syntactic features, leading to substantial misclassification when such cues are removed through obfuscation. This work highlights the need for more robust detection tools capable of capturing deeper semantic meanings in source code. We propose our curated Obfuscated corpus (containing 34 Java source files) as a reusable proof-of-concept benchmark for evaluating state-of-the-art design pattern detectors on their true semantic generalization capabilities.", "published": "2025-12-08T06:10:34Z", "updated": "2025-12-08T06:10:34Z", "authors": ["Manthan Shenoy", "Andreas Rausch"], "pdf_url": "https://arxiv.org/pdf/2512.07193v1"}
{"id": "http://arxiv.org/abs/2512.07122v1", "title": "RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations", "summary": "Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.", "published": "2025-12-08T03:05:27Z", "updated": "2025-12-08T03:05:27Z", "authors": ["Liping Han", "Tingting Nie", "Le Yu", "Mingzhe Hu", "Tao Yue"], "pdf_url": "https://arxiv.org/pdf/2512.07122v1"}
{"id": "http://arxiv.org/abs/2512.07097v1", "title": "TagLabel: RFID Based Orientation and Material Sensing for Automated Package Inspection", "summary": "Modern logistics systems face increasing difficulty in identifying counterfeit products, fraudulent returns, and hazardous items concealed within packages, yet current package screening methods remain too slow, expensive, and impractical for widespread use. This paper presents TagLabel, an RFID based system that determines both the orientation and contents of packages using low cost passive UHF tags. By analyzing how materials change RSSI and phase, the system identifies the contents of a package without opening it. Using orientation inferred from phase differences, tag occlusion, and antenna gain patterns, the system selects the tag with the greatest occlusion for accurate material sensing. We evaluate two and three tag configurations, and show that both can deliver high orientation and material sensing performance through the use of machine learning classifiers, even in realistic RF environments. When combined into a unified pipeline, TagLabel achieves more than 80 percent accuracy across all package orientations. Because it requires only standard RFID hardware and offers fast scanning times, this approach provides a practical way to enhance package inspection and improve automation in logistics operations.", "published": "2025-12-08T02:26:25Z", "updated": "2025-12-08T02:26:25Z", "authors": ["David Wang", "Jiale Zhang", "Pei Zhang"], "pdf_url": "https://arxiv.org/pdf/2512.07097v1"}
