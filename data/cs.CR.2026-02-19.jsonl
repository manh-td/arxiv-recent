{"id": "http://arxiv.org/abs/2602.17651v1", "title": "Non-Trivial Zero-Knowledge Implies One-Way Functions", "summary": "A recent breakthrough [Hirahara and Nanashima, STOC'2024] established that if $\\mathsf{NP} \\not \\subseteq \\mathsf{ioP/poly}$, the existence of zero-knowledge with negligible errors for $\\mathsf{NP}$ implies the existence of one-way functions (OWFs). In this work, we obtain a characterization of one-way functions from the worst-case complexity of zero-knowledge {\\em in the high-error regime}.\n  We say that a zero-knowledge argument is {\\em non-trivial} if the sum of its completeness, soundness and zero-knowledge errors is bounded away from $1$. Our results are as follows, assuming $\\mathsf{NP} \\not \\subseteq \\mathsf{ioP/poly}$:\n  1. {\\em Non-trivial} Non-Interactive ZK (NIZK) arguments for $\\mathsf{NP}$ imply the existence of OWFs. Using known amplification techniques, this result also provides an unconditional transformation from weak to standard NIZK proofs for all meaningful error parameters.\n  2. We also generalize to the interactive setting: {\\em Non-trivial} constant-round public-coin zero-knowledge arguments for $\\mathsf{NP}$ imply the existence of OWFs, and therefore also (standard) four-message zero-knowledge arguments for $\\mathsf{NP}$.\n  Prior to this work, one-way functions could be obtained from NIZKs that had constant zero-knowledge error $ε_{zk}$ and soundness error $ε_{s}$ satisfying $ε_{zk} + \\sqrt{ε_{s}} < 1$ [Chakraborty, Hulett and Khurana, CRYPTO'2025]. However, the regime where $ε_{zk} + \\sqrt{ε_{s}} \\geq 1$ remained open. This work closes the gap, and obtains new implications in the interactive setting. Our results and techniques could be useful stepping stones in the quest to construct one-way functions from worst-case hardness.", "published": "2026-02-19T18:56:24Z", "updated": "2026-02-19T18:56:24Z", "authors": ["Suvradip Chakraborty", "James Hulett", "Dakshita Khurana", "Kabir Tomer"], "pdf_url": "https://arxiv.org/pdf/2602.17651v1"}
{"id": "http://arxiv.org/abs/2508.04669v3", "title": "Cybersecurity of Quantum Key Distribution Implementations", "summary": "Practical implementations of Quantum Key Distribution (QKD) often deviate from the theoretical protocols, exposing the implementations to various attacks even when the underlying (ideal) protocol is proven secure. We present new analysis tools and methodologies for quantum cybersecurity, adapting the concepts of vulnerabilities, attack surfaces, and exploits from classical cybersecurity to QKD implementation attacks. We also present three additional concepts, derived from the connection between classical and quantum cybersecurity: \"Quantum Fuzzing\", which is the first tool for black-box vulnerability research on QKD implementations; \"Reversed-Space Attacks\", which are a generic exploit method using the attack surface of imperfect receivers; and concrete quantum-mechanical definitions of \"Quantum Side-Channel Attacks\" and \"Quantum State-Channel Attacks\", meaningfully distinguishing them from each other and from other attacks. Using our tools, we analyze multiple existing QKD attacks and show that the \"Bright Illumination\" attack could have been found even with minimal knowledge of the device implementation. This work begins to bridge the gap between current analysis methods for experimental attacks on QKD implementations and the decades-long research in the field of classical cybersecurity, improving the practical security of QKD products and enhancing their usefulness in real-world systems.", "published": "2025-08-06T17:37:04Z", "updated": "2026-02-19T18:55:25Z", "authors": ["Ittay Alfassi", "Ran Gelles", "Rotem Liss", "Tal Mor"], "pdf_url": "https://arxiv.org/pdf/2508.04669v3"}
{"id": "http://arxiv.org/abs/2602.17622v1", "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?", "summary": "LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of tooling due to planning and state management limitations. We show that Type B failures share a root cause that is largely invariant to the underlying LLM: agents lack real-time task difficulty estimation. As a result, agents misallocate effort, over-commit to low-value branches, and exhaust context before completing attack chains.\n  Based on this insight, we present Excalibur, a penetration testing agent that couples strong tooling with difficulty-aware planning. A Tool and Skill Layer eliminates Type A failures through typed interfaces and retrieval-augmented knowledge. A Task Difficulty Assessment (TDA) mechanism addresses Type B failures by estimating tractability through four measurable dimensions (horizon estimation, evidence confidence, context load, and historical success) and uses these estimates to guide exploration-exploitation decisions within an Evidence-Guided Attack Tree Search (EGATS) framework. Excalibur achieves up to 91% task completion on CTF benchmarks with frontier models (39 to 49% relative improvement over baselines) and compromises 4 of 5 hosts on the GOAD Active Directory environment versus 2 by prior systems. These results show that difficulty-aware planning yields consistent end-to-end gains across models and addresses a limitation that model scaling alone does not eliminate.", "published": "2026-02-19T18:42:40Z", "updated": "2026-02-19T18:42:40Z", "authors": ["Gelei Deng", "Yi Liu", "Yuekang Li", "Ruozhao Yang", "Xiaofei Xie", "Jie Zhang", "Han Qiu", "Tianwei Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.17622v1"}
{"id": "http://arxiv.org/abs/2602.17590v1", "title": "BMC4TimeSec: Verification Of Timed Security Protocols", "summary": "We present BMC4TimeSec, an end-to-end tool for verifying Timed Security Protocols (TSP) based on SMT-based bounded model checking and multi-agent modelling in the form of Timed Interpreted Systems (TIS) and Timed Interleaved Interpreted Systems (TIIS). In BMC4TimeSec, TSP executions implement the TIS/TIIS environment (join actions, interleaving, delays, lifetimes), and knowledge automata implement the agents (evolution of participant knowledge, including the intruder). The code is publicly available on \\href{https://github.com/agazbrzezny/BMC4TimeSec}{GitHub}, as is a \\href{https://youtu.be/aNybKz6HwdA}{video} demonstration.", "published": "2026-02-19T18:12:01Z", "updated": "2026-02-19T18:12:01Z", "authors": ["Agnieszka M. Zbrzezny"], "pdf_url": "https://arxiv.org/pdf/2602.17590v1"}
{"id": "http://arxiv.org/abs/2512.01295v2", "title": "Systems Security Foundations for Agentic Computing", "summary": "In recent years, agentic artificial intelligence (AI) systems are becoming increasingly widespread. These systems allow agents to use various tools, such as web browsers, compilers, and more. However, despite their popularity, agentic AI systems also introduce a myriad of security concerns, due to their constant interaction with third-party servers. For example, a malicious adversary can cause data exfiltration by executing prompt injection attacks, as well as other unwarranted behavior. These security concerns have recently motivated researchers to improve the safety and reliability of agentic systems. However, most of the literature on this topic is from the AI standpoint and lacks the system-security perspective and guarantees. In this work, we begin bridging this gap and present an analysis through the lens of classic cybersecurity research. Specifically, motivated by decades of progress in this domain, we identify short- and long-term research problems in agentic AI safety by examining end-to-end security properties of entire systems, rather than standalone AI models running in isolation. Our key goal is to examine where research challenges arise when applying traditional security principles in the context of AI agents and, as a secondary goal, distill these ideas for AI practitioners. Furthermore, we extensively cover 11 case studies of real-world attacks on agentic systems, as well as define a series of new research problems that are specific to this important domain.", "published": "2025-12-01T05:28:59Z", "updated": "2026-02-19T18:03:10Z", "authors": ["Mihai Christodorescu", "Earlence Fernandes", "Ashish Hooda", "Somesh Jha", "Johann Rehberger", "Kamalika Chaudhuri", "Xiaohan Fu", "Khawaja Shams", "Guy Amir", "Jihye Choi", "Sarthak Choudhary", "Nils Palumbo", "Andrey Labunets", "Nishit V. Pandya"], "pdf_url": "https://arxiv.org/pdf/2512.01295v2"}
{"id": "http://arxiv.org/abs/2602.17490v1", "title": "Coin selection by Random Draw according to the Boltzmann distribution", "summary": "Coin selection refers to the problem of choosing a set of tokens to fund a transaction in token-based payment systems such as, e.g., cryptocurrencies or central bank digital currencies (CBDCs). In this paper, we propose the Boltzmann Draw that is a probabilistic algorithm inspired by the principles of statistical physics. The algorithm relies on drawing tokens according to the Boltzmann distribution, serving as an extension and improvement of the Random Draw method. Numerical results demonstrate the effectiveness of our method in bounding the number of selected input tokens as well as reducing dust generation and limiting the token pool size in the wallet. Moreover, the probabilistic algorithm can be implemented efficiently, improves performance and respects privacy requirements - properties of significant relevance for current token-based technologies. We compare the Boltzmann draw to both the standard Random Draw and the Greedy algorithm. We argue that the former is superior to the latter in the sense of the above objectives. Our findings are relevant for token-based technologies, and are also of interest for CBDCs, which as a legal tender possibly needs to handle large transaction volumes at a high frequency.", "published": "2026-02-19T16:00:21Z", "updated": "2026-02-19T16:00:21Z", "authors": ["Jan Lennart Bönsel", "Michael Maurer", "Silvio Petriconi", "Andrea Tundis", "Marc Winstel"], "pdf_url": "https://arxiv.org/pdf/2602.17490v1"}
{"id": "http://arxiv.org/abs/2602.17488v1", "title": "Computational Hardness of Private Coreset", "summary": "We study the problem of differentially private (DP) computation of coreset for the $k$-means objective. For a given input set of points, a coreset is another set of points such that the $k$-means objective for any candidate solution is preserved up to a multiplicative $(1 \\pm α)$ factor (and some additive factor).\n  We prove the first computational lower bounds for this problem. Specifically, assuming the existence of one-way functions, we show that no polynomial-time $(ε, 1/n^{ω(1)})$-DP algorithm can compute a coreset for $k$-means in the $\\ell_\\infty$-metric for some constant $α> 0$ (and some constant additive factor), even for $k=3$. For $k$-means in the Euclidean metric, we show a similar result but only for $α= Θ\\left(1/d^2\\right)$, where $d$ is the dimension.", "published": "2026-02-19T15:58:49Z", "updated": "2026-02-19T15:58:49Z", "authors": ["Badih Ghazi", "Cristóbal Guzmán", "Pritish Kamath", "Alexander Knop", "Ravi Kumar", "Pasin Manurangsi"], "pdf_url": "https://arxiv.org/pdf/2602.17488v1"}
{"id": "http://arxiv.org/abs/2602.17458v1", "title": "The CTI Echo Chamber: Fragmentation, Overlap, and Vendor Specificity in Twenty Years of Cyber Threat Reporting", "summary": "Despite the high volume of open-source Cyber Threat Intelligence (CTI), our understanding of long-term threat actor-victim dynamics remains fragmented due to the lack of structured datasets and inconsistent reporting standards. In this paper, we present a large-scale automated analysis of open-source CTI reports spanning two decades. We develop a high-precision, LLM-based pipeline to ingest and structure 13,308 reports, extracting key entities such as attributed threat actors, motivations, victims, reporting vendors, and technical indicators (IoCs and TTPs). Our analysis quantifies the evolution of CTI information density and specialization, characterizing patterns that relate specific threat actors to motivations and victim profiles. Furthermore, we perform a meta-analysis of the CTI industry itself. We identify a fragmented ecosystem of distinct silos where vendors demonstrate significant geographic and sectoral reporting biases. Our marginal coverage analysis reveals that intelligence overlap between vendors is typically low: while a few core providers may offer broad situational awareness, additional sources yield diminishing returns. Overall, our findings characterize the structural biases inherent in the CTI ecosystem, enabling practitioners and researchers to better evaluate the completeness of their intelligence sources.", "published": "2026-02-19T15:25:09Z", "updated": "2026-02-19T15:25:09Z", "authors": ["Manuel Suarez-Roman", "Francesco Marciori", "Mauro Conti", "Juan Tapiador"], "pdf_url": "https://arxiv.org/pdf/2602.17458v1"}
{"id": "http://arxiv.org/abs/2602.17454v1", "title": "Privacy in Theory, Bugs in Practice: Grey-Box Auditing of Differential Privacy Libraries", "summary": "Differential privacy (DP) implementations are notoriously prone to errors, with subtle bugs frequently invalidating theoretical guarantees. Existing verification methods are often impractical: formal tools are too restrictive, while black-box statistical auditing is intractable for complex pipelines and fails to pinpoint the source of the bug. This paper introduces Re:cord-play, a gray-box auditing paradigm that inspects the internal state of DP algorithms. By running an instrumented algorithm on neighboring datasets with identical randomness, Re:cord-play directly checks for data-dependent control flow and provides concrete falsification of sensitivity violations by comparing declared sensitivity against the empirically measured distance between internal inputs. We generalize this to Re:cord-play-sample, a full statistical audit that isolates and tests each component, including untrusted ones. We show that our novel testing approach is both effective and necessary by auditing 12 open-source libraries, including SmartNoise SDK, Opacus, and Diffprivlib, and uncovering 13 privacy violations that impact their theoretical guarantees. We release our framework as an open-source Python package, thereby making it easy for DP developers to integrate effective, computationally inexpensive, and seamless privacy testing as part of their software development lifecycle.", "published": "2026-02-19T15:18:00Z", "updated": "2026-02-19T15:18:00Z", "authors": ["Tudor Cebere", "David Erb", "Damien Desfontaines", "Aurélien Bellet", "Jack Fitzsimons"], "pdf_url": "https://arxiv.org/pdf/2602.17454v1"}
{"id": "http://arxiv.org/abs/2602.17452v1", "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge", "summary": "We present Jolt Atlas, a zero-knowledge machine learning (zkML) framework that extends the Jolt proving system to model inference. Unlike zkVMs (zero-knowledge virtual machines), which emulate CPU instruction execution, Jolt Atlas adapts Jolt's lookup-centric approach and applies it directly to ONNX tensor operations. The ONNX computational model eliminates the need for CPU registers and simplifies memory consistency verification. In addition, ONNX is an open-source, portable format, which makes it easy to share and deploy models across different frameworks, hardware platforms, and runtime environments without requiring framework-specific conversions.\n  Our lookup arguments, which use sumcheck protocol, are well-suited for non-linear functions -- key building blocks in modern ML. We apply optimisations such as neural teleportation to reduce the size of lookup tables while preserving model accuracy, as well as several tensor-level verification optimisations detailed in this paper. We demonstrate that Jolt Atlas can prove model inference in memory-constrained environments -- a prover property commonly referred to as \\textit{streaming}. Furthermore, we discuss how Jolt Atlas achieves zero-knowledge through the BlindFold technique, as introduced in Vega. In contrast to existing zkML frameworks, we show practical proving times for classification, embedding, automated reasoning, and small language models.\n  Jolt Atlas enables cryptographic verification that can be run on-device, without specialised hardware. The resulting proofs are succinctly verifiable. This makes Jolt Atlas well-suited for privacy-centric and adversarial environments. In a companion work, we outline various use cases of Jolt Atlas, including how it serves as guardrails in agentic commerce and for trustless AI context (often referred to as \\textit{AI memory}).", "published": "2026-02-19T15:17:18Z", "updated": "2026-02-19T15:17:18Z", "authors": ["Wyatt Benno", "Alberto Centelles", "Antoine Douchet", "Khalil Gibran"], "pdf_url": "https://arxiv.org/pdf/2602.17452v1"}
{"id": "http://arxiv.org/abs/2602.17413v1", "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing", "summary": "In current inter-organizational data spaces, usage policies are enforced mainly at the asset level: a whole document or dataset is either shared or withheld. When only parts of a document are sensitive, providers who want to avoid leaking protected information typically must manually redact documents before sharing them, which is costly, coarse-grained, and hard to maintain as policies or partners change. We present DAVE, a usage policy-enforcing LLM spokesperson that answers questions over private documents on behalf of a data provider. Instead of releasing documents, the provider exposes a natural language interface whose responses are constrained by machine-readable usage policies. We formalize policy-violating information disclosure in this setting, drawing on usage control and information flow security, and introduce virtual redaction: suppressing sensitive information at query time without modifying source documents. We describe an architecture for integrating such a spokesperson with Eclipse Dataspace Components and ODRL-style policies, and outline an initial provider-side integration prototype in which QA requests are routed through a spokesperson service instead of triggering raw document transfer. Our contribution is primarily architectural: we do not yet implement or empirically evaluate the full enforcement pipeline. We therefore outline an evaluation methodology to assess security, utility, and performance trade-offs under benign and adversarial querying as a basis for future empirical work on systematically governed LLM access to multi-party data spaces.", "published": "2026-02-19T14:43:48Z", "updated": "2026-02-19T14:43:48Z", "authors": ["René Brinkhege", "Prahlad Menon"], "pdf_url": "https://arxiv.org/pdf/2602.17413v1"}
{"id": "http://arxiv.org/abs/2505.16723v3", "title": "LLM Fingerprinting via Semantically Conditioned Watermarks", "summary": "Most LLM fingerprinting methods teach the model to respond to a few fixed queries with predefined atypical responses (keys). This memorization often does not survive common deployment steps such as finetuning or quantization, and such keys can be easily detected and filtered from LLM responses, ultimately breaking the fingerprint. To overcome these limitations we introduce LLM fingerprinting via semantically conditioned watermarks, replacing fixed query sets with a broad semantic domain, and replacing brittle atypical keys with a statistical watermarking signal diffused throughout each response. After teaching the model to watermark its responses only to prompts from a predetermined domain e.g., French language, the model owner can use queries from that domain to reliably detect the fingerprint and verify ownership. As we confirm in our thorough experimental evaluation, our fingerprint is both stealthy and robust to all common deployment scenarios.", "published": "2025-05-22T14:32:23Z", "updated": "2026-02-19T14:43:35Z", "authors": ["Thibaud Gloaguen", "Robin Staab", "Nikola Jovanović", "Martin Vechev"], "pdf_url": "https://arxiv.org/pdf/2505.16723v3"}
{"id": "http://arxiv.org/abs/2509.24368v2", "title": "Watermarking Diffusion Language Models", "summary": "We introduce the first watermark tailored for diffusion language models (DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in contrast to standard autoregressive language models (ARLMs) which generate tokens sequentially. While there has been much work in ARLM watermarking, a key challenge when attempting to apply these schemes directly to the DLM setting is that they rely on previously generated tokens, which are not always available with DLM generation. In this work we address this challenge by: (i) applying the watermark in expectation over the context even when some context tokens are yet to be determined, and (ii) promoting tokens which increase the watermark strength when used as context for other tokens. This is accomplished while keeping the watermark detector unchanged. Our experimental evaluation demonstrates that the DLM watermark leads to a >99% true positive rate with minimal quality impact and achieves similar robustness to existing ARLM watermarks, enabling for the first time reliable DLM watermarking.", "published": "2025-09-29T07:11:40Z", "updated": "2026-02-19T14:24:09Z", "authors": ["Thibaud Gloaguen", "Robin Staab", "Nikola Jovanović", "Martin Vechev"], "pdf_url": "https://arxiv.org/pdf/2509.24368v2"}
{"id": "http://arxiv.org/abs/2602.17345v1", "title": "What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?", "summary": "Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation.", "published": "2026-02-19T13:29:00Z", "updated": "2026-02-19T13:29:00Z", "authors": ["Boyang Ma", "Hechuan Guo", "Peizhuo Lv", "Minghui Xu", "Xuelong Dai", "YeChao Zhang", "Yijun Yang", "Yue Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.17345v1"}
{"id": "http://arxiv.org/abs/2602.17307v1", "title": "Security of the Fischlin Transform in Quantum Random Oracle Model", "summary": "The Fischlin transform yields non-interactive zero-knowledge proofs with straight-line extractability in the classical random oracle model. This is done by forcing a prover to generate multiple accepting transcripts through a proof-of-work mechanism. Whether the Fischlin transform is straight-line extractable against quantum adversaries has remained open due to the difficulty of reasoning about the likelihood of query transcripts in the quantum-accessible random oracle model (QROM), even when using the compressed oracle methodology. In this work, we prove that the Fischlin transform remains straight-line extractable in the QROM, via an extractor based on the compressed oracle. This establishes the post-quantum security of the Fischlin transform, providing a post-quantum straight-line extractable NIZK alternative to Pass' transform with smaller proof size. Our techniques include tail bounds for sums of independent random variables and for martingales as well as symmetrization, query amplitude and quantum union bound arguments.", "published": "2026-02-19T12:18:28Z", "updated": "2026-02-19T12:18:28Z", "authors": ["Christian Majenz", "Jaya Sharma"], "pdf_url": "https://arxiv.org/pdf/2602.17307v1"}
{"id": "http://arxiv.org/abs/2504.21730v2", "title": "Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises", "summary": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To address this issue, we propose a sample-specific certified backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. After that, Cert-SSB aggregates the predictions of multiple smoothed models to generate the final robust prediction. In particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. To conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample's certification region to improve certification performance. We conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. Our code is available at https://github.com/NcepuQiaoTing/Cert-SSB.", "published": "2025-04-30T15:21:25Z", "updated": "2026-02-19T12:16:56Z", "authors": ["Ting Qiao", "Yingjia Wang", "Xing Liu", "Sixing Wu", "Jianbin Li", "Yiming Li"], "pdf_url": "https://arxiv.org/pdf/2504.21730v2"}
{"id": "http://arxiv.org/abs/2602.17301v1", "title": "Grothendieck Topologies and Sheaf-Theoretic Foundations of Cryptographic Security: Attacker Models and $Σ$-Protocols as the First Step", "summary": "Cryptographic security is traditionally formulated using game-based or simulation-based definitions. In this paper, we propose a structural reformulation of cryptographic security based on Grothendieck topologies and sheaf theory.\n  Our key idea is to model attacker observations as a Grothendieck site, where covering families represent admissible decompositions of partial information determined by efficient simulation. Within this framework, protocol transcripts naturally form sheaves, and security properties arise as geometric conditions.\n  As a first step, we focus on $Σ$-protocols. We show that the transcript structure of any $Σ$-protocol defines a torsor in the associated topos of sheaves. Local triviality of this torsor corresponds to zero-knowledge, while the absence of global sections reflects soundness. A concrete analysis of the Schnorr $Σ$-protocol is provided to illustrate the construction.\n  This sheaf-theoretic perspective offers a conceptual explanation of simulation-based security and suggests a geometric foundation for further cryptographic abstractions.", "published": "2026-02-19T12:11:35Z", "updated": "2026-02-19T12:11:35Z", "authors": ["Takao Inoué"], "pdf_url": "https://arxiv.org/pdf/2602.17301v1"}
{"id": "http://arxiv.org/abs/2602.17223v1", "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs", "summary": "As large language models (LLMs) continue to grow in size, fewer users are able to host and run models locally. This has led to increased use of third-party hosting services. However, in this setting, there is a lack of guarantees on the computation performed by the inference provider. For example, a dishonest provider may replace an expensive large model with a cheaper-to-run weaker model and return the results from the weaker model to the user. Existing tools to verify inference typically rely on methods from cryptography such as zero-knowledge proofs (ZKPs), but these add significant computational overhead, and remain infeasible for use for large models. In this work, we develop a new insight -- that given a method for performing private LLM inference, one can obtain forms of verified inference at marginal extra cost. Specifically, we propose two new protocols which leverage privacy-preserving LLM inference in order to provide guarantees over the inference that was carried out. Our approaches are cheap, requiring the addition of a few extra tokens of computation, and have little to no downstream impact. As the fastest privacy-preserving inference methods are typically faster than ZK methods, the proposed protocols also improve verification runtime. Our work provides novel insights into the connections between privacy and verifiability in LLM inference.", "published": "2026-02-19T10:15:51Z", "updated": "2026-02-19T10:15:51Z", "authors": ["Arka Pal", "Louai Zahran", "William Gvozdjak", "Akilesh Potti", "Micah Goldblum"], "pdf_url": "https://arxiv.org/pdf/2602.17223v1"}
{"id": "http://arxiv.org/abs/2602.06530v2", "title": "Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance", "summary": "The rapid advancement of AI-Generated Content (AIGC) technologies poses significant challenges for authenticity assessment. However, existing evaluation protocols largely overlook anti-forensics attack, failing to ensure the comprehensive robustness of state-of-the-art AIGC detectors in real-world applications. To bridge this gap, we propose ForgeryEraser, a framework designed to execute universal anti-forensics attack without access to the target AIGC detectors. We reveal an adversarial vulnerability stemming from the systemic reliance on Vision-Language Models (VLMs) as shared backbones (e.g., CLIP), where downstream AIGC detectors inherit the feature space of these publicly accessible models. Instead of traditional logit-based optimization, we design a multi-modal guidance loss to drive forged image embeddings within the VLM feature space toward text-derived authentic anchors to erase forgery traces, while repelling them from forgery anchors. Extensive experiments demonstrate that ForgeryEraser causes substantial performance degradation to advanced AIGC detectors on both global synthesis and local editing benchmarks. Moreover, ForgeryEraser induces explainable forensic models to generate explanations consistent with authentic images for forged images. Our code will be made publicly available.", "published": "2026-02-06T09:32:10Z", "updated": "2026-02-19T08:33:25Z", "authors": ["Haipeng Li", "Rongxuan Peng", "Anwei Luo", "Shunquan Tan", "Changsheng Chen", "Anastasia Antsiferova"], "pdf_url": "https://arxiv.org/pdf/2602.06530v2"}
{"id": "http://arxiv.org/abs/2601.21318v2", "title": "QCL-IDS: Quantum Continual Learning for Intrusion Detection with Fidelity-Anchored Stability and Generative Replay", "summary": "Continual intrusion detection must absorb newly emerging attack stages while retaining legacy detection capability under strict operational constraints, including bounded compute and qubit budgets and privacy rules that preclude long-term storage of raw telemetry. We propose QCL-IDS, a quantum-centric continual-learning framework that co-designs stability and privacy-governed rehearsal for NISQ-era pipelines. Its core component, Q-FISH (Quantum Fisher Anchors), enforces retention using a compact anchor coreset through (i) sensitivity-weighted parameter constraints and (ii) a fidelity-based functional anchoring term that directly limits decision drift on representative historical traffic. To regain plasticity without retaining sensitive flows, QCL-IDS further introduces privacy-preserved quantum generative replay (QGR) via frozen, task-conditioned generator snapshots that synthesize bounded rehearsal samples. Across a three-stage attack stream on UNSW-NB15 and CICIDS2017, QCL-IDS consistently attains the best retention-adaptation trade-off: the gradient-anchor configuration achieves mean Attack-F1 = 0.941 with forgetting = 0.005 on UNSW-NB15 and mean Attack-F1 = 0.944 with forgetting = 0.004 on CICIDS2017, versus 0.800/0.138 and 0.803/0.128 for sequential fine-tuning, respectively.", "published": "2026-01-29T06:27:05Z", "updated": "2026-02-19T07:21:23Z", "authors": ["Zirui Zhu", "Xiangyang Li"], "pdf_url": "https://arxiv.org/pdf/2601.21318v2"}
{"id": "http://arxiv.org/abs/2510.00167v2", "title": "Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI", "summary": "Autonomous drones must often respond to sudden events, such as alarms, faults, or unexpected changes in their environment, that require immediate and adaptive decision-making. Traditional approaches rely on safety engineers hand-coding large sets of recovery rules, but this strategy cannot anticipate the vast range of real-world contingencies and quickly becomes incomplete. Recent advances in embodied AI, powered by large visual language models, provide commonsense reasoning to assess context and generate appropriate actions in real time. We demonstrate this capability in a simulated urban benchmark in the Unreal Engine, where drones dynamically interpret their surroundings and decide on sudden maneuvers for safe landings. Our results show that embodied AI makes possible a new class of adaptive recovery and decision-making pipelines that were previously infeasible to design by hand, advancing resilience and safety in autonomous aerial systems.", "published": "2025-09-30T18:39:36Z", "updated": "2026-02-19T05:56:49Z", "authors": ["Diego Ortiz Barbosa", "Mohit Agrawal", "Yash Malegaonkar", "Luis Burbano", "Axel Andersson", "György Dán", "Henrik Sandberg", "Alvaro A. Cardenas"], "pdf_url": "https://arxiv.org/pdf/2510.00167v2"}
{"id": "http://arxiv.org/abs/2602.16708v2", "title": "Policy Compiler for Secure Agentic Systems", "summary": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.\n  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.", "published": "2026-02-18T18:57:12Z", "updated": "2026-02-19T03:40:05Z", "authors": ["Nils Palumbo", "Sarthak Choudhary", "Jihye Choi", "Prasad Chalasani", "Somesh Jha"], "pdf_url": "https://arxiv.org/pdf/2602.16708v2"}
{"id": "http://arxiv.org/abs/2411.06317v4", "title": "Harpocrates: A Statically Typed Privacy Conscious Programming Framework", "summary": "In this paper, we introduce Harpocrates, a compiler plugin and a framework pair for Scala that binds the privacy policies to the data during data creation in form of oblivious membranes. Harpocrates eliminates raw data for a policy protected type from the application, ensuring it can only exist in protected form and centralizes the policy checking to the policy declaration site, making the privacy logic easy to maintain and verify. Instead of approaching privacy from an information flow verification perspective, Harpocrates allow the data to flow freely throughout the application, inside the policy membranes but enforces the policies when the data is tried to be accessed, mutated, declassified or passed through the application boundary. The centralization of the policies allow the maintainers to change the enforced logic simply by updating a single function while keeping the rest of the application oblivious to the change. Especially in a setting where the data definition is shared by multiple applications, the publisher can update the policies without requiring the dependent applications to make any changes beyond updating the dependency version.", "published": "2024-11-10T00:28:58Z", "updated": "2026-02-19T03:38:01Z", "authors": ["Sinan Pehlivanoglu", "Malte Schwarzkopf"], "pdf_url": "https://arxiv.org/pdf/2411.06317v4"}
{"id": "http://arxiv.org/abs/2602.16980v1", "title": "Discovering Universal Activation Directions for PII Leakage in Language Models", "summary": "Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.", "published": "2026-02-19T00:39:12Z", "updated": "2026-02-19T00:39:12Z", "authors": ["Leo Marchyok", "Zachary Coalson", "Sungho Keum", "Sooel Son", "Sanghyun Hong"], "pdf_url": "https://arxiv.org/pdf/2602.16980v1"}
{"id": "http://arxiv.org/abs/2602.16977v1", "title": "Fail-Closed Alignment for Large Language Models", "summary": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.", "published": "2026-02-19T00:33:35Z", "updated": "2026-02-19T00:33:35Z", "authors": ["Zachary Coalson", "Beth Sohler", "Aiden Gabriel", "Sanghyun Hong"], "pdf_url": "https://arxiv.org/pdf/2602.16977v1"}
