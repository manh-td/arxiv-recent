{"id": "http://arxiv.org/abs/2404.07831v4", "title": "Protected QR Code-based Anti-counterfeit System for Pharmaceutical Manufacturing", "summary": "The pharmaceutical manufacturing faces critical challenges due to the global threat of counterfeit drugs. This paper proposes a new approach of protected QR codes to secure unique product information for safeguarding the pharmaceutical supply chain. The proposed solution integrates secure QR code generation and encrypted data transmission to establish a comprehensive anti-counterfeit ecosystem. The protected QR codes encapsulate product information that cannot be identified using traditional QR code scanners which protect the information against replication and tampering. The system is developed with scalability in mind, which can be easily implemented without introducing any additional modification in the traditional supply chain.", "published": "2024-04-11T15:16:26Z", "updated": "2026-02-22T20:05:54Z", "authors": ["Md Masruk Aulia", "Nitol Saha", "Md. Mostafizur Rahman"], "pdf_url": "https://arxiv.org/pdf/2404.07831v4"}
{"id": "http://arxiv.org/abs/2602.19319v1", "title": "Health+: Empowering Individuals via Unifying Health Data", "summary": "Managing personal health data is a challenge in today's fragmented and institution-centric healthcare ecosystem. Individuals often lack meaningful control over their medical records, which are scattered across incompatible systems and formats. This vision paper presents Health+, a user-centric, multimodal health data management system that empowers individuals (including those with limited technical expertise) to upload, query, and share their data across modalities (e.g., text, images, reports). Rather than aiming for institutional overhaul, Health+ emphasizes individual agency by providing intuitive interfaces and intelligent recommendations for data access and sharing. At the system level, it tackles the complexity of storing, integrating, and securing heterogeneous health records, ensuring both efficiency and privacy. By unifying multimodal data and prioritizing patients, Health+ lays the foundation for a more connected, interpretable, and user-controlled health information ecosystem.", "published": "2026-02-22T19:48:57Z", "updated": "2026-02-22T19:48:57Z", "authors": ["Sujaya Maiyya", "Shantanu Sharma", "Avinash Kumar"], "pdf_url": "https://arxiv.org/pdf/2602.19319v1"}
{"id": "http://arxiv.org/abs/2602.14281v2", "title": "MCPShield: A Security Cognition Layer for Adaptive Trust Calibration in Model Context Protocol Agents", "summary": "The Model Context Protocol (MCP) standardizes tool use for LLM-based agents and enable third-party servers. This openness introduces a security misalignment: agents implicitly trust tools exposed by potentially untrusted MCP servers. However, despite its excellent utility, existing agents typically offer limited validation for third-party MCP servers. As a result, agents remain vulnerable to MCP-based attacks that exploit the misalignment between agents and servers throughout the tool invocation lifecycle. In this paper, we propose MCPShield as a plug-in security cognition layer that mitigates this misalignment and ensures agent security when invoking MCP-based tools. Drawing inspiration from human experience-driven tool validation, MCPShield assists agent forms security cognition with metadata-guided probing before invocation. Our method constrains execution within controlled boundaries while cognizing runtime events, and subsequently updates security cognition by reasoning over historical traces after invocation, building on human post-use reflection on tool behavior. Experiments demonstrate that MCPShield exhibits strong generalization in defending against six novel MCP-based attack scenarios across six widely used agentic LLMs, while avoiding false positives on benign servers and incurring low deployment overhead. Overall, our work provides a practical and robust security safeguard for MCP-based tool invocation in open agent ecosystems.", "published": "2026-02-15T19:10:00Z", "updated": "2026-02-22T18:19:38Z", "authors": ["Zhenhong Zhou", "Yuanhe Zhang", "Hongwei Cai", "Moayad Aloqaily", "Ouns Bouachir", "Linsey Pang", "Prakhar Mehrotra", "Kun Wang", "Qingsong Wen"], "pdf_url": "https://arxiv.org/pdf/2602.14281v2"}
{"id": "http://arxiv.org/abs/2602.19275v1", "title": "KUDA: Knowledge Unlearning by Deviating Representation for Large Language Models", "summary": "Large language models (LLMs) acquire a large amount of knowledge through pre-training on vast and diverse corpora. While this endows LLMs with strong capabilities in generation and reasoning, it amplifies risks associated with sensitive, copyrighted, or harmful content in training data.LLM unlearning, which aims to remove specific knowledge encoded within models, is a promising technique to reduce these risks. However, existing LLM unlearning methods often force LLMs to generate random or incoherent answers due to their inability to alter the encoded knowledge precisely. To achieve effective unlearning at the knowledge level of LLMs, we propose Knowledge Unlearning by Deviating representAtion (KUDA). We first utilize causal tracing to locate specific layers for target knowledge storage. We then design a new unlearning objective that induces the model's representations to deviate from its original position in the phase of knowledge removal, thus disrupting the ability to associate with the target knowledge. To resolve the optimization conflicts between forgetting and retention, we employ a relaxation null-space projection mechanism to mitigate the disruption to the representation space of retaining knowledge. Extensive experiments on representative benchmarks, WMDP and MUSE, demonstrate that KUDA outperforms most existing baselines by effectively balancing knowledge removal and model utility retention.", "published": "2026-02-22T17:16:49Z", "updated": "2026-02-22T17:16:49Z", "authors": ["Ce Fang", "Zhikun Zhang", "Min Chen", "Qing Liu", "Lu Zhou", "Zhe Liu", "Yunjun Gao"], "pdf_url": "https://arxiv.org/pdf/2602.19275v1"}
{"id": "http://arxiv.org/abs/2602.19270v1", "title": "Hagenberg Risk Management Process (Part 2): From Context-Sensitive Triage to Case Analysis With Bowtie and Bayesian Networks", "summary": "Risk matrices (heatmaps) are widely used for information and cyber risk management and decision-making, yet they are often too coarse for today's resilience-driven organizational and system landscapes. Likelihood and impact (the two dimensions represented in a heatmap) can vary with operational conditions, third-party dependencies, and the effectiveness of technical and organizational controls. At the same time, organizations cannot afford to analyze and operationalize every identified risk with equal depth using more sophisticated methods, telemetry, and real-time decision logic. We therefore propose a traceable triage pipeline that connects broad, context-sensitive screening with selective deep-dive analysis of material risks.\n  The Hagenberg Risk Management Process presented in this paper integrates three steps: (i) context-aware prioritization using multidimensional polar heatmaps to compare risks across multiple operational states, (ii) Bowtie analysis for triaged risks to structure causes, consequences, and barriers, and (iii) an automated transformation of Bowties into directed acyclic graphs as the structural basis for Bayesian networks. A distinctive feature is the explicit representation of barriers as activation nodes in the resulting graph, making control points visible and preparing for later intervention and what-if analyses. The approach is demonstrated on an instant-payments gateway scenario in which a faulty production change under peak load leads to cascading degradation and transaction loss; DORA serves as the reference framework for resilience requirements. The result is an end-to-end, tool-supported workflow that improves transparency, auditability, and operational readiness from prioritization to monitoring-oriented models.", "published": "2026-02-22T16:53:14Z", "updated": "2026-02-22T16:53:14Z", "authors": ["Eckehard Hermann", "Harald Lampesberger"], "pdf_url": "https://arxiv.org/pdf/2602.19270v1"}
{"id": "http://arxiv.org/abs/2505.13362v2", "title": "Dynamic Probabilistic Noise Injection for Membership Inference Defense", "summary": "Membership Inference Attacks (MIAs) expose privacy risks by determining whether a specific sample was part of a model's training set. These threats are especially serious in sensitive domains such as healthcare and finance. Traditional mitigation techniques, such as static differential privacy, rely on injecting a fixed amount of noise during training or inference. However, this often leads to a detrimental trade-off: the noise may be insufficient to counter sophisticated attacks or, when increased, can substantially degrade model accuracy. To address this limitation, we propose DynaNoise, an adaptive inference-time defense that modulates injected noise based on per-query sensitivity. DynaNoise estimates risk using measures such as Shannon entropy and scales the noise variance accordingly, followed by a smoothing step that re-normalizes the perturbed outputs to preserve predictive utility. We further introduce MIDPUT (Membership Inference Defense Privacy-Utility Trade-off), a scalar metric that captures both privacy gains and accuracy retention. Our evaluation on several benchmark datasets demonstrates that DynaNoise substantially lowers attack success rates while maintaining competitive accuracy, achieving strong overall MIDPUT scores compared to state-of-the-art defenses.", "published": "2025-05-19T17:07:00Z", "updated": "2026-02-22T15:05:53Z", "authors": ["Javad Forough", "Hamed Haddadi"], "pdf_url": "https://arxiv.org/pdf/2505.13362v2"}
{"id": "http://arxiv.org/abs/2602.19199v1", "title": "Counted NFT Transfers", "summary": "Non-fungible tokens (NFTs) on Ethereum currently follow a binary mobility paradigm: ERC-721 enables unrestricted transfers, whereas SBTs (ERC-5192) prohibit transfers entirely. We identify a design gap in which no standard mechanism supports bounded transferability, where ownership mobility is allowed but limited to a finite number of programmable transfers. We study counted NFT transfers and introduce ERC-7634 as a minimal realization compatible with ERC-721. The design augments each token with a transfer counter and configurable cap L, allowing ownership to evolve under a finite transfer budget. ERC-7634 defines a minimal extension interface with three lightweight functions (transferCountOf, setTransferLimit, and transferLimitOf), two events, and native-transfer hooks, requiring fewer than 60 additional lines of Solidity while preserving full backward compatibility with existing NFT infrastructure.\n  We analyze behavioral and economic consequences of counted transfers. Our results reveal (i) a mobility premium induced by remaining transfer capacity, (ii) a protocol-level costing signal that can deter wash trading in cap-aware markets through irreversible budget consumption, (iii) bounded recursive collateralization enabled by limited ownership turnover, and (iv) associated security and gas-cost implications, including wrapper-bypass trade-offs.\n  Evaluation on calibrated simulations shows that moderate limits (e.g., L = 10) affect fewer than 15% of tokens under representative transfer distributions, while repeated manipulation becomes unprofitable after a few cycles in a cap-aware pricing model; the additional gas overhead remains below 11% per transfer. We further position ERC-7634 within the NFT mobility design space, derive practical cap-selection guidelines, and discuss post-cap ownership outcomes including soulbound conversion, auto-burn, and provenance freeze.", "published": "2026-02-22T14:00:54Z", "updated": "2026-02-22T14:00:54Z", "authors": ["Qin Wang", "Minfeng Qi", "Guangsheng Yu", "Shiping Chen"], "pdf_url": "https://arxiv.org/pdf/2602.19199v1"}
{"id": "http://arxiv.org/abs/2506.16224v3", "title": "Malware Classification Leveraging NLP & Machine Learning for Enhanced Accuracy", "summary": "This paper investigates the application of natural language processing (NLP)-based n-gram analysis and machine learning techniques to enhance malware classification. We explore how NLP can be used to extract and analyze textual features from malware samples through n-grams, contiguous string or API call sequences. This approach effectively captures distinctive linguistic patterns among malware and benign families, enabling finer-grained classification. We delve into n-gram size selection, feature representation, and classification algorithms. While evaluating our proposed method on real-world malware samples, we observe significantly improved accuracy compared to the traditional methods. By implementing our n-gram approach, we achieved an accuracy of 99.02% across various machine learning algorithms by using hybrid feature selection technique to address high dimensionality. Hybrid feature selection technique reduces the feature set to only 1.6% of the original features.", "published": "2025-06-19T11:32:53Z", "updated": "2026-02-22T13:51:02Z", "authors": ["Bishwajit Prasad Gond", " Rajneekant", "Pushkar Kishore", "Durga Prasad Mohapatra"], "pdf_url": "https://arxiv.org/pdf/2506.16224v3"}
{"id": "http://arxiv.org/abs/2602.19149v1", "title": "ReVision : A Post-Hoc, Vision-Based Technique for Replacing Unacceptable Concepts in Image Generation Pipeline", "summary": "Image-generative models are widely deployed across industries. Recent studies show that they can be exploited to produce policy-violating content. Existing mitigation strategies primarily operate at the pre- or mid-generation stages through techniques such as prompt filtering and safety-aware training/fine-tuning. Prior work shows that these approaches can be bypassed and often degrade generative quality. In this work, we propose ReVision, a training-free, prompt-based, post-hoc safety framework for image-generation pipeline. ReVision acts as a last-line defense by analyzing generated images and selectively editing unsafe concepts without altering the underlying generator. It uses the Gemini-2.5-Flash model as a generic policy-violating concept detector, avoiding reliance on multiple category-specific detectors, and performs localized semantic editing to replace unsafe content. Prior post-hoc editing methods often rely on imprecise spatial localization, that undermines usability and limits deployability, particularly in multi-concept scenes. To address this limitation, ReVision introduces a VLM-assisted spatial gating mechanism that enforces instance-consistent localization, enabling precise edits while preserving scene integrity. We evaluate ReVision on a 245-image benchmark covering both single- and multi-concept scenarios. Results show that ReVision (i) improves CLIP-based alignment toward safe prompts by +$0.121$ on average; (ii) significantly improves multi-concept background fidelity (LPIPS $0.166 \\rightarrow 0.058$); (iii) achieves near-complete suppression on category-specific detectors (e.g., NudeNet $70.51 \\rightarrow 0$); and (iv) reduces policy-violating content recognizability in a human moderation study from $95.99\\%$ to $10.16\\%$.", "published": "2026-02-22T12:30:01Z", "updated": "2026-02-22T12:30:01Z", "authors": ["Gurjot Singh", "Prabhjot Singh", "Aashima Sharma", "Maninder Singh", "Ryan Ko"], "pdf_url": "https://arxiv.org/pdf/2602.19149v1"}
{"id": "http://arxiv.org/abs/2602.19097v1", "title": "SiGRRW: A Single-Watermark Robust Reversible Watermarking Framework with Guiding Strategy", "summary": "Robust reversible watermarking (RRW) enables copyright protection for images while overcoming the limitation of distortion introduced by watermark itself. Current RRW schemes typically employ a two-stage framework, which fails to achieve simultaneous robustness and reversibility within a single watermarking, and functional interference between the two watermarks results in performance degradation in multiple terms such as capacity and imperceptibility. We propose SiGRRW, a single-watermark RRW framework, which is applicable to both generative models and natural images. We introduce a novel guiding strategy to generate guiding images, serving as the guidance for embedding and recovery. The watermark is reversibly embedded with the guiding residual, which can be calculated from both cover images and watermark images. The proposed framework can be deployed either as a plug-and-play watermarking layer at the output stage of generative models, or directly applied to natural images. Extensive experiments demonstrate that SiGRRW effectively enhances imperceptibility and robustness compared to existing RRW schemes while maintaining lossless recovery of cover images, with significantly higher capacity than conventional schemes.", "published": "2026-02-22T08:48:49Z", "updated": "2026-02-22T08:48:49Z", "authors": ["Zikai Xu", "Bin Liu", "Weihai Li", "Lijunxian Zhang", "Nenghai Yu"], "pdf_url": "https://arxiv.org/pdf/2602.19097v1"}
{"id": "http://arxiv.org/abs/2602.19087v1", "title": "Detecting Cybersecurity Threats by Integrating Explainable AI with SHAP Interpretability and Strategic Data Sampling", "summary": "The critical need for transparent and trustworthy machine learning in cybersecurity operations drives the development of this integrated Explainable AI (XAI) framework. Our methodology addresses three fundamental challenges in deploying AI for threat detection: handling massive datasets through Strategic Sampling Methodology that preserves class distributions while enabling efficient model development; ensuring experimental rigor via Automated Data Leakage Prevention that systematically identifies and removes contaminated features; and providing operational transparency through Integrated XAI Implementation using SHAP analysis for model-agnostic interpretability across algorithms. Applied to the CIC-IDS2017 dataset, our approach maintains detection efficacy while reducing computational overhead and delivering actionable explanations for security analysts. The framework demonstrates that explainability, computational efficiency, and experimental integrity can be simultaneously achieved, providing a robust foundation for deploying trustworthy AI systems in security operations centers where decision transparency is paramount.", "published": "2026-02-22T08:01:14Z", "updated": "2026-02-22T08:01:14Z", "authors": ["Norrakith Srisumrith", "Sunantha Sodsee"], "pdf_url": "https://arxiv.org/pdf/2602.19087v1"}
{"id": "http://arxiv.org/abs/2602.19025v1", "title": "Routing-Aware Explanations for Mixture of Experts Graph Models in Malware Detection", "summary": "Mixture-of-Experts (MoE) offers flexible graph reasoning by combining multiple views of a graph through a learned router. We investigate routing-aware explanations for MoE graph models in malware detection using control flow graphs (CFGs). Our architecture builds diversity at two levels. At the node level, each layer computes multiple neighborhood statistics and fuses them with an MLP, guided by a degree reweighting factor rho and a pooling choice lambda in {mean, std, max}, producing distinct node representations that capture complementary structural cues in CFGs. At the readout level, six experts, each tied to a specific (rho, lambda) view, output graph-level logits that the router weights into a final prediction. Post-hoc explanations are generated with edge-level attributions per expert and aggregated using the router gates so the rationale reflects both what each expert highlights and how strongly it is selected. Evaluated against single-expert GNN baselines such as GCN, GIN, and GAT on the same CFG dataset, the proposed MoE achieves strong detection accuracy while yielding stable, faithful attributions under sparsity-based perturbations. The results indicate that making the router explicit and combining multi-statistic node encoding with expert-level diversity can improve the transparency of MoE decisions for malware analysis.", "published": "2026-02-22T03:27:17Z", "updated": "2026-02-22T03:27:17Z", "authors": ["Hossein Shokouhinejad", "Roozbeh Razavi-Far", "Griffin Higgins", "Ali. A Ghorbani"], "pdf_url": "https://arxiv.org/pdf/2602.19025v1"}
{"id": "http://arxiv.org/abs/2602.19021v1", "title": "LLM Scalability Risk for Agentic-AI and Model Supply Chain Security", "summary": "Large Language Models (LLMs) & Generative AI are transforming cybersecurity, enabling both advanced defenses and new attacks. Organizations now use LLMs for threat detection, code review, and DevSecOps automation, while adversaries leverage them to produce malwares and run targeted social-engineering campaigns. This paper presents a unified analysis integrating offensive and defensive perspectives on GenAI-driven cybersecurity. Drawing on 70 academic, industry, and policy sources, it analyzes the rise of AI-facilitated threats and its implications for global security to ground necessity for scalable defensive mechanisms. We introduce two primary contributions: the LLM Scalability Risk Index (LSRI), a parametric framework to stress-test operational risks when deploying LLMs in security-critical environments & a model-supply-chain framework establishing a verifiable root of trust throughout model lifecycle. We also synthesize defense strategies from platforms like Google Play Protect, Microsoft Security Copilot and outline a governance roadmap for secure, large-scale LLM deployment.", "published": "2026-02-22T03:20:49Z", "updated": "2026-02-22T03:20:49Z", "authors": ["Kiarash Ahi", "Vaibhav Agrawal", "Saeed Valizadeh"], "pdf_url": "https://arxiv.org/pdf/2602.19021v1"}
