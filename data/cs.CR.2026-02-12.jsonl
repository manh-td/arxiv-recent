{"id": "http://arxiv.org/abs/2602.12106v1", "title": "MedExChain: Enabling Secure and Efffcient PHR Sharing Across Heterogeneous Blockchains", "summary": "With the proliferation of intelligent healthcare systems, patients' Personal Health Records (PHR) generated by the Internet of Medical Things (IoMT) in real-time play a vital role in disease diagnosis. The integration of emerging blockchain technologies signiffcantly enhanced the data security inside intelligent medical systems. However, data sharing across different systems based on varied blockchain architectures is still constrained by the unsolved performance and security challenges. This paper constructs a cross-chain data sharing scheme, termed MedExChain, which aims to securely share PHR across heterogeneous blockchain systems. The MedExChain scheme ensures that PHR can be shared across chains even under the performance limitations of IoMT devices. Additionally, the scheme incorporates Cryptographic Reverse Firewall (CRF) and a blockchain audit mechanism to defend against both internal and external security threats. The robustness of our scheme is validated through BAN logic, Scyther tool, Chosen Plaintext Attack (CPA) and Algorithm Substitution Attack (ASA) security analysis veriffcation. Extensive evaluations demonstrate that MedExChain signiffcantly minimizes computation and communication overhead, making it suitable for IoMT devices and fostering the efffcient circulation of PHR across diverse blockchain systems.", "published": "2026-02-12T15:59:19Z", "updated": "2026-02-12T15:59:19Z", "authors": ["Yongyang Lv", "Xiaohong Li", "Kui Chen", "Zhe Hou", "Guangdong Bai", "Ruitao Feng"], "pdf_url": "https://arxiv.org/pdf/2602.12106v1"}
{"id": "http://arxiv.org/abs/2508.13220v3", "title": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols", "summary": "Large Language Models (LLMs) are increasingly integrated into real-world applications via the Model Context Protocol (MCP), a universal open standard for connecting AI agents with data sources and external tools. While MCP enhances the capabilities of LLM-based agents, it also introduces new security risks and significantly expands their attack surface. In this paper, we present the first formalization of a secure MCP and its required specifications. Based on this foundation, we establish a comprehensive MCP security taxonomy that extends existing models by incorporating protocol-level and host-side threats, identifying 17 distinct attack types across four primary attack surfaces. Building on these specifications, we introduce MCPSecBench, a systematic security benchmark and playground that integrates prompt datasets, MCP servers, MCP clients, attack scripts, a GUI test harness, and protection mechanisms to evaluate these threats across three major MCP platforms. MCPSecBench is designed to be modular and extensible, allowing researchers to incorporate custom implementations of clients, servers, and transport protocols for rigorous assessment. Our evaluation across three major MCP platforms reveals that all attack surfaces yield successful compromises. Core vulnerabilities universally affect Claude, OpenAI, and Cursor, while server-side and specific client-side attacks exhibit considerable variability across different hosts and models. Furthermore, current protection mechanisms proved largely ineffective, achieving an average success rate of less than 30%. Overall, MCPSecBench standardizes the evaluation of MCP security and enables rigorous testing across all protocol layers.", "published": "2025-08-17T11:49:16Z", "updated": "2026-02-12T15:51:08Z", "authors": ["Yixuan Yang", "Cuifeng Gao", "Daoyuan Wu", "Yufan Chen", "Yingjiu Li", "Shuai Wang"], "pdf_url": "https://arxiv.org/pdf/2508.13220v3"}
{"id": "http://arxiv.org/abs/2602.12092v1", "title": "DeepSight: An All-in-One LM Safety Toolkit", "summary": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.", "published": "2026-02-12T15:43:14Z", "updated": "2026-02-12T15:43:14Z", "authors": ["Bo Zhang", "Jiaxuan Guo", "Lijun Li", "Dongrui Liu", "Sujin Chen", "Guanxu Chen", "Zhijie Zheng", "Qihao Lin", "Lewen Yan", "Chen Qian", "Yijin Zhou", "Yuyao Wu", "Shaoxiong Guo", "Tianyi Du", "Jingyi Yang", "Xuhao Hu", "Ziqi Miao", "Xiaoya Lu", "Jing Shao", "Xia Hu"], "pdf_url": "https://arxiv.org/pdf/2602.12092v1"}
{"id": "http://arxiv.org/abs/2506.06119v2", "title": "SATversary: Adversarial Attacks and Defenses for Satellite Fingerprinting", "summary": "Due to the increasing threat of attacks on satellite systems, novel countermeasures have been developed to provide additional security. Among these, there has been a particular interest in transmitter fingerprinting, which authenticates transmitters by looking at characteristics expressed in the physical layer signal. These systems rely heavily upon statistical methods and machine learning, and are therefore vulnerable to a range of attacks. The severity of this threat in a fingerprinting context is currently not well understood.\n  In this paper we evaluate a range of attacks against satellite fingerprinting, building on previous works by looking at attacks optimized to target the fingerprinting system for maximal impact. We design optimized jamming, dataset poisoning, and spoofing attacks, evaluating them in the real world against the SatIQ fingerprinting system designed to authenticate Iridium transmitters, and using a wireless channel emulator to achieve realistic channel conditions. We show that an optimized jamming signal can cause a 50% error rate with attacker-to-victim ratios as low as -30dB (far less power than traditional jamming techniques), and demonstrate successful spoofing attacks, with an attacker successfully removing their own transmitter's fingerprint from messages. We also present a viable dataset poisoning attack, enabling persistent message spoofing by altering stored data to include the fingerprint of the attacker's transmitter.\n  Finally, we show that a model trained to optimize spoofing attacks can also be used to detect spoofing and replay attacks, even when it has never seen the attacker's transmitter before. This technique works even when the training dataset includes only a single transmitter, enabling fingerprinting to be used to protect small constellations and even individual satellites, providing additional protection where it is needed the most.", "published": "2025-06-06T14:27:19Z", "updated": "2026-02-12T15:36:52Z", "authors": ["Joshua Smailes", "Sebastian Köhler", "Simon Birnbach", "Martin Strohmeier", "Ivan Martinovic"], "pdf_url": "https://arxiv.org/pdf/2506.06119v2"}
{"id": "http://arxiv.org/abs/2602.12059v1", "title": "Evaluation of Security-Induced Latency on 5G RAN Interfaces and User Plane Communication", "summary": "5G promises enhanced performance-not only in bandwidth and capacity, but also latency and security. Its ultra-reliable low-latency configuration targets round-trip times below 1 ms, while optional security controls extend protection across all interfaces, making 5G attractive for mission-critical applications. A key enabler of low latency is the disaggregation of network components, including the RAN, allowing user-plane functions to be deployed nearer to end users. However, this split introduces additional interfaces, whose protection increases latency overhead. In this paper, guided by discussions with a network operator and a 5G manufacturer, we evaluate the latency overhead of enabling optional 5G security controls across internal RAN interfaces and the 5G user plane. To this end, we deploy the first testbed implementing a disaggregated RAN with standardized optional security mechanisms. Our results show that disaggregated RAN deployments retain a latency advantage over monolithic designs, even with security enabled. However, achieving sub-1 ms round-trip times remains challenging, as cryptographic overhead alone can already exceed this target.", "published": "2026-02-12T15:20:10Z", "updated": "2026-02-12T15:20:10Z", "authors": ["Sotiris Michaelides", "Jakub Lapawa", "Daniel Eguiguren Chavez", "Martin Henze"], "pdf_url": "https://arxiv.org/pdf/2602.12059v1"}
{"id": "http://arxiv.org/abs/2602.11954v1", "title": "PAC to the Future: Zero-Knowledge Proofs of PAC Private Systems", "summary": "Privacy concerns in machine learning systems have grown significantly with the increasing reliance on sensitive user data for training large-scale models. This paper introduces a novel framework combining Probably Approximately Correct (PAC) Privacy with zero-knowledge proofs (ZKPs) to provide verifiable privacy guarantees in trustless computing environments. Our approach addresses the limitations of traditional privacy-preserving techniques by enabling users to verify both the correctness of computations and the proper application of privacy-preserving noise, particularly in cloud-based systems. We leverage non-interactive ZKP schemes to generate proofs that attest to the correct implementation of PAC privacy mechanisms while maintaining the confidentiality of proprietary systems. Our results demonstrate the feasibility of achieving verifiable PAC privacy in outsourced computation, offering a practical solution for maintaining trust in privacy-preserving machine learning and database systems while ensuring computational integrity.", "published": "2026-02-12T13:49:22Z", "updated": "2026-02-12T13:49:22Z", "authors": ["Guilhem Repetto", "Nojan Sheybani", "Gabrielle De Micheli", "Farinaz Koushanfar"], "pdf_url": "https://arxiv.org/pdf/2602.11954v1"}
{"id": "http://arxiv.org/abs/2602.11897v1", "title": "Agentic AI for Cybersecurity: A Meta-Cognitive Architecture for Governable Autonomy", "summary": "Contemporary AI-driven cybersecurity systems are predominantly architected as model-centric detection and automation pipelines optimized for task-level performance metrics such as accuracy and response latency. While effective for bounded classification tasks, these architectures struggle to support accountable decision-making under adversarial uncertainty, where actions must be justified, governed, and aligned with organizational and regulatory constraints. This paper argues that cybersecurity orchestration should be reconceptualized as an agentic, multi-agent cognitive system, rather than a linear sequence of detection and response components. We introduce a conceptual architectural framework in which heterogeneous AI agents responsible for detection, hypothesis formation, contextual interpretation, explanation, and governance are coordinated through an explicit meta-cognitive judgement function. This function governs decision readiness and dynamically calibrates system autonomy when evidence is incomplete, conflicting, or operationally risky. By synthesizing distributed cognition theory, multi-agent systems research, and responsible AI governance frameworks, we demonstrate that modern security operations already function as distributed cognitive systems, albeit without an explicit organizing principle. Our contribution is to make this cognitive structure architecturally explicit and governable by embedding meta-cognitive judgement as a first-class system function. We discuss implications for security operations centers, accountable autonomy, and the design of next-generation AI-enabled cyber defence architectures. The proposed framework shifts the focus of AI in cybersecurity from optimizing isolated predictions to governing autonomy under uncertainty.", "published": "2026-02-12T12:52:49Z", "updated": "2026-02-12T12:52:49Z", "authors": ["Andrei Kojukhov", "Arkady Bovshover"], "pdf_url": "https://arxiv.org/pdf/2602.11897v1"}
{"id": "http://arxiv.org/abs/2602.11887v1", "title": "Verifiable Provenance of Software Artifacts with Zero-Knowledge Compilation", "summary": "Verifying that a compiled binary originates from its claimed source code is a fundamental security requirement, called source code provenance. Achieving verifiable source code provenance in practice remains challenging. The most popular technique, called reproducible builds, requires difficult matching and reexecution of build toolchains and environments. We propose a novel approach to verifiable provenance based on compiling software with zero-knowledge virtual machines (zkVMs). By executing a compiler within a zkVM, our system produces both the compiled output and a cryptographic proof attesting that the compilation was performed on the claimed source code with the claimed compiler. We implement a proof-of-concept implementation using the RISC Zero zkVM and the ChibiCC C compiler, and evaluate it on 200 synthetic programs as well as 31 OpenSSL and 21 libsodium source files. Our results show that zk-compilation is applicable to real-world software and provides strong security guarantees: all adversarial tests targeting compiler substitution, source tampering, output manipulation, and replay attacks are successfully blocked.", "published": "2026-02-12T12:36:36Z", "updated": "2026-02-12T12:36:36Z", "authors": ["Javier Ron", "Martin Monperrus"], "pdf_url": "https://arxiv.org/pdf/2602.11887v1"}
{"id": "http://arxiv.org/abs/2408.10963v5", "title": "KeySpace: Enhancing Public Key Infrastructure for Interplanetary Networks", "summary": "As the use of satellites continues to grow, new networking paradigms are emerging to support the scale and long distance communication inherent to these networks. In particular, interplanetary communication relays connect distant network segments together, but result in a sparsely connected network with long-distance links that are frequently interrupted. In this new context, traditional Public Key Infrastructure (PKI) becomes difficult to implement, due to the impossibility of low-latency queries to a central authority. This paper addresses the challenge of implementing PKI in these complex networks, identifying the essential goals and requirements.\n  Using these requirements, we develop the KeySpace framework, comprising a set of standardized experiments and metrics for comparing PKI systems across various network topologies, evaluating their performance and security. This enables the testing of different protocols and configurations in a standard, repeatable manner, so that improvements can be more fairly tested and clearly demonstrated. We use KeySpace to test two standard PKI protocols in use in terrestrial networks (OCSP and CRLs), demonstrating for the first time that both can be effectively utilized even in interplanetary networks with high latency and frequent interruptions, provided authority is properly distributed throughout the network. Finally, we propose and evaluate a number of novel techniques extending standard OCSP to improve the overhead of connection establishment, reduce link congestion, and limit the reach of an attacker with a compromised key. Using KeySpace we validate these claims, demonstrating their improved performance over the state of the art.", "published": "2024-08-20T16:00:17Z", "updated": "2026-02-12T12:06:55Z", "authors": ["Joshua Smailes", "Filip Futera", "Sebastian Köhler", "Simon Birnbach", "Martin Strohmeier", "Ivan Martinovic"], "pdf_url": "https://arxiv.org/pdf/2408.10963v5"}
{"id": "http://arxiv.org/abs/2602.11851v1", "title": "Resource-Aware Deployment Optimization for Collaborative Intrusion Detection in Layered Networks", "summary": "Collaborative Intrusion Detection Systems (CIDS) are increasingly adopted to counter cyberattacks, as their collaborative nature enables them to adapt to diverse scenarios across heterogeneous environments. As distributed critical infrastructure operates in rapidly evolving environments, such as drones in both civil and military domains, there is a growing need for CIDS architectures that can flexibly accommodate these dynamic changes. In this study, we propose a novel CIDS framework designed for easy deployment across diverse distributed environments. The framework dynamically optimizes detector allocation per node based on available resources and data types, enabling rapid adaptation to new operational scenarios with minimal computational overhead. We first conducted a comprehensive literature review to identify key characteristics of existing CIDS architectures. Based on these insights and real-world use cases, we developed our CIDS framework, which we evaluated using several distributed datasets that feature different attack chains and network topologies. Notably, we introduce a public dataset based on a realistic cyberattack targeting a ground drone aimed at sabotaging critical infrastructure. Experimental results demonstrate that the proposed CIDS framework can achieve adaptive, efficient intrusion detection in distributed settings, automatically reconfiguring detectors to maintain an optimal configuration, without requiring heavy computation, since all experiments were conducted on edge devices.", "published": "2026-02-12T11:42:58Z", "updated": "2026-02-12T11:42:58Z", "authors": ["André García Gómez", "Ines Rieger", "Wolfgang Hotwagner", "Max Landauer", "Markus Wurzenberger", "Florian Skopik", "Edgar Weippl"], "pdf_url": "https://arxiv.org/pdf/2602.11851v1"}
{"id": "http://arxiv.org/abs/2602.11820v1", "title": "Solving the Post-Quantum Control Plane Bottleneck: Energy-Aware Cryptographic Scheduling in Open RAN", "summary": "The Open Radio Access Network (O-RAN) offers flexibility and innovation but introduces unique security vulnerabilities, particularly from cryptographically relevant quantum computers. While Post-Quantum Cryptography (PQC) is the primary scalable defence, its computationally intensive handshakes create a significant bottleneck for the RAN control plane, posing sustainability challenges. This paper proposes an energy-aware framework to solve this PQC bottleneck, ensuring quantum resilience without sacrificing operational energy efficiency. The system employs an O-RAN aligned split: a Crypto Policy rApp residing in the Non-Real-Time (Non-RT) RIC defines the strategic security envelope (including PQC suites), while a Security Operations Scheduling (SOS) xApp in the Near-RT RIC converts these into tactical timing and placement intents. Cryptographic enforcement remains at standards-compliant endpoints: the Open Fronthaul utilizes Media Access Control Security (MACsec) at the O-DU/O-RU, while the xhaul (midhaul and backhaul) utilizes IP Security (IPsec) at tunnel terminators. The SOS xApp reduces PQC overhead by batching non-urgent handshakes, prioritizing session resumption, and selecting parameters that meet slice SLAs while minimizing joules per secure connection. We evaluate the architecture via a Discrete-Event Simulation (DES) using 3GPP-aligned traffic profiles and verified hardware benchmarks from literature. Results show that intelligent scheduling can reduce per-handshake energy by approximately 60 percent without violating slice latency targets.", "published": "2026-02-12T10:58:54Z", "updated": "2026-02-12T10:58:54Z", "authors": ["Neha Gupta", "Hamed Alimohammadi", "Mohammad Shojafar", "De Mi", "Muhammad N. M. Bhutta"], "pdf_url": "https://arxiv.org/pdf/2602.11820v1"}
{"id": "http://arxiv.org/abs/2602.10915v2", "title": "Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System", "summary": "The evolution of Large Language Models (LLMs) has shifted mobile computing from App-centric interactions to system-level autonomous agents. Current implementations predominantly rely on a \"Screen-as-Interface\" paradigm, which inherits structural vulnerabilities and conflicts with the mobile ecosystem's economic foundations. In this paper, we conduct a systematic security analysis of state-of-the-art mobile agents using Doubao Mobile Assistant as a representative case. We decompose the threat landscape into four dimensions - Agent Identity, External Interface, Internal Reasoning, and Action Execution - revealing critical flaws such as fake App identity, visual spoofing, indirect prompt injection, and unauthorized privilege escalation stemming from a reliance on unstructured visual data.\n  To address these challenges, we propose Aura, an Agent Universal Runtime Architecture for a clean-slate secure agent OS. Aura replaces brittle GUI scraping with a structured, agent-native interaction model. It adopts a Hub-and-Spoke topology where a privileged System Agent orchestrates intent, sandboxed App Agents execute domain-specific tasks, and the Agent Kernel mediates all communication. The Agent Kernel enforces four defense pillars: (i) cryptographic identity binding via a Global Agent Registry; (ii) semantic input sanitization through a multilayer Semantic Firewall; (iii) cognitive integrity via taint-aware memory and plan-trajectory alignment; and (iv) granular access control with non-deniable auditing. Evaluation on MobileSafetyBench shows that, compared to Doubao, Aura improves low-risk Task Success Rate from roughly 75% to 94.3%, reduces high-risk Attack Success Rate from roughly 40% to 4.4%, and achieves near-order-of-magnitude latency gains. These results demonstrate Aura as a viable, secure alternative to the \"Screen-as-Interface\" paradigm.", "published": "2026-02-11T14:52:27Z", "updated": "2026-02-12T10:44:59Z", "authors": ["Zhenhua Zou", "Sheng Guo", "Qiuyang Zhan", "Lepeng Zhao", "Shuo Li", "Qi Li", "Ke Xu", "Mingwei Xu", "Zhuotao Liu"], "pdf_url": "https://arxiv.org/pdf/2602.10915v2"}
{"id": "http://arxiv.org/abs/2505.10297v3", "title": "Defending the Edge: Representative-Attention Defense against Backdoor Attacks in Federated Learning", "summary": "Federated learning (FL) remains highly vulnerable to adaptive backdoor attacks that preserve stealth by closely imitating benign update statistics. Existing defenses predominantly rely on anomaly detection in parameter or gradient space, overlooking behavioral constraints that backdoor attacks must satisfy to ensure reliable trigger activation. These anomaly-centric methods fail against adaptive attacks that normalize update magnitudes and mimic benign statistical patterns while preserving backdoor functionality, creating a fundamental detection gap. To address this limitation, this paper introduces FeRA (Federated Representative Attention) -- a novel attention-driven defense that shifts the detection paradigm from anomaly-centric to consistency-centric analysis. FeRA exploits the intrinsic need for backdoor persistence across training rounds, identifying malicious clients through suppressed representation-space variance, an orthogonal property to traditional magnitude-based statistics. The framework conducts multi-dimensional behavioral analysis combining spectral and spatial attention, directional alignment, mutual similarity, and norm inflation across two complementary detection mechanisms: consistency analysis and norm-inflation detection. Through this mechanism, FeRA isolates malicious clients that exhibit low-variance consistency or magnitude amplification. Extensive evaluation across six datasets, nine attacks, and three model architectures under both Independent and Identically Distributed (IID) and non-IID settings confirm FeRA achieves superior backdoor mitigation. Under different non-IID settings, FeRA achieved the lowest average Backdoor Accuracy (BA), about 1.67% while maintaining high clean accuracy compared to other state-of-the-art defenses. The code is available at https://github.com/Peatech/FeRA_defense.git.", "published": "2025-05-15T13:44:32Z", "updated": "2026-02-12T10:29:31Z", "authors": ["Chibueze Peace Obioma", "Youcheng Sun", "Mustafa A. Mustafa"], "pdf_url": "https://arxiv.org/pdf/2505.10297v3"}
{"id": "http://arxiv.org/abs/2602.11793v1", "title": "More Haste, Less Speed: Weaker Single-Layer Watermark Improves Distortion-Free Watermark Ensembles", "summary": "Watermarking has emerged as a crucial technique for detecting and attributing content generated by large language models. While recent advancements have utilized watermark ensembles to enhance robustness, prevailing methods typically prioritize maximizing the strength of the watermark at every individual layer. In this work, we identify a critical limitation in this \"stronger-is-better\" approach: strong watermarks significantly reduce the entropy of the token distribution, which paradoxically weakens the effectiveness of watermarking in subsequent layers. We theoretically and empirically show that detectability is bounded by entropy and that watermark ensembles induce a monotonic decrease in both entropy and the expected green-list ratio across layers. To address this inherent trade-off, we propose a general framework that utilizes weaker single-layer watermarks to preserve the entropy required for effective multi-layer ensembling. Empirical evaluations demonstrate that this counter-intuitive strategy mitigates signal decay and consistently outperforms strong baselines in both detectability and robustness.", "published": "2026-02-12T10:18:16Z", "updated": "2026-02-12T10:18:16Z", "authors": ["Ruibo Chen", "Yihan Wu", "Xuehao Cui", "Jingqi Zhang", "Heng Huang"], "pdf_url": "https://arxiv.org/pdf/2602.11793v1"}
{"id": "http://arxiv.org/abs/2602.11764v1", "title": "Reliable and Private Anonymous Routing for Satellite Constellations", "summary": "Shared, dynamic network infrastructures, such as dual-use LEO satellite constellations, pose critical threats to metadata privacy, particularly for state actors operating in mixed-trust environments. This work proposes an enhanced anonymity architecture, evolving the Loopix mix-network, to provide robust security and reliability in these volatile topologies. We introduce three primary contributions: (1) A multi-path transport protocol utilizing $(n, k)$ erasure codes, which is demonstrated to counteract the high link volatility and intermittent connectivity that renders standard mix-networks unreliable. (2) The integration of a computationally efficient Private Information Retrieval (PIR) protocol during route discovery. (3) The introduction of adaptive, centrality-based delay strategies that efficiently mitigate the inherent topological bias of LEO networks, providing a superior anonymity-to-latency trade-off. This mechanism provably prevents metadata leakage at the user-provider directory, mitigating profiling and correlation attacks. We validate this architecture via high-fidelity, packet-level simulations of a LEO constellation. Empirical results show our multi-path transport achieves near-zero message loss, establishing a quantifiable trade-off between reliability and bandwidth overhead. Furthermore, microbenchmarks of the PIR protocol quantify its computational and latency overheads, confirming its feasibility for practical deployment. This work provides a validated blueprint for deployable high-anonymity communication systems, demonstrating the viability of securely multiplexing sensitive operations within large-scale commercial network infrastructures.", "published": "2026-02-12T09:43:55Z", "updated": "2026-02-12T09:43:55Z", "authors": ["Nilesh Vyas", "Fabien Geyer", "Svetoslav Duhovnikov"], "pdf_url": "https://arxiv.org/pdf/2602.11764v1"}
{"id": "http://arxiv.org/abs/2509.01835v2", "title": "From CVE Entries to Verifiable Exploits: An Automated Multi-Agent Framework for Reproducing CVEs", "summary": "High-quality datasets of real-world vulnerabilities and their corresponding verifiable exploits are crucial resources in software security research. Yet such resources remain scarce, as their creation demands intensive manual effort and deep security expertise. In this paper, we present CVE-GENIE, an automated, large language model (LLM)-based multi-agent framework designed to reproduce real-world vulnerabilities, provided in Common Vulnerabilities and Exposures (CVE) format, to enable creation of high-quality vulnerability datasets. Given a CVE entry as input, CVE-GENIE gathers the relevant resources of the CVE, automatically reconstructs the vulnerable environment, and (re)produces a verifiable exploit. Our systematic evaluation highlights the efficiency and robustness of CVE-GENIE's design and successfully reproduces approximately 51% (428 of 841) CVEs published in 2024-2025, complete with their verifiable exploits, at an average cost of $2.77 per CVE. Our pipeline offers a robust method to generate reproducible CVE benchmarks, valuable for diverse applications such as fuzzer evaluation, vulnerability patching, and assessing AI's security capabilities.", "published": "2025-09-01T23:37:44Z", "updated": "2026-02-12T09:05:40Z", "authors": ["Saad Ullah", "Praneeth Balasubramanian", "Wenbo Guo", "Amanda Burnett", "Hammond Pearce", "Christopher Kruegel", "Giovanni Vigna", "Gianluca Stringhini"], "pdf_url": "https://arxiv.org/pdf/2509.01835v2"}
{"id": "http://arxiv.org/abs/2602.11655v1", "title": "LoRA-based Parameter-Efficient LLMs for Continuous Learning in Edge-based Malware Detection", "summary": "The proliferation of edge devices has created an urgent need for security solutions capable of detecting malware in real time while operating under strict computational and memory constraints. Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in recognizing complex patterns, yet their deployment on edge devices remains impractical due to their resource demands. However, in edge malware detection, static or centrally retrained models degrade under evolving threats and heterogeneous traffic; locally trained models become siloed and fail to transfer across domains. To overcome these limitations, in this paper, we present a continuous learning architecture for edge-based malware detection that combines local adaptation on each device with global knowledge sharing through parameter-efficient LoRA adapters. Lightweight transformer models (DistilBERT, DistilGPT-2, TinyT5) run on edge nodes and are incrementally fine-tuned on device-specific traffic; only the resulting LoRA modules are aggregated by a lightweight coordinator and redistributed, enabling cross-device generalization without exchanging raw data. We evaluate on two public IoT security datasets, Edge-IIoTset and TON-IoT, under multi-round learning to simulate evolving threats. Compared to isolated fine-tuning, the LoRA-based exchange yields up to 20-25% accuracy gains when models encounter previously unseen attacks from another domain, while maintaining stable loss and F1 across rounds. LoRA adds less than 1% to model size (~0.6-1.8 MB), making updates practical for constrained edge hardware.", "published": "2026-02-12T07:20:26Z", "updated": "2026-02-12T07:20:26Z", "authors": ["Christian Rondanini", "Barbara Carminati", "Elena Ferrari", "Niccolò Lardo", "Ashish Kundu"], "pdf_url": "https://arxiv.org/pdf/2602.11655v1"}
{"id": "http://arxiv.org/abs/2602.11651v1", "title": "DMind-3: A Sovereign Edge--Local--Cloud AI System with Controlled Deliberation and Correction-Based Tuning for Safe, Low-Latency Transaction Execution", "summary": "This paper introduces DMind-3, a sovereign Edge-Local-Cloud intelligence stack designed to secure irreversible financial execution in Web3 environments against adversarial risks and strict latency constraints. While existing cloud-centric assistants compromise privacy and fail under network congestion, and purely local solutions lack global ecosystem context, DMind-3 resolves these tensions by decomposing capability into three cooperating layers: a deterministic signing-time intent firewall at the edge, a private high-fidelity reasoning engine on user hardware, and a policy-governed global context synthesizer in the cloud. We propose policy-driven selective offloading to route computation based on privacy sensitivity and uncertainty, supported by two novel training objectives: Hierarchical Predictive Synthesis (HPS) for fusing time-varying macro signals, and Contrastive Chain-of-Correction Supervised Fine-Tuning (C$^3$-SFT) to enhance local verification reliability. Extensive evaluations demonstrate that DMind-3 achieves a 93.7% multi-turn success rate in protocol-constrained tasks and superior domain reasoning compared to general-purpose baselines, providing a scalable framework where safety is bound to the edge execution primitive while maintaining sovereignty over sensitive user intent.", "published": "2026-02-12T07:03:08Z", "updated": "2026-02-12T07:03:08Z", "authors": ["Enhao Huang", "Frank Li", "Tony Lin", "Lowes Yang"], "pdf_url": "https://arxiv.org/pdf/2602.11651v1"}
{"id": "http://arxiv.org/abs/2512.13628v3", "title": "Certified-Everlasting Quantum NIZK Proofs", "summary": "We study non-interactive zero-knowledge proofs (NIZKs) for NP satisfying: 1) statistical soundness, 2) computational zero-knowledge and 3) certified-everlasting zero-knowledge (CE-ZK). The CE-ZK property allows a verifier of a quantum proof to revoke the proof in a way that can be checked (certified) by the prover. Conditioned on successful certification, the verifier's state can be efficiently simulated with only the statement, in a statistically indistinguishable way. Our contributions regarding these certified-everlasting NIZKs (CE-NIZKs) are as follows:\n  - We identify a barrier to obtaining CE-NIZKs in the CRS model via generalizations of known interactive zero-knowledge proofs that satisfy CE-ZK.\n  - We circumvent this by constructing CE-NIZK from black-box use of NIZK for NP satisfying certain properties, along with OWFs. As a result, we obtain CE-NIZKs for NP in the CRS model, based on polynomial hardness of the learning with errors (LWE) assumption.\n  - In addition, we observe that the aforementioned barrier does not apply to the shared EPR model. We leverage this fact to construct a CE-NIZK for NP in this model based on any statistical binding hidden-bits generator, which can be based on LWE. The only quantum computation in this protocol involves single-qubit measurements of the shared EPR pairs.", "published": "2025-12-15T18:23:48Z", "updated": "2026-02-12T06:00:17Z", "authors": ["Nikhil Pappu"], "pdf_url": "https://arxiv.org/pdf/2512.13628v3"}
{"id": "http://arxiv.org/abs/2602.11606v1", "title": "QDBFT: A Dynamic Consensus Algorithm for Quantum-Secured Blockchain", "summary": "The security foundation of blockchain system relies primarily on classical cryptographic methods and consensus algorithms. However, the advent of quantum computing poses a significant threat to conventional public-key cryptosystems based on computational hardness assumptions. In particular, Shor's algorithm can efficiently solve discrete logarithm and integer factorization problems in polynomial time, thereby undermining the immutability and security guarantees of existing systems. Moreover, current Practical Byzantine Fault Tolerance (PBFT) protocols, widely adopted in consortium blockchains, suffer from high communication overhead and limited efficiency when coping with dynamic node reconfigurations, while offering no intrinsic protection against quantum adversaries.\n  To address these challenges, we propose QDBFT, a quantum-secured dynamic consensus algorithm, with two main contributions: first,we design a primary node automatic rotation mechanism based on a consistent hash ring to enable consensus under dynamic membership changes, ensuring equitable authority distribution; second, we integrate Quantum Key Distribution (QKD) networks to provide message authentication for inter-node communication, thereby achieving information-theoretic security in the consensus process. Experimental evaluations demonstrate that QDBFT achieves performance comparable to traditional PBFT while delivering strong resilience against quantum attacks, making it a promising solution for future quantum-secure decentralized infrastructures.", "published": "2026-02-12T05:53:51Z", "updated": "2026-02-12T05:53:51Z", "authors": ["Fei Xu", "Cheng Ye", "Jie OuYang", "Ziqiang Wu", "Haoze Chen", "An Hua", "Meifeng Gao", "Qiandong Zhang", "Minghan Li", "Feilong Li", "Yajun Miao", "Wei Qi"], "pdf_url": "https://arxiv.org/pdf/2602.11606v1"}
{"id": "http://arxiv.org/abs/2602.09319v2", "title": "Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation", "summary": "Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.", "published": "2026-02-10T01:27:46Z", "updated": "2026-02-12T04:41:35Z", "authors": ["Zhisheng Qi", "Utkarsh Sahu", "Li Ma", "Haoyu Han", "Ryan Rossi", "Franck Dernoncourt", "Mahantesh Halappanavar", "Nesreen Ahmed", "Yushun Dong", "Yue Zhao", "Yu Zhang", "Yu Wang"], "pdf_url": "https://arxiv.org/pdf/2602.09319v2"}
{"id": "http://arxiv.org/abs/2410.21088v5", "title": "Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models", "summary": "The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, Shallow Diffuse decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our Shallow Diffuse outperforms existing watermarking methods in terms of robustness and consistency. The codes are released at https://github.com/liwd190019/Shallow-Diffuse.", "published": "2024-10-28T14:51:04Z", "updated": "2026-02-12T03:47:01Z", "authors": ["Wenda Li", "Huijie Zhang", "Qing Qu"], "pdf_url": "https://arxiv.org/pdf/2410.21088v5"}
{"id": "http://arxiv.org/abs/2602.11528v1", "title": "Stop Tracking Me! Proactive Defense Against Attribute Inference Attack in LLMs", "summary": "Recent studies have shown that large language models (LLMs) can infer private user attributes (e.g., age, location, gender) from user-generated text shared online, enabling rapid and large-scale privacy breaches. Existing anonymization-based defenses are coarse-grained, lacking word-level precision in anonymizing privacy-leaking elements. Moreover, they are inherently limited as altering user text to hide sensitive cues still allows attribute inference to occur through models' reasoning capabilities. To address these limitations, we propose a unified defense framework that combines fine-grained anonymization (TRACE) with inference-preventing optimization (RPS). TRACE leverages attention mechanisms and inference chain generation to identify and anonymize privacy-leaking textual elements, while RPS employs a lightweight two-stage optimization strategy to induce model rejection behaviors, thereby preventing attribute inference. Evaluations across diverse LLMs show that TRACE-RPS reduces attribute inference accuracy from around 50\\% to below 5\\% on open-source models. In addition, our approach offers strong cross-model generalization, prompt-variation robustness, and utility-privacy tradeoffs. Our code is available at https://github.com/Jasper-Yan/TRACE-RPS.", "published": "2026-02-12T03:37:50Z", "updated": "2026-02-12T03:37:50Z", "authors": ["Dong Yan", "Jian Liang", "Ran He", "Tieniu Tan"], "pdf_url": "https://arxiv.org/pdf/2602.11528v1"}
{"id": "http://arxiv.org/abs/2412.03441v4", "title": "PBP: Post-training Backdoor Purification for Malware Classifiers", "summary": "In recent years, the rise of machine learning (ML) in cybersecurity has brought new challenges, including the increasing threat of backdoor poisoning attacks on ML malware classifiers. For instance, adversaries could inject malicious samples into public malware repositories, contaminating the training data and potentially misclassifying malware by the ML model. Current countermeasures predominantly focus on detecting poisoned samples by leveraging disagreements within the outputs of a diverse set of ensemble models on training data points. However, these methods are not suitable for scenarios where Machine Learning-as-a-Service (MLaaS) is used or when users aim to remove backdoors from a model after it has been trained. Addressing this scenario, we introduce PBP, a post-training defense for malware classifiers that mitigates various types of backdoor embeddings without assuming any specific backdoor embedding mechanism. Our method exploits the influence of backdoor attacks on the activation distribution of neural networks, independent of the trigger-embedding method. In the presence of a backdoor attack, the activation distribution of each layer is distorted into a mixture of distributions. By regulating the statistics of the batch normalization layers, we can guide a backdoored model to perform similarly to a clean one. Our method demonstrates substantial advantages over several state-of-the-art methods, as evidenced by experiments on two datasets, two types of backdoor methods, and various attack configurations. Notably, our approach requires only a small portion of the training data -- only 1\\% -- to purify the backdoor and reduce the attack success rate from 100\\% to almost 0\\%, a 100-fold improvement over the baseline methods. Our code is available at https://github.com/judydnguyen/pbp-backdoor-purification-official.", "published": "2024-12-04T16:30:03Z", "updated": "2026-02-12T03:20:07Z", "authors": ["Dung Thuy Nguyen", "Ngoc N. Tran", "Taylor T. Johnson", "Kevin Leach"], "pdf_url": "https://arxiv.org/pdf/2412.03441v4"}
{"id": "http://arxiv.org/abs/2512.21561v2", "title": "A Quantitative Method for Evaluating Security Boundaries in Quantum Key Distribution Combined with Block Ciphers", "summary": "With the rapid development of quantum computing, classical cryptography systems are increasingly vulnerable to security threats, thereby highlighting the urgency of constructing architectures that are resilient to quantum computing attacks. While Quantum Key Distribution (QKD) offers security with information-theoretic guarantees, its relatively low key generation rate necessitates integration with classical cryptographic techniques, particularly block ciphers such as AES and SM4, to facilitate practical applications. However, when a single QKD-key is employed to encrypt multiple data blocks, the reduction in cryptographic security strength has not yet been quantitatively analyzed. In this work, we focus on the security strength in the application scenario where QKD is combined with block ciphers. We propose a quantitative evaluation method for the security benefits of the QKD-key renewal period, aiming to provide a precise measure of the cryptographic security strength in such hybrid systems. Our method is based on concrete security paradigm of block cipher modes of operation. We demonstrate that under practical security level requirements, for files consisting of specific blocks, rekeying k times can provide an additional log2(k) to 2log2(k) bits of security. Our research offers a novel perspective on balancing the security and efficiency of QKD-based encryption.", "published": "2025-12-25T08:13:02Z", "updated": "2026-02-12T03:14:38Z", "authors": ["Xiaoming Chen", "Haoze Chen", "Fei Xu", "Meifeng Gao", "Jianguo Xie", "Cheng Ye", "An Hua", "Shichang Jiang", "Jiao Zhao", "Minghan Li", "Feilong Li", "Yajun Miao", "Wei Qi"], "pdf_url": "https://arxiv.org/pdf/2512.21561v2"}
{"id": "http://arxiv.org/abs/2507.12314v3", "title": "Thought Purity: A Defense Framework For Chain-of-Thought Attack", "summary": "Large Reasoning Models (LRMs) leverage Chain-of-Thought (CoT) reasoning to solve complex tasks, but this explicit reasoning process introduces a critical vulnerability: adversarial manipulation of the thought chain itself, known as Chain-of-Thought Attacks (CoTA). Such attacks subtly corrupt the reasoning path to produce erroneous outputs, challenging conventional defenses that often sacrifice model utility for safety. To address this, we propose Thought Purity(TP), a defense framework that shifts from passive refusal to active reasoning recovery. TP integrates a safety-aware data pipeline with reinforcement learning, employing a dual-reward mechanism to teach models to dynamically identify and isolate malicious logic while preserving correct reasoning. Experiments on multiple model families demonstrate that TP significantly reduces the attack success rate of CoTA while maintaining or enhancing the model's performance on benign tasks.", "published": "2025-07-16T15:09:13Z", "updated": "2026-02-12T03:14:01Z", "authors": ["Zihao Xue", "Zhen Bi", "Long Ma", "Zhenlin Hu", "Yan Wang", "Xueshu Chen", "Zhenfang Liu", "Kang Zhao", "Jie Xiao", "Jungang Lou"], "pdf_url": "https://arxiv.org/pdf/2507.12314v3"}
{"id": "http://arxiv.org/abs/2602.11513v1", "title": "Differentially Private and Communication Efficient Large Language Model Split Inference via Stochastic Quantization and Soft Prompt", "summary": "Large Language Models (LLMs) have achieved remarkable performance and received significant research interest. The enormous computational demands, however, hinder the local deployment on devices with limited resources. The current prevalent LLM inference paradigms require users to send queries to the service providers for processing, which raises critical privacy concerns. Existing approaches propose to allow the users to obfuscate the token embeddings before transmission and utilize local models for denoising. Nonetheless, transmitting the token embeddings and deploying local models may result in excessive communication and computation overhead, preventing practical implementation. In this work, we propose \\textbf{DEL}, a framework for \\textbf{D}ifferentially private and communication \\textbf{E}fficient \\textbf{L}LM split inference. More specifically, an embedding projection module and a differentially private stochastic quantization mechanism are proposed to reduce the communication overhead in a privacy-preserving manner. To eliminate the need for local models, we adapt soft prompt at the server side to compensate for the utility degradation caused by privacy. To the best of our knowledge, this is the first work that utilizes soft prompt to improve the trade-off between privacy and utility in LLM inference, and extensive experiments on text generation and natural language understanding benchmarks demonstrate the effectiveness of the proposed method.", "published": "2026-02-12T03:13:16Z", "updated": "2026-02-12T03:13:16Z", "authors": ["Yujie Gu", "Richeng Jin", "Xiaoyu Ji", "Yier Jin", "Wenyuan Xu"], "pdf_url": "https://arxiv.org/pdf/2602.11513v1"}
{"id": "http://arxiv.org/abs/2602.11495v1", "title": "Jailbreaking Leaves a Trace: Understanding and Detecting Jailbreak Attacks from Internal Representations of Large Language Models", "summary": "Jailbreaking large language models (LLMs) has emerged as a critical security challenge with the widespread deployment of conversational AI systems. Adversarial users exploit these models through carefully crafted prompts to elicit restricted or unsafe outputs, a phenomenon commonly referred to as Jailbreaking. Despite numerous proposed defense mechanisms, attackers continue to develop adaptive prompting strategies, and existing models remain vulnerable. This motivates approaches that examine the internal behavior of LLMs rather than relying solely on prompt-level defenses. In this work, we study jailbreaking from both security and interpretability perspectives by analyzing how internal representations differ between jailbreak and benign prompts. We conduct a systematic layer-wise analysis across multiple open-source models, including GPT-J, LLaMA, Mistral, and the state-space model Mamba, and identify consistent latent-space patterns associated with harmful inputs. We then propose a tensor-based latent representation framework that captures structure in hidden activations and enables lightweight jailbreak detection without model fine-tuning or auxiliary LLM-based detectors. We further demonstrate that the latent signals can be used to actively disrupt jailbreak execution at inference time. On an abliterated LLaMA-3.1-8B model, selectively bypassing high-susceptibility layers blocks 78% of jailbreak attempts while preserving benign behavior on 94% of benign prompts. This intervention operates entirely at inference time and introduces minimal overhead, providing a scalable foundation for achieving stronger coverage by incorporating additional attack distributions or more refined susceptibility thresholds. Our results provide evidence that jailbreak behavior is rooted in identifiable internal structures and suggest a complementary, architecture-agnostic direction for improving LLM security.", "published": "2026-02-12T02:43:17Z", "updated": "2026-02-12T02:43:17Z", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "pdf_url": "https://arxiv.org/pdf/2602.11495v1"}
{"id": "http://arxiv.org/abs/2602.09369v2", "title": "Timing and Memory Telemetry on GPUs for AI Governance", "summary": "The rapid expansion of GPU-accelerated computing has enabled major advances in large-scale artificial intelligence (AI), while heightening concerns about how accelerators are observed or governed once deployed. Governance is essential to ensure that large-scale compute infrastructure is not silently repurposed for training models, circumventing usage policies, or operating outside legal oversight. Because current GPUs expose limited trusted telemetry and can be modified or virtualized by adversaries, we explore whether compute-based measurements can provide actionable signals of utilization when host and device are untrusted. We introduce a measurement framework that leverages architectural characteristics of modern GPUs to generate timing- and memory-based observables that correlate with compute activity. Our design draws on four complementary primitives: (1) a probabilistic, workload-driven mechanism inspired by Proof-of-Work (PoW) to expose parallel effort, (2) sequential, latency-sensitive workloads derived via Verifiable Delay Functions (VDFs) to characterize scalar execution pressure, (3) General Matrix Multiplication (GEMM)-based tensor-core measurements that reflect dense linear-algebra throughput, and (4) a VRAM-residency test that distinguishes on-device memory locality from off-chip access through bandwidth-dependent hashing. These primitives provide statistical and behavioral indicators of GPU engagement that remain observable even without trusted firmware, enclaves, or vendor-controlled counters. We evaluate their responses to contention, architectural alignment, memory pressure, and power overhead, showing that timing shifts and residency latencies reveal meaningful utilization patterns. Our results illustrate why compute-based telemetry can complement future accountability mechanisms by exposing architectural signals relevant to post-deployment GPU governance.", "published": "2026-02-10T03:20:06Z", "updated": "2026-02-12T01:42:39Z", "authors": ["Saleh K. Monfared", "Fatemeh Ganji", "Dan Holcomb", "Shahin Tajik"], "pdf_url": "https://arxiv.org/pdf/2602.09369v2"}
{"id": "http://arxiv.org/abs/2602.11472v1", "title": "Future Mining: Learning for Safety and Security", "summary": "Mining is rapidly evolving into an AI driven cyber physical ecosystem where safety and operational reliability depend on robust perception, trustworthy distributed intelligence, and continuous monitoring of miners and equipment. However, real world mining environments impose severe constraints, including poor illumination, GPS denied conditions, irregular underground topologies and intermittent connectivity. These factors degrade perception accuracy, disrupt situational awareness and weaken distributed learning systems. At the same time, emerging cyber physical threats such as backdoor triggers, sensor spoofing, label flipping attacks, and poisoned model updates further jeopardize operational safety as mines adopt autonomous vehicles, humanoid assistance, and federated learning for collaborative intelligence. Energy constrained sensors also experience uneven battery depletion, creating blind spots in safety coverage and disrupting hazard detection pipelines. This paper presents a vision for a Unified Smart Safety and Security Architecture that integrates multimodal perception, secure federated learning, reinforcement learning, DTN enabled communication, and energy aware sensing into a cohesive safety framework. We introduce five core modules: Miner Finder, Multimodal Situational Awareness, Backdoor Attack Monitor, TrustFed LFD, and IoT driven Equipment Health Monitoring. These modules collectively address miner localization, hazard understanding, federated robustness, and predictive maintenance. Together, they form an end to end framework capable of guiding miners through obstructed pathways, identifying compromised models or sensors, and ensuring mission critical equipment reliability. This work outlines a comprehensive research vision for building a resilient and trustworthy intelligent mining system capable of maintaining operational continuity under adversarial conditions.", "published": "2026-02-12T01:13:16Z", "updated": "2026-02-12T01:13:16Z", "authors": ["Md Sazedur Rahman", "Mizanur Rahman Jewel", "Sanjay Madria"], "pdf_url": "https://arxiv.org/pdf/2602.11472v1"}
{"id": "http://arxiv.org/abs/2602.11470v1", "title": "Cachemir: Fully Homomorphic Encrypted Inference of Generative Large Language Model with KV Cache", "summary": "Generative large language models (LLMs) have revolutionized multiple domains. Modern LLMs predominantly rely on an autoregressive decoding strategy, which generates output tokens sequentially and employs a key-value cache (KV cache) to avoid redundant computation. However, the widespread deployment of LLMs has raised serious privacy concerns, as users are feeding all types of data into the model, motivating the development of secure inference frameworks based on fully homomorphic encryption (FHE). A major limitation of existing FHE-based frameworks is their inability to effectively integrate the KV cache, resulting in prohibitively high latency for autoregressive decoding. In this paper, we propose Cachemir, a KV Cache Accelerated Homomorphic Encrypted LLM Inference Regime to overcome this limitation. Cachemir comprises three key technical contributions: 1) a set of novel HE packing algorithms specifically designed to leverage the computational advantages of the KV cache; 2) an interleaved replicated packing algorithm to efficiently compute the vector-matrix multiplications that result from using the KV cache in Transformer linear layers; and 3) an augmented bootstrapping placement strategy that accounts for the KV cache to minimize bootstrapping cost. We demonstrate that Cachemir achieves $48.83\\times$ and $67.16\\times$ speedup over MOAI (ICML'25) and THOR (CCS'25) respectively on CPU and consumes less than 100 seconds on GPU to generate an output token for Llama-3-8B.", "published": "2026-02-12T01:01:38Z", "updated": "2026-02-12T01:01:38Z", "authors": ["Ye Yu", "Yifan Zhou", "Yi Chen", "Pedro Soto", "Wenjie Xiong", "Meng Li"], "pdf_url": "https://arxiv.org/pdf/2602.11470v1"}
