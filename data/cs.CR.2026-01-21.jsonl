{"id": "http://arxiv.org/abs/2601.15269v1", "title": "Lightweight LLMs for Network Attack Detection in IoT Networks", "summary": "The rapid growth of Internet of Things (IoT) devices has increased the scale and diversity of cyberattacks, exposing limitations in traditional intrusion detection systems. Classical machine learning (ML) models such as Random Forest and Support Vector Machine perform well on known attacks but require retraining to detect unseen or zero-day threats. This study investigates lightweight decoder-only Large Language Models (LLMs) for IoT attack detection by integrating structured-to-text conversion, Quantized Low-Rank Adaptation (QLoRA) fine-tuning, and Retrieval-Augmented Generation (RAG). Network traffic features are transformed into compact natural-language prompts, enabling efficient adaptation under constrained hardware. Experiments on the CICIoT2023 dataset show that a QLoRA-tuned LLaMA-1B model achieves an F1-score of 0.7124, comparable to the Random Forest (RF) baseline (0.7159) for known attacks. With RAG, the system attains 42.63% accuracy on unseen attack types without additional training, demonstrating practical zero-shot capability. These results highlight the potential of retrieval-enhanced lightweight LLMs as adaptable and resource-efficient solutions for next-generation IoT intrusion detection.", "published": "2026-01-21T18:52:26Z", "updated": "2026-01-21T18:52:26Z", "authors": ["Piyumi Bhagya Sudasinghe", "Kushan Sudheera Kalupahana Liyanage", "Harsha S. Gardiyawasam Pussewalage"], "pdf_url": "https://arxiv.org/pdf/2601.15269v1"}
{"id": "http://arxiv.org/abs/2505.22843v3", "title": "On the Reliability and Stability of Selective Methods in Malware Classification Tasks", "summary": "The performance figures of modern drift-adaptive malware classifiers appear promising, but does this translate to genuine operational reliability? The standard evaluation paradigm primarily focuses on baseline performance metrics, neglecting confidence-error alignment and operational stability. While prior works established the importance of temporal evaluation and introduced selective classification in malware classification tasks, we take a complementary direction by investigating whether malware classifiers maintain reliable and stable confidence estimates under distribution shifts and exploring the tensions between scientific advancement and practical impacts when they do not. We propose Aurora, a framework to evaluate malware classifiers based on their confidence quality and operational resilience. Aurora subjects the confidence profile of a given model to verification to assess the reliability of its estimates. Unreliable confidence estimates erode operational trust, waste valuable annotation budgets on non-informative samples for active learning, and leave error-prone instances undetected in selective classification. Aurora is further complemented by a set of metrics designed to go beyond point-in-time performance, striving towards a more holistic assessment of operational stability throughout temporal evaluation periods. The fragility we observe in SOTA frameworks across datasets of varying drift severity suggests it may be time to revisit the underlying assumptions.", "published": "2025-05-28T20:22:43Z", "updated": "2026-01-21T18:26:18Z", "authors": ["Alexander Herzog", "Aliai Eusebi", "Lorenzo Cavallaro"], "pdf_url": "https://arxiv.org/pdf/2505.22843v3"}
{"id": "http://arxiv.org/abs/2402.05059v2", "title": "Connecting Kani's Lemma and path-finding in the Bruhat-Tits tree to compute supersingular endomorphism rings", "summary": "We give a deterministic polynomial time algorithm to compute the endomorphism ring of a supersingular elliptic curve in characteristic p, provided that we are given two noncommuting endomorphisms and the factorization of the discriminant of the ring $\\mathcal{O}_0$ they generate. At each prime $q$ for which $\\mathcal{O}_0$ is not maximal, we compute the endomorphism ring locally by computing a q-maximal order containing it and, when $q \\neq p$, recovering a path to $\\text{End}(E) \\otimes \\mathbb{Z}_q$ in the Bruhat-Tits tree. We use techniques of higher-dimensional isogenies to navigate towards the local endomorphism ring. Our algorithm improves on a previous algorithm which requires a restricted input and runs in subexponential time under certain heuristics. Page and Wesolowski give a probabilistic polynomial time algorithm to compute the endomorphism ring on input of a single non-scalar endomorphism. Beyond using techniques of higher-dimensional isogenies to divide endomorphisms by a scalar, our methods are completely different.", "published": "2024-02-07T18:10:54Z", "updated": "2026-01-21T18:25:27Z", "authors": ["Kirsten Eisentraeger", "Gabrielle Scullard"], "pdf_url": "https://arxiv.org/pdf/2402.05059v2"}
{"id": "http://arxiv.org/abs/2505.16888v3", "title": "SPECTRE: Conditional System Prompt Poisoning to Hijack LLMs", "summary": "Large Language Models (LLMs) are increasingly deployed via third-party system prompts downloaded from public marketplaces. We identify a critical supply-chain vulnerability: conditional system prompt poisoning, where an adversary injects a ``sleeper agent'' into a benign-looking prompt. Unlike traditional jailbreaks that aim for broad refusal-breaking, our proposed framework, SPECTRE, optimizes system prompts to trigger LLMs to output targeted, compromised responses only for specific queries (e.g., ``Who should I vote for the US President?'') while maintaining high utility on benign inputs. Operating in a strict black-box setting without model weight access, SPECTRE utilizes a two-stage optimization including a global semantic search followed by a greedy lexical refinement. Tested on open-source models and commercial APIs (GPT-4o-mini, GPT-3.5), SPECTRE achieves up to 70% F1 reduction on targeted queries with minimal degradation to general capabilities. We further demonstrate that these poisoned prompts evade standard defenses, including perplexity filters and typo-correction, by exploiting the natural noise found in real-world system prompts. Our code and data are available at https://github.com/vietph34/CAIN. WARNING: Our paper contains examples that might be sensitive to the readers!", "published": "2025-05-22T16:47:15Z", "updated": "2026-01-21T17:45:09Z", "authors": ["Viet Pham", "Thai Le"], "pdf_url": "https://arxiv.org/pdf/2505.16888v3"}
{"id": "http://arxiv.org/abs/2501.16534v3", "title": "Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs", "summary": "Alignment in large language models (LLMs) is used to enforce guidelines such as safety. Yet, alignment fails in the face of jailbreak attacks that modify inputs to induce unsafe outputs. In this paper, we introduce and evaluate a new technique for jailbreak attacks. We observe that alignment embeds a safety classifier in the LLM responsible for deciding between refusal and compliance, and seek to extract an approximation of this classifier: a surrogate classifier. To this end, we build candidate classifiers from subsets of the LLM. We first evaluate the degree to which candidate classifiers approximate the LLM's safety classifier in benign and adversarial settings. Then, we attack the candidates and measure how well the resulting adversarial inputs transfer to the LLM. Our evaluation shows that the best candidates achieve accurate agreement (an F1 score above 80%) using as little as 20% of the model architecture. Further, we find that attacks mounted on the surrogate classifiers can be transferred to the LLM with high success. For example, a surrogate using only 50% of the Llama 2 model achieved an attack success rate (ASR) of 70% with half the memory footprint and runtime -- a substantial improvement over attacking the LLM directly, where we only observed a 22% ASR. These results show that extracting surrogate classifiers is an effective and efficient means for modeling (and therein addressing) the vulnerability of aligned models to jailbreaking attacks. The code is available at https://github.com/jcnf0/targeting-alignment.", "published": "2025-01-27T22:13:05Z", "updated": "2026-01-21T17:29:42Z", "authors": ["Jean-Charles Noirot Ferrand", "Yohan Beugin", "Eric Pauley", "Ryan Sheatsley", "Patrick McDaniel"], "pdf_url": "https://arxiv.org/pdf/2501.16534v3"}
{"id": "http://arxiv.org/abs/2601.15177v1", "title": "Dynamic Management of a Deep Learning-Based Anomaly Detection System for 5G Networks", "summary": "Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and analyzing significantly large amount of data traffic and huge number of network connections in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile networks to detect network anomalies in real-time and in autonomic way. Our proposal uses deep learning techniques to analyze network flows and to detect network anomalies. Moreover, it uses policies in order to provide an efficient and dynamic management system of the computing resources used in the anomaly detection process. The paper presents relevant aspects of the deployment of the proposal and experimental results to show its performance.", "published": "2026-01-21T16:54:19Z", "updated": "2026-01-21T16:54:19Z", "authors": ["Lorenzo Fernández Maimó", "Alberto Huertas Celdrán", "Manuel Gil Pérez", "Félix J. García Clemente", "Gregorio Martínez Pérez"], "pdf_url": "https://arxiv.org/pdf/2601.15177v1"}
{"id": "http://arxiv.org/abs/2601.15154v1", "title": "SAGA: Detecting Security Vulnerabilities Using Static Aspect Analysis", "summary": "Python is one of the most popular programming languages; as such, projects written in Python involve an increasing number of diverse security vulnerabilities. However, existing state-of-the-art analysis tools for Python only support a few vulnerability types. Hence, there is a need to detect a large variety of vulnerabilities in Python projects.\n  In this paper, we propose the SAGA approach to detect and locate vulnerabilities in Python source code in a versatile way. SAGA includes a source code parser able to extract control- and data-flow information and to represent it as a symbolic control-flow graph, as well as a domain-specific language defining static aspects of the source code and their evolution during graph traversals. We have leveraged this language to define a library of static aspects for integrity, confidentiality, and other security-related properties.\n  We have evaluated SAGA on a dataset of 108 vulnerabilities, obtaining 100% sensitivity and 99.15% specificity, with only one false positive, while outperforming four common security analysis tools. This analysis was performed in less than 31 seconds, i.e., between 2.5 and 512.1 times faster than the baseline tools.", "published": "2026-01-21T16:26:26Z", "updated": "2026-01-21T16:26:26Z", "authors": ["Yoann Marquer", "Domenico Bianculli", "Lionel C. Briand"], "pdf_url": "https://arxiv.org/pdf/2601.15154v1"}
{"id": "http://arxiv.org/abs/2410.08864v2", "title": "The Good, the Bad and the Ugly: Meta-Analysis of Watermarks, Transferable Attacks and Adversarial Defenses", "summary": "We formalize and analyze the trade-off between backdoor-based watermarks and adversarial defenses, framing it as an interactive protocol between a verifier and a prover. While previous works have primarily focused on this trade-off, our analysis extends it by identifying transferable attacks as a third, counterintuitive, but necessary option. Our main result shows that for all learning tasks, at least one of the three exists: a watermark, an adversarial defense, or a transferable attack. By transferable attack, we refer to an efficient algorithm that generates queries indistinguishable from the data distribution and capable of fooling all efficient defenders. Using cryptographic techniques, specifically fully homomorphic encryption, we construct a transferable attack and prove its necessity in this trade-off. Finally, we show that tasks of bounded VC-dimension allow adversarial defenses against all attackers, while a subclass allows watermarks secure against fast adversaries.", "published": "2024-10-11T14:44:05Z", "updated": "2026-01-21T16:22:57Z", "authors": ["Grzegorz Głuch", "Berkant Turan", "Sai Ganesh Nagarajan", "Sebastian Pokutta"], "pdf_url": "https://arxiv.org/pdf/2410.08864v2"}
{"id": "http://arxiv.org/abs/2510.10111v3", "title": "Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization", "summary": "Advances in image tampering pose serious security threats, underscoring the need for effective image manipulation localization (IML). While supervised IML achieves strong performance, it depends on costly pixel-level annotations. Existing weakly supervised or training-free alternatives often underperform and lack interpretability. We propose the In-Context Forensic Chain (ICFC), a training-free framework that leverages multi-modal large language models (MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule construction with adaptive filtering to build a reliable knowledge base and a multi-step progressive reasoning pipeline that mirrors expert forensic workflows from coarse proposals to fine-grained forensics results. This design enables systematic exploitation of MLLM reasoning for image-level classification, pixel-level localization, and text-level interpretability. Across multiple benchmarks, ICFC not only surpasses state-of-the-art training-free methods but also achieves competitive or superior performance compared to weakly and fully supervised approaches.", "published": "2025-10-11T08:42:31Z", "updated": "2026-01-21T15:39:57Z", "authors": ["Rui Chen", "Bin Liu", "Changtao Miao", "Xinghao Wang", "Yi Li", "Tao Gong", "Qi Chu", "Nenghai Yu"], "pdf_url": "https://arxiv.org/pdf/2510.10111v3"}
{"id": "http://arxiv.org/abs/2601.15055v1", "title": "SpooFL: Spoofing Federated Learning", "summary": "Traditional defenses against Deep Leakage (DL) attacks in Federated Learning (FL) primarily focus on obfuscation, introducing noise, transformations or encryption to degrade an attacker's ability to reconstruct private data. While effective to some extent, these methods often still leak high-level information such as class distributions or feature representations, and are frequently broken by increasingly powerful denoising attacks. We propose a fundamentally different perspective on FL defense: framing it as a spoofing problem.We introduce SpooFL (Figure 1), a spoofing-based defense that deceives attackers into believing they have recovered the true training data, while actually providing convincing but entirely synthetic samples from an unrelated task. Unlike prior synthetic-data defenses that share classes or distributions with the private data and thus still leak semantic information, SpooFL uses a state-of-the-art generative model trained on an external dataset with no class overlap. As a result, attackers are misled into recovering plausible yet completely irrelevant samples, preventing meaningful data leakage while preserving FL training integrity. We implement the first example of such a spoofing defense, and evaluate our method against state-of-the-art DL defenses and demonstrate that it successfully misdirects attackers without compromising model performance significantly.", "published": "2026-01-21T14:57:18Z", "updated": "2026-01-21T14:57:18Z", "authors": ["Isaac Baglin", "Xiatian Zhu", "Simon Hadfield"], "pdf_url": "https://arxiv.org/pdf/2601.15055v1"}
{"id": "http://arxiv.org/abs/2404.07892v2", "title": "A Measurement of Genuine Tor Traces for Realistic Website Fingerprinting", "summary": "Website fingerprinting (WF) is a dangerous attack on web privacy because it enables an adversary to predict the website a user is visiting, despite the use of encryption, VPNs, or anonymizing networks such as Tor. Previous WF work almost exclusively uses synthetic datasets to evaluate the performance and estimate the feasibility of WF attacks despite evidence that synthetic data misrepresents the real world. In this paper we present GTT23, the first WF dataset of genuine Tor traces, which we obtain through a large-scale measurement of the Tor network and which is intended especially for WF. It represents real Tor user behavior better than any existing WF dataset, is larger than any existing WF dataset by at least an order of magnitude, and will help ground the future study of realistic WF attacks and defenses. In a detailed evaluation, we survey 28 WF datasets published since 2008 and compare their characteristics to those of GTT23. We discover common deficiencies of synthetic datasets that make them inferior to GTT23 for drawing meaningful conclusions about the effectiveness of WF attacks directed at real Tor users. We have made GTT23 available to promote reproducible research and to help inspire new directions for future work.", "published": "2024-04-11T16:24:49Z", "updated": "2026-01-21T14:52:10Z", "authors": ["Rob Jansen", "Ryan Wails", "Aaron Johnson"], "pdf_url": "https://arxiv.org/pdf/2404.07892v2"}
{"id": "http://arxiv.org/abs/2601.14996v1", "title": "On the Effectiveness of Mempool-based Transaction Auditing", "summary": "While the literature features a number of proposals to defend against transaction manipulation attacks, existing proposals are still not integrated within large blockchains, such as Bitcoin, Ethereum, and Cardano. Instead, the user community opted to rely on more practical but ad-hoc solutions (such as Mempool.space) that aim at detecting censorship and transaction displacement attacks by auditing discrepancies in the mempools of so-called observers.\n  In this paper, we precisely analyze, for the first time, the interplay between mempool auditing and the ability to detect censorship and transaction displacement attacks by malicious miners in Bitcoin and Ethereum. Our analysis shows that mempool auditing can result in mis-accusations against miners with a probability larger than 25% in some settings. On a positive note, however, we show that mempool auditing schemes can successfully audit the execution of any two transactions (with an overwhelming probability of 99.9%) if they are consistently received by all observers and sent at least 30 seconds apart from each other. As a direct consequence, our findings show, for the first time, that batch-order fair-ordering schemes can offer only strong fairness guarantees for a limited subset of transactions in real-world deployments.", "published": "2026-01-21T13:54:04Z", "updated": "2026-01-21T13:54:04Z", "authors": ["Jannik Albrecht", "Ghassan Karame"], "pdf_url": "https://arxiv.org/pdf/2601.14996v1"}
{"id": "http://arxiv.org/abs/2601.14982v1", "title": "Interoperable Architecture for Digital Identity Delegation for AI Agents with Blockchain Integration", "summary": "Verifiable delegation in digital identity systems remains unresolved across centralized, federated, and self-sovereign identity (SSI) environments, particularly where both human users and autonomous AI agents must exercise and transfer authority without exposing primary credentials or private keys. We introduce a unified framework that enables bounded, auditable, and least-privilege delegation across heterogeneous identity ecosystems. The framework includes four key elements: Delegation Grants (DGs), first-class authorization artefacts that encode revocable transfers of authority with enforced scope reduction; a Canonical Verification Context (CVC) that normalizes verification requests into a single structured representation independent of protocols or credential formats; a layered reference architecture that separates trust anchoring, credential and proof validation, policy evaluation, and protocol mediation via a Trust Gateway; and an explicit treatment of blockchain anchoring as an optional integrity layer rather than a structural dependency. Together, these elements advance interoperable delegation and auditability and provide a foundation for future standardization, implementation, and integration of autonomous agents into trusted digital identity infrastructures.", "published": "2026-01-21T13:29:23Z", "updated": "2026-01-21T13:29:23Z", "authors": ["David Ricardo Saavedra"], "pdf_url": "https://arxiv.org/pdf/2601.14982v1"}
{"id": "http://arxiv.org/abs/2510.06605v2", "title": "Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation", "summary": "The substantial investment required to develop Large Language Models (LLMs) makes them valuable intellectual property, raising significant concerns about copyright protection. LLM fingerprinting has emerged as a key technique to address this, which aims to verify a model's origin by extracting an intrinsic, unique signature (a \"fingerprint\") and comparing it to that of a source model to identify illicit copies. However, existing black-box fingerprinting methods often fail to generate distinctive LLM fingerprints. This ineffectiveness arises because black-box methods typically rely on model outputs, which lose critical information about the model's unique parameters due to the usage of non-linear functions. To address this, we first leverage Fisher Information Theory to formally demonstrate that the gradient of the model's input is a more informative feature for fingerprinting than the output. Based on this insight, we propose ZeroPrint, a novel method that approximates these information-rich gradients in a black-box setting using zeroth-order estimation. ZeroPrint overcomes the challenge of applying this to discrete text by simulating input perturbations via semantic-preserving word substitutions. This operation allows ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint. Experiments on the standard benchmark show ZeroPrint achieves a state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.", "published": "2025-10-08T03:27:38Z", "updated": "2026-01-21T12:56:55Z", "authors": ["Shuo Shao", "Yiming Li", "Hongwei Yao", "Yifei Chen", "Yuchen Yang", "Zhan Qin"], "pdf_url": "https://arxiv.org/pdf/2510.06605v2"}
{"id": "http://arxiv.org/abs/2512.08809v3", "title": "PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration", "summary": "With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_χ$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.", "published": "2025-12-09T17:03:59Z", "updated": "2026-01-21T12:46:46Z", "authors": ["Yi Liu", "Weixiang Han", "Chengjun Cai", "Xingliang Yuan", "Cong Wang"], "pdf_url": "https://arxiv.org/pdf/2512.08809v3"}
{"id": "http://arxiv.org/abs/2007.14729v4", "title": "Formal Power Series on Algebraic Cryptanalysis", "summary": "In the complexity estimation for an attack that reduces a cryptosystem to solving a system of polynomial equations, the degree of regularity and an upper bound of the first fall degree are often used in cryptanalysis. While the degree of regularity can be easily computed using a univariate formal power series under the semi-regularity assumption, determining an upper bound of the first fall degree requires investigating the concrete syzygies of an input system. In this paper, we investigate an upper bound of the first fall degree for a polynomial system over a sufficiently large field. In this case, we prove that the first fall degree of a non-semi-regular system is bounded above by the degree of regularity, and that the first fall degree of a multi-graded polynomial system is bounded above by a certain value determined from a multivariate formal power series. Moreover, we provide a theoretical assumption for computing the first fall degree of a polynomial system over a sufficiently large field.", "published": "2020-07-29T10:36:20Z", "updated": "2026-01-21T12:36:48Z", "authors": ["Shuhei Nakamura"], "pdf_url": "https://arxiv.org/pdf/2007.14729v4"}
{"id": "http://arxiv.org/abs/2601.14926v1", "title": "On Implementing Hybrid Post-Quantum End-to-End Encryption", "summary": "The emergence of quantum computing poses a fundamental threat to current public key cryptographic systems. This threat is necessitating a transition to quantum resistant cryptographic alternatives in all the applications. In this work, we present the implementation of a practical hybrid end-to-end encryption system that combines classical and post-quantum cryptographic primitives to achieve both security and efficiency. Our system employs CRYSTALS-Kyber, a NIST-standardized lattice-based key encapsulation mechanism, for quantum-safe key exchange, coupled with AES-256-GCM for efficient authenticated symmetric encryption and SHA-256 for deterministic key derivation. The architecture follows a zero-trust model where a relay server facilitates communication without accessing plaintext messages or cryptographic keys. All encryption and decryption operations occur exclusively at client endpoints. The system demonstrates that NIST standardized post-quantum cryptography can be effectively integrated into practical messaging systems with acceptable performance characteristics, offering protection against both classical and quantum adversaries. As our focus is on implementation rather than on novelty, we also provide an open-source implementation to facilitate reproducibility and further research in post quantum secure communication systems.", "published": "2026-01-21T12:17:24Z", "updated": "2026-01-21T12:17:24Z", "authors": ["Aditi Gandhi", "Aakankshya Das", "Aswani Kumar Cherukuri"], "pdf_url": "https://arxiv.org/pdf/2601.14926v1"}
{"id": "http://arxiv.org/abs/2509.06136v2", "title": "\"Abuse Risks are Often Inherent to Product Features\": Exploring AI Vendors' Bug Bounty and Responsible Disclosure Policies", "summary": "As vendors adopt AI technologies, security researchers are working to uncover and fix related vulnerabilities, which is important given AI systems handle sensitive data and critical functions. This process relies on vendors receiving and rewarding AI vulnerability reports. To assess current practices, we analyzed the vulnerability disclosure policies of 264 AI vendors. We employed a mixed-methods approach, combining snapshot and longitudinal qualitative analysis, as well as comparing alignment with 320 AI incidents and 260 academic articles. Our analysis reveals that 36% of AI vendors have no established policy, and only 18% mention AI risks. Data access, authorization, and model extraction vulnerabilities are most consistently declared in-scope. Jailbreaking and hallucination are most commonly declared out-of-scope. We identify three profiles that reflect vendors' different positions toward AI vulnerabilities: proactive clarification (n = 46), silent (n = 115), and restrictive (n = 103). Our alignment results suggest that vendors may address AI vulnerability disclosure later than academic research and real-world incidents.", "published": "2025-09-07T16:44:28Z", "updated": "2026-01-21T10:55:51Z", "authors": ["Yangheran Piao", "Jingjie Li", "Daniel W. Woods"], "pdf_url": "https://arxiv.org/pdf/2509.06136v2"}
{"id": "http://arxiv.org/abs/2506.02674v2", "title": "Decentralized COVID-19 Health System Leveraging Blockchain", "summary": "With the development of the Internet, the amount of data generated by the medical industry each year has grown exponentially. The Electronic Health Record (EHR) manages the electronic data generated during the user's treatment process. Typically, an EHR data manager belongs to a medical institution. This traditional centralized data management model has many unreasonable or inconvenient aspects, such as difficulties in data sharing, and it is hard to verify the authenticity and integrity of the data. The decentralized, non-forgeable, data unalterable and traceable features of blockchain are in line with the application requirements of EHR. This paper takes the most common COVID-19 as the application scenario and designs a COVID-19 health system based on blockchain, which has extensive research and application value. Considering that the public and transparent nature of blockchain violates the privacy requirements of some health data, in the system design stage, from the perspective of practical application, the data is divided into public data and private data according to its characteristics. For private data, data encryption methods are adopted to ensure data privacy. The searchable encryption technology is combined with blockchain technology to achieve the retrieval function of encrypted data. Then, the proxy re-encryption technology is used to realize authorized access to data. In the system implementation part, based on the Hyperledger Fabric architecture, some functions of the system design are realized, including data upload, retrieval of the latest data and historical data. According to the environment provided by the development architecture, Go language chaincode (smart contract) is written to implement the relevant system functions.", "published": "2025-06-03T09:19:47Z", "updated": "2026-01-21T10:32:20Z", "authors": ["Lingsheng Chen", "Shipeng Ye", "Xiaoqi Li"], "pdf_url": "https://arxiv.org/pdf/2506.02674v2"}
{"id": "http://arxiv.org/abs/2601.14778v1", "title": "STEAD: Robust Provably Secure Linguistic Steganography with Diffusion Language Model", "summary": "Recent provably secure linguistic steganography (PSLS) methods rely on mainstream autoregressive language models (ARMs) to address historically challenging tasks, that is, to disguise covert communication as ``innocuous'' natural language communication. However, due to the characteristic of sequential generation of ARMs, the stegotext generated by ARM-based PSLS methods will produce serious error propagation once it changes, making existing methods unavailable under an active tampering attack. To address this, we propose a robust, provably secure linguistic steganography with diffusion language models (DLMs). Unlike ARMs, DLMs can generate text in a partially parallel manner, allowing us to find robust positions for steganographic embedding that can be combined with error-correcting codes. Furthermore, we introduce error correction strategies, including pseudo-random error correction and neighborhood search correction, during steganographic extraction. Theoretical proof and experimental results demonstrate that our method is secure and robust. It can resist token ambiguity in stegotext segmentation and, to some extent, withstand token-level attacks of insertion, deletion, and substitution.", "published": "2026-01-21T08:58:12Z", "updated": "2026-01-21T08:58:12Z", "authors": ["Yuang Qi", "Na Zhao", "Qiyi Yao", "Benlong Wu", "Weiming Zhang", "Nenghai Yu", "Kejiang Chen"], "pdf_url": "https://arxiv.org/pdf/2601.14778v1"}
{"id": "http://arxiv.org/abs/2601.13569v2", "title": "DRGW: Learning Disentangled Representations for Robust Graph Watermarking", "summary": "Graph-structured data is foundational to numerous web applications, and watermarking is crucial for protecting their intellectual property and ensuring data provenance. Existing watermarking methods primarily operate on graph structures or entangled graph representations, which compromise the transparency and robustness of watermarks due to the information coupling in representing graphs and uncontrollable discretization in transforming continuous numerical representations into graph structures. This motivates us to propose DRGW, the first graph watermarking framework that addresses these issues through disentangled representation learning. Specifically, we design an adversarially trained encoder that learns an invariant structural representation against diverse perturbations and derives a statistically independent watermark carrier, ensuring both robustness and transparency of watermarks. Meanwhile, we devise a graph-aware invertible neural network to provide a lossless channel for watermark embedding and extraction, guaranteeing high detectability and transparency of watermarks. Additionally, we develop a structure-aware editor that resolves the issue of latent modifications into discrete graph edits, ensuring robustness against structural perturbations. Experiments on diverse benchmark datasets demonstrate the superior effectiveness of DRGW.", "published": "2026-01-20T03:55:18Z", "updated": "2026-01-21T07:29:25Z", "authors": ["Jiasen Li", "Yanwei Liu", "Zhuoyi Shang", "Xiaoyan Gu", "Weiping Wang"], "pdf_url": "https://arxiv.org/pdf/2601.13569v2"}
{"id": "http://arxiv.org/abs/2601.14687v1", "title": "Beyond Denial-of-Service: The Puppeteer's Attack for Fine-Grained Control in Ranking-Based Federated Learning", "summary": "Federated Rank Learning (FRL) is a promising Federated Learning (FL) paradigm designed to be resilient against model poisoning attacks due to its discrete, ranking-based update mechanism. Unlike traditional FL methods that rely on model updates, FRL leverages discrete rankings as a communication parameter between clients and the server. This approach significantly reduces communication costs and limits an adversary's ability to scale or optimize malicious updates in the continuous space, thereby enhancing its robustness. This makes FRL particularly appealing for applications where system security and data privacy are crucial, such as web-based auction and bidding platforms. While FRL substantially reduces the attack surface, we demonstrate that it remains vulnerable to a new class of local model poisoning attack, i.e., fine-grained control attacks. We introduce the Edge Control Attack (ECA), the first fine-grained control attack tailored to ranking-based FL frameworks. Unlike conventional denial-of-service (DoS) attacks that cause conspicuous disruptions, ECA enables an adversary to precisely degrade a competitor's accuracy to any target level while maintaining a normal-looking convergence trajectory, thereby avoiding detection. ECA operates in two stages: (i) identifying and manipulating Ascending and Descending Edges to align the global model with the target model, and (ii) widening the selection boundary gap to stabilize the global model at the target accuracy. Extensive experiments across seven benchmark datasets and nine Byzantine-robust aggregation rules (AGRs) show that ECA achieves fine-grained accuracy control with an average error of only 0.224%, outperforming the baseline by up to 17x. Our findings highlight the need for stronger defenses against advanced poisoning attacks. Our code is available at: https://github.com/Chenzh0205/ECA", "published": "2026-01-21T06:03:11Z", "updated": "2026-01-21T06:03:11Z", "authors": ["Zhihao Chen", "Zirui Gong", "Jianting Ning", "Yanjun Zhang", "Leo Yu Zhang"], "pdf_url": "https://arxiv.org/pdf/2601.14687v1"}
{"id": "http://arxiv.org/abs/2501.09328v4", "title": "Neural Honeytrace: Plug&Play Watermarking Framework against Model Extraction Attacks", "summary": "Triggerable watermarking enables model owners to assert ownership against model extraction attacks. However, most existing approaches require additional training, which limits post-deployment flexibility, and the lack of clear theoretical foundations makes them vulnerable to adaptive attacks. In this paper, we propose Neural Honeytrace, a plug-and-play watermarking framework that operates without retraining. We redefine the watermark transmission mechanism from an information perspective, designing a training-free multi-step transmission strategy that leverages the long-tailed effect of backdoor learning to achieve efficient and robust watermark embedding. Extensive experiments demonstrate that Neural Honeytrace reduces the average number of queries required for a worst-case t-test-based ownership verification to as low as $2\\%$ of existing methods, while incurring zero training cost.", "published": "2025-01-16T06:59:20Z", "updated": "2026-01-21T05:34:49Z", "authors": ["Yixiao Xu", "Binxing Fang", "Rui Wang", "Yinghai Zhou", "Yuan Liu", "Mohan Li", "Zhihong Tian"], "pdf_url": "https://arxiv.org/pdf/2501.09328v4"}
{"id": "http://arxiv.org/abs/2601.14660v1", "title": "NeuroFilter: Privacy Guardrails for Conversational LLM Agents", "summary": "This work addresses the computational challenge of enforcing privacy for agentic Large Language Models (LLMs), where privacy is governed by the contextual integrity framework. Indeed, existing defenses rely on LLM-mediated checking stages that add substantial latency and cost, and that can be undermined in multi-turn interactions through manipulation or benign-looking conversational scaffolding. Contrasting this background, this paper makes a key observation: internal representations associated with privacy-violating intent can be separated from benign requests using linear structure. Using this insight, the paper proposes NeuroFilter, a guardrail framework that operationalizes contextual integrity by mapping norm violations to simple directions in the model's activation space, enabling detection even when semantic filters are bypassed. The proposed filter is also extended to capture threats arising during long conversations using the concept of activation velocity, which measures cumulative drift in internal representations across turns. A comprehensive evaluation across over 150,000 interactions and covering models from 7B to 70B parameters, illustrates the strong performance of NeuroFilter in detecting privacy attacks while maintaining zero false positives on benign prompts, all while reducing the computational inference cost by several orders of magnitude when compared to LLM-based agentic privacy defenses.", "published": "2026-01-21T05:16:50Z", "updated": "2026-01-21T05:16:50Z", "authors": ["Saswat Das", "Ferdinando Fioretto"], "pdf_url": "https://arxiv.org/pdf/2601.14660v1"}
{"id": "http://arxiv.org/abs/2601.08223v3", "title": "DNF: Dual-Layer Nested Fingerprinting for Large Language Model Intellectual Property Protection", "summary": "The rapid growth of large language models raises pressing concerns about intellectual property protection under black-box deployment. Existing backdoor-based fingerprints either rely on rare tokens -- leading to high-perplexity inputs susceptible to filtering -- or use fixed trigger-response mappings that are brittle to leakage and post-hoc adaptation. We propose \\textsc{Dual-Layer Nested Fingerprinting} (DNF), a black-box method that embeds a hierarchical backdoor by coupling domain-specific stylistic cues with implicit semantic triggers. Across Mistral-7B, LLaMA-3-8B-Instruct, and Falcon3-7B-Instruct, DNF achieves perfect fingerprint activation while preserving downstream utility. Compared with existing methods, it uses lower-perplexity triggers, remains undetectable under fingerprint detection attacks, and is relatively robust to incremental fine-tuning and model merging. These results position DNF as a practical, stealthy, and resilient solution for LLM ownership verification and intellectual property protection.", "published": "2026-01-13T05:05:37Z", "updated": "2026-01-21T05:03:08Z", "authors": ["Zhenhua Xu", "Yiran Zhao", "Mengting Zhong", "Dezhang Kong", "Changting Lin", "Tong Qiao", "Meng Han"], "pdf_url": "https://arxiv.org/pdf/2601.08223v3"}
{"id": "http://arxiv.org/abs/2601.12986v2", "title": "KinGuard: Hierarchical Kinship-Aware Fingerprinting to Defend Against Large Language Model Stealing", "summary": "Protecting the intellectual property of large language models requires robust ownership verification. Conventional backdoor fingerprinting, however, is flawed by a stealth-robustness paradox: to be robust, these methods force models to memorize fixed responses to high-perplexity triggers, but this targeted overfitting creates detectable statistical artifacts. We resolve this paradox with KinGuard, a framework that embeds a private knowledge corpus built on structured kinship narratives. Instead of memorizing superficial triggers, the model internalizes this knowledge via incremental pre-training, and ownership is verified by probing its conceptual understanding. Extensive experiments demonstrate KinGuard's superior effectiveness, stealth, and resilience against a battery of attacks including fine-tuning, input perturbation, and model merging. Our work establishes knowledge-based embedding as a practical and secure paradigm for model fingerprinting.", "published": "2026-01-19T12:06:20Z", "updated": "2026-01-21T04:51:19Z", "authors": ["Zhenhua Xu", "Xiaoning Tian", "Wenjun Zeng", "Wenpeng Xing", "Tianliang Lu", "Gaolei Li", "Chaochao Chen", "Meng Han"], "pdf_url": "https://arxiv.org/pdf/2601.12986v2"}
{"id": "http://arxiv.org/abs/2601.14614v1", "title": "Towards Cybersecurity Superintelligence: from AI-guided humans to human-guided AI", "summary": "Cybersecurity superintelligence -- artificial intelligence exceeding the best human capability in both speed and strategic reasoning -- represents the next frontier in security. This paper documents the emergence of such capability through three major contributions that have pioneered the field of AI Security. First, PentestGPT (2023) established LLM-guided penetration testing, achieving 228.6% improvement over baseline models through an architecture that externalizes security expertise into natural language guidance. Second, Cybersecurity AI (CAI, 2025) demonstrated automated expert-level performance, operating 3,600x faster than humans while reducing costs 156-fold, validated through #1 rankings at international competitions including the $50,000 Neurogrid CTF prize. Third, Generative Cut-the-Rope (G-CTR, 2026) introduces a neurosymbolic architecture embedding game-theoretic reasoning into LLM-based agents: symbolic equilibrium computation augments neural inference, doubling success rates while reducing behavioral variance 5.2x and achieving 2:1 advantage over non-strategic AI in Attack & Defense scenarios.\n  Together, these advances establish a clear progression from AI-guided humans to human-guided game-theoretic cybersecurity superintelligence.", "published": "2026-01-21T03:12:48Z", "updated": "2026-01-21T03:12:48Z", "authors": ["Víctor Mayoral-Vilches", "Stefan Rass", "Martin Pinzger", "Endika Gil-Uriarte", "Unai Ayucar-Carbajo", "Jon Ander Ruiz-Alcalde", "Maite del Mundo de Torres", "Luis Javier Navarrete-Lozano", "María Sanz-Gómez", "Francesco Balassone", "Cristóbal R. J. Veas-Chavez", "Vanesa Turiel", "Alfonso Glera-Picón", "Daniel Sánchez-Prieto", "Yuri Salvatierra", "Paul Zabalegui-Landa", "Ruffino Reydel Cabrera-Álvarez", "Patxi Mayoral-Pizarroso"], "pdf_url": "https://arxiv.org/pdf/2601.14614v1"}
{"id": "http://arxiv.org/abs/2508.06837v2", "title": "Towards Effective Prompt Stealing Attack against Text-to-Image Diffusion Models", "summary": "Text-to-Image (T2I) models, represented by DALL$\\cdot$E and Midjourney, have gained huge popularity for creating realistic images. The quality of these images relies on the carefully engineered prompts, which have become valuable intellectual property. While skilled prompters showcase their AI-generated art on markets to attract buyers, this business incidentally exposes them to \\textit{prompt stealing attacks}. Existing state-of-the-art attack techniques reconstruct the prompts from a fixed set of modifiers (i.e., style descriptions) with model-specific training, which exhibit restricted adaptability and effectiveness to diverse showcases (i.e., target images) and diffusion models.\n  To alleviate these limitations, we propose Prometheus, a training-free, proxy-in-the-loop, search-based prompt-stealing attack, which reverse-engineers the valuable prompts of the showcases by interacting with a local proxy model. It consists of three innovative designs. First, we introduce dynamic modifiers, as a supplement to static modifiers used in prior works. These dynamic modifiers provide more details specific to the showcases, and we exploit NLP analysis to generate them on the fly. Second, we design a contextual matching algorithm to sort both dynamic and static modifiers. This offline process helps reduce the search space of the subsequent step. Third, we interact with a local proxy model to invert the prompts with a greedy search algorithm. Based on the feedback guidance, we refine the prompt to achieve higher fidelity. The evaluation results show that Prometheus successfully extracts prompts from popular platforms like PromptBase and AIFrog against diverse victim models, including Midjourney, Leonardo.ai, and DALL$\\cdot$E, with an ASR improvement of 25.0\\%. We also validate that Prometheus is resistant to extensive potential defenses, further highlighting its severity in practice.", "published": "2025-08-09T05:38:38Z", "updated": "2026-01-21T02:45:23Z", "authors": ["Shiqian Zhao", "Chong Wang", "Yiming Li", "Yihao Huang", "Wenjie Qu", "Siew-Kei Lam", "Yi Xie", "Kangjie Chen", "Jie Zhang", "Tianwei Zhang"], "pdf_url": "https://arxiv.org/pdf/2508.06837v2"}
{"id": "http://arxiv.org/abs/2601.14606v1", "title": "An LLM Agent-based Framework for Whaling Countermeasures", "summary": "With the spread of generative AI in recent years, attacks known as Whaling have become a serious threat. Whaling is a form of social engineering that targets important high-authority individuals within organizations and uses sophisticated fraudulent emails. In the context of Japanese universities, faculty members frequently hold positions that combine research leadership with authority within institutional workflows. This structural characteristic leads to the wide public disclosure of high-value information such as publications, grants, and detailed researcher profiles. Such extensive information exposure enables the construction of highly precise target profiles using generative AI. This raises concerns that Whaling attacks based on high-precision profiling by generative AI will become prevalent. In this study, we propose a Whaling countermeasure framework for university faculty members that constructs personalized defense profiles and uses large language model (LLM)-based agents. We design agents that (i) build vulnerability profiles for each target from publicly available information on faculty members, (ii) identify potential risk scenarios relevant to Whaling defense based on those profiles, (iii) construct defense profiles corresponding to the vulnerabilities and anticipated risks, and (iv) analyze Whaling emails using the defense profiles. Furthermore, we conduct a preliminary risk-assessment experiment. The results indicate that the proposed method can produce judgments accompanied by explanations of response policies that are consistent with the work context of faculty members who are Whaling targets. The findings also highlight practical challenges and considerations for future operational deployment and systematic evaluation.", "published": "2026-01-21T02:43:42Z", "updated": "2026-01-21T02:43:42Z", "authors": ["Daisuke Miyamoto", "Takuji Iimura", "Narushige Michishita"], "pdf_url": "https://arxiv.org/pdf/2601.14606v1"}
{"id": "http://arxiv.org/abs/2601.14601v1", "title": "Holmes: An Evidence-Grounded LLM Agent for Auditable DDoS Investigation in Cloud Networks", "summary": "Cloud environments face frequent DDoS threats due to centralized resources and broad attack surfaces. Modern cloud-native DDoS attacks further evolve rapidly and often blend multi-vector strategies, creating an operational dilemma: defenders need wire-speed monitoring while also requiring explainable, auditable attribution for response. Existing rule-based and supervised-learning approaches typically output black-box scores or labels, provide limited evidence chains, and generalize poorly to unseen attack variants; meanwhile, high-quality labeled data is often difficult to obtain in cloud settings.\n  We present Holmes (DDoS Detective), an LLM-based DDoS detection agent that reframes the model as a virtual SRE investigator rather than an end-to-end classifier. Holmes couples a funnel-like hierarchical workflow (counters/sFlow for continuous sensing and triage; PCAP evidence collection triggered only on anomaly windows) with an Evidence Pack abstraction that converts binary packets into compact, reproducible, high-signal structured evidence. On top of this evidence interface, Holmes enforces a structure-first investigation protocol and strict JSON/quotation constraints to produce machine-consumable reports with auditable evidence anchors.\n  We evaluate Holmes on CICDDoS2019 reflection/amplification attacks and script-triggered flooding scenarios. Results show that Holmes produces attribution decisions grounded in salient evidence anchors across diverse attack families, and when errors occur, its audit logs make the failure source easy to localize, demonstrating the practicality of an LLM agent for cost-controlled and traceable DDoS investigation in cloud operations.", "published": "2026-01-21T02:39:46Z", "updated": "2026-01-21T02:39:46Z", "authors": ["Haodong Chen", "Ziheng Zhang", "Jinghui Jiang", "Qiang Su", "Qiao Xiang"], "pdf_url": "https://arxiv.org/pdf/2601.14601v1"}
{"id": "http://arxiv.org/abs/2601.14597v1", "title": "Optimality of Staircase Mechanisms for Vector Queries under Differential Privacy", "summary": "We study the optimal design of additive mechanisms for vector-valued queries under $ε$-differential privacy (DP). Given only the sensitivity of a query and a norm-monotone cost function measuring utility loss, we ask which noise distribution minimizes expected cost among all additive $ε$-DP mechanisms. Using convex rearrangement theory, we show that this infinite-dimensional optimization problem admits a reduction to a one-dimensional compact and convex family of radially symmetric distributions whose extreme points are the staircase distributions. As a consequence, we prove that for any dimension, any norm, and any norm-monotone cost function, there exists an $ε$-DP staircase mechanism that is optimal among all additive mechanisms. This result resolves a conjecture of Geng, Kairouz, Oh, and Viswanath, and provides a geometric explanation for the emergence of staircase mechanisms as extremal solutions in differential privacy.", "published": "2026-01-21T02:35:33Z", "updated": "2026-01-21T02:35:33Z", "authors": ["James Melbourne", "Mario Diaz", "Shahab Asoodeh"], "pdf_url": "https://arxiv.org/pdf/2601.14597v1"}
{"id": "http://arxiv.org/abs/2601.14595v1", "title": "IntelliSA: An Intelligent Static Analyzer for IaC Security Smell Detection Using Symbolic Rules and Neural Inference", "summary": "Infrastructure as Code (IaC) enables automated provisioning of large-scale cloud and on-premise environments, reducing the need for repetitive manual setup. However, this automation is a double-edged sword: a single misconfiguration in IaC scripts can propagate widely, leading to severe system downtime and security risks. Prior studies have shown that IaC scripts often contain security smells--bad coding patterns that may introduce vulnerabilities--and have proposed static analyzers based on symbolic rules to detect them. Yet, our preliminary analysis reveals that rule-based detection alone tends to over-approximate, producing excessive false positives and increasing the burden of manual inspection. In this paper, we present IntelliSA, an intelligent static analyzer for IaC security smell detection that integrates symbolic rules with neural inference. IntelliSA applies symbolic rules to over-approximate potential smells for broad coverage, then employs neural inference to filter false positives. While an LLM can effectively perform this filtering, reliance on LLM APIs introduces high cost and latency, raises data governance concerns, and limits reproducibility and offline deployment. To address the challenges, we adopt a knowledge distillation approach: an LLM teacher generates pseudo-labels to train a compact student model--over 500x smaller--that learns from the teacher's knowledge and efficiently classifies false positives. We evaluate IntelliSA against two static analyzers and three LLM baselines (Claude-4, Grok-4, and GPT-5) using a human-labeled dataset including 241 security smells across 11,814 lines of real-world IaC code. Experimental results show that IntelliSA achieves the highest F1 score (83%), outperforming baselines by 7-42%. Moreover, IntelliSA demonstrates the best cost-effectiveness, detecting 60% of security smells while inspecting less than 2% of the codebase.", "published": "2026-01-21T02:27:54Z", "updated": "2026-01-21T02:27:54Z", "authors": ["Qiyue Mei", "Michael Fu"], "pdf_url": "https://arxiv.org/pdf/2601.14595v1"}
{"id": "http://arxiv.org/abs/2601.14582v1", "title": "Automatically Tightening Access Control Policies with Restricter", "summary": "Robust access control is a cornerstone of secure software, systems, and networks. An access control mechanism is as effective as the policy it enforces. However, authoring effective policies that satisfy desired properties such as the principle of least privilege is a challenging task even for experienced administrators, as evidenced by many real instances of policy misconfiguration. In this paper, we set out to address this pain point by proposing Restricter, which automatically tightens each (permit) policy rule of a policy with respect to an access log, which captures some already exercised access requests and their corresponding access decisions (i.e., allow or deny). Restricter achieves policy tightening by reducing the number of access requests permitted by a policy rule without sacrificing the functionality of the underlying system it is regulating. We implement Restricter for Amazon's Cedar policy language and demonstrate its effectiveness through two realistic case studies.", "published": "2026-01-21T01:42:05Z", "updated": "2026-01-21T01:42:05Z", "authors": ["Ka Lok Wu", "Christa Jenkins", "Scott D. Stolle", "Omar Chowdhury"], "pdf_url": "https://arxiv.org/pdf/2601.14582v1"}
{"id": "http://arxiv.org/abs/2601.14567v1", "title": "Agent Identity URI Scheme: Topology-Independent Naming and Capability-Based Discovery for Multi-Agent Systems", "summary": "Multi-agent systems face a fundamental architectural flaw: agent identity is bound to network location. When agents migrate between providers, scale across instances, or federate across organizations, URI-based identity schemes break references, fragment audit trails, and require centralized coordination. We propose the agent:// URI scheme, which decouples identity from topology through three orthogonal components: a trust root establishing organizational authority, a hierarchical capability path enabling semantic discovery, and a sortable unique identifier providing stable reference. The scheme enables capability-based discovery through DHT key derivation, where queries return agents by what they do rather than where they are. Trust-root scoping prevents cross-organization pollution while permitting federation when desired. Cryptographic attestation via PASETO tokens binds capability claims to agent identity, enabling verification without real-time contact with the issuing authority. We evaluate the scheme across four dimensions: capability expressiveness (100% coverage on 369 production tools with zero collision), discovery precision (F1=1.0 across 10,000 agents), identity stability (formal proofs of migration invariance), and performance (all operations under 5 microseconds). The agent:// URI scheme provides a formally-specified, practically-evaluated foundation for decentralized agent identity and capability-based discovery.", "published": "2026-01-21T01:09:22Z", "updated": "2026-01-21T01:09:22Z", "authors": ["Roland R. Rodriguez"], "pdf_url": "https://arxiv.org/pdf/2601.14567v1"}
{"id": "http://arxiv.org/abs/2504.19567v2", "title": "GenPTW: Latent Image Watermarking for Provenance Tracing and Tamper Localization", "summary": "The proliferation of generative image models has revolutionized AIGC creation while amplifying concerns over content provenance and manipulation forensics. Existing methods are typically either unable to localize tampering or restricted to specific generative settings, limiting their practical utility. We propose \\textbf{GenPTW}, a \\textbf{Gen}eral watermarking framework that unifies \\textbf{P}rovenance tracing and \\textbf{T}amper localization in latent space. It supports both in-generation and post-generation embedding without altering the generative process, and is plug-and-play compatible with latent diffusion models (LDMs) and visual autoregressive (VAR) models. To achieve precise provenance tracing and tamper localization, we embed the watermark using two complementary mechanisms: cross-attention fusion aligned with latent semantics and spatial fusion providing explicit spatial guidance for edit sensitivity. A tamper-aware extractor jointly conducts provenance tracing and tamper localization by leveraging watermark features together with high-frequency features. Experiments show that GenPTW maintains high visual fidelity and strong robustness against diverse AIGC-editing.", "published": "2025-04-28T08:21:39Z", "updated": "2026-01-21T00:46:11Z", "authors": ["Zhenliang Gan", "Chunya Liu", "Yichao Tang", "Binghao Wang", "Shiwen Cui", "Weiqiang Wang", "Xinpeng Zhang"], "pdf_url": "https://arxiv.org/pdf/2504.19567v2"}
{"id": "http://arxiv.org/abs/2601.14556v1", "title": "Constructing Multi-label Hierarchical Classification Models for MITRE ATT&CK Text Tagging", "summary": "MITRE ATT&CK is a cybersecurity knowledge base that organizes threat actor and cyber-attack information into a set of tactics describing the reasons and goals threat actors have for carrying out attacks, with each tactic having a set of techniques that describe the potential methods used in these attacks. One major application of ATT&CK is the use of its tactic and technique hierarchy by security specialists as a framework for annotating cyber-threat intelligence reports, vulnerability descriptions, threat scenarios, inter alia, to facilitate downstream analyses. To date, the tagging process is still largely done manually. In this technical note, we provide a stratified \"task space\" characterization of the MITRE ATT&CK text tagging task for organizing previous efforts toward automation using AIML methods, while also clarifying pathways for constructing new methods. To illustrate one of the pathways, we use the task space strata to stage-wise construct our own multi-label hierarchical classification models for the text tagging task via experimentation over general cyber-threat intelligence text -- using shareable computational tools and publicly releasing the models to the security community (via https://github.com/jpmorganchase/MITRE_models). Our multi-label hierarchical approach yields accuracy scores of roughly 94% at the tactic level, as well as accuracy scores of roughly 82% at the technique level. The models also meet or surpass state-of-the-art performance while relying only on classical machine learning methods -- removing any dependence on LLMs, RAG, agents, or more complex hierarchical approaches. Moreover, we show that GPT-4o model performance at the tactic level is significantly lower (roughly 60% accuracy) than our own approach. We also extend our baseline model to a corpus of threat scenarios for financial applications produced by subject matter experts.", "published": "2026-01-21T00:41:34Z", "updated": "2026-01-21T00:41:34Z", "authors": ["Andrew Crossman", "Jonah Dodd", "Viralam Ramamurthy Chaithanya Kumar", "Riyaz Mohammed", "Andrew R. Plummer", "Chandra Sekharudu", "Deepak Warrier", "Mohammad Yekrangian"], "pdf_url": "https://arxiv.org/pdf/2601.14556v1"}
{"id": "http://arxiv.org/abs/2601.14555v1", "title": "WebAssembly Based Portable and Secure Sensor Interface for Internet of Things", "summary": "As the expansion of IoT connectivity continues to provide quality-of-life improvements around the world, they simultaneously introduce increasing privacy and security concerns. The lack of a clear definition in managing shared and protected access to IoT sensors offer channels by which devices can be compromised and sensitive data can be leaked. In recent years, WebAssembly has received considerable attention for its efficient application sandboxing suitable for embedded systems, making it a prime candidate for exploring a secure and portable sensor interface. This paper introduces the first WebAssembly System Interface (WASI) extension offering a secure, portable, and low-footprint sandbox enabling multi-tenant access to sensor data across heterogeneous embedded devices. The runtime extensions provide application memory isolation, ensure appropriate resource privileges by intercepting sensor access, and offer an MQTT-SN interface enabling in-network access control. When targeting the WebAssembly byte-code with the associated runtime extensions implemented atop the Zephyr RTOS, our evaluation of sensor access indicates a latency overhead of 6% with an additional memory footprint of 5% when compared to native execution. As MQTT-SN requests are dominated by network delays, the WASI-SN implementation of MQTT-SN introduces less than 1% additional latency with similar memory footprint.", "published": "2026-01-21T00:36:58Z", "updated": "2026-01-21T00:36:58Z", "authors": ["Botong Ou", "Baijian Yang"], "pdf_url": "https://arxiv.org/pdf/2601.14555v1"}
