{"id": "http://arxiv.org/abs/2602.22020v1", "title": "Detecting UX smells in Visual Studio Code using LLMs", "summary": "Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concentrated in informativeness, clarity, intuitiveness, and efficiency, qualities that developers value most.", "published": "2026-02-25T15:32:17Z", "updated": "2026-02-25T15:32:17Z", "authors": ["Andrés Rodriguez", "Juan Cruz Gardey", "Alejandra Garrido"], "pdf_url": "https://arxiv.org/pdf/2602.22020v1"}
{"id": "http://arxiv.org/abs/2602.21997v1", "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code", "summary": "Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods.", "published": "2026-02-25T15:16:43Z", "updated": "2026-02-25T15:16:43Z", "authors": ["WeiZhe Xu", "Mengyu Liu", "Fanxin Kong"], "pdf_url": "https://arxiv.org/pdf/2602.21997v1"}
{"id": "http://arxiv.org/abs/2506.08980v4", "title": "Towards Better Code Generation: Adaptive Decoding with Uncertainty Guidance", "summary": "The success of code synthesis using large language models (LLMs) depends heavily on navigating critical decision points during the decoding process. Standard uniform strategies, such as greedy decoding, often fall short because they fail to distinguish between deterministic steps and those characterized by high logical ambiguity. Our empirical analysis identifies a recurring failure mode: \"logic drift\" caused by the model's inability to correctly rank viable candidates during high-uncertainty intervals, even when the ground-truth token is available.\n  To resolve this, we present AdaDec, a framework that introduces a selective pause-then-rerank mechanism into the decoding pipeline. Unlike static methods, AdaDec utilizes learned, model-specific entropy thresholds to identify when the model is \"confused\" and dynamically triggers a lookahead-based evaluation to re-score candidate tokens.\n  Across benchmarks including HumanEval+, MBPP+, and DevEval, AdaDec achieves significant performance breakthroughs, boosting Pass@1 accuracy by up to 20.9% absolute over greedy decoding. The framework not only surpasses traditional Beam Search and specialized methods like AdapT in terms of reliability but also maintains high inference efficiency by intervening only at the most consequential steps. These results suggest that uncertainty-aware adaptive strategies are key to making LLM-driven code generation both robust and practical.", "published": "2025-06-10T16:49:46Z", "updated": "2026-02-25T15:08:15Z", "authors": ["Kaifeng He", "Mingwei Liu", "Chong Wang", "Zike Li", "Yanlin Wang", "Xin Peng", "Zibin Zheng"], "pdf_url": "https://arxiv.org/pdf/2506.08980v4"}
{"id": "http://arxiv.org/abs/2509.11787v3", "title": "CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings", "summary": "Static analysis tools are widely used to detect bugs, vulnerabilities, and code smells. Traditionally, developers must resolve these warnings manually. Because this process is tedious, developers sometimes ignore warnings, leading to an accumulation of warnings and a degradation of code quality. This paper presents CodeCureAgent, an approach that harnesses LLM-based agents to automatically analyze, classify, and repair static analysis warnings. Unlike previous work, our method does not follow a predetermined algorithm. Instead, we adopt an agentic framework that iteratively invokes tools to gather additional information from the codebase (e.g., via code search) and edit the codebase to resolve the warning. CodeCureAgent detects and suppresses false positives, while fixing true positives when identified. We equip CodeCureAgent with a three-step heuristic to approve patches: (1) build the project, (2) verify that the warning disappears without introducing new warnings, and (3) run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube warnings found in 106 Java projects and covering 291 distinct rules. Our approach produces plausible fixes for 96.8% of the warnings, outperforming state-of-the-art baseline approaches by 29.2%-34.0% in plausible-fix rate. Manual inspection of 291 cases reveals a correct-fix rate of 86.3%, showing that CodeCureAgent can reliably repair static analysis warnings. The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end processing time of about four minutes per warning. We envision CodeCureAgent helping to clean existing codebases and being integrated into CI/CD pipelines to prevent the accumulation of static analysis warnings.", "published": "2025-09-15T11:16:04Z", "updated": "2026-02-25T12:42:03Z", "authors": ["Pascal Joos", "Islem Bouzenia", "Michael Pradel"], "pdf_url": "https://arxiv.org/pdf/2509.11787v3"}
{"id": "http://arxiv.org/abs/2602.21833v1", "title": "From Restructuring to Stabilization: A Large-Scale Experiment on Iterative Code Readability Refactoring with Large Language Models", "summary": "Large language models (LLMs) are increasingly used for automated code refactoring tasks. Although these models can quickly refactor code, the quality may exhibit inconsistencies and unpredictable behavior. In this article, we systematically study the capabilities of LLMs for code refactoring with a specific focus on improving code readability.\n  We conducted a large-scale experiment using GPT5.1 with 230 Java snippets, each systematically varied and refactored regarding code readability across five iterations under three different prompting strategies. We categorized fine-grained code changes during the refactoring into implementation, syntactic, and comment-level transformations. Subsequently, we investigated the functional correctness and tested the robustness of the results with novel snippets.\n  Our results reveal three main insights: First, iterative code refactoring exhibits an initial phase of restructuring followed by stabilization. This convergence tendency suggests that LLMs possess an internalized understanding of an \"optimally readable\" version of code. Second, convergence patterns are fairly robust across different code variants. Third, explicit prompting toward specific readability factors slightly influences the refactoring dynamics.\n  These insights provide an empirical foundation for assessing the reliability of LLM-assisted code refactoring, which opens pathways for future research, including comparative analyses across models and a systematic evaluation of additional software quality dimensions in LLM-refactored code.", "published": "2026-02-25T12:05:25Z", "updated": "2026-02-25T12:05:25Z", "authors": ["Norman Peitek", "Julia Hess", "Sven Apel"], "pdf_url": "https://arxiv.org/pdf/2602.21833v1"}
{"id": "http://arxiv.org/abs/2602.21806v1", "title": "An Empirical Study of Bugs in Modern LLM Agent Frameworks", "summary": "LLM agents have been widely adopted in real-world applications, relying on agent frameworks for workflow execution and multi-agent coordination. As these systems scale, understanding bugs in the underlying agent frameworks becomes critical. However, existing work mainly focuses on agent-level failures, overlooking framework-level bugs. To address this gap, we conduct an empirical study of 998 bug reports from CrewAI and LangChain, constructing a taxonomy of 15 root causes and 7 observable symptoms across five agent lifecycle stages: 'Agent Initialization','Perception', 'Self-Action', 'Mutual Interaction' and 'Evolution'. Our findings show that agent framework bugs mainly arise from 'API misuse', 'API incompatibility', and 'Documentation Desync', largely concentrated in the 'Self-Action' stage. Symptoms typically appear as 'Functional Error', 'Crash', and 'Build Failure', reflecting disruptions to task progression and control flow.", "published": "2026-02-25T11:34:17Z", "updated": "2026-02-25T11:34:17Z", "authors": ["Xinxue Zhu", "Jiacong Wu", "Xiaoyu Zhang", "Tianlin Li", "Yanzhou Mu", "Juan Zhai", "Chao Shen", "Yang Liu"], "pdf_url": "https://arxiv.org/pdf/2602.21806v1"}
{"id": "http://arxiv.org/abs/2602.21800v1", "title": "An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention", "summary": "The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained by fixed context lengths, limiting its ability to generalize across long, domain-specific code sequences. To address this challenge, we investigate zero-shot, inference-only methods aimed at improving position encodings and optimizing attention mechanisms. Our goal is to provide a thorough analysis of current approaches that facilitate context length extrapolation in code, particularly in the context of long code completion tasks.", "published": "2026-02-25T11:27:34Z", "updated": "2026-02-25T11:27:34Z", "authors": ["Madhusudan Ghosh", "Rishabh Gupta"], "pdf_url": "https://arxiv.org/pdf/2602.21800v1"}
{"id": "http://arxiv.org/abs/2602.21734v1", "title": "Proto-ML: An IDE for ML Solution Prototyping", "summary": "Prototyping plays a critical role in the development of machine learning (ML) solutions, yet existing tools often provide limited support for effective collaboration and knowledge reuse among stakeholders. This paper introduces Proto-ML, an IDE designed to strengthen ML prototyping workflows. By addressing key deficiencies such as insufficient stakeholder involvement, limited cross-project knowledge reuse, and fragmented tool support, Proto-ML offers a unified framework that enables structured documentation of prototyping activities and promotes knowledge sharing across projects.\n  The Proto-ML IDE consists of three extension bundles: prototype implementation, analysis, and knowledge management. These extensions support tasks ranging from evaluating prototype quality against defined criteria to incorporating stakeholder perspectives throughout the development process. Preliminary user feedback suggests that Proto-ML can increase prototyping efficiency and foster more transparent and reusable ML solution development.", "published": "2026-02-25T09:43:56Z", "updated": "2026-02-25T09:43:56Z", "authors": ["Selin Coban", "Miguel Perez", "Horst Lichter"], "pdf_url": "https://arxiv.org/pdf/2602.21734v1"}
{"id": "http://arxiv.org/abs/2602.16291v3", "title": "A Calculus of Inheritance", "summary": "Just as the $λ$-calculus uses three primitives (abstraction, application, variable) as the foundation of functional programming, inheritance-calculus uses three primitives (record, definition, inheritance) as the foundation of declarative programming. It trivially embeds the $λ$-calculus, although the entire semantics rests solely on naive set theory; as a consequence, all constructs including inheritance are inherently commutative, idempotent, and associative; the linearization problem of multiple inheritance does not arise. This induces a fully abstract semantics of the lazy $λ$-calculus with respect to Böhm tree equivalence~\\cite{barendregt1984lambda}. Inheritance-calculus is distilled from MIXINv2, a practical implementation in which we observed further emergent phenomena: the same code acts as different function colors~\\cite{nystrom2015color}; ordinary arithmetic yields the relational semantics of logic programming~\\cite{vanemden1976semantics}; self-reference resolves to multiple targets; and programs are immune to the Expression Problem~\\cite{wadler1998expression}. This makes inheritance-calculus strictly more expressive than the $λ$-calculus in both common sense and Felleisen's sense~\\cite{felleisen1991expressive}. These properties suggest applications to configuration languages, dependency injection, object-oriented programming, composable effect systems, modular software architectures, file-system-as-compiler, general-purpose programming, and no-code development.", "published": "2026-02-18T09:17:20Z", "updated": "2026-02-25T09:21:01Z", "authors": ["Bo Yang"], "pdf_url": "https://arxiv.org/pdf/2602.16291v3"}
{"id": "http://arxiv.org/abs/2602.21697v1", "title": "EditFlow: Benchmarking and Optimizing Code Edit Recommendation Systems via Reconstruction of Developer Flows", "summary": "Large language models (LLMs) for code editing have achieved remarkable progress, yet recent empirical studies reveal a fundamental disconnect between technical accuracy and developer productivity. Despite their strong benchmark performance, developers complete tasks 19% slower when using AI assistance, with over 68.81% of recommendations disrupting their mental flow. This misalignment stems from the use of static commit snapshots that lack temporal information, causing models to optimize for end results rather than the incremental, context-sensitive steps that align with developers' natural reasoning process.\n  To bridge this gap, we present EditFlow, which benchmarks and optimizes subsequent code edit recommendation systems through the reconstruction of developer editing flows. EditFlow addresses three key challenges. First, collecting edit-order data that reflects developers' flow is inherently difficult: manual annotation introduces prohibitive overhead, while development logs capture only single trajectories instead of all plausible editing flows. Second, benchmarking recommendation performance against developers' ongoing editing flow requires a digital-twin-like simulation that can faithfully simulate the editing process. Third, existing heterogeneous systems vary drastically in scale and architecture, posing challenges for developing a unified optimization strategy that endows all models with mental-flow awareness regardless of design or capability.\n  ......", "published": "2026-02-25T09:02:45Z", "updated": "2026-02-25T09:02:45Z", "authors": ["Chenyan Liu", "Yun Lin", "Jiaxin Chang", "Jiawei Liu", "Binhang Qi", "Bo Jiang", "Zhiyong Huang", "Jin Song Dong"], "pdf_url": "https://arxiv.org/pdf/2602.21697v1"}
{"id": "http://arxiv.org/abs/2602.21681v1", "title": "AkiraRust: Re-thinking LLM-aided Rust Repair Using a Feedback-guided Thinking Switch", "summary": "Eliminating undefined behaviors (UBs) in Rust programs requires a deep semantic understanding to enable accurate and reliable repair. While existing studies have demonstrated the potential of LLMs to support Rust code analysis and repair, most frameworks remain constrained by inflexible templates or lack grounding in executable semantics, resulting in limited contextual awareness and semantic incorrectness. Here, we present AkiraRust, an LLM-driven repair and verification framework that incorporates a finite-state machine to dynamically adapt its detection and repair flow to runtime semantic conditions. AkiraRust introduces a dual-mode reasoning strategy that coordinates fast and slow thinking across multiple agents. Each agent is mapped to an FSM state, and a waveform-driven transition controller manages state switching, rollback decisions, and semantic check pointing, enabling context-aware and runtime-adaptive repair. Experimental results show that AkiraRust achieves about 92% semantic correctness and delivers a 2.2x average speedup compared to SOTA.", "published": "2026-02-25T08:34:27Z", "updated": "2026-02-25T08:34:27Z", "authors": ["Renshuang Jiang", "Yichong Wang", "Pan Dong", "Xiaoxiang Fang", "Zhenling Duan", "Tinglue Wang", "Yuchen Hu", "Jie Yu", "Zhe Jiang"], "pdf_url": "https://arxiv.org/pdf/2602.21681v1"}
{"id": "http://arxiv.org/abs/2602.21641v1", "title": "Uncertainty Modeling for SysML v2", "summary": "Uncertainty is inherent in modern engineered systems, including cyber-physical systems, autonomous systems, and large-scale software-intensive infrastructures (such as microservice-based systems) operating in dynamic and partially observable environments. The recent publication of Precise Semantics for Uncertainty Modeling (PSUM) by the Object Management Group represents the first standardized specification for uncertainty modeling within the Model-Based Systems Engineering (MBSE) community, providing formally defined semantics for representing and reasoning about uncertainty in models. In parallel, the second version of Systems Modeling Language (SysML v2) was released as the next-generation systems modeling language, offering improved semantic rigor and reusability, yet lacking native constructs aligned with PSUM for first-class uncertainty representation. This paper proposes a systematic extension of SysML v2 that incorporates the PSUM metamodel into its modeling framework. The extension enables explicit specification of indeterminacy sources, structured characterization of uncertainties, and consistent propagation of uncertainty within system models, while preserving conformance with SysML v2 syntax and semantics. We validate the approach through seven case studies. Results demonstrate that the proposed extension (PSUM-SysMLv2) is expressive and applicable for uncertainty-aware MBSE, and potentially enables uncertainty and uncertainty propagation analyses.", "published": "2026-02-25T07:10:53Z", "updated": "2026-02-25T07:10:53Z", "authors": ["Man Zhang", "Yunyang Li", "Tao Yue"], "pdf_url": "https://arxiv.org/pdf/2602.21641v1"}
{"id": "http://arxiv.org/abs/2602.20610v2", "title": "SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference", "summary": "Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions.", "published": "2026-02-24T07:01:17Z", "updated": "2026-02-25T06:38:21Z", "authors": ["Cuong Chi Le", "Minh V. T Pham", "Tung Vu Duy", "Cuong Duc Van", "Huy N. Phan", "Hoang N. Phan", "Tien N. Nguyen"], "pdf_url": "https://arxiv.org/pdf/2602.20610v2"}
{"id": "http://arxiv.org/abs/2602.21611v1", "title": "Structurally Aligned Subtask-Level Memory for Software Engineering Agents", "summary": "Large Language Models (LLMs) have demonstrated significant potential as autonomous software engineering (SWE) agents. Recent work has further explored augmenting these agents with memory mechanisms to support long-horizon reasoning. However, these approaches typically operate at a coarse instance granularity, treating the entire problem-solving episode as the atomic unit of storage and retrieval. We empirically demonstrate that instance-level memory suffers from a fundamental granularity mismatch, resulting in misguided retrieval when tasks with similar surface descriptions require distinct reasoning logic at specific stages. To address this, we propose Structurally Aligned Subtask-Level Memory, a method that aligns memory storage, retrieval, and updating with the agent's functional decomposition. Extensive experiments on SWE-bench Verified demonstrate that our method consistently outperforms both vanilla agents and strong instance-level memory baselines across diverse backbones, improving mean Pass@1 over the vanilla agent by +4.7 pp on average (e.g., +6.8 pp on Gemini 2.5 Pro). Performance gains grow with more interaction steps, showing that leveraging past experience benefits long-horizon reasoning in complex software engineering tasks.", "published": "2026-02-25T06:13:25Z", "updated": "2026-02-25T06:13:25Z", "authors": ["Kangning Shen", "Jingyuan Zhang", "Chenxi Sun", "Wencong Zeng", "Yang Yue"], "pdf_url": "https://arxiv.org/pdf/2602.21611v1"}
{"id": "http://arxiv.org/abs/2602.21568v1", "title": "From Ad-Hoc Scripts to Orchestrated Pipelines: Architecting a Resilient ELT Framework for Developer Productivity Metrics", "summary": "Developer Productivity Dashboards are essential for visualizing DevOps performance metrics such as Deployment Frequency and Change Failure Rate (DORA). However, the utility of these dashboards is frequently undermined by data reliability issues. In early iterations of our platform, ad-hoc ingestion scripts (Cron jobs) led to \"silent failures,\" where data gaps went undetected for days, eroding organizational trust. This paper reports on our experience migrating from legacy scheduling to a robust Extract-Load-Transform (ELT) pipeline using Directed Acyclic Graph (DAG) orchestration and Medallion Architecture. We detail the operational benefits of decoupling data extraction from transformation, the necessity of immutable raw history for metric redefinition, and the implementation of state-based dependency management. Our experience suggests that treating the metrics pipeline as a production-grade distributed system is a prerequisite for sustainable engineering analytics.", "published": "2026-02-25T04:46:08Z", "updated": "2026-02-25T04:46:08Z", "authors": ["Yuvraj Agrawal", "Pallav Jain"], "pdf_url": "https://arxiv.org/pdf/2602.21568v1"}
{"id": "http://arxiv.org/abs/2507.17691v2", "title": "CASCADE: LLM-Powered JavaScript Deobfuscator at Google", "summary": "Software obfuscation, particularly prevalent in JavaScript, hinders code comprehension and analysis, posing significant challenges to software testing, static analysis, and malware detection. This paper introduces CASCADE, a novel hybrid approach that integrates the advanced coding capabilities of Gemini with the deterministic transformation capabilities of a compiler Intermediate Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to identify critical prelude functions, the foundational components underlying the most prevalent obfuscation techniques, and leveraging JSIR for subsequent code transformations, CASCADE effectively recovers semantic elements like original strings and API names, and reveals original program behaviors. This method overcomes limitations of existing static and dynamic deobfuscation techniques, eliminating hundreds to thousands of hardcoded rules while achieving reliability and flexibility. CASCADE is already deployed in Google's production environment, demonstrating substantial improvements in JavaScript deobfuscation efficiency and reducing reverse engineering efforts.", "published": "2025-07-23T16:57:32Z", "updated": "2026-02-25T04:00:38Z", "authors": ["Shan Jiang", "Pranoy Kovuri", "David Tao", "Zhixun Tan"], "pdf_url": "https://arxiv.org/pdf/2507.17691v2"}
{"id": "http://arxiv.org/abs/2406.11935v4", "title": "A Problem-Oriented Perspective and Anchor Verification for Code Optimization", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in solving various programming tasks, such as code generation. However, their potential for code optimization, particularly in performance enhancement, remains largely unexplored. This paper investigates the capabilities of LLMs in optimizing code for minimal execution time, addressing a critical gap in current research. The recently proposed code optimization methods construct program optimization pairs based on iterative submissions from the same programmer for the same problem. However, this approach confines LLMs to local performance improvements, neglecting global algorithmic innovation. To overcome this limitation, we adopt a completely different perspective by reconstructing the optimization pairs into a problem-oriented approach. This allows for the integration of various ideas from multiple programmers tackling the same problem. Furthermore, we observe that code optimization presents greater challenges compared to code generation, often accompanied by \"optimization tax\". Recognizing the inherent trade-offs in correctness and efficiency, we introduce a novel anchor verification framework to mitigate this \"optimization tax\". Ultimately, the problem oriented perspective combined with the anchor verification framework significantly enhances both the correct optimization ratio and speedup to new levels.", "published": "2024-06-17T16:10:10Z", "updated": "2026-02-25T02:17:57Z", "authors": ["Tong Ye", "Tengfei Ma", "Xuhong Zhang", "Hang Yu", "Jianwei Yin", "Wenhai Wang"], "pdf_url": "https://arxiv.org/pdf/2406.11935v4"}
{"id": "http://arxiv.org/abs/2507.02376v2", "title": "On the Inference (In-)Security of Vertical Federated Learning: Efficient Auditing against Inference Tampering Attack", "summary": "Vertical Federated Learning (VFL) is an emerging distributed learning paradigm for cross-silo collaboration without accessing participants' data. However, existing VFL work lacks a mechanism to audit the inference correctness of the data party. The malicious data party can modify the local data and model to mislead the joint inference results. To exploit this vulnerability, we design a novel Vertical Federated Inference Tampering (VeFIT) attack, allowing the data party to covertly tamper with the local inference and mislead results on the task party's final prediction. VeFIT can decrease the task party's inference accuracy by an average of 34.49%. Existing defense mechanisms can not effectively detect this attack, and the detection performance is near random guessing. To mitigate the attack, we further design a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task party to audit whether the data party's inferences are executed as expected during large-scale online inference. VeFIA does not leak the data party's privacy nor introduce additional latency. The core design is that the task party can use the inference results from a framework with Trusted Execution Environments (TEE) and the coordinator to validate the correctness of the data party's computation results. VeFIA guarantees that, as long as the proportion of inferences attacked by VeFIT exceeds 5.4%, the task party can detect the malicious behavior of the data party with a probability of 99.99%, without any additional online overhead. VeFIA's random sampling validation of VeFIA achieves 100% positive predictive value, negative predictive value, and true positive rate in detecting VeFIT. We further validate VeFIA's effectiveness in terms of privacy protection and scalability on real-world datasets. To the best of our knowledge, this is the first paper discussing the inference auditing problem towards VFL.", "published": "2025-07-03T07:17:49Z", "updated": "2026-02-25T01:55:14Z", "authors": ["Chung-ju Huang", "Ziqi Zhang", "Yinggui Wang", "Binghui Wang", "Tao Wei", "Leye Wang"], "pdf_url": "https://arxiv.org/pdf/2507.02376v2"}
