{"id": "http://arxiv.org/abs/2601.08806v1", "title": "APEX-SWE", "summary": "We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).", "published": "2026-01-13T18:44:08Z", "updated": "2026-01-13T18:44:08Z", "authors": ["Abhi Kottamasu", "Akul Datta", "Aakash Barthwal", "Chirag Mahapatra", "Ajay Arun", "Adarsh Hiremath", "Brendan Foody", "Bertie Vidgen"], "pdf_url": "https://arxiv.org/pdf/2601.08806v1"}
{"id": "http://arxiv.org/abs/2601.08773v1", "title": "Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs", "summary": "Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.\n  Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.", "published": "2026-01-13T18:03:41Z", "updated": "2026-01-13T18:03:41Z", "authors": ["Manideep Reddy Chinthareddy"], "pdf_url": "https://arxiv.org/pdf/2601.08773v1"}
{"id": "http://arxiv.org/abs/2601.05467v2", "title": "STELP: Secure Transpilation and Execution of LLM-Generated Programs", "summary": "Rapid evolution of Large Language Models (LLMs) has achieved major advances in reasoning, planning, and function-calling capabilities. Multi-agentic collaborative frameworks using such LLMs place them at the center of solving software development-related tasks such as code generation. However, direct use of LLM generated code in production software development systems is problematic. The code could be unstable or erroneous and contain vulnerabilities such as data poisoning, malicious attacks, and hallucinations that could lead to widespread system malfunctions. This prohibits the adoption of LLM generated code in production AI systems where human code reviews and traditional secure testing tools are impractical or untrustworthy. In this paper, we discuss safety and reliability problems with the execution of LLM generated code and propose a Secure Transpiler and Executor of LLM-Generated Program (STELP), capable of executing LLM-generated code in a controlled and safe manner. STELP secures autonomous production AI systems involving code generation, filling the critical void left by the impracticality or limitations of traditional secure testing methodologies and human oversight. This includes applications such as headless code generation-execution and LLMs that produce executable code snippets as an action plan to be executed in real time. We contribute a human-validated dataset of insecure code snippets and benchmark our approach on publicly available datasets for correctness, safety, and latency. Our results demonstrate that our approach outperforms an existing method by a significant margin, particularly in its ability to safely execute risky code snippets. Warning: This paper contains malicious code snippets that should be run with caution.", "published": "2026-01-09T01:49:41Z", "updated": "2026-01-13T17:55:11Z", "authors": ["Swapnil Shinde", "Sahil Wadhwa", "Andy Luo", "Akshay Gupta", "Mohammad Shahed Sorower"], "pdf_url": "https://arxiv.org/pdf/2601.05467v2"}
{"id": "http://arxiv.org/abs/2601.08734v1", "title": "TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback", "summary": "Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.", "published": "2026-01-13T17:08:30Z", "updated": "2026-01-13T17:08:30Z", "authors": ["Prithwish Jana", "Sam Davidson", "Bhavana Bhasker", "Andrey Kan", "Anoop Deoras", "Laurent Callot"], "pdf_url": "https://arxiv.org/pdf/2601.08734v1"}
{"id": "http://arxiv.org/abs/2601.08729v1", "title": "Revisiting \"Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and Distribution-Aware Criterion\": A Critical Review and Implications on DNN Coverage Testing", "summary": "We present a critical review of Neural Coverage (NLC), a state-of-the-art DNN coverage criterion by Yuan et al. at ICSE 2023. While NLC proposes to satisfy eight design requirements and demonstrates strong empirical performance, we question some of their theoretical and empirical assumptions. We observe that NLC deviates from core principles of coverage criteria, such as monotonicity and test suite order independence, and could more fully account for key properties of the covariance matrix. Additionally, we note threats to the validity of the empirical study, related to the ground truth ordering of test suites. Through our empirical validation, we substantiate our claims and propose improvements for future DNN coverage metrics. Finally, we conclude by discussing the implications of these insights.", "published": "2026-01-13T16:58:44Z", "updated": "2026-01-13T16:58:44Z", "authors": ["Jinhan Kim", "Nargiz Humbatova", "Gunel Jahangirova", "Shin Yoo", "Paolo Tonella"], "pdf_url": "https://arxiv.org/pdf/2601.08729v1"}
{"id": "http://arxiv.org/abs/2601.08706v1", "title": "\"Where is My Troubleshooting Procedure?\": Studying the Potential of RAG in Assisting Failure Resolution of Large Cyber-Physical System", "summary": "In today's complex industrial environments, operators must often navigate through extensive technical manuals to identify troubleshooting procedures that may help react to some observed failure symptoms. These manuals, written in natural language, describe many steps in detail. Unfortunately, the number, magnitude, and articulation of these descriptions can significantly slow down and complicate the retrieval of the correct procedure during critical incidents. Interestingly, Retrieval Augmented Generation (RAG) enables the development of tools based on conversational interfaces that can assist operators in their retrieval tasks, improving their capability to respond to incidents. This paper presents the results of a set of experiments that derive from the analysis of the troubleshooting procedures available in Fincantieri, a large international company developing complex naval cyber-physical systems. Results show that RAG can assist operators in reacting promptly to failure symptoms, although specific measures have to be taken into consideration to cross-validate recommendations before actuating them.", "published": "2026-01-13T16:34:43Z", "updated": "2026-01-13T16:34:43Z", "authors": ["Maria Teresa Rossi", "Leonardo Mariani", "Oliviero Riganelli", "Giuseppe Filomento", "Danilo Giannone", "Paolo Gavazzo"], "pdf_url": "https://arxiv.org/pdf/2601.08706v1"}
{"id": "http://arxiv.org/abs/2601.08691v1", "title": "LLMs in Code Vulnerability Analysis: A Proof of Concept", "summary": "Context: Traditional software security analysis methods struggle to keep pace with the scale and complexity of modern codebases, requiring intelligent automation to detect, assess, and remediate vulnerabilities more efficiently and accurately. Objective: This paper explores the incorporation of code-specific and general-purpose Large Language Models (LLMs) to automate critical software security tasks, such as identifying vulnerabilities, predicting severity and access complexity, and generating fixes as a proof of concept. Method: We evaluate five pairs of recent LLMs, including both code-based and general-purpose open-source models, on two recognized C/C++ vulnerability datasets, namely Big-Vul and Vul-Repair. Additionally, we compare fine-tuning and prompt-based approaches. Results: The results show that fine-tuning uniformly outperforms both zero-shot and few-shot approaches across all tasks and models. Notably, code-specialized models excel in zero-shot and few-shot settings on complex tasks, while general-purpose models remain nearly as effective. Discrepancies among CodeBLEU, CodeBERTScore, BLEU, and ChrF highlight the inadequacy of current metrics for measuring repair quality. Conclusions: This study contributes to the software security community by investigating the potential of advanced LLMs to improve vulnerability analysis and remediation.", "published": "2026-01-13T16:16:11Z", "updated": "2026-01-13T16:16:11Z", "authors": ["Shaznin Sultana", "Sadia Afreen", "Nasir U. Eisty"], "pdf_url": "https://arxiv.org/pdf/2601.08691v1"}
{"id": "http://arxiv.org/abs/2411.00634v3", "title": "Does GenAI Make Usability Testing Obsolete?", "summary": "Ensuring usability is crucial for the success of mobile apps. Usability issues can compromise user experience and negatively impact the perceived app quality. This paper presents UX-LLM, a novel tool powered by a Large Vision-Language Model that predicts usability issues in iOS apps. To evaluate the performance of UX-LLM, we predicted usability issues in two open-source apps of a medium complexity and asked two usability experts to assess the predictions. We also performed traditional usability testing and expert review for both apps and compared the results to those of UX-LLM. UX-LLM demonstrated precision ranging from 0.61 and 0.66 and recall between 0.35 and 0.38, indicating its ability to identify valid usability issues, yet failing to capture the majority of issues. Finally, we conducted a focus group with an app development team of a capstone project developing a transit app for visually impaired persons. The focus group expressed positive perceptions of UX-LLM as it identified unknown usability issues in their app. However, they also raised concerns about its integration into the development workflow, suggesting potential improvements. Our results show that UX-LLM cannot fully replace traditional usability evaluation methods but serves as a valuable supplement particularly for small teams with limited resources, to identify issues in less common user paths, due to its ability to inspect the source code.", "published": "2024-11-01T14:45:34Z", "updated": "2026-01-13T15:54:40Z", "authors": ["Ali Ebrahimi Pourasad", "Walid Maalej"], "pdf_url": "https://arxiv.org/pdf/2411.00634v3"}
{"id": "http://arxiv.org/abs/2601.08609v1", "title": "Coverage-Guided Road Selection and Prioritization for Efficient Testing in Autonomous Driving Systems", "summary": "Autonomous Driving Assistance Systems (ADAS) rely on extensive testing to ensure safety and reliability, yet road scenario datasets often contain redundant cases that slow down the testing process without improving fault detection. To address this issue, we present a novel test prioritization framework that reduces redundancy while preserving geometric and behavioral diversity. Road scenarios are clustered based on geometric and dynamic features of the ADAS driving behavior, from which representative cases are selected to guarantee coverage. Roads are finally prioritized based on geometric complexity, driving difficulty, and historical failures, ensuring that the most critical and challenging tests are executed first. We evaluate our framework on the OPENCAT dataset and the Udacity self-driving car simulator using two ADAS models. On average, our approach achieves an 89% reduction in test suite size while retaining an average of 79% of failed road scenarios. The prioritization strategy improves early failure detection by up to 95x compared to random baselines.", "published": "2026-01-13T14:55:27Z", "updated": "2026-01-13T14:55:27Z", "authors": ["Qurban Ali", "Andrea Stocco", "Leonardo Mariani", "Oliviero Riganelli"], "pdf_url": "https://arxiv.org/pdf/2601.08609v1"}
{"id": "http://arxiv.org/abs/2601.06789v2", "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences", "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.", "published": "2026-01-11T06:41:26Z", "updated": "2026-01-13T14:48:24Z", "authors": ["Qihao Wang", "Ziming Cheng", "Shuo Zhang", "Fan Liu", "Rui Xu", "Heng Lian", "Kunyi Wang", "Xiaoming Yu", "Jianghao Yin", "Sen Hu", "Yue Hu", "Shaolei Zhang", "Yanbing Liu", "Ronghao Chen", "Huacan Wang"], "pdf_url": "https://arxiv.org/pdf/2601.06789v2"}
{"id": "http://arxiv.org/abs/2601.08545v1", "title": "Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement", "summary": "With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \\textbf{LPR} (\\textbf{L}earner-Tailored \\textbf{P}rogram \\textbf{R}epair). We then propose a novel and effective framework, \\textbf{\\textsc{\\MethodName{}}} (\\textbf{L}earner-Tailored \\textbf{S}olution \\textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.", "published": "2026-01-13T13:31:11Z", "updated": "2026-01-13T13:31:11Z", "authors": ["Zhenlong Dai", "Zhuoluo Zhao", "Hengning Wang", "Xiu Tang", "Sai Wu", "Chang Yao", "Zhipeng Gao", "Jingyuan Chen"], "pdf_url": "https://arxiv.org/pdf/2601.08545v1"}
{"id": "http://arxiv.org/abs/2601.08477v1", "title": "Do You Understand How I Feel?: Towards Verified Empathy in Therapy Chatbots", "summary": "Conversational agents are increasingly used as support tools along mental therapeutic pathways with significant societal impacts. In particular, empathy is a key non-functional requirement in therapeutic contexts, yet current chatbot development practices provide no systematic means to specify or verify it. This paper envisions a framework integrating natural language processing and formal verification to deliver empathetic therapy chatbots. A Transformer-based model extracts dialogue features, which are then translated into a Stochastic Hybrid Automaton model of dyadic therapy sessions. Empathy-related properties can then be verified through Statistical Model Checking, while strategy synthesis provides guidance for shaping agent behavior. Preliminary results show that the formal model captures therapy dynamics with good fidelity and that ad-hoc strategies improve the probability of satisfying empathy requirements.", "published": "2026-01-13T12:08:58Z", "updated": "2026-01-13T12:08:58Z", "authors": ["Francesco Dettori", "Matteo Forasassi", "Lorenzo Veronese", "Livia Lestingi", "Vincenzo Scotti", "Matteo Giovanni Rossi"], "pdf_url": "https://arxiv.org/pdf/2601.08477v1"}
{"id": "http://arxiv.org/abs/2601.07526v2", "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era", "summary": "The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.", "published": "2026-01-12T13:25:33Z", "updated": "2026-01-13T12:02:57Z", "authors": ["Lei Zhang", "Mouxiang Chen", "Ruisheng Cao", "Jiawei Chen", "Fan Zhou", "Yiheng Xu", "Jiaxi Yang", "Zeyao Ma", "Liang Chen", "Changwei Luo", "Kai Zhang", "Fan Yan", "KaShun Shum", "Jiajun Zhang", "Zeyu Cui", "Feng Hu", "Junyang Lin", "Binyuan Hui", "Min Yang"], "pdf_url": "https://arxiv.org/pdf/2601.07526v2"}
{"id": "http://arxiv.org/abs/2601.08367v1", "title": "A Methodological Analysis of Empirical Studies in Quantum Software Testing", "summary": "In quantum software engineering (QSE), quantum software testing (QST) has attracted increasing attention as quantum software systems grow in scale and complexity. Since QST evaluates quantum programs through execution under designed test inputs, empirical studies are widely used to assess the effectiveness of testing approaches. However, the design and reporting of empirical studies in QST remain highly diverse, and a shared methodological understanding has yet to emerge, making it difficult to interpret results and compare findings across studies. This paper presents a methodological analysis of empirical studies in QST through a systematic examination of 59 primary studies identified from a literature pool of size 384. We organize our analysis around ten research questions that cover key methodological dimensions of QST empirical studies, including objects under test, baseline comparison, testing setup, experimental configuration, and tool and artifact support. Through cross-study analysis along these dimensions, we characterize current empirical practices in QST, identify recurring limitations and inconsistencies, and highlight open methodological challenges. Based on our findings, we derive insights and recommendations to inform the design, execution, and reporting of future empirical studies in QST.", "published": "2026-01-13T09:29:00Z", "updated": "2026-01-13T09:29:00Z", "authors": ["Yuechen Li", "Minqi Shao", "Jianjun Zhao", "Qichen Wang"], "pdf_url": "https://arxiv.org/pdf/2601.08367v1"}
{"id": "http://arxiv.org/abs/2510.12566v2", "title": "Power Assumptions Matter: Evaluating End-user Laptop Energy Models for Sustainability Reporting of Browser-Based Web Services", "summary": "Sustainability reporting for web-based services often relies on simplified end-user energy models that assume constant laptop power during browser interactions. Energy models such as Digst and DIMPACT apply fixed power values (15-22W), yet the validity of this approach for realistic browsing remains underexplored.\n  We empirically evaluate constant-power assumptions in a controlled user study where ten participants repeatedly complete eight representative user flows across shopping, booking, navigation, and news services on four laptop platforms, while device energy is measured. Typical power is 9--13~W, substantially below current reporting standards, implying systematic overestimation. Moreover, the error scales proportionally with task duration, indicating systematic bias rather than random noise.\n  Comparing progressively refined constant-power models, we find that category-specific parameters improve accuracy more than hardware-only parameters and approach flow-specific performance. The best fit is obtained by combining category (or flow) with hardware, while category-level models retain most of the benefit with fewer parameters, making them a practical upgrade for sustainability reporting.", "published": "2025-10-14T14:25:26Z", "updated": "2026-01-13T09:07:45Z", "authors": ["Maja H. Kirkeby", "Timmie Lagermann"], "pdf_url": "https://arxiv.org/pdf/2510.12566v2"}
{"id": "http://arxiv.org/abs/2509.23824v2", "title": "SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation", "summary": "The rise of blockchain has brought smart contracts into mainstream use, creating a demand for smart contract generation tools. While large language models (LLMs) excel at generating code in general-purpose languages, their effectiveness on Solidity, the primary language for smart contracts, remains underexplored. Solidity constitutes only a small portion of typical LLM training data and differs from general-purpose languages in its version-sensitive syntax and limited flexibility. These factors raise concerns about the reliability of existing LLMs for Solidity code generation. Critically, existing evaluations, focused on isolated functions and synthetic inputs, fall short of assessing models' capabilities in real-world contract development.\n  To bridge this gap, we introduce SolContractEval, the first contract-level benchmark for Solidity code generation. It comprises 124 tasks drawn from real on-chain contracts across nine major domains. Each task input, consisting of complete context dependencies, a structured contract framework, and a concise task prompt, is independently annotated and cross-validated by experienced developers. To enable precise and automated evaluation of functional correctness, we also develop a dynamic evaluation framework based on historical transaction replay. Building on SolContractEval, we perform a systematic evaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the highest overall performance, though evaluated models underperform relative to their capabilities on class-level generation tasks in general-purpose programming languages. Second, current models perform better on tasks that follow standard patterns but struggle with complex logic and inter-contract dependencies. Finally, they exhibit limited understanding of Solidity-specific features and contextual dependencies.", "published": "2025-09-28T11:53:41Z", "updated": "2026-01-13T06:58:18Z", "authors": ["Zhifan Ye", "Jiachi Chen", "Zhenzhe Shao", "Lingfeng Bao", "Xiaohu Yang", "Zhongxin Liu"], "pdf_url": "https://arxiv.org/pdf/2509.23824v2"}
{"id": "http://arxiv.org/abs/2512.23742v2", "title": "AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization", "summary": "With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools.", "published": "2025-12-26T01:34:08Z", "updated": "2026-01-13T06:50:48Z", "authors": ["Guangxi Fan", "Tianliang Ma", "Xuguang Sun", "Xun Wang", "Kain Lu Low", "Leilai Shao"], "pdf_url": "https://arxiv.org/pdf/2512.23742v2"}
{"id": "http://arxiv.org/abs/2505.03307v2", "title": "Qimax: Efficient quantum simulation via GPU-accelerated extended stabilizer formalism", "summary": "Simulating Clifford and near-Clifford circuits using the extended stabilizer formalism has become increasingly popular, particularly in quantum error correction. Compared to the state-vector approach, the extended stabilizer formalism can solve the same problems with fewer computational resources, as it operates on stabilizers rather than full state vectors. Most existing studies on near-Clifford circuits focus on balancing the trade-off between the number of ancilla qubits and simulation accuracy, often overlooking performance considerations. Furthermore, in the presence of high-rank stabilizers, performance is limited by the sequential property of the stabilizer formalism. In this work, we introduce a parallelized version of the extended stabilizer formalism, enabling efficient execution on multi-core devices such as GPU. Experimental results demonstrate that, in certain scenarios, our Python-based implementation outperforms state-of-the-art simulators such as Qiskit and Pennylane.", "published": "2025-05-06T08:41:28Z", "updated": "2026-01-13T05:08:13Z", "authors": ["Vu Tuan Hai", "Bui Cao Doanh", "Le Vu Trung Duong", "Pham Hoai Luan", "Yasuhiko Nakashima"], "pdf_url": "https://arxiv.org/pdf/2505.03307v2"}
{"id": "http://arxiv.org/abs/2503.16913v4", "title": "FGIT: Fault-Guided Fine-Tuning for Code Generation", "summary": "Modern instruction-tuned large language models (LLMs) have made remarkable progress in code generation. However, these LLMs fine-tuned with standard supervised fine-tuning (SFT) sometimes generate plausible-looking but functionally incorrect code variants. This issue likely stems from the limitation of standard SFT, which treats all tokens equally during optimization and fails to emphasize the error-sensitive segments-specific code differences between correct implementations and similar incorrect variants. To address this problem, we propose Fault-Guided Fine-Tuning (FGIT), a novel fine-tuning technique that enhances LLMs' code generation by (1) extracting multi-granularity (line/token-level) differences between correct and incorrect yet similar implementations to identify error-sensitive segments, and (2) dynamically prioritizing those segments during training via dynamic loss weighting. Through extensive experiments on seven LLMs across three widely-used benchmarks, our method achieves an average relative improvement of 6.9% on pass@1 with some enhanced 6.7B LLMs outperforming closed-source models, e.g., GPT-3.5-Turbo. Furthermore, our fine-tuning technique demonstrates strong generalization with performance improvements ranging from 3.8% to 19.1% across diverse instruction-tuned LLMs, and our ablation studies confirm the contributions of different granularities of differences and hyperparameters.", "published": "2025-03-21T07:23:26Z", "updated": "2026-01-13T04:42:20Z", "authors": ["Lishui Fan", "Zhongxin Liu", "Haoye Wang", "Lingfeng Bao", "Xin Xia", "Shanping Li"], "pdf_url": "https://arxiv.org/pdf/2503.16913v4"}
{"id": "http://arxiv.org/abs/2601.08196v1", "title": "Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis", "summary": "The integration of large language models (LLMs) into autonomous agents has enabled complex tool use, yet in high-stakes domains, these systems must strictly adhere to regulatory standards beyond simple functional correctness. However, existing benchmarks often overlook implicit regulatory compliance, thus failing to evaluate whether LLMs can autonomously enforce mandatory safety constraints. To fill this gap, we introduce LogiSafetyGen, a framework that converts unstructured regulations into Linear Temporal Logic oracles and employs logic-guided fuzzing to synthesize valid, safety-critical traces. Building on this framework, we construct LogiSafetyBench, a benchmark comprising 240 human-verified tasks that require LLMs to generate Python programs that satisfy both functional objectives and latent compliance rules. Evaluations of 13 state-of-the-art (SOTA) LLMs reveal that larger models, despite achieving better functional correctness, frequently prioritize task completion over safety, which results in non-compliant behavior.", "published": "2026-01-13T03:55:18Z", "updated": "2026-01-13T03:55:18Z", "authors": ["Da Song", "Yuheng Huang", "Boqi Chen", "Tianshuo Cong", "Randy Goebel", "Lei Ma", "Foutse Khomh"], "pdf_url": "https://arxiv.org/pdf/2601.08196v1"}
{"id": "http://arxiv.org/abs/2601.06266v2", "title": "Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with ML and Non-ML Software", "summary": "Self-admitted technical debt (SATD), referring to comments flagged by developers that explicitly acknowledge suboptimal code or incomplete functionality, has received extensive attention in machine learning (ML) and traditional (Non-ML) software. However, little is known about how SATD manifests and evolves in contemporary Large Language Model (LLM)-based systems, whose architectures, workflows, and dependencies differ fundamentally from both traditional and pre-LLM ML software. In this paper, we conduct the first empirical study of SATD in the LLM era, replicating and extending prior work on ML technical debt to modern LLM-based systems. We compare SATD prevalence across LLM, ML, and non-ML repositories across a total of 477 repositories (159 per category). We perform survival analysis of SATD introduction and removal to understand the dynamics of technical debt across different development paradigms. Surprisingly, despite their architectural complexity, our results reveal that LLM repositories accumulate SATD at similar rates to ML systems (3.95% vs. 4.10%). However, we observe that LLM repositories remain debt-free 2.4x longer than ML repositories (a median of 492 days vs. 204 days), and then start to accumulate technical debt rapidly. Moreover, our qualitative analysis of 377 SATD instances reveals three new forms of technical debt unique to LLM-based development that have not been reported in prior research: Model-Stack Workaround Debt, Model Dependency Debt, and Performance Optimization Debt. Finally, by mapping SATD to stages of the LLM development pipeline, we observe that debt concentrates", "published": "2026-01-09T19:25:48Z", "updated": "2026-01-13T02:51:00Z", "authors": ["Niruthiha Selvanayagam", "Taher A. Ghaleb", "Manel Abdellatif"], "pdf_url": "https://arxiv.org/pdf/2601.06266v2"}
{"id": "http://arxiv.org/abs/2508.04295v4", "title": "EvoC2Rust: A Skeleton-guided Framework for Project-Level C-to-Rust Translation", "summary": "Translating legacy C codebases to Rust is increasingly demanded for building safety-critical systems. While various approaches have emerged for this task, they face inherent trade-offs: rule-based methods often struggle to satisfy code safety and idiomaticity requirements, while LLM-based methods frequently fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting complete C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros, and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates functions, replacing the corresponding stub placeholders; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates the superior performance of EvoC2Rust in project-level C-to-Rust translation. The results show that our approach outperforms the strongest LLM-based baseline by 17.24% in syntax accuracy and 14.32% in semantic accuracy, while also achieving a 43.59% higher code safety rate than the best rule-based tool.", "published": "2025-08-06T10:31:23Z", "updated": "2026-01-13T02:38:08Z", "authors": ["Chaofan Wang", "Tingrui Yu", "Beijun Shen", "Jie Wang", "Dong Chen", "Wenrui Zhang", "Yuling Shi", "Chen Xie", "Xiaodong Gu"], "pdf_url": "https://arxiv.org/pdf/2508.04295v4"}
