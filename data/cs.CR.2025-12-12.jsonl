{"id": "http://arxiv.org/abs/2512.11783v1", "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously", "summary": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.\n  Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.", "published": "2025-12-12T18:52:09Z", "updated": "2025-12-12T18:52:09Z", "authors": ["Andrew Adiletta", "Kathryn Adiletta", "Kemal Derya", "Berk Sunar"], "pdf_url": "https://arxiv.org/pdf/2512.11783v1"}
{"id": "http://arxiv.org/abs/2512.11775v1", "title": "Hypergraph based Multi-Party Payment Channel", "summary": "Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments.\n  We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs.\n  Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs.", "published": "2025-12-12T18:37:28Z", "updated": "2025-12-12T18:37:28Z", "authors": ["Ayush Nainwal", "Atharva Kamble", "Nitin Awathare"], "pdf_url": "https://arxiv.org/pdf/2512.11775v1"}
{"id": "http://arxiv.org/abs/2312.11533v3", "title": "Cryptanalysis of PLWE based on zero-trace quadratic roots", "summary": "We extend two of the attacks on the PLWE problem presented in (Y. Elias, K. E. Lauter, E. Ozman, and K. E. Stange, Ring-LWE Cryptography for the Number Theorist, in Directions in Number Theory, E. E. Eischen, L. Long, R. Pries, and K. E. Stange, eds., vol. 3 of Association for Women in Mathematics Series, Cham, 2016, Springer International Publishing, pp. 271-290) to a ring $R_q=\\mathbb{F}_q[x]/(f(x))$ where the irreducible monic polynomial $f(x)\\in\\mathbb{Z}[x]$ has an irreducible quadratic factor over $\\mathbb{F}_q[x]$ of the form $x^2+ρ$ with $ρ$ of suitable multiplicative order in $\\mathbb{F}_q$. Our attack exploits the fact that the trace of the root is zero, and has overwhelming success probability as a function of the number of samples taken as input. An implementation in Maple and some examples of our attack are also provided.", "published": "2023-12-15T16:22:59Z", "updated": "2025-12-12T17:42:07Z", "authors": ["Beatriz Barbero-Lucas", "Iván Blanco-Chacón", "Raúl Durán-Díaz", "Rodrigo Martín Sánchez-Ledesma", "Rahinatou Yuh Njah Nchiwo"], "pdf_url": "https://arxiv.org/pdf/2312.11533v3"}
{"id": "http://arxiv.org/abs/2512.11699v1", "title": "SoK: Demystifying the multiverse of MPC protocols", "summary": "This paper systematizes knowledge on the performance of Multi-Party Computation (MPC) protocols. Despite strong privacy and correctness guarantees, MPC adoption in real-world applications remains limited by high costs (especially in the malicious setting) and lack of guidance on choosing suitable protocols for concrete workloads. We identify the theoretical and practical parameters that shape MPC efficiency and conduct an extensive experimental study across diverse benchmarks. Our analysis discusses the trade-offs between protocols, and highlights which techniques align best with different application scenarios and needs. By providing actionable guidance for developers and outlining open challenges for researchers, this work seeks to narrow the gap between MPC theory and practice.", "published": "2025-12-12T16:31:14Z", "updated": "2025-12-12T16:31:14Z", "authors": ["Roberta De Viti", "Vaastav Anand", "Pierfrancesco Ingo", "Deepak Garg"], "pdf_url": "https://arxiv.org/pdf/2512.11699v1"}
{"id": "http://arxiv.org/abs/2512.11690v1", "title": "Leveraging FPGAs for Homomorphic Matrix-Vector Multiplication in Oblivious Message Retrieval", "summary": "While end-to-end encryption protects the content of messages, it does not secure metadata, which exposes sender and receiver information through traffic analysis. A plausible approach to protecting this metadata is to have senders post encrypted messages on a public bulletin board and receivers scan it for relevant messages. Oblivious message retrieval (OMR) leverages homomorphic encryption (HE) to improve user experience in this solution by delegating the scan to a resource-rich server while preserving privacy. A key process in OMR is the homomorphic detection of pertinent messages for the receiver from the bulletin board. It relies on a specialized matrix-vector multiplication algorithm, which involves extensive multiplications between ciphertext vectors and plaintext matrices, as well as homomorphic rotations. The computationally intensive nature of this process limits the practicality of OMR. To address this challenge, this paper proposes a hardware architecture to accelerate the matrix-vector multiplication algorithm. The building homomorphic operators in this algorithm are implemented using high-level synthesis, with design parameters for different parallelism levels. These operators are then deployed on a field-programmable gate array platform using an efficient design space exploration strategy to accelerate homomorphic matrix-vector multiplication. Compared to a software implementation, the proposed hardware accelerator achieves a 13.86x speedup.", "published": "2025-12-12T16:12:02Z", "updated": "2025-12-12T16:12:02Z", "authors": ["Grant Bosworth", "Keewoo Lee", "Sunwoong Kim"], "pdf_url": "https://arxiv.org/pdf/2512.11690v1"}
{"id": "http://arxiv.org/abs/2408.08772v2", "title": "Vital: Vulnerability-Oriented Symbolic Execution via Type-Unsafe Pointer-Guided Monte Carlo Tree Search", "summary": "How to find memory safety bugs efficiently when navigating a symbolic execution tree that suffers from path explosion? Existing solutions either adopt path search heuristics to maximize coverage rate or chopped symbolic execution to skip uninteresting code (i.e., manually labeled as vulnerability-unrelated) during path exploration. However, most existing search heuristics are not vulnerability-oriented, and manual labeling of irrelevant code-to-be-skipped relies heavily on prior expert knowledge, making it hard to detect vulnerabilities effectively in practice.\n  This paper proposes Vital, a new vulnerability-oriented path exploration for symbolic execution with two innovations. First, a new indicator (i.e., type-unsafe pointers) is suggested to approximate vulnerable paths. A pointer that is type-unsafe cannot be statically proven to be safely dereferenced without memory corruption. Our key hypothesis is that a path with more type-unsafe pointers is more likely to be vulnerable. Second, a new type-unsafe pointer-guided Monte Carlo Tree Search algorithm is implemented to guide the path exploration towards the areas that contain more unsafe pointers, aiming to increase the likelihood of detecting vulnerabilities. We built Vital on top of KLEE and compared it with existing path searching strategies and chopped symbolic execution. In the former, the results demonstrate that Vital could cover up to 90.03% more unsafe pointers and detect up to 57.14% more unique memory errors. In the latter, the results show that Vital could achieve a speedup of up to 30x execution time and a reduction of up to 20x memory consumption to detect known vulnerabilities without prior expert knowledge automatically. In practice, Vital also detected one previously unknown vulnerability (a new CVE ID is assigned), which has been fixed by developers.", "published": "2024-08-16T14:29:57Z", "updated": "2025-12-12T15:20:57Z", "authors": ["Haoxin Tu", "Lingxiao Jiang", "Marcel Böhme"], "pdf_url": "https://arxiv.org/pdf/2408.08772v2"}
{"id": "http://arxiv.org/abs/2512.11602v1", "title": "Granite: Granular Runtime Enforcement for GitHub Actions Permissions", "summary": "Modern software projects use automated CI/CD pipelines to streamline their development, build, and deployment processes. GitHub Actions is a popular CI/CD platform that enables project maintainers to create custom workflows -- collections of jobs composed of sequential steps -- using reusable components known as actions. Wary of the security risks introduced by fully-privileged actions, GitHub provides a job-level permission model for controlling workflow access to repository resources. Unfortunately, this model is too coarse-grained to reduce the attack surface pertaining to permission misuse attacks: All actions within a job share the same permissions granted to the job. This violates the principle of least privilege and can lead to broader software supply chain attacks, whenever a compromised action exploits the granted permissions to compromise the repository resources. In this paper, we present Granite, a runtime proxy-based system that enforces fine-grained permissions for GitHub Actions at the step-level granularity within a job. Granite transparently monitors requests made by JavaScript and composite actions during workflow execution and checks them against predefined step-level policies at runtime. We evaluate Granite in terms of compatibility, security, and performance overhead using a dataset of 500 workflows comprising 12,916 jobs from the most-starred GitHub repositories that use GitHub Actions. Our analysis reveals that 52.7% of the jobs can be protected by Granite against permission misuse attacks. We evaluate Granite on 20 top-starred repositories (63 actions, 58 workflows), validate attack prevention using 10 permission misuse attacks across 42 overprivileged jobs, and measure an average overhead of 55% (3.67 seconds) per job, concluding that Granite effectively reduces CI/CD attack surfaces.", "published": "2025-12-12T14:38:45Z", "updated": "2025-12-12T14:38:45Z", "authors": ["Mojtaba Moazen", "Amir. M Ahmadian", "Musard Balliu"], "pdf_url": "https://arxiv.org/pdf/2512.11602v1"}
{"id": "http://arxiv.org/abs/2512.11484v1", "title": "Capacitive Touchscreens at Risk: Recovering Handwritten Trajectory on Smartphone via Electromagnetic Emanations", "summary": "This paper reveals and exploits a critical security vulnerability: the electromagnetic (EM) side channel of capacitive touchscreens leaks sufficient information to recover fine-grained, continuous handwriting trajectories. We present Touchscreen Electromagnetic Side-channel Leakage Attack (TESLA), a non-contact attack framework that captures EM signals generated during on-screen writing and regresses them into two-dimensional (2D) handwriting trajectories in real time. Extensive evaluations across a variety of commercial off-the-shelf (COTS) smartphones show that TESLA achieves 77% character recognition accuracy and a Jaccard index of 0.74, demonstrating its capability to recover highly recognizable motion trajectories that closely resemble the original handwriting under realistic attack conditions.", "published": "2025-12-12T11:33:05Z", "updated": "2025-12-12T11:33:05Z", "authors": ["Yukun Cheng", "Shiyu Zhu", "Changhai Ou", "Xingshuo Han", "Yuan Li", "Shihui Zheng"], "pdf_url": "https://arxiv.org/pdf/2512.11484v1"}
{"id": "http://arxiv.org/abs/2512.11482v1", "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models", "summary": "Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.", "published": "2025-12-12T11:31:13Z", "updated": "2025-12-12T11:31:13Z", "authors": ["Melih Catal", "Pooja Rani", "Harald C. Gall"], "pdf_url": "https://arxiv.org/pdf/2512.11482v1"}
{"id": "http://arxiv.org/abs/2505.01012v3", "title": "Quantum Support Vector Regression for Robust Anomaly Detection", "summary": "Anomaly Detection (AD) is critical in data analysis, particularly within the domain of IT security. In this study, we explore the potential of Quantum Machine Learning for application to AD with special focus on the robustness to noise and adversarial attacks. We build upon previous work on Quantum Support Vector Regression (QSVR) for semisupervised AD by conducting a comprehensive benchmark on IBM quantum hardware using eleven datasets. Our results demonstrate that QSVR achieves strong classification performance and even outperforms the noiseless simulation on two of these datasets. Moreover, we investigate the influence of - in the NISQ-era inevitable - quantum noise on the performance of the QSVR. Our findings reveal that the model exhibits robustness to depolarizing, phase damping, phase flip, and bit flip noise, while amplitude damping and miscalibration noise prove to be more disruptive. Finally, we explore the domain of Quantum Adversarial Machine Learning by demonstrating that QSVR is highly vulnerable to adversarial attacks, with neither quantum noise nor adversarial training improving the model's robustness against such attacks.", "published": "2025-05-02T05:23:34Z", "updated": "2025-12-12T10:41:27Z", "authors": ["Kilian Tscharke", "Maximilian Wendlinger", "Sebastian Issel", "Pascal Debus"], "pdf_url": "https://arxiv.org/pdf/2505.01012v3"}
{"id": "http://arxiv.org/abs/2512.11431v1", "title": "Proving DNSSEC Correctness: A Formal Approach to Secure Domain Name Resolution", "summary": "The Domain Name System Security Extensions (DNSSEC) are critical for preventing DNS spoofing, yet its specifications contain ambiguities and vulnerabilities that elude traditional \"break-and-fix\" approaches. A holistic, foundational security analysis of the protocol has thus remained an open problem. This paper introduces DNSSECVerif, the first framework for comprehensive, automated formal security analysis of the DNSSEC protocol suite. Built on the SAPIC+ symbolic verifier, our high-fidelity model captures protocol-level interactions, including cryptographic operations and stateful caching with fine-grained concurrency control. Using DNSSECVerif, we formally prove four of DNSSEC's core security guarantees and uncover critical ambiguities in the standards--notably, the insecure coexistence of NSEC and NSEC3. Our model also automatically rediscovers three classes of known attacks, demonstrating fundamental weaknesses in the protocol design. To bridge the model-to-reality gap, we validate our findings through targeted testing of mainstream DNS software and a large-scale measurement study of over 2.2 million open resolvers, confirming the real-world impact of these flaws. Our work provides crucial, evidence-based recommendations for hardening DNSSEC specifications and implementations.", "published": "2025-12-12T10:12:06Z", "updated": "2025-12-12T10:12:06Z", "authors": ["Qifan Zhang", "Zilin Shen", "Imtiaz Karim", "Elisa Bertino", "Zhou Li"], "pdf_url": "https://arxiv.org/pdf/2512.11431v1"}
{"id": "http://arxiv.org/abs/2501.05928v3", "title": "Towards Backdoor Stealthiness in Model Parameter Space", "summary": "Recent research on backdoor stealthiness focuses mainly on indistinguishable triggers in input space and inseparable backdoor representations in feature space, aiming to circumvent backdoor defenses that examine these respective spaces. However, existing backdoor attacks are typically designed to resist a specific type of backdoor defense without considering the diverse range of defense mechanisms. Based on this observation, we pose a natural question: Are current backdoor attacks truly a real-world threat when facing diverse practical defenses?\n  To answer this question, we examine 12 common backdoor attacks that focus on input-space or feature-space stealthiness and 17 diverse representative defenses. Surprisingly, we reveal a critical blind spot: Backdoor attacks designed to be stealthy in input and feature spaces can be mitigated by examining backdoored models in parameter space. To investigate the underlying causes behind this common vulnerability, we study the characteristics of backdoor attacks in the parameter space. Notably, we find that input- and feature-space attacks introduce prominent backdoor-related neurons in parameter space, which are not thoroughly considered by current backdoor attacks. Taking comprehensive stealthiness into account, we propose a novel supply-chain attack called Grond. Grond limits the parameter changes by a simple yet effective module, Adversarial Backdoor Injection (ABI), which adaptively increases the parameter-space stealthiness during the backdoor injection. Extensive experiments demonstrate that Grond outperforms all 12 backdoor attacks against state-of-the-art (including adaptive) defenses on CIFAR-10, GTSRB, and a subset of ImageNet. In addition, we show that ABI consistently improves the effectiveness of common backdoor attacks.", "published": "2025-01-10T12:49:12Z", "updated": "2025-12-12T07:43:57Z", "authors": ["Xiaoyun Xu", "Zhuoran Liu", "Stefanos Koffas", "Stjepan Picek"], "pdf_url": "https://arxiv.org/pdf/2501.05928v3"}
{"id": "http://arxiv.org/abs/2512.11316v1", "title": "Visualisation for the CIS benchmark scanning results", "summary": "In this paper, we introduce GraphSecure, a web application that provides advanced analysis and visualisation of security scanning results. GraphSecure enables users to initiate scans for their AWS account, validate them against specific Center for Internet Security (CIS) Benchmarks and return results, showcase those returned results in the form of statistical charts and warn the users about their account status.", "published": "2025-12-12T06:31:58Z", "updated": "2025-12-12T06:31:58Z", "authors": ["Zhenshuo Zhao", "Maria Spichkova", "Duttkumari Champavat", "Juilee N. Kulkarni", "Sahil Singla", "Muhammad A. Zulkefli", "Pradhuman Khandelwal"], "pdf_url": "https://arxiv.org/pdf/2512.11316v1"}
{"id": "http://arxiv.org/abs/2507.05578v2", "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they also exhibit memorization of their training data. This phenomenon raises critical questions about model behavior, privacy risks, and the boundary between learning and memorization. Addressing these concerns, this paper synthesizes recent studies and investigates the landscape of memorization, the factors influencing it, and methods for its detection and mitigation. We explore key drivers, including training data duplication, training dynamics, and fine-tuning procedures that influence data memorization. In addition, we examine methodologies such as prefix-based extraction, membership inference, and adversarial prompting, assessing their effectiveness in detecting and measuring memorized content. Beyond technical analysis, we also explore the broader implications of memorization, including the legal and ethical implications. Finally, we discuss mitigation strategies, including data cleaning, differential privacy, and post-training unlearning, while highlighting open challenges in balancing the need to minimize harmful memorization with model utility. This paper provides a comprehensive overview of the current state of research on LLM memorization across technical, privacy, and performance dimensions, identifying critical directions for future work.", "published": "2025-07-08T01:30:46Z", "updated": "2025-12-12T06:27:27Z", "authors": ["Alexander Xiong", "Xuandong Zhao", "Aneesh Pappu", "Dawn Song"], "pdf_url": "https://arxiv.org/pdf/2507.05578v2"}
{"id": "http://arxiv.org/abs/2512.11286v1", "title": "A Survey of OAM-Encoded High-Dimensional Quantum Key Distribution: Foundations, Experiments, and Recent Trends", "summary": "High-dimensional quantum key distribution (HD-QKD) enhances information efficiency and noise tolerance by encoding data in large Hilbert spaces. The orbital angular momentum (OAM) of light provides a scalable basis for such encoding and supports high-dimensional photonic communication. Practical OAM-based implementations remain constrained by challenges in state generation, transmission, and detection. This survey offers a consolidated overview of OAM-encoded HD-QKD, outlining fundamental principles, representative experiments, and system-level limitations. Recent progress in hybrid encodings, mode sorting, adaptive optics, and TF, CV, MDI, and DI frameworks is summarized with emphasis on practical feasibility.", "published": "2025-12-12T05:19:51Z", "updated": "2025-12-12T05:19:51Z", "authors": ["Huan Zhang", "Zhenyu Cao", "Yu Sun", "Hu Jin"], "pdf_url": "https://arxiv.org/pdf/2512.11286v1"}
{"id": "http://arxiv.org/abs/2506.16666v3", "title": "The Hitchhiker's Guide to Efficient, End-to-End, and Tight DP Auditing", "summary": "In this paper, we systematize research on auditing Differential Privacy (DP) techniques, aiming to identify key insights and open challenges. First, we introduce a comprehensive framework for reviewing work in the field and establish three cross-contextual desiderata that DP audits should target -- namely, efficiency, end-to-end-ness, and tightness. Then, we systematize the modes of operation of state-of-the-art DP auditing techniques, including threat models, attacks, and evaluation functions. This allows us to highlight key details overlooked by prior work, analyze the limiting factors to achieving the three desiderata, and identify open research problems. Overall, our work provides a reusable and systematic methodology geared to assess progress in the field and identify friction points and future directions for our community to focus on.", "published": "2025-06-20T00:32:59Z", "updated": "2025-12-12T05:16:09Z", "authors": ["Meenatchi Sundaram Muthu Selva Annamalai", "Borja Balle", "Jamie Hayes", "Georgios Kaissis", "Emiliano De Cristofaro"], "pdf_url": "https://arxiv.org/pdf/2506.16666v3"}
{"id": "http://arxiv.org/abs/2512.09385v2", "title": "BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks", "summary": "The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts.", "published": "2025-12-10T07:30:03Z", "updated": "2025-12-12T04:56:44Z", "authors": ["Uisang Lee", "Changhoon Chung", "Junmo Lee", "Soo-Mook Moon"], "pdf_url": "https://arxiv.org/pdf/2512.09385v2"}
{"id": "http://arxiv.org/abs/2512.11272v1", "title": "Vision-Based Learning for Cyberattack Detection in Blockchain Smart Contracts and Transactions", "summary": "Blockchain technology has experienced rapid growth and has been widely adopted across various sectors, including healthcare, finance, and energy. However, blockchain platforms remain vulnerable to a broad range of cyberattacks, particularly those aimed at exploiting transactions and smart contracts (SCs) to steal digital assets or compromise system integrity. To address this issue, we propose a novel and effective framework for detecting cyberattacks within blockchain systems. Our framework begins with a preprocessing tool that uses Natural Language Processing (NLP) techniques to transform key features of blockchain transactions into image representations. These images are then analyzed through vision-based analysis using Vision Transformers (ViT), a recent advancement in computer vision known for its superior ability to capture complex patterns and semantic relationships. By integrating NLP-based preprocessing with vision-based learning, our framework can detect a wide variety of attack types. Experimental evaluations on benchmark datasets demonstrate that our approach significantly outperforms existing state-of-the-art methods in terms of both accuracy (achieving 99.5%) and robustness in cyberattack detection for blockchain transactions and SCs.", "published": "2025-12-12T04:28:06Z", "updated": "2025-12-12T04:28:06Z", "authors": ["Do Hai Son", "Le Vu Hieu", "Tran Viet Khoa", "Yibeltal F. Alem", "Hoang Trong Minh", "Tran Thi Thuy Quynh", "Nguyen Viet Ha", "Nguyen Linh Trung"], "pdf_url": "https://arxiv.org/pdf/2512.11272v1"}
{"id": "http://arxiv.org/abs/2512.11269v1", "title": "A Scalable Multi-GPU Framework for Encrypted Large-Model Inference", "summary": "Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.", "published": "2025-12-12T04:15:38Z", "updated": "2025-12-12T04:15:38Z", "authors": ["Siddharth Jayashankar", "Joshua Kim", "Michael B. Sullivan", "Wenting Zheng", "Dimitrios Skarlatos"], "pdf_url": "https://arxiv.org/pdf/2512.11269v1"}
{"id": "http://arxiv.org/abs/2504.21228v2", "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks", "summary": "Large Language Models (LLMs) are susceptible to indirect prompt injection attacks, in which the model inadvertently responds to task messages injected within the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune, a defense method that identifies and prunes task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to interpret the input prompt context purely as data rather than as cues for instruction following. To identify these neurons, we introduce a neural attribution mechanism guided by a preferential attribution loss, which enables effective attribution with only a few samples while preserving response quality after pruning. We further enhance the efficacy of neural attribution by leveraging an observed triggering effect inherent in the model's response generation behavior. Notably, our approach does not impose additional formatting on the prompt or introduce extra test-time LLM calls. Experiments show that CachePrune can significantly reduce attack success rates while maintaining clean response quality.", "published": "2025-04-29T23:42:21Z", "updated": "2025-12-12T02:25:34Z", "authors": ["Rui Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ruiyi Zhang", "Ryan Rossi", "Subrata Mitra", "Lina Yao", "Julian McAuley"], "pdf_url": "https://arxiv.org/pdf/2504.21228v2"}
