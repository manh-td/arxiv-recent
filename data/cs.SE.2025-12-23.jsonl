{"id": "http://arxiv.org/abs/2512.20482v1", "title": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization", "summary": "Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.", "published": "2025-12-23T16:18:39Z", "updated": "2025-12-23T16:18:39Z", "authors": ["Revanth Gangi Reddy", "Ye Liu", "Wenting Zhao", "JaeHyeok Doo", "Tarun Suresh", "Daniel Lee", "Caiming Xiong", "Yingbo Zhou", "Semih Yavuz", "Shafiq Joty"], "pdf_url": "https://arxiv.org/pdf/2512.20482v1"}
{"id": "http://arxiv.org/abs/2511.21878v2", "title": "Advancing Automated In-Isolation Validation in Repository-Level Code Translation", "summary": "Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.", "published": "2025-11-26T19:53:46Z", "updated": "2025-12-23T15:17:24Z", "authors": ["Kaiyao Ke", "Ali Reza Ibrahimzada", "Rangeet Pan", "Saurabh Sinha", "Reyhaneh Jabbarvand"], "pdf_url": "https://arxiv.org/pdf/2511.21878v2"}
{"id": "http://arxiv.org/abs/2512.20402v1", "title": "iblock: Accurate and Scalable Bitcoin Simulations with OMNeT++", "summary": "This paper proposes iblock, a comprehensive C++ library for Bitcoin simulation, designed for OMNeT++. iblock offers superior efficiency and scalability with respect to state-of-the-art simulators, which are typically written in high-level languages. Moreover, the possible integration with other OMNeT++ libraries allows highly detailed simulations. We measure iblock's performance against a state-of-the-art blockchain simulator, proving that it is more efficient at the same level of simulation detail. We also validate iblock by using it to simulate different scenarios such as the normal Bitcoin operation and the selfish mine attack, showing that simulation results are coherent with theoretical expectations.", "published": "2025-12-23T14:43:03Z", "updated": "2025-12-23T14:43:03Z", "authors": ["Niccolò Scatena", "Pericle Perazzo", "Giovanni Nardini"], "pdf_url": "https://arxiv.org/pdf/2512.20402v1"}
{"id": "http://arxiv.org/abs/2512.20396v1", "title": "Symmaries: Automatic Inference of Formal Security Summaries for Java Programs", "summary": "We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.", "published": "2025-12-23T14:33:31Z", "updated": "2025-12-23T14:33:31Z", "authors": ["Narges Khakpour", "Nicolas Berthier"], "pdf_url": "https://arxiv.org/pdf/2512.20396v1"}
{"id": "http://arxiv.org/abs/2512.20381v1", "title": "Identifying Appropriately-Sized Services with Deep Reinforcement Learning", "summary": "Service-based architecture (SBA) has gained attention in industry and academia as a means to modernize legacy systems. It refers to a design style that enables systems to be developed as suites of small, loosely coupled, and autonomous components (services) that encapsulate functionality and communicate via language-agnostic APIs. However, defining appropriately sized services that capture cohesive subsets of system functionality remains challenging. Existing work often relies on the availability of documentation, access to project personnel, or a priori knowledge of the target number of services, assumptions that do not hold in many real-world scenarios. Our work addresses these limitations using a deep reinforcement learning-based approach to identify appropriately sized services directly from implementation artifacts. We present Rake, a reinforcement learning-based technique that leverages available system documentation and source code to guide service decomposition at the level of implementation methods. Rake does not require specific documentation or access to project personnel and is language-agnostic. It also supports a customizable objective function that balances modularization quality and business capability alignment, i.e., the degree to which a service covers the targeted business capability. We applied Rake to four open-source legacy projects and compared it with two state-of-the-art techniques. On average, Rake achieved 7-14 percent higher modularization quality and 18-22 percent stronger business capability alignment. Our results further show that optimizing solely for business context can degrade decomposition quality in tightly coupled systems, highlighting the need for balanced objectives.", "published": "2025-12-23T14:12:02Z", "updated": "2025-12-23T14:12:02Z", "authors": ["Syeda Tasnim Fabiha", "Saad Shafiq", "Wesley Klewerton Guez Assunção", "Nenad Medvidović"], "pdf_url": "https://arxiv.org/pdf/2512.20381v1"}
{"id": "http://arxiv.org/abs/2512.20345v1", "title": "A Comprehensive Study of Bugs in Modern Distributed Deep Learning Systems", "summary": "In today's data-driven era, deep learning is vital for processing massive datasets, yet single-device training is constrained by computational and memory limits. Distributed deep learning overcomes these challenges by leveraging multiple GPUs or machines in parallel. While general-purpose frameworks (e.g., TensorFlow and PyTorch) provide distributed capabilities, these are often add-on features that demand significant manual effort for advanced parallelism, underscoring the need for specialized frameworks. This study conducts the first large-scale empirical analysis of practitioner challenges in dedicated distributed frameworks. We examine 849 real-world issues from DeepSpeed, Megatron-LM, and Colossal-AI and construct a taxonomy of 34 bug symptoms, 28 root causes, and 6 fix patterns. Crucially, we establish explicit mappings between symptoms, causes, and fixes across distributed training stages, enabling a systematic understanding of how issues emerge and are resolved. Our results show that 45.1\\% of bug symptoms are unique to distributed frameworks, with setup failures, memory issues, and performance anomalies being the most prevalent. Moreover, 95\\% of issues in the communication setup stage occur exclusively in distributed contexts. We also find over 60\\% of cases can be resolved through version and dependency management, and distributed feature, API, and communication tuning. Based on these findings, we provide actionable implications.", "published": "2025-12-23T13:27:05Z", "updated": "2025-12-23T13:27:05Z", "authors": ["Xiaoxue Ma", "Wanwei Zhan", "Jiale Chen", "Yishu Li", "Jacky Keung", "Federica Sarro"], "pdf_url": "https://arxiv.org/pdf/2512.20345v1"}
{"id": "http://arxiv.org/abs/2512.20334v1", "title": "Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation", "summary": "With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.\n  This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.", "published": "2025-12-23T13:08:19Z", "updated": "2025-12-23T13:08:19Z", "authors": ["Yuan Huang", "Yukang Zhou", "Xiangping Chen", "Zibin Zheng"], "pdf_url": "https://arxiv.org/pdf/2512.20334v1"}
{"id": "http://arxiv.org/abs/2512.20328v1", "title": "Toward Explaining Large Language Models in Software Engineering Tasks", "summary": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.", "published": "2025-12-23T12:56:18Z", "updated": "2025-12-23T12:56:18Z", "authors": ["Antonio Vitale", "Khai-Nguyen Nguyen", "Denys Poshyvanyk", "Rocco Oliveto", "Simone Scalabrino", "Antonio Mastropaolo"], "pdf_url": "https://arxiv.org/pdf/2512.20328v1"}
{"id": "http://arxiv.org/abs/2512.20279v1", "title": "Auditing Reproducibility in Non-Targeted Analysis: 103 LC/GC--HRMS Tools Reveal Temporal Divergence Between Openness and Operability", "summary": "In 2008, melamine in infant formula forced laboratories across three continents to verify a compound they had never monitored. Non-targeted analysis using LC/GC-HRMS handles these cases. But when findings trigger regulatory action, reproducibility becomes operational: can an independent laboratory repeat the analysis and reach the same conclusion?\n  We assessed 103 tools (2004-2025) against six pillars drawn from FAIR and BP4NTA principles: laboratory validation (C1), data availability (C2), code availability (C3), standardised formats (C4), knowledge integration (C5), and portable implementation (C6). Health contributed 51 tools, Pharma 31, and Chemistry 21.\n  Nine in ten tools shared data (C2, 90/103, 87%). Fewer than four in ten supported portable implementations (C6, 40/103, 39%). Validation and portability rarely appeared together (C1+C6, 18/103, 17%). Over twenty-one years, openness climbed from 56% to 86% while operability dropped from 55% to 43%. No tool addressed food safety.\n  Journal data-sharing policies increased what authors share but not what reviewers can run. Tools became easier to find but harder to execute. Strengthening C1, C4, and C6 would turn documented artifacts into workflows that external laboratories can replay.", "published": "2025-12-23T11:35:30Z", "updated": "2025-12-23T11:35:30Z", "authors": ["Sarah Alsubaie", "Sakhaa Alsaedi", "Xin Gao"], "pdf_url": "https://arxiv.org/pdf/2512.20279v1"}
{"id": "http://arxiv.org/abs/2512.20245v1", "title": "Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds", "summary": "The memory of contemporary Large Language Models is bound by a physical paradox: as they learn, they fill up. The linear accumulation (O(N)) of Key-Value states treats context as a warehouse of static artifacts, eventually forcing a destructive choice between amnesia and latency. We challenge this discrete orthodoxy, proposing that long-term memory is not the storage of items, but the persistence of a trajectory. We introduce Phonetic Trajectory Memory (PTM), a neuro-symbolic architecture that encodes language not as a sequence of tensors, but as a continuous path on an ergodic manifold governed by irrational rotation matrices. By decoupling the navigation (an invariant O(1) geometric signal) from the reconstruction (a probabilistic generative act), PTM achieves a compression magnitude of greater than 3,000x relative to dense caches. We demonstrate that retrieval becomes a process of resonance: the phonetic trace stabilizes the model against hallucination via \"Signal Consensus\" mechanism, securing up to approximately 92% factual accuracy. While this aggressive abstraction alters generative texture, it unlocks immediate access latency (approximately 34ms) independent of depth. Our results suggest that infinite context does not require infinite silicon; it requires treating memory not as data to be stored, but as a reconstructive process acting on a conserved, undying physical signal.", "published": "2025-12-23T10:55:32Z", "updated": "2025-12-23T10:55:32Z", "authors": ["Tarik Houichime", "Abdelghani Souhar", "Younes El Amrani"], "pdf_url": "https://arxiv.org/pdf/2512.20245v1"}
{"id": "http://arxiv.org/abs/2512.18261v2", "title": "Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective", "summary": "Artificial Intelligence (AI) has revolutionized software development, particularly by automating repetitive tasks and improving developer productivity. While these advancements are well-documented, the use of AI-powered tools for Software Vulnerability Management (SVM), such as vulnerability detection and repair, remains underexplored in industry settings. To bridge this gap, our study aims to determine the extent of the adoption of AI-powered tools for SVM, identify barriers and facilitators to the use, and gather insights to help improve the tools to meet industry needs better. We conducted a survey study involving 60 practitioners from diverse industry sectors across 27 countries. The survey incorporates both quantitative and qualitative questions to analyze the adoption trends, assess tool strengths, identify practical challenges, and uncover opportunities for improvement. Our findings indicate that AI-powered tools are used throughout the SVM life cycle, with 69% of users reporting satisfaction with their current use. Practitioners value these tools for their speed, coverage, and accessibility. However, concerns about false positives, missing context, and trust issues remain prevalent. We observe a socio-technical adoption pattern in which AI outputs are filtered through human oversight and organizational governance. To support safe and effective use of AI for SVM, we recommend improvements in explainability, contextual awareness, integration workflows, and validation practices. We assert that these findings can offer practical guidance for practitioners, tool developers, and researchers seeking to enhance secure software development through the use of AI.", "published": "2025-12-20T07:58:35Z", "updated": "2025-12-23T10:10:18Z", "authors": ["M. Mehdi Kholoosi", "Triet Huynh Minh Le", "M. Ali Babar"], "pdf_url": "https://arxiv.org/pdf/2512.18261v2"}
{"id": "http://arxiv.org/abs/2512.20203v1", "title": "Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair", "summary": "The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.\n  To tackle the two limitations, we propose \\sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \\sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \\sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \\sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \\sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \\sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.", "published": "2025-12-23T09:54:22Z", "updated": "2025-12-23T09:54:22Z", "authors": ["Zhenlei Ye", "Xiaobing Sun", "Sicong Cao", "Lili Bo", "Bin Li"], "pdf_url": "https://arxiv.org/pdf/2512.20203v1"}
{"id": "http://arxiv.org/abs/2510.04711v2", "title": "Rethinking the Evaluation of Microservice RCA with a Fault Propagation-Aware Benchmark", "summary": "While cloud-native microservice architectures have revolutionized software development, their inherent operational complexity makes failure Root Cause Analysis (RCA) a critical yet challenging task. Numerous data-driven RCA models have been proposed to address this challenge. However, we find that the benchmarks used to evaluate these models are often too simple to reflect real-world scenarios. Our preliminary study reveals that simple rule-based methods can achieve performance comparable to or even surpassing state-of-the-art (SOTA) models on four widely used public benchmarks. This finding suggests that the oversimplification of existing benchmarks might lead to an overestimation of the performance of RCA methods. To further investigate the oversimplification issue, we conduct a systematic analysis of popular public RCA benchmarks, identifying key limitations in their fault injection strategies, call graph structures, and telemetry signal patterns. Based on these insights, we propose an automated framework for generating more challenging and comprehensive benchmarks that include complex fault propagation scenarios. Our new dataset contains 1,430 validated failure cases from 9,152 fault injections, covering 25 fault types across 6 categories, dynamic workloads, and hierarchical ground-truth labels that map failures from services down to code-level causes. Crucially, to ensure the failure cases are relevant to IT operations, each case is validated to have a discernible impact on user-facing SLIs. Our re-evaluation of 11 SOTA models on this new benchmark shows that they achieve low Top@1 accuracies, averaging 0.21, with the best-performing model reaching merely 0.37, and execution times escalating from seconds to hours.", "published": "2025-10-06T11:30:03Z", "updated": "2025-12-23T09:28:44Z", "authors": ["Aoyang Fang", "Songhan Zhang", "Yifan Yang", "Haotong Wu", "Junjielong Xu", "Xuyang Wang", "Rui Wang", "Manyi Wang", "Qisheng Lu", "Pinjia He"], "pdf_url": "https://arxiv.org/pdf/2510.04711v2"}
{"id": "http://arxiv.org/abs/2512.20159v1", "title": "AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration", "summary": "Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.\n  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...", "published": "2025-12-23T08:39:22Z", "updated": "2025-12-23T08:39:22Z", "authors": ["Ruiqi Wang", "Xinchen Wang", "Cuiyun Gao", "Chun Yong Chong", "Xin Xia", "Qing Liao"], "pdf_url": "https://arxiv.org/pdf/2512.20159v1"}
{"id": "http://arxiv.org/abs/2512.17814v2", "title": "LLM-based Behaviour Driven Development for Hardware Design", "summary": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.\n  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.", "published": "2025-12-19T17:19:08Z", "updated": "2025-12-23T08:00:41Z", "authors": ["Rolf Drechsler", "Qian Liu"], "pdf_url": "https://arxiv.org/pdf/2512.17814v2"}
{"id": "http://arxiv.org/abs/2510.06187v3", "title": "Automated Program Repair of Uncompilable Student Code", "summary": "A significant portion of student programming submissions in CS1 learning environments are uncompilable, limiting their use in student modeling and downstream knowledge tracing. Traditional modeling pipelines often exclude these cases, discarding observations of student learning. This study investigates automated program repair as a strategy to recover uncompilable code while preserving students' structural intent for use in student modeling. Within this framework, we assess large language models (LLMs) as repair agents under high- and low-context prompting conditions. Repairs were evaluated for compilability, edit distance, and preservation of students' original structure and logic. While all models produced compilable repairs, they differed in how well they preserve students' control flow and code structure, affecting their pedagogical utility. By recovering uncompilable submissions, this work enables richer and more comprehensive analyses of learners' coding processes and development over time.", "published": "2025-10-07T17:46:33Z", "updated": "2025-12-23T07:41:07Z", "authors": ["Griffin Pitts", "Aum Pandya", "Darsh Rank", "Tirth Bhatt", "Muntasir Hoq", "Bita Akram"], "pdf_url": "https://arxiv.org/pdf/2510.06187v3"}
{"id": "http://arxiv.org/abs/2512.20083v1", "title": "Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing", "summary": "As embodied agents advance toward real-world deployment, ensuring optimal decisions becomes critical for resource-constrained applications. Current evaluation methods focus primarily on functional correctness, overlooking the non-functional optimality of generated plans. This gap can lead to significant performance degradation and resource waste. We identify and formalize the problem of Non-optimal Decisions (NoDs), where agents complete tasks successfully but inefficiently. We present NoD-DGMT, a systematic framework for detecting NoDs in embodied agent task planning via diversity-guided metamorphic testing. Our key insight is that optimal planners should exhibit invariant behavioral properties under specific transformations. We design four novel metamorphic relations capturing fundamental optimality properties: position detour suboptimality, action optimality completeness, condition refinement monotonicity, and scene perturbation invariance. To maximize detection efficiency, we introduce a diversity-guided selection strategy that actively selects test cases exploring different violation categories, avoiding redundant evaluations while ensuring comprehensive diversity coverage. Extensive experiments on the AI2-THOR simulator with four state-of-the-art planning models demonstrate that NoD-DGMT achieves violation detection rates of 31.9% on average, with our diversity-guided filter improving rates by 4.3% and diversity scores by 3.3 on average. NoD-DGMT significantly outperforms six baseline methods, with 16.8% relative improvement over the best baseline, and demonstrates consistent superiority across different model architectures and task complexities.", "published": "2025-12-23T06:27:18Z", "updated": "2025-12-23T06:27:18Z", "authors": ["Wenzhao Wu", "Yahui Tang", "Mingfei Cheng", "Wenbing Tang", "Yuan Zhou", "Yang Liu"], "pdf_url": "https://arxiv.org/pdf/2512.20083v1"}
{"id": "http://arxiv.org/abs/2512.19997v1", "title": "BacAlarm: Mining and Simulating Composite API Traffic to Prevent Broken Access Control Violations", "summary": "Broken Access Control (BAC) violations, which consistently rank among the top five security risks in the OWASP API Security Top 10, refer to unauthorized access attempts arising from BAC vulnerabilities, whose successful exploitation can impose significant risks on exposed application programming interfaces (APIs). In recent years, learning-based methods have demonstrated promising prospects in detecting various types of malicious activities. However, in real-network operation and maintenance scenarios, leveraging learning-based methods for BAC detection faces two critical challenges. Firstly, under the RESTful API design principles, most systems omit recording composite traffic for performance, and together with ethical and legal bans on directly testing real-world systems, this leads to a critical shortage of training data for detecting BAC violations. Secondly, common malicious behaviors such as SQL injection typically generate individual access traffic that is inherently anomalous. In contrast, BAC is usually composed of multiple correlated access requests that appear normal when examined in isolation. To tackle these problems, we introduce \\BAC, an approach for establishing a BAC violation detection model by generating and utilizing API traffic data. The \\BAC consists of an API Traffic Generator and a BAC Detector. Experimental results show that \\BAC outperforms current state-of-the-art invariant-based and learning-based methods with the $\\text{F}_1$ and MCC improving by 21.2\\% and 24.1\\%.", "published": "2025-12-23T02:45:36Z", "updated": "2025-12-23T02:45:36Z", "authors": ["Yanjing Yang", "He Zhang", "Bohan Liu", "Jinwei Xu", "Jinghao Hu", "Liming Dong", "Zhewen Mao", "Dongxue Pan"], "pdf_url": "https://arxiv.org/pdf/2512.19997v1"}
{"id": "http://arxiv.org/abs/2512.19980v1", "title": "Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?", "summary": "Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.", "published": "2025-12-23T02:04:13Z", "updated": "2025-12-23T02:04:13Z", "authors": ["Zhe Yin", "Xiaodong Gu", "Beijun Shen"], "pdf_url": "https://arxiv.org/pdf/2512.19980v1"}
{"id": "http://arxiv.org/abs/2406.03839v6", "title": "PCART: Automated Repair of Python API Parameter Compatibility Issues", "summary": "In modern software development, Python third-party libraries play a critical role, especially in fields like deep learning and scientific computing. However, API parameters in these libraries often change during evolution, leading to compatibility issues for client applications reliant on specific versions. Python's flexible parameter-passing mechanism further complicates this, as different passing methods can result in different API compatibility. Currently, no tool can automatically detect and repair Python API parameter compatibility issues. To fill this gap, we introduce PCART, the first solution to fully automate the process of API extraction, code instrumentation, API mapping establishment, compatibility assessment, repair, and validation. PCART handles various types of Python API parameter compatibility issues, including parameter addition, removal, renaming, reordering, and the conversion of positional to keyword parameters. To evaluate PCART, we construct PCBENCH, a large-scale benchmark comprising 47,478 test cases mutated from 844 parameter-changed APIs across 33 popular Python libraries. Evaluation results demonstrate that PCART is both effective and efficient, significantly outperforming existing tools (MLCatchUp and Relancer) and the large language model ChatGPT (GPT-4o), achieving an F1-score of 96.51% in detecting API parameter compatibility issues and a repair precision of 91.97%. Further evaluation on 30 real-world Python projects from GitHub confirms PCART's practicality. We believe PCART can significantly reduce the time programmers spend maintaining Python API updates and advance the automation of Python API compatibility issue repair.", "published": "2024-06-06T08:15:12Z", "updated": "2025-12-23T01:41:38Z", "authors": ["Shuai Zhang", "Guanping Xiao", "Jun Wang", "Huashan Lei", "Gangqiang He", "Yepang Liu", "Zheng Zheng"], "pdf_url": "https://arxiv.org/pdf/2406.03839v6"}
{"id": "http://arxiv.org/abs/2406.15596v3", "title": "DiVerify: Hardening Identity-Based Software Signing with Diverse-Context Scopes", "summary": "Identity-based code signing enables software developers to digitally sign their code using cryptographic keys. This key is then linked to an identity (e.g., through an identity provider), allowing signers to verify both the code's origin and integrity. However, this code-identity binding is only as trustworthy as the mechanisms enforcing it. State-of-the-art identity-based code signing schemes present a major shortcoming: these schemes fail to provide verifiable information about the context in which a signature is generated. This verifiability is crucial given that modern attackers have subverted long-established security assumptions, namely, that the identity provider ecosystem, as well as signing software itself, is trusted.\n  To address these issues, this paper introduces a diverse identity verification framework, DiVerify, that distributes identity-based verification across multiple entities and enforces stronger guarantees about the signing context. DiVerify makes it possible to provide end-to-end verifiability of not only a signer's identity (via multiple such signals), but also a signer's software stack (e.g., to verify no malware is present on a system at the time of signing). DiVerify is aimed at deployability, and leverages a meta-protocol to gather various trust signals and a binding mechanism to address the aforementioned, novel software supply chain attack vectors. We evaluate DiVerify's performance and confirm it is cheap to deploy and non-intrusive to developers: it only incurs a few kilobytes of additional storage (less than 0.4 percent of the average package size in widely used ecosystems like PyPI), and signing completes in under 100ms on a server-grade deployment.", "published": "2024-06-21T18:53:52Z", "updated": "2025-12-23T01:14:43Z", "authors": ["Chinenye Okafor", "James C. Davis", "Santiago Torres-Arias"], "pdf_url": "https://arxiv.org/pdf/2406.15596v3"}
{"id": "http://arxiv.org/abs/2512.19958v1", "title": "Towards Analysing Invoices and Receipts with Amazon Textract", "summary": "This paper presents an evaluation of the AWS Textract in the context of extracting data from receipts. We analyse Textract functionalities using a dataset that includes receipts of varied formats and conditions. Our analysis provided a qualitative view of Textract strengths and limitations. While the receipts totals were consistently detected, we also observed typical issues and irregularities that were often influenced by image quality and layout. Based on the analysis of the observations, we propose mitigation strategies.", "published": "2025-12-23T01:10:25Z", "updated": "2025-12-23T01:10:25Z", "authors": ["Sneha Oommen", "Gabby Sanchez", "Cassandra T. Britto", "Di Wang", "Jordan Chiou", "Maria Spichkova"], "pdf_url": "https://arxiv.org/pdf/2512.19958v1"}
