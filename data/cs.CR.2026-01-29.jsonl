{"id": "http://arxiv.org/abs/2505.00554v4", "title": "Notes on Univariate Sumcheck", "summary": "Two candidate approaches for univariate sumcheck over roots of unity are presented. The first takes the form of a multilinear evaluation protocol, which can be combined with the standard multivariate sumcheck protocol. The other consists of a direct reduction from univariate sumcheck to multilinear evaluation, which can be combined with Gemini (Bootle et al., Eurocrypt 2022). Both approaches optionally support a very natural exponential round reduction from $m$ to $\\log(m)$ while retaining asymptotically linear prover time.", "published": "2025-05-01T14:29:32Z", "updated": "2026-01-29T13:59:30Z", "authors": ["Malcom Mohamed"], "pdf_url": "https://arxiv.org/pdf/2505.00554v4"}
{"id": "http://arxiv.org/abs/2601.21682v1", "title": "FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning", "summary": "Large language models (LLMs) demonstrate impressive capabilities across diverse tasks but raise concerns about privacy, copyright, and harmful materials. Existing LLM unlearning methods rarely consider the continual and high-volume nature of real-world deletion requests, which can cause utility degradation and catastrophic forgetting as requests accumulate. To address this challenge, we introduce \\fit, a framework for continual unlearning that handles large numbers of deletion requests while maintaining robustness against both catastrophic forgetting and post-unlearning recovery. \\fit mitigates degradation through rigorous data \\underline{F}iltering, \\underline{I}mportance-aware updates, and \\underline{T}argeted layer attribution, enabling stable performance across long sequences of unlearning operations and achieving a favorable balance between forgetting effectiveness and utility retention. To support realistic evaluation, we present \\textbf{PCH}, a benchmark covering \\textbf{P}ersonal information, \\textbf{C}opyright, and \\textbf{H}armful content in sequential deletion scenarios, along with two symmetric metrics, Forget Degree (F.D.) and Retain Utility (R.U.), which jointly assess forgetting quality and utility preservation. Extensive experiments on four open-source LLMs with hundreds of deletion requests show that \\fit achieves the strongest trade-off between F.D. and R.U., surpasses existing methods on MMLU, CommonsenseQA, and GSM8K, and remains resistant against both relearning and quantization recovery attacks.", "published": "2026-01-29T13:15:32Z", "updated": "2026-01-29T13:15:32Z", "authors": ["Xiaoyu Xu", "Minxin Du", "Kun Fang", "Zi Liang", "Yaxin Xiao", "Zhicong Huang", "Cheng Hong", "Qingqing Ye", "Haibo Hu"], "pdf_url": "https://arxiv.org/pdf/2601.21682v1"}
{"id": "http://arxiv.org/abs/2601.21680v1", "title": "Incremental Fingerprinting in an Open World", "summary": "Network protocol fingerprinting is used to identify a protocol implementation by analyzing its input-output behavior. Traditionally, fingerprinting operates under a closed-world assumption, where models of all implementations are assumed to be available. However, this assumption is unrealistic in practice. When this assumption does not hold, fingerprinting results in numerous misclassifications without indicating that a model for an implementation is missing. Therefore, we introduce an open-world variant of the fingerprinting problem, where not all models are known in advance. We propose an incremental fingerprinting approach to solve the problem by combining active automata learning with closed-world fingerprinting. Our approach quickly determines whether the implementation under consideration matches an available model using fingerprinting and conformance checking. If no match is found, it learns a new model by exploiting the structure of available models. We prove the correctness of our approach and improvements in asymptotic complexity compared to naive baselines. Moreover, experimental results on a variety of protocols demonstrate a significant reduction in misclassifications and interactions with these black-boxes.", "published": "2026-01-29T13:14:15Z", "updated": "2026-01-29T13:14:15Z", "authors": ["Loes Kruger", "Paul Kobialka", "Andrea Pferscher", "Einar Broch Johnsen", "Sebastian Junges", "Jurriaan Rot"], "pdf_url": "https://arxiv.org/pdf/2601.21680v1"}
{"id": "http://arxiv.org/abs/2601.19684v2", "title": "LLM-Assisted Authentication and Fraud Detection", "summary": "User authentication and fraud detection face growing challenges as digital systems expand and adversaries adopt increasingly sophisticated tactics. Traditional knowledge-based authentication remains rigid, requiring exact word-for-word string matches that fail to accommodate natural human memory and linguistic variation. Meanwhile, fraud-detection pipelines struggle to keep pace with rapidly evolving scam behaviors, leading to high false-positive rates and frequent retraining cycles required. This work introduces two complementary LLM-enabled solutions, namely, an LLM-assisted authentication mechanism that evaluates semantic correctness rather than exact wording, supported by document segmentation and a hybrid scoring method combining LLM judgement with cosine-similarity metrics and a RAG-based fraud-detection pipeline that grounds LLM reasoning in curated evidence to reduce hallucinations and adapt to emerging scam patterns without model retraining. Experiments show that the authentication system accepts 99.5% of legitimate non-exact answers while maintaining a 0.1% false-acceptance rate, and that the RAG-enhanced fraud detection reduces false positives from 17.2% to 3.5%. Together, these findings demonstrate that LLMs can significantly improve both usability and robustness in security workflows, offering a more adaptive , explainable, and human-aligned approach to authentication and fraud detection.", "published": "2026-01-27T15:01:37Z", "updated": "2026-01-29T13:12:00Z", "authors": ["Emunah S-S. Chan", "Aldar C-F. Chan"], "pdf_url": "https://arxiv.org/pdf/2601.19684v2"}
{"id": "http://arxiv.org/abs/2601.21657v1", "title": "Authenticated encryption for space telemetry", "summary": "We explore how command stack protection requirements outlined in NASA-STD-1006A can be satisfied within the context of emergency space telemetry. Proposed implementation of lightweight authenticated encryption offers strong security without sacrificing performance in resource-constrained environments. It produces fixed-length messages, maintaining compatibility with the underlying data transport protocols. By focusing on predictable properties and robust authentication, we create a scheme that protects the confidentiality, integrity and authenticity of telemetry data in emergency communications while balancing security requirements with the operational constraints.", "published": "2026-01-29T12:57:27Z", "updated": "2026-01-29T12:57:27Z", "authors": ["Andrew Savchenko"], "pdf_url": "https://arxiv.org/pdf/2601.21657v1"}
{"id": "http://arxiv.org/abs/2601.00752v3", "title": "Three results on twisted $G-$codes and skew twisted $G-$codes", "summary": "In this paper we solve an open question formulated in the original paper of twisted skew group codes regarding when a twisted skew group code is checkable. Also, we prove that all ideals of dimension 3 over a twisted group algebra are abelian group codes, generalising another previous result over group algebras. Finally, we prove a bound on the dimension and distance of a twisted group code, as well as when such bound is reached.", "published": "2026-01-02T17:16:09Z", "updated": "2026-01-29T12:45:05Z", "authors": ["Alvaro Otero Sanchez"], "pdf_url": "https://arxiv.org/pdf/2601.00752v3"}
{"id": "http://arxiv.org/abs/2601.21636v1", "title": "Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation", "summary": "We study privacy amplification for differentially private model training with matrix factorization under random allocation (also known as the balls-in-bins model). Recent work by Choquette-Choo et al. (2025) proposes a sampling-based Monte Carlo approach to compute amplification parameters in this setting. However, their guarantees either only hold with some high probability or require random abstention by the mechanism. Furthermore, the required number of samples for ensuring $(ε,δ)$-DP is inversely proportional to $δ$. In contrast, we develop sampling-free bounds based on Rényi divergence and conditional composition. The former is facilitated by a dynamic programming formulation to efficiently compute the bounds. The latter complements it by offering stronger privacy guarantees for small $ε$, where Rényi divergence bounds inherently lead to an over-approximation. Our framework applies to arbitrary banded and non-banded matrices. Through numerical comparisons, we demonstrate the efficacy of our approach across a broad range of matrix mechanisms used in research and practice.", "published": "2026-01-29T12:40:29Z", "updated": "2026-01-29T12:40:29Z", "authors": ["Jan Schuchardt", "Nikita Kalinin"], "pdf_url": "https://arxiv.org/pdf/2601.21636v1"}
{"id": "http://arxiv.org/abs/2601.21628v1", "title": "Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise", "summary": "Diffusion models have achieved remarkable progress in image generation, but their increasing deployment raises serious concerns about privacy. In particular, fine-tuned models are highly vulnerable, as they are often fine-tuned on small and private datasets. Membership inference attacks (MIAs) are used to assess privacy risks by determining whether a specific sample was part of a model's training data. Existing MIAs against diffusion models either assume obtaining the intermediate results or require auxiliary datasets for training the shadow model. In this work, we utilized a critical yet overlooked vulnerability: the widely used noise schedules fail to fully eliminate semantic information in the images, resulting in residual semantic signals even at the maximum noise step. We empirically demonstrate that the fine-tuned diffusion model captures hidden correlations between the residual semantics in initial noise and the original images. Building on this insight, we propose a simple yet effective membership inference attack, which injects semantic information into the initial noise and infers membership by analyzing the model's generation result. Extensive experiments demonstrate that the semantic initial noise can strongly reveal membership information, highlighting the vulnerability of diffusion models to MIAs.", "published": "2026-01-29T12:29:01Z", "updated": "2026-01-29T12:29:01Z", "authors": ["Puwei Lian", "Yujun Cai", "Songze Li", "Bingkun Bao"], "pdf_url": "https://arxiv.org/pdf/2601.21628v1"}
{"id": "http://arxiv.org/abs/2601.21586v1", "title": "ICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses", "summary": "In-context learning (ICL) has become a powerful, data-efficient paradigm for text classification using large language models. However, its robustness against realistic adversarial threats remains largely unexplored. We introduce ICL-Evader, a novel black-box evasion attack framework that operates under a highly practical zero-query threat model, requiring no access to model parameters, gradients, or query-based feedback during attack generation. We design three novel attacks, Fake Claim, Template, and Needle-in-a-Haystack, that exploit inherent limitations of LLMs in processing in-context prompts. Evaluated across sentiment analysis, toxicity, and illicit promotion tasks, our attacks significantly degrade classifier performance (e.g., achieving up to 95.3% attack success rate), drastically outperforming traditional NLP attacks which prove ineffective under the same constraints. To counter these vulnerabilities, we systematically investigate defense strategies and identify a joint defense recipe that effectively mitigates all attacks with minimal utility loss (<5% accuracy degradation). Finally, we translate our defensive insights into an automated tool that proactively fortifies standard ICL prompts against adversarial evasion. This work provides a comprehensive security assessment of ICL, revealing critical vulnerabilities and offering practical solutions for building more robust systems. Our source code and evaluation datasets are publicly available at: https://github.com/ChaseSecurity/ICL-Evader .", "published": "2026-01-29T11:50:50Z", "updated": "2026-01-29T11:50:50Z", "authors": ["Ningyuan He", "Ronghong Huang", "Qianqian Tang", "Hongyu Wang", "Xianghang Mi", "Shanqing Guo"], "pdf_url": "https://arxiv.org/pdf/2601.21586v1"}
{"id": "http://arxiv.org/abs/2505.20955v3", "title": "Enhancing Membership Inference Attacks on Diffusion Models from a Frequency-Domain Perspective", "summary": "Diffusion models have achieved tremendous success in image generation, but they also raise significant concerns regarding privacy and copyright issues. Membership Inference Attacks (MIAs) are designed to ascertain whether specific data were utilized during a model's training phase. As current MIAs for diffusion models typically exploit the model's image prediction ability, we formalize them into a unified general paradigm which computes the membership score for membership identification. Under this paradigm, we empirically find that existing attacks overlook the inherent deficiency in how diffusion models process high-frequency information. Consequently, this deficiency leads to member data with more high-frequency content being misclassified as hold-out data, and hold-out data with less high-frequency content tend to be misclassified as member data. Moreover, we theoretically demonstrate that this deficiency reduces the membership advantage of attacks, thereby interfering with the effective discrimination of member data and hold-out data. Based on this insight, we propose a plug-and-play high-frequency filter module to mitigate the adverse effects of the deficiency, which can be seamlessly integrated into any attacks within this general paradigm without additional time costs. Extensive experiments corroborate that this module significantly improves the performance of baseline attacks across different datasets and models.", "published": "2025-05-27T09:50:11Z", "updated": "2026-01-29T11:43:24Z", "authors": ["Puwei Lian", "Yujun Cai", "Songze Li", "Bingkun Bao"], "pdf_url": "https://arxiv.org/pdf/2505.20955v3"}
{"id": "http://arxiv.org/abs/2510.02999v3", "title": "Untargeted Jailbreak Attack", "summary": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs) typically optimize adversarial suffixes to align the LLM output with predefined target responses. However, restricting the objective as inducing fixed targets inherently constrains the adversarial search space, limiting the overall attack efficacy. Furthermore, existing methods typically require numerous optimization iterations to fulfill the large gap between the fixed target and the original LLM output, resulting in low attack efficiency. To overcome these limitations, we propose the first gradient-based untargeted jailbreak attack (UJA), which relies on an untargeted objective to maximize the unsafety probability of the LLM output, without enforcing any response patterns. For tractable optimization, we further decompose this objective into two differentiable sub-objectives to search the optimal harmful response and the corresponding adversarial prompt, with a theoretical analysis to validate the decomposition. In contrast to existing attacks, UJA's unrestricted objective significantly expands the search space, enabling more flexible and efficient exploration of LLM vulnerabilities. Extensive evaluations show that UJA achieves over 80\\% attack success rates against recent safety-aligned LLMs with only 100 optimization iterations, outperforming the state-of-the-art gradient-based attacks by over 30\\%.", "published": "2025-10-03T13:38:56Z", "updated": "2026-01-29T11:30:58Z", "authors": ["Xinzhe Huang", "Wenjing Hu", "Tianhang Zheng", "Kedong Xiu", "Xiaojun Jia", "Di Wang", "Zhan Qin", "Kui Ren"], "pdf_url": "https://arxiv.org/pdf/2510.02999v3"}
{"id": "http://arxiv.org/abs/2412.00990v2", "title": "Seldom: An Anonymity Network with Selective Deanonymization", "summary": "While anonymity networks such as Tor provide invaluable privacy guarantees to society, they also enable all kinds of criminal activities. Consequently, many blameless citizens shy away from protecting their privacy using such technology for fear of being associated with criminals. To grasp the potential for alternative privacy protection for those users, we design Seldom, an anonymity network with integrated selective deanonymization that disincentivizes criminal activity. Seldom enables law enforcement agencies to selectively access otherwise anonymized identities of misbehaving users while providing technical guarantees preventing these access rights from being misused. Seldom further ensures translucency, as each access request is approved by a trustworthy consortium of impartial entities and eventually disclosed to the public (without interfering with ongoing investigations). To demonstrate Seldom's feasibility and applicability, we base our implementation on Tor, the most widely used anonymity network. Our evaluation indicates minimal latency, processing, and bandwidth overheads compared to Tor; Seldom's main costs stem from storing flow records and encrypted identities. With at most 636 TB of storage required in total to retain the encrypted identifiers of a Tor-sized network for two years, Seldom provides a practical and deployable technical solution to the inherent problem of criminal activities in anonymity networks. As such, Seldom sheds new light on the potentials and limitations when integrating selective deanonymization into anonymity networks.", "published": "2024-12-01T22:31:31Z", "updated": "2026-01-29T11:13:52Z", "authors": ["Eric Wagner", "Roman Matzutt", "Martin Henze"], "pdf_url": "https://arxiv.org/pdf/2412.00990v2"}
{"id": "http://arxiv.org/abs/2509.23694v4", "title": "SafeSearch: Automated Red-Teaming of LLM-Based Search Agents", "summary": "Search agents connect LLMs to the Internet, enabling them to access broader and more up-to-date information. However, this also introduces a new threat surface: unreliable search results can mislead agents into producing unsafe outputs. Real-world incidents and our two in-the-wild observations show that such failures can occur in practice. To study this threat systematically, we propose SafeSearch, an automated red-teaming framework that is scalable, cost-efficient, and lightweight, enabling harmless safety evaluation of search agents. Using this, we generate 300 test cases spanning five risk categories (e.g., misinformation and prompt injection) and evaluate three search agent scaffolds across 17 representative LLMs. Our results reveal substantial vulnerabilities in LLM-based search agents, with the highest ASR reaching 90.5% for GPT-4.1-mini in a search-workflow setting. Moreover, we find that common defenses, such as reminder prompting, offer limited protection. Overall, SafeSearch provides a practical way to measure and improve the safety of LLM-based search agents. Our codebase and test cases are publicly available: https://github.com/jianshuod/SafeSearch.", "published": "2025-09-28T07:05:17Z", "updated": "2026-01-29T10:58:06Z", "authors": ["Jianshuo Dong", "Sheng Guo", "Hao Wang", "Xun Chen", "Zhuotao Liu", "Tianwei Zhang", "Ke Xu", "Minlie Huang", "Han Qiu"], "pdf_url": "https://arxiv.org/pdf/2509.23694v4"}
{"id": "http://arxiv.org/abs/2601.21531v1", "title": "On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression", "summary": "Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.", "published": "2026-01-29T10:47:21Z", "updated": "2026-01-29T10:47:21Z", "authors": ["Xinwei Zhang", "Hangcheng Liu", "Li Bai", "Hao Wang", "Qingqing Ye", "Tianwei Zhang", "Haibo Hu"], "pdf_url": "https://arxiv.org/pdf/2601.21531v1"}
{"id": "http://arxiv.org/abs/2509.23019v4", "title": "LLM Watermark Evasion via Bias Inversion", "summary": "Watermarking offers a promising solution for detecting LLM-generated content, yet its robustness under realistic query-free (black-box) evasion remains an open challenge. Existing query-free attacks often achieve limited success or severely distort semantic meaning. We bridge this gap by theoretically analyzing rewriting-based evasion, demonstrating that reducing the average conditional probability of sampling green tokens by a small margin causes the detection probability to decay exponentially. Guided by this insight, we propose the Bias-Inversion Rewriting Attack (BIRA), a practical query-free method that applies a negative logit bias to a proxy suppression set identified via token surprisal. Empirically, BIRA achieves state-of-the-art evasion rates (>99%) across diverse watermarking schemes while preserving semantic fidelity substantially better than prior baselines. Our findings reveal a fundamental vulnerability in current watermarking methods and highlight the need for rigorous stress tests.", "published": "2025-09-27T00:24:57Z", "updated": "2026-01-29T10:31:53Z", "authors": ["Jeongyeon Hwang", "Sangdon Park", "Jungseul Ok"], "pdf_url": "https://arxiv.org/pdf/2509.23019v4"}
{"id": "http://arxiv.org/abs/2510.02422v3", "title": "Dynamic Target Attack", "summary": "Existing gradient-based jailbreak attacks typically optimize an adversarial suffix to induce a fixed affirmative response, e.g., ``Sure, here is...''. However, this fixed target usually resides in an extremely low-density region of a safety-aligned LLM's output distribution. Due to the substantial discrepancy between the fixed target and the output distribution, existing attacks require numerous iterations to optimize the adversarial prompt, which might still fail to induce the low-probability target response. To address this limitation, we propose Dynamic Target Attack (DTA), which leverages the target LLM's own responses as adaptive targets. In each optimization round, DTA samples multiple candidates from the output distribution conditioned on the current prompt, and selects the most harmful one as a temporary target for prompt optimization. Extensive experiments demonstrate that, under the white-box setting, DTA achieves over 87% average attack success rate (ASR) within 200 optimization iterations on recent safety-aligned LLMs, exceeding the state-of-the-art baselines by over 15% and reducing wall-clock time by 2-26x. Under the black-box setting, DTA employs a white-box LLM as a surrogate model for gradient-based optimization, achieving an average ASR of 77.5% against black-box models, exceeding prior transfer-based attacks by over 12%.", "published": "2025-10-02T16:40:51Z", "updated": "2026-01-29T09:33:04Z", "authors": ["Kedong Xiu", "Churui Zeng", "Tianhang Zheng", "Xinzhe Huang", "Xiaojun Jia", "Di Wang", "Puning Zhao", "Zhan Qin", "Kui Ren"], "pdf_url": "https://arxiv.org/pdf/2510.02422v3"}
{"id": "http://arxiv.org/abs/2601.21380v1", "title": "RerouteGuard: Understanding and Mitigating Adversarial Risks for LLM Routing", "summary": "Recent advancements in multi-model AI systems have leveraged LLM routers to reduce computational cost while maintaining response quality by assigning queries to the most appropriate model. However, as classifiers, LLM routers are vulnerable to novel adversarial attacks in the form of LLM rerouting, where adversaries prepend specially crafted triggers to user queries to manipulate routing decisions. Such attacks can lead to increased computational cost, degraded response quality, and even bypass safety guardrails, yet their security implications remain largely underexplored. In this work, we bridge this gap by systematizing LLM rerouting threats based on the adversary's objectives (i.e., cost escalation, quality hijacking, and safety bypass) and knowledge. Based on the threat taxonomy, we conduct a measurement study of real-world LLM routing systems against existing LLM rerouting attacks. The results reveal that existing routing systems are vulnerable to rerouting attacks, especially in the cost escalation scenario. We then characterize existing rerouting attacks using interpretability techniques, revealing that they exploit router decision boundaries through confounder gadgets that prepend queries to force misrouting. To mitigate these risks, we introduce RerouteGuard, a flexible and scalable guardrail framework for LLM rerouting. RerouteGuard filters adversarial rerouting prompts via dynamic embedding-based detection and adaptive thresholding. Extensive evaluations in three attack settings and four benchmarks demonstrate that RerouteGuard achieves over 99% detection accuracy against state-of-the-art rerouting attacks, while maintaining negligible impact on legitimate queries. The experimental results indicate that RerouteGuard offers a principled and practical solution for safeguarding multi-model AI systems against adversarial rerouting.", "published": "2026-01-29T08:17:08Z", "updated": "2026-01-29T08:17:08Z", "authors": ["Wenhui Zhang", "Huiyu Xu", "Zhibo Wang", "Zhichao Li", "Zeqing He", "Xuelin Wei", "Kui Ren"], "pdf_url": "https://arxiv.org/pdf/2601.21380v1"}
{"id": "http://arxiv.org/abs/2601.19768v2", "title": "GAVEL: Towards rule-based safety through activation monitoring", "summary": "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.", "published": "2026-01-27T16:31:39Z", "updated": "2026-01-29T08:02:45Z", "authors": ["Shir Rozenfeld", "Rahul Pankajakshan", "Itay Zloczower", "Eyal Lenga", "Gilad Gressel", "Yisroel Mirsky"], "pdf_url": "https://arxiv.org/pdf/2601.19768v2"}
{"id": "http://arxiv.org/abs/2601.21353v1", "title": "SecIC3: Customizing IC3 for Hardware Security Verification", "summary": "Recent years have seen significant advances in using formal verification to check hardware security properties. Of particular practical interest are checking confidentiality and integrity of secrets, by checking that there is no information flow between the secrets and observable outputs. A standard method for checking information flow is to translate the corresponding non-interference hyperproperty into a safety property on a self-composition of the design, which has two copies of the design composed together. Although prior efforts have aimed to reduce the size of the self-composed design, there are no state-of-the-art model checkers that exploit their special structure for hardware security verification. In this paper, we propose SecIC3, a hardware model checking algorithm based on IC3 that is customized to exploit this self-composition structure. SecIC3 utilizes this structure in two complementary techniques: symmetric state exploration and adding equivalence predicates. We implement SecIC3 on top of two open-source IC3 implementations and evaluate it on a non-interference checking benchmark consisting of 10 designs. The experiment results show that SecIC3 significantly reduces the time for finding security proofs, with up to 49.3x proof speedup compared to baseline implementations.", "published": "2026-01-29T07:24:08Z", "updated": "2026-01-29T07:24:08Z", "authors": ["Qinhan Tan", "Akash Gaonkar", "Yu-Wei Fan", "Aarti Gupta", "Sharad Malik"], "pdf_url": "https://arxiv.org/pdf/2601.21353v1"}
{"id": "http://arxiv.org/abs/2601.21318v1", "title": "QCL-IDS: Quantum Continual Learning for Intrusion Detection with Fidelity-Anchored Stability and Generative Replay", "summary": "Continual intrusion detection must absorb newly emerging attack stages while retaining legacy detection capability under strict operational constraints, including bounded compute and qubit budgets and privacy rules that preclude long-term storage of raw telemetry. We propose QCL-IDS, a quantum-centric continual-learning framework that co-designs stability and privacy-governed rehearsal for NISQ-era pipelines. Its core component, Q-FISH (Quantum Fisher Anchors), enforces retention using a compact anchor coreset through (i) sensitivity-weighted parameter constraints and (ii) a fidelity-based functional anchoring term that directly limits decision drift on representative historical traffic. To regain plasticity without retaining sensitive flows, QCL-IDS further introduces privacy-preserved quantum generative replay (QGR) via frozen, task-conditioned generator snapshots that synthesize bounded rehearsal samples. Across a three-stage attack stream on UNSW-NB15 and CICIDS2017, QCL-IDS consistently attains the best retention-adaptation trade-off: the gradient-anchor configuration achieves mean Attack-F1 = 0.941 with forgetting = 0.005 on UNSW-NB15 and mean Attack-F1 = 0.944 with forgetting = 0.004 on CICIDS2017, versus 0.800/0.138 and 0.803/0.128 for sequential fine-tuning, respectively.", "published": "2026-01-29T06:27:05Z", "updated": "2026-01-29T06:27:05Z", "authors": ["Zirui Zhu", "Xiangyang Li"], "pdf_url": "https://arxiv.org/pdf/2601.21318v1"}
{"id": "http://arxiv.org/abs/2601.00065v2", "title": "The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition", "summary": "The open-weight language model ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single breaker token that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and evades outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge", "published": "2025-12-31T19:00:03Z", "updated": "2026-01-29T06:04:53Z", "authors": ["Xiaoze Liu", "Weichen Yu", "Matt Fredrikson", "Xiaoqian Wang", "Jing Gao"], "pdf_url": "https://arxiv.org/pdf/2601.00065v2"}
{"id": "http://arxiv.org/abs/2601.21287v1", "title": "Towards Zero Rotation and Beyond: Architecting Neural Networks for Fast Secure Inference with Homomorphic Encryption", "summary": "Privacy-preserving deep learning addresses privacy concerns in Machine Learning as a Service (MLaaS) by using Homomorphic Encryption (HE) for linear computations. However, the computational overhead remains a major challenge. While prior work has improved efficiency, most approaches build on models originally designed for plaintext inference. Such models incur architectural inefficiencies when adapted to HE. We argue that substantial gains require networks tailored to HE rather than retrofitting plaintext architectures. Our design has two components: the building block and the overall architecture. First, StriaBlock targets the most expensive HE operation, rotation. It integrates ExRot-Free Convolution and a novel Cross Kernel, eliminating external rotations and requiring only 19% of the internal rotations used by plaintext models. Second, our architectural principles include (i) the Focused Constraint Principle, which limits cost-sensitive factors while preserving flexibility elsewhere, and (ii) the Channel Packing-Aware Scaling Principle, which adapts bottleneck ratios to ciphertext channel capacity that varies with depth. Together, these strategies control both local and end-to-end HE cost, enabling a balanced HE-tailored network. We evaluate the resulting StriaNet across datasets of varying scales, including ImageNet, Tiny ImageNet, and CIFAR-10. At comparable accuracy, StriaNet achieves speedups of 9.78x, 6.01x, and 9.24x on ImageNet, Tiny ImageNet, and CIFAR-10, respectively.", "published": "2026-01-29T05:40:05Z", "updated": "2026-01-29T05:40:05Z", "authors": ["Yifei Cai", "Yizhou Feng", "Qiao Zhang", "Chunsheng Xin", "Hongyi Wu"], "pdf_url": "https://arxiv.org/pdf/2601.21287v1"}
{"id": "http://arxiv.org/abs/2601.21261v1", "title": "User-Centric Phishing Detection: A RAG and LLM-Based Approach", "summary": "The escalating sophistication of phishing emails necessitates a shift beyond traditional rule-based and conventional machine-learning-based detectors. Although large language models (LLMs) offer strong natural language understanding, using them as standalone classifiers often yields elevated falsepositive (FP) rates, which mislabel legitimate emails as phishing and create significant operational burden. This paper presents a personalized phishing detection framework that integrates LLMs with retrieval-augmented generation (RAG). For each message, the system constructs user-specific context by retrieving a compact set of the user's historical legitimate emails and enriching it with real-time domain and URL reputation from a cyber-threat intelligence platform, then conditions the LLM's decision on this evidence. We evaluate four open-source LLMs (Llama4-Scout, DeepSeek-R1, Mistral-Saba, and Gemma2) on an email dataset collected from public and institutional sources. Results show high performance; for example, Llama4-Scout attains an F1-score of 0.9703 and achieves a 66.7% reduction in FPs with RAG. These findings validate that a RAG-based, user-profiling approach is both feasible and effective for building high-precision, low-friction email security systems that adapt to individual communication patterns.", "published": "2026-01-29T04:42:18Z", "updated": "2026-01-29T04:42:18Z", "authors": ["Abrar Hamed Al Barwani", "Abdelaziz Amara Korba", "Raja Waseem Anwar"], "pdf_url": "https://arxiv.org/pdf/2601.21261v1"}
{"id": "http://arxiv.org/abs/2601.21258v1", "title": "Virtualization-based Penetration Testing Study for Detecting Accessibility Abuse Vulnerabilities in Banking Apps in East and Southeast Asia", "summary": "Android banking applications have revolutionized financial management by allowing users to perform various financial activities through mobile devices. However, this convenience has attracted cybercriminals who exploit security vulnerabilities to access sensitive financial data. FjordPhantom, a malware identified by our industry collaborator, uses virtualization and hooking to bypass the detection of malicious accessibility services, allowing it to conduct keylogging, screen scraping, and unauthorized data access. This malware primarily affects banking and finance apps across East and Southeast Asia region where our industry partner's clients are primarily based in. It requires users to be deceived into installing a secondary malicious component and activating a malicious accessibility service. In our study, we conducted an empirical study on the susceptibility of banking apps in the region to FjordPhantom, analyzed the effectiveness of protective measures currently implemented in those apps, and discussed ways to detect and prevent such attacks by identifying and mitigating the vulnerabilities exploited by this malware.", "published": "2026-01-29T04:37:53Z", "updated": "2026-01-29T04:37:53Z", "authors": ["Wei Minn", "Phong Phan", "Vikas K. Malviya", "Benjamin Adolphi", "Yan Naing Tun", "Henning Benzon Treichl", "Albert Ching", "Lwin Khin Shar", "David Lo"], "pdf_url": "https://arxiv.org/pdf/2601.21258v1"}
{"id": "http://arxiv.org/abs/2601.21252v1", "title": "Lossless Copyright Protection via Intrinsic Model Fingerprinting", "summary": "The exceptional performance of diffusion models establishes them as high-value intellectual property but exposes them to unauthorized replication. Existing protection methods either modify the model to embed watermarks, which impairs performance, or extract model fingerprints by manipulating the denoising process, rendering them incompatible with black-box APIs. In this paper, we propose TrajPrint, a completely lossless and training-free framework that verifies model copyright by extracting unique manifold fingerprints formed during deterministic generation. Specifically, we first utilize a watermarked image as an anchor and exactly trace the path back to its trajectory origin, effectively locking the model fingerprint mapped by this path. Subsequently, we implement a joint optimization strategy that employs dual-end anchoring to synthesize a specific fingerprint noise, which strictly adheres to the target manifold for robust watermark recovery. As input, it enables the protected target model to recover the watermarked image, while failing on non-target models. Finally, we achieved verification via atomic inference and statistical hypothesis testing. Extensive experiments demonstrate that TrajPrint achieves lossless verification in black-box API scenarios with superior robustness against model modifications.", "published": "2026-01-29T04:18:07Z", "updated": "2026-01-29T04:18:07Z", "authors": ["Lingxiao Chen", "Liqin Wang", "Wei Lu", "Xiangyang Luo"], "pdf_url": "https://arxiv.org/pdf/2601.21252v1"}
{"id": "http://arxiv.org/abs/2504.02194v3", "title": "FairDAG: Consensus Fairness over Multi-Proposer Causal Design", "summary": "The rise of cryptocurrencies like Bitcoin and Ethereum has driven interest in blockchain database technology, with smart contracts enabling the growth of decentralized finance (DeFi). However, research has shown that adversaries exploit transaction ordering to extract profits through attacks like front-running, sandwich attacks, and liquidation manipulation. This issue affects blockchains where block proposers have full control over transaction ordering. To address this, a more fair transaction ordering mechanism is essential.\n  Existing fairness protocols, such as Pompe and Themis, operate on leader-based consensus protocols, which not only suffer from low throughput caused by the single-leader bottleneck, but also allow adversarial block proposers to manipulate transaction ordering. To address these limitations, we propose a new framework, FairDAG, that runs fairness protocols on top of DAG-based consensus protocols. FairDAG improves protocol performance in both throughput and fairness quality by leveraging the multi-proposer design and validity property of DAG-based consensus protocols.\n  We conducted a comprehensive analytical and experimental evaluation of two FairDAG variants - FairDAG-AB and FairDAG-RL. Our results demonstrate that FairDAG outperforms prior fairness protocols in both throughput and fairness quality.", "published": "2025-04-03T00:38:03Z", "updated": "2026-01-29T03:53:56Z", "authors": ["Dakai Kang", "Junchao Chen", "Tien Tuan Anh Dinh", "Mohammad Sadoghi"], "pdf_url": "https://arxiv.org/pdf/2504.02194v3"}
{"id": "http://arxiv.org/abs/2601.21211v1", "title": "SPOILER-GUARD: Gating Latency Effects of Memory Accesses through Randomized Dependency Prediction", "summary": "Modern microprocessors depend on speculative execution, creating vulnerabilities that enable transient execution attacks. Prior defenses target speculative data leakage but overlook false dependencies from partial address aliasing, where repeated squash and reissue events increase the load-store latency, which is exploited by the SPOILER attack. We present SPOILER-GUARD, a hardware defense that obfuscates speculative dependency resolution by dynamically randomizing the physical address bits used for load-store comparisons and tagging store entries to prevent latency-amplifying misspeculations. Implemented in gem5 and evaluated with SPEC 2017, SPOILER-GUARD reduces misspeculation to 0.0004 percent and improves integer and floating-point performance by 2.12 and 2.87 percent. HDL synthesis with Synopsys Design Compiler at 14 nm node demonstrates minimal overheads - 69 ps latency in critical path, 0.064 square millimeter in area, and 5.863 mW in power.", "published": "2026-01-29T03:22:58Z", "updated": "2026-01-29T03:22:58Z", "authors": ["Gayathri Subramanian", "Girinath P", "Nitya Ranganathan", "Kamakoti Veezhinathan", "Gopalakrishnan Srinivasan"], "pdf_url": "https://arxiv.org/pdf/2601.21211v1"}
{"id": "http://arxiv.org/abs/2509.18874v3", "title": "When Ads Become Profiles: Uncovering the Invisible Risk of Web Advertising at Scale with LLMs", "summary": "Regulatory limits on explicit targeting have not eliminated algorithmic profiling on the Web, as optimisation systems still adapt ad delivery to users' private attributes. The widespread availability of powerful zero-shot multimodal Large Language Models (LLMs) has dramatically lowered the barrier for exploiting these latent signals for adversarial inference. We investigate this emerging societal risk, specifically how adversaries can now exploit these signals to reverse-engineer private attributes from ad exposure alone. We introduce a novel pipeline that leverages LLMs as adversarial inference engines to perform natural language profiling. Applying this method to a longitudinal dataset comprising over 435,000 Facebook ad impressions collected from 891 users, we conducted a large-scale study to assess the feasibility and precision of inferring private attributes from passive online ad observations. Our results demonstrate that off-the-shelf LLMs can accurately reconstruct complex user private attributes, including party preference, employment status, and education level, consistently outperforming strong census-based priors and matching or exceeding human social perception at only a fraction of the cost (223x lower) and time (52x faster) required by humans. Critically, actionable profiling is feasible even within short observation windows, indicating that prolonged tracking is not a prerequisite for a successful attack. These findings provide the first empirical evidence that ad streams serve as a high-fidelity digital footprint, enabling off-platform profiling that inherently bypasses current platform safeguards, highlighting a systemic vulnerability in the ad ecosystem and the urgent need for responsible web AI governance in the generative AI era. The code is available at https://github.com/Breezelled/when-ads-become-profiles.", "published": "2025-09-23T10:10:37Z", "updated": "2026-01-29T03:16:51Z", "authors": ["Baiyu Chen", "Benjamin Tag", "Hao Xue", "Daniel Angus", "Flora Salim"], "pdf_url": "https://arxiv.org/pdf/2509.18874v3"}
{"id": "http://arxiv.org/abs/2601.21189v1", "title": "Adaptive and Robust Cost-Aware Proof of Quality for Decentralized LLM Inference Networks", "summary": "Decentralized large language model inference networks require lightweight mechanisms to reward high quality outputs under heterogeneous latency and cost. Proof of Quality provides scalable verification by sampling evaluator nodes that score candidate outputs, then aggregating their scores into a consensus signal that determines rewards. However, evaluator heterogeneity and malicious score manipulation can distort consensus and inflate payouts, which weakens incentive alignment in open participation settings.\n  This paper extends a cost-aware Proof of Quality mechanism by adding adversary-resilient consensus formation. We study robust aggregation rules, including median and trimmed mean, and an adaptive trust-weighted consensus that updates evaluator weights from deviation signals. Using question answering and summarization workloads with a ground truth proxy for offline analysis, we quantify evaluator reliability and show strong variance across evaluators, including task-dependent misalignment that can invert correlations. We then evaluate robustness under four adversarial strategies, including noise injection, boosting, sabotage, and intermittent manipulation, across a sweep of malicious ratios and evaluator sample sizes. Our results show that robust aggregation improves consensus alignment with the ground truth proxy and reduces sensitivity to noisy and strategic attacks compared with simple averaging. We further characterize the operational trade-off introduced by evaluator sampling, where larger evaluator sets reduce evaluator rewards and increase payoff variance while inference rewards remain relatively stable in our configuration. These findings motivate robust consensus as a default component for cost-aware Proof of Quality and provide practical guidance for selecting evaluator sampling parameters under adversarial risk and resource constraints.", "published": "2026-01-29T02:39:40Z", "updated": "2026-01-29T02:39:40Z", "authors": ["Arther Tian", "Alex Ding", "Frank Chen", "Simon Wu", "Aaron Chan"], "pdf_url": "https://arxiv.org/pdf/2601.21189v1"}
{"id": "http://arxiv.org/abs/2601.18842v2", "title": "GUIGuard: Toward a General Framework for Privacy-Preserving GUI Agents", "summary": "GUI agents enable end-to-end automation through direct perception of and interaction with on-screen interfaces. However, these agents frequently access interfaces containing sensitive personal information, and screenshots are often transmitted to remote models, creating substantial privacy risks. These risks are particularly severe in GUI workflows: GUIs expose richer, more accessible private information, and privacy risks depend on interaction trajectories across sequential scenes. We propose GUIGuard, a three-stage framework for privacy-preserving GUI agents: (1) privacy recognition, (2) privacy protection, and (3) task execution under protection. We further construct GUIGuard-Bench, a cross-platform benchmark with 630 trajectories and 13,830 screenshots, annotated with region-level privacy grounding and fine-grained labels of risk level, privacy category, and task necessity. Evaluations reveal that existing agents exhibit limited privacy recognition, with state-of-the-art models achieving only 13.3% accuracy on Android and 1.4% on PC. Under privacy protection, task-planning semantics can still be maintained, with closed-source models showing stronger semantic consistency than open-source ones. Case studies on MobileWorld show that carefully designed protection strategies achieve higher task accuracy while preserving privacy. Our results highlight privacy recognition as a critical bottleneck for practical GUI agents. Project: https://futuresis.github.io/GUIGuard-page/", "published": "2026-01-26T11:33:40Z", "updated": "2026-01-29T01:37:29Z", "authors": ["Yanxi Wang", "Zhiling Zhang", "Wenbo Zhou", "Weiming Zhang", "Jie Zhang", "Qiannan Zhu", "Yu Shi", "Shuxin Zheng", "Jiyan He"], "pdf_url": "https://arxiv.org/pdf/2601.18842v2"}
