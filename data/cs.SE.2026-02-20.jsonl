{"id": "http://arxiv.org/abs/2601.14434v2", "title": "CMind: An AI Agent for Localizing C Memory Bugs", "summary": "This demonstration paper presents CMind, an artificial intelligence agent for localizing C memory bugs. The novel aspect to CMind is that it follows steps that we observed human programmers perform during empirical study of those programmers finding memory bugs in C programs. The input to the tool is a C program's source code and a bug report describing the problem. The output is the tool's hypothesis about the reason for the bug and its location. CMind reads the bug report to find potential entry points to the program, then navigates the program's source code, analyzes that source code, and generates a hypothesis location and rationale that fit a template. The tool combines large language model reasoning with guided decision making we encoded to mimic human behavior. The video demonstration is available at https://youtu.be/_vVd0LRvVHI.", "published": "2026-01-20T19:43:11Z", "updated": "2026-02-20T18:56:02Z", "authors": ["Chia-Yi Su", "Collin McMillan"], "pdf_url": "https://arxiv.org/pdf/2601.14434v2"}
{"id": "http://arxiv.org/abs/2602.17037v2", "title": "Wink: Recovering from Misbehaviors in Coding Agents", "summary": "Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.\n  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.", "published": "2026-02-19T03:15:00Z", "updated": "2026-02-20T18:13:08Z", "authors": ["Rahul Nanda", "Chandra Maddila", "Smriti Jha", "Euna Mehnaz Khan", "Matteo Paltenghi", "Satish Chandra"], "pdf_url": "https://arxiv.org/pdf/2602.17037v2"}
{"id": "http://arxiv.org/abs/2602.18357v1", "title": "Statistical Confidence in Functional Correctness: An Approach for AI Product Functional Correctness Evaluation", "summary": "The quality assessment of Artificial Intelligence (AI) systems is a fundamental challenge due to their inherently probabilistic nature. Standards such as ISO/IEC 25059 provide a quality model, but they lack practical and statistically robust methods for assessing functional correctness. This paper proposes and evaluates the Statistical Confidence in Functional Correctness (SCFC) approach, which seeks to fill this gap by connecting business requirements to a measure of statistical confidence that considers both the model's average performance and its variability. The approach consists of four steps: defining quantitative specification limits, performing stratified and probabilistic sampling, applying bootstrapping to estimate a confidence interval for the performance metric, and calculating a capability index as a final indicator. The approach was evaluated through a case study on two real-world AI systems in industry involving interviews with AI experts. Valuable insights were collected from the experts regarding the utility, ease of use, and intention to adopt the methodology in practical scenarios. We conclude that the proposed approach is a feasible and valuable way to operationalize the assessment of functional correctness, moving the evaluation from a point estimate to a statement of statistical confidence.", "published": "2026-02-20T17:06:38Z", "updated": "2026-02-20T17:06:38Z", "authors": ["Wallace Albertini", "Marina Condé Araújo", "Júlia Condé Araújo", "Antonio Pedro Santos Alves", "Marcos Kalinowski"], "pdf_url": "https://arxiv.org/pdf/2602.18357v1"}
{"id": "http://arxiv.org/abs/2602.18352v1", "title": "Qualitative Coding Analysis through Open-Source Large Language Models: A User Study and Design Recommendations", "summary": "Qualitative data analysis is labor-intensive, yet the privacy risks associated with commercial Large Language Models (LLMs) often preclude their use in sensitive research. To address this, we introduce ChatQDA, an on-device framework powered by open-source LLMs designed for privacy-preserving open coding. Our mixed-methods user study reveals that while participants rated the system highly for usability and perceived efficiency, they exhibited \"conditional trust\", valuing the tool for surface-level extraction while questioning its interpretive nuance and consistency. Furthermore, despite the technical security of local deployment, participants reported epistemic uncertainty regarding data protection, suggesting that invisible security measures are insufficient to foster trust. We conclude with design recommendations for local-first analysis tools that prioritize verifiable privacy and methodological rigor.", "published": "2026-02-20T17:04:02Z", "updated": "2026-02-20T17:04:02Z", "authors": ["Tung T. Ngo", "Dai Nguyen Van", "Anh-Minh Nguyen", "Phuong-Anh Do", "Anh Nguyen-Quoc"], "pdf_url": "https://arxiv.org/pdf/2602.18352v1"}
{"id": "http://arxiv.org/abs/2510.00674v2", "title": "PyTrim: A Practical Tool for Reducing Python Dependency Bloat", "summary": "Dependency bloat is a persistent challenge in Python projects, which increases maintenance costs and security risks. While numerous tools exist for detecting unused dependencies in Python, removing these dependencies across the source code and configuration files of a project requires manual effort and expertise.\n  To tackle this challenge we introduce PYTRIM, an end-to-end system to automate this process. PYTRIM eliminates unused imports and package declarations across a variety of file types, including Python source and configuration files such as requirements.txt and setup.py. PYTRIM's modular design makes it agnostic to the source of dependency bloat information, enabling integration with any detection tool. Beyond its contribution when it comes to automation, PYTRIM also incorporates a novel dynamic analysis component that improves dependency detection recall.\n  Our evaluation of PYTRIM's end-to-end effectiveness on a ground-truth dataset of 37 merged pull requests from prior work, shows that PYTRIM achieves 98.3% accuracy in replicating human-made changes. To show its practical impact, we run PYTRIM on 971 open-source packages, identifying and trimming bloated dependencies in 39 of them. For each case, we submit a corresponding pull request, 6 of which have already been accepted and merged. PYTRIM is available as an open-source project, encouraging community contributions and further development.\n  Video demonstration: https://youtu.be/LqTEdOUbJRI\n  Code repository: https://github.com/TrimTeam/PyTrim", "published": "2025-10-01T08:58:55Z", "updated": "2026-02-20T16:06:47Z", "authors": ["Konstantinos Karakatsanis", "Georgios Alexopoulos", "Ioannis Karyotakis", "Foivos Timotheos Proestakis", "Evangelos Talos", "Panos Louridas", "Dimitris Mitropoulos"], "pdf_url": "https://arxiv.org/pdf/2510.00674v2"}
{"id": "http://arxiv.org/abs/2602.18307v1", "title": "VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean", "summary": "Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.", "published": "2026-02-20T16:05:06Z", "updated": "2026-02-20T16:05:06Z", "authors": ["Yutong Xin", "Qiaochu Chen", "Greg Durrett", "Işil Dillig"], "pdf_url": "https://arxiv.org/pdf/2602.18307v1"}
{"id": "http://arxiv.org/abs/2602.18306v1", "title": "ReqElicitGym: An Evaluation Environment for Interview Competence in Conversational Requirements Elicitation", "summary": "With the rapid improvement of LLMs' coding capabilities, the bottleneck of LLM-based automated software development is shifting from generating correct code to eliciting users' requirements. Despite growing interest, the interview competence of LLMs in conversational requirements elicitation remains fully underexplored. Existing evaluations often depend on a few scenarios, real user interaction, and subjective human scoring, which hinders systematic and quantitative comparison. To address these challenges, we propose ReqElicitGym, an interactive and automatic evaluation environment for assessing interview competence in conversational requirements elicitation. Specifically, ReqElicitGym introduces a new evaluation dataset and designs both an interactive oracle user and a task evaluator. The dataset contains 101 website requirements elicitation scenarios spanning 10 application types. Both the oracle user and the task evaluator achieve high agreement with real users and expert judgment. Using our ReqElicitGym, any automated conversational requirements elicitation approach (e.g., LLM-based agents) can be evaluated in a reproducible and quantitative manner through interaction with the environment. Based on our ReqElicitGym, we conduct a systematic empirical study on seven representative LLMs, and the results show that current LLMs still exhibit limited interview competence in uncovering implicit requirements. Particularly, they elicit less than half of the users' implicit requirements, and their effective elicitation questions often emerge in later turns of the dialogue. Besides, we found LLMs can elicit interaction and content implicit requirements, but consistently struggle with style-related requirements. We believe ReqElicitGym will facilitate the evaluation and development of automated conversational requirements elicitation.", "published": "2026-02-20T16:02:13Z", "updated": "2026-02-20T16:02:13Z", "authors": ["Dongming Jin", "Zhi Jin", "Zheng Fang", "Linyu Li", "XiaoTian Yang", "Yuanpeng He", "Xiaohong Chen"], "pdf_url": "https://arxiv.org/pdf/2602.18306v1"}
{"id": "http://arxiv.org/abs/2602.18270v1", "title": "Many Tools, Few Exploitable Vulnerabilities: A Survey of 246 Static Code Analyzers for Security", "summary": "Static security analysis is a widely used technique for detecting software vulnerabilities across a wide range of weaknesses, application domains, and programming languages. While prior work surveyed static analyzes for specific weaknesses or application domains, no overview of the entire security landscape exists. We present a systematic literature review of 246 static security analyzers concerning their targeted vulnerabilities, application domains, analysis techniques, evaluation methods, and limitations. We observe that most analyzers focus on a limited set of weaknesses, that the vulnerabilities they detect are rarely exploitable, and that evaluations use custom benchmarks that are too small to enable robust assessment.", "published": "2026-02-20T14:52:18Z", "updated": "2026-02-20T14:52:18Z", "authors": ["Kevin Hermann", "Sven Peldszus", "Thorsten Berger"], "pdf_url": "https://arxiv.org/pdf/2602.18270v1"}
{"id": "http://arxiv.org/abs/2602.18190v1", "title": "Role and Identity Work of Software Engineering Professionals in the Generative AI Era", "summary": "The adoption of Generative AI (GenAI) suggests major changes for software engineering, including technical aspects but also human aspects of the professionals involved. One of these aspects is how individuals perceive themselves regarding their work, i.e., their work identity, and the processes they perform to form, adapt and reject these identities, i.e., identity work. Existent studies provide evidence of such identity work of software professionals triggered by the adoption of GenAI, however they do not consider differences among diverse roles, such as developers and testers. In this paper, we argue the need for considering the role as a factor defining the identity work of software professionals. To support our claim, we review some studies regarding different roles and also recent studies on how to adopt GenAI in software engineering. Then, we propose a research agenda to better understand how the role influences identity work of software professionals triggered by the adoption of GenAI, and, based on that, to propose new artifacts to support this adoption. We also discuss the potential implications for practice of the results to be obtained.", "published": "2026-02-20T12:53:43Z", "updated": "2026-02-20T12:53:43Z", "authors": ["Jorge Melegati"], "pdf_url": "https://arxiv.org/pdf/2602.18190v1"}
{"id": "http://arxiv.org/abs/2601.09282v2", "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing", "summary": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration and presents a proof-of-concept design.", "published": "2026-01-14T08:36:21Z", "updated": "2026-02-20T11:40:54Z", "authors": ["Leszek Sliwko", "Jolanta Mizeria-Pietraszko"], "pdf_url": "https://arxiv.org/pdf/2601.09282v2"}
{"id": "http://arxiv.org/abs/2602.18142v1", "title": "Toward Automated Virtual Electronic Control Unit (ECU) Twins for Shift-Left Automotive Software Testing", "summary": "Automotive software increasingly outpaces hardware availability, forcing late integration and expensive hardware-in-the-loop (HiL) bottlenecks. The InnoRegioChallenge project investigated whether a virtual test and integration environment can reproduce electronic control unit (ECU) behavior early enough to run real software binaries before physical hardware exists. We report a prototype that generates instruction-accurate processor models in SystemC/TLM~2.0 using an agentic, feedback-driven workflow coupled to a reference simulator via the GNU Debugger (GDB). The results indicate that the most critical technical risk -- CPU behavioral fidelity -- can be reduced through automated differential testing and iterative model correction. We summarize the architecture, the agentic modeling loop, and project outcomes, and we extrapolate plausible technical details consistent with the reported qualitative findings. While cloud-scale deployment and full toolchain integration remain future work, the prototype demonstrates a viable shift-left path for virtual ECU twins, enabling reproducible tests, non-intrusive tracing, and fault-injection campaigns aligned with safety standards.", "published": "2026-02-20T11:03:46Z", "updated": "2026-02-20T11:03:46Z", "authors": ["Sebastian Dingler", "Frederik Boenke"], "pdf_url": "https://arxiv.org/pdf/2602.18142v1"}
{"id": "http://arxiv.org/abs/2601.01944v2", "title": "The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities", "summary": "In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.\n  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.", "published": "2026-01-05T09:50:37Z", "updated": "2026-02-20T10:14:27Z", "authors": ["Matteo Esposito", "Andrea Janes", "Valentina Lenarduzzi", "Davide Taibi"], "pdf_url": "https://arxiv.org/pdf/2601.01944v2"}
{"id": "http://arxiv.org/abs/2506.03283v2", "title": "Exploring Generalizable Automated Program Repair with Large Language Models", "summary": "Automated Program Repair (APR) proposes bug fixes to aid developers in maintaining software. The state of the art in this domain focuses on LLMs, leveraging their strong capabilities to comprehend specifications in natural language and to generate program code. However, despite the APR community's research achievements and industry deployments, APR still cannot generalize broadly. In this work, we present an intensive empirical evaluation of LLMs' capabilities in APR. We evaluate a diverse set of 13 recent open and closed models. In particular, we explore language-agnostic repair by utilizing benchmarks for Java, JavaScript, Python, and PHP. Besides the generalization across languages and levels of patch complexity, we also investigate the effects of fault localization (FL). Our key results include: (1) Different LLMs tend to perform best for different languages, which makes it hard to develop cross-platform, single-LLM repair techniques. (2) Combining models by pooling repairs adds value with respect to uniquely fixed bugs, so a committee of expert models should be considered. (3) Under realistic assumptions of imperfect FL, we observe significant drops in accuracy from the usual practice of using perfect FL. Our insights will help develop reliable and generalizable APR techniques and evaluate them in realistic and fair environments.", "published": "2025-06-03T18:15:14Z", "updated": "2026-02-20T10:01:46Z", "authors": ["Viola Campos", "Ridwan Shariffdeen", "Adrian Ulges", "Yannic Noller"], "pdf_url": "https://arxiv.org/pdf/2506.03283v2"}
{"id": "http://arxiv.org/abs/2602.18012v1", "title": "DeCEAT: Decoding Carbon Emissions for AI-driven Software Testing", "summary": "The increasing use of language models in automated software testing raises concerns about their environmental impact, yet existing sustainability analyses focus almost exclusively on large language models. As a result, the energy and carbon characteristics of small language models (SLMs) during test generation remain largely unexplored. To address this gap, this work introduces the DeCEAT framework, which systematically evaluates the environmental and performance trade-offs of SLMs using the HumanEval benchmark and adaptive prompt variants (based on the Anthropic template). The framework quantifies emission and time-aware behavior under controlled conditions, with CodeCarbon measuring energy consumption and carbon emissions, and unit test coverage assessing the quality of generated tests. Our results show that different SLMs exhibit distinct sustainability strengths: some prioritize lower energy use and faster execution, while others maintain higher stability or accuracy under carbon constraints. These findings demonstrate that sustainability in the generation of SLM-driven tests is multidimensional and strongly shaped by prompt design. This work provides a focused sustainability evaluation framework specifically tailored to automated SLM-based test generation, clarifying how prompt structure and model choice jointly influence environmental and performance outcomes.", "published": "2026-02-20T05:54:58Z", "updated": "2026-02-20T05:54:58Z", "authors": ["Pragati Kumari", "Novarun Deb"], "pdf_url": "https://arxiv.org/pdf/2602.18012v1"}
{"id": "http://arxiv.org/abs/2602.17955v1", "title": "Mining Type Constructs Using Patterns in AI-Generated Code", "summary": "Artificial Intelligence (AI) increasingly automates various parts of the software development tasks. Although AI has enhanced the productivity of development tasks, it remains unstudied whether AI essentially outperforms humans in type-related programming tasks, such as employing type constructs properly for type safety, during its tasks. Moreover, there is no systematic study that evaluates whether AI agents overuse or misuse the type constructs under the complicated type systems to the same extent as humans. In this study, we present the first empirical analysis to answer these questions in the domain of TypeScript projects. Our findings show that, in contrast to humans, AI agents are 9x more prone to use the 'any' keyword. In addition, we observed that AI agents use advanced type constructs, including those that ignore type checks, more often compared to humans. Surprisingly, even with all these issues, Agentic pull requests (PRs) have 1.8x higher acceptance rates compared to humans for TypeScript. We encourage software developers to carefully confirm the type safety of their codebases whenever they coordinate with AI agents in the development process.", "published": "2026-02-20T03:17:42Z", "updated": "2026-02-20T03:17:42Z", "authors": ["Imgyeong Lee", "Tayyib Ul Hassan", "Abram Hindle"], "pdf_url": "https://arxiv.org/pdf/2602.17955v1"}
{"id": "http://arxiv.org/abs/2510.26075v2", "title": "FGGM: Formal Grey-box Gradient Method for Attacking DRL-based MU-MIMO Scheduler", "summary": "In 5G mobile communication systems, MU-MIMO has been applied to enhance spectral efficiency and support high data rates. To maximize spectral efficiency while providing fairness among users, the base station (BS) needs to selects a subset of users for data transmission. Given that this problem is NP-hard, DRL-based methods have been proposed to infer the near-optimal solutions in real-time, yet this approach has an intrinsic security problem. This paper investigates how a group of adversarial users can exploit unsanitized raw CSIs to launch a throughput degradation attack. Most existing studies only focused on systems in which adversarial users can obtain the exact values of victims' CSIs, but this is impractical in the case of uplink transmission in LTE/5G mobile systems. We note that the DRL policy contains an observation normalizer which has the mean and variance of the observation to improve training convergence. Adversarial users can then estimate the upper and lower bounds of the local observations including the CSIs of victims based solely on that observation normalizer. We develop an attacking scheme FGGM by leveraging polytope abstract domains, a technique used to bound the outputs of a neural network given the input ranges. Our goal is to find one set of intentionally manipulated CSIs which can achieve the attacking goals for the whole range of local observations of victims. Experimental results demonstrate that FGGM can determine a set of adversarial CSI vector controlled by adversarial users, then reuse those CSIs throughout the simulation to reduce the network throughput of a victim up to 70\\% without knowing the exact value of victims' local observations. This study serves as a case study and can be applied to many other DRL-based problems, such as a knapsack-oriented resource allocation problems.", "published": "2025-10-30T02:17:32Z", "updated": "2026-02-20T03:03:41Z", "authors": ["Thanh Le", "Hai Duong", "Yusheng Ji", "ThanhVu Nguyen", "John C. S. Lui"], "pdf_url": "https://arxiv.org/pdf/2510.26075v2"}
{"id": "http://arxiv.org/abs/2602.08801v3", "title": "Verifying DNN-based Semantic Communication Against Generative Adversarial Noise", "summary": "Safety-critical applications like autonomous vehicles and industrial IoT are adopting semantic communication (SemCom) systems using deep neural networks to reduce bandwidth and increase transmission speed by transmitting only task-relevant semantic features.\n  However, adversarial attacks against these DNN-based SemCom systems can cause catastrophic failures by manipulating transmitted semantic features.\n  Existing defense mechanisms rely on empirical approaches provide no formal guarantees against the full spectrum of adversarial perturbations.\n  We present VSCAN, a neural network verification framework that provides mathematical robustness guarantees by formulating adversarial noise generation as mixed integer programming and verifying end-to-end properties across multiple interconnected networks (encoder, decoder, and task model).\n  Our key insight is that realistic adversarial constraints (power limitations and statistical undetectability) can be encoded as logical formulae to enable efficient verification using state-of-the-art DNN verifiers.\n  Our evaluation on 600 verification properties characterizing various attacker's capabilities shows VSCAN matches attack methods in finding vulnerabilities while providing formal robustness guarantees for 44% of properties -- a significant achievement given the complexity of multi-network verification.\n  Moreover, we reveal a fundamental security-efficiency tradeoff: compact 16-dimensional latent spaces achieve 50% verified robustness compared to 64-dimensional spaces.", "published": "2026-02-09T15:40:13Z", "updated": "2026-02-20T02:50:14Z", "authors": ["Thanh Le", "Hai Duong", "ThanhVu Nguyen", "Takeshi Matsumura"], "pdf_url": "https://arxiv.org/pdf/2602.08801v3"}
