{"id": "http://arxiv.org/abs/2602.05780v1", "title": "Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes", "summary": "Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.", "published": "2026-02-05T15:38:54Z", "updated": "2026-02-05T15:38:54Z", "authors": ["Ulrich Finkler", "Irene Manotas", "Wei Zhang", "Geert Janssen", "Octavian Popescu", "Shyam Ramji"], "pdf_url": "https://arxiv.org/pdf/2602.05780v1"}
{"id": "http://arxiv.org/abs/2602.05762v1", "title": "RocqSmith: Can Automatic Optimization Forge Better Proof Agents?", "summary": "This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.", "published": "2026-02-05T15:28:26Z", "updated": "2026-02-05T15:28:26Z", "authors": ["Andrei Kozyrev", "Nikita Khramov", "Denis Lochmelis", "Valerio Morelli", "Gleb Solovev", "Anton Podkopaev"], "pdf_url": "https://arxiv.org/pdf/2602.05762v1"}
{"id": "http://arxiv.org/abs/2602.05759v1", "title": "Toward Quantum-Safe Software Engineering: A Vision for Post-Quantum Cryptography Migration", "summary": "The quantum threat to cybersecurity has accelerated the standardization of Post-Quantum Cryptography (PQC). Migrating legacy software to these quantum-safe algorithms is not a simple library swap, but a new software engineering challenge: existing vulnerability detection, refactoring, and testing tools are not designed for PQC's probabilistic behavior, side-channel sensitivity, and complex performance trade-offs. To address these challenges, this paper outlines a vision for a new class of tools and introduces the Automated Quantum-safe Adaptation (AQuA) framework, with a three-pillar agenda for PQC-aware detection, semantic refactoring, and hybrid verification, thereby motivating Quantum-Safe Software Engineering (QSSE) as a distinct research direction.", "published": "2026-02-05T15:27:30Z", "updated": "2026-02-05T15:27:30Z", "authors": ["Lei Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.05759v1"}
{"id": "http://arxiv.org/abs/2602.05739v1", "title": "A Bayesian Optimization-Based AutoML Framework for Non-Intrusive Load Monitoring", "summary": "Non-Intrusive Load Monitoring (NILM), commonly known as energy disaggregation, aims to estimate the power consumption of individual appliances by analyzing a home's total electricity usage. This method provides a cost-effective alternative to installing dedicated smart meters for each appliance. In this paper, we introduce a novel framework that incorporates Automated Machine Learning (AutoML) into the NILM domain, utilizing Bayesian Optimization for automated model selection and hyperparameter tuning. This framework empowers domain practitioners to effectively apply machine learning techniques without requiring advanced expertise in data science or machine learning. To support further research and industry adoption, we present AutoML4NILM, a flexible and extensible open-source toolkit designed to streamline the deployment of AutoML solutions for energy disaggregation. Currently, this framework supports 11 algorithms, each with different hyperparameters; however, its flexible design allows for the extension of both the algorithms and their hyperparameters.", "published": "2026-02-05T15:05:24Z", "updated": "2026-02-05T15:05:24Z", "authors": ["Nazanin Siavash", "Armin Moin"], "pdf_url": "https://arxiv.org/pdf/2602.05739v1"}
{"id": "http://arxiv.org/abs/2602.03070v2", "title": "ProOPF: Benchmarking and Improving LLMs for Professional-Grade Power Systems Optimization Modeling", "summary": "Growing renewable penetration introduces substantial uncertainty into power system operations, necessitating frequent adaptation of dispatch objectives and constraints and challenging expertise-intensive, near-real-time modeling workflows. Large Language Models (LLMs) provide a promising avenue for automating this process by translating natural-language (NL) operational requirements into executable optimization models via semantic reasoning and code synthesis. Yet existing LLM datasets and benchmarks for optimization modeling primarily target coarse-grained cross-domain generalization, offering limited, rigorous evaluation in power-system settings, particularly for Optimal Power Flow (OPF). We therefore introduce \\textbf{ProOPF-D} and \\textbf{ProOPF-B}, a dataset and benchmark for professional-grade OPF modeling: ProOPF-D contains 12K instances pairing NL requests with parameter adjustments and structural extensions to a canonical OPF, together with executable implementations; ProOPF-B provides 121 expert-annotated test cases with ground-truth code, enabling end-to-end evaluation under both concrete and abstract OPF modeling regimes.", "published": "2026-02-03T03:59:03Z", "updated": "2026-02-05T15:04:25Z", "authors": ["Chao Shen", "Zihan Guo", "Xu Wan", "Zhenghao Yang", "Yifan Zhang", "Wengi Huang", "Jie Song", "Zongyan Zhang", "Mingyang Sun"], "pdf_url": "https://arxiv.org/pdf/2602.03070v2"}
{"id": "http://arxiv.org/abs/2602.05721v1", "title": "A Dual-Loop Agent Framework for Automated Vulnerability Reproduction", "summary": "Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \\textit{Tactical Loop} for code-level refinement, while the \\textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\\% and 54.3\\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\\% and 20.4\\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.", "published": "2026-02-05T14:47:48Z", "updated": "2026-02-05T14:47:48Z", "authors": ["Bin Liu", "Yanjie Zhao", "Zhenpeng Chen", "Guoai Xu", "Haoyu Wang"], "pdf_url": "https://arxiv.org/pdf/2602.05721v1"}
{"id": "http://arxiv.org/abs/2602.05712v1", "title": "Towards Green AI: Decoding the Energy of LLM Inference in Software Development", "summary": "Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.", "published": "2026-02-05T14:38:19Z", "updated": "2026-02-05T14:38:19Z", "authors": ["Lola Solovyeva", "Fernando Castor"], "pdf_url": "https://arxiv.org/pdf/2602.05712v1"}
{"id": "http://arxiv.org/abs/2602.05703v1", "title": "SEAL: Symbolic Execution with Separation Logic (Competition Contribution)", "summary": "SEAL is a static analyser for the verification of programs that manipulate unbounded linked data structures. It is based on separation logic to represent abstract memory states and, unlike other separation-logic-based approaches, it employs a general-purpose separation logic solver Astral for satisfiability and entailment checking, which itself is based on translation to SMT. This design results in a modular architecture intended to be easier to extend and to combine with reasoning in other theories. Although still a prototype, SEAL achieved competitive results in the LinkedLists base category and was one of only four analysers capable of verifying programs with unbounded lists. We believe that the tool's extensibility, combined with further development, can lead to significant improvements in future competitions.", "published": "2026-02-05T14:29:10Z", "updated": "2026-02-05T14:29:10Z", "authors": ["Tomáš Brablec", "Tomáš Dacík", "Tomáš Vojnar"], "pdf_url": "https://arxiv.org/pdf/2602.05703v1"}
{"id": "http://arxiv.org/abs/2409.12519v3", "title": "Multi-View Adaptive Contrastive Learning for Information Retrieval Based Fault Localization", "summary": "Most studies focused on information retrieval-based techniques for fault localization, which built representations for bug reports and source code files and matched their semantic vectors through similarity measurement. However, such approaches often ignore some useful information that might help improve localization performance, such as 1) the interaction relationship between bug reports and source code files; 2) the similarity relationship between bug reports; and 3) the co-citation relationship between source code files. In this paper, we propose a novel approach named Multi-View Adaptive Contrastive Learning for Information Retrieval Fault Localization (MACL-IRFL) to learn the above-mentioned relationships for software fault localization. Specifically, we first generate data augmentations from report-code interaction view, report-report similarity view and code-code co-citation view separately, and adopt graph neural network to aggregate the information of bug reports or source code files from the three views in the embedding process. Moreover, we perform contrastive learning across these views. Our design of contrastive learning task will force the bug report representations to encode information shared by report-report and report-code views,and the source code file representations shared by code-code and report-code views, thereby alleviating the noise from auxiliary information. Finally, to evaluate the performance of our approach, we conduct extensive experiments on five open-source Java projects. The results show that our model can improve over the best baseline up to 28.93%, 25.57% and 20.35% on Accuracy@1, MAP and MRR, respectively.", "published": "2024-09-19T07:20:10Z", "updated": "2026-02-05T11:31:43Z", "authors": ["Chunying Zhou", "Xiaoyuan Xie", "Gong Chen", "Peng He", "Bing Li"], "pdf_url": "https://arxiv.org/pdf/2409.12519v3"}
{"id": "http://arxiv.org/abs/2602.05550v1", "title": "ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval", "summary": "ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.", "published": "2026-02-05T11:15:34Z", "updated": "2026-02-05T11:15:34Z", "authors": ["Yulong He", "Artem Ermakov", "Sergey Kovalchuk", "Artem Aliev", "Dmitry Shalymov"], "pdf_url": "https://arxiv.org/pdf/2602.05550v1"}
{"id": "http://arxiv.org/abs/2602.05523v1", "title": "Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations", "summary": "Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.", "published": "2026-02-05T10:30:57Z", "updated": "2026-02-05T10:30:57Z", "authors": ["Shahin Honarvar", "Amber Gorzynski", "James Lee-Jones", "Harry Coppock", "Marek Rei", "Joseph Ryan", "Alastair F. Donaldson"], "pdf_url": "https://arxiv.org/pdf/2602.05523v1"}
{"id": "http://arxiv.org/abs/2602.05486v1", "title": "Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems", "summary": "Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.", "published": "2026-02-05T09:49:04Z", "updated": "2026-02-05T09:49:04Z", "authors": ["Matteo Esposito", "Lodovica Marchesi", "Roberto Tonelli", "Valentina Lenarduzzi"], "pdf_url": "https://arxiv.org/pdf/2602.05486v1"}
{"id": "http://arxiv.org/abs/2602.05465v1", "title": "Can We Classify Flaky Tests Using Only Test Code? An LLM-Based Empirical Study", "summary": "Flaky tests yield inconsistent results when they are repeatedly executed on the same code revision. They interfere with automated quality assurance of code changes and hinder efficient software testing. Previous work evaluated approaches to train machine learning models to classify flaky tests based on identifiers in the test code. However, the resulting classifiers have been shown to lack generalizability, hindering their applicability in practical environments. Recently, pre-trained Large Language Models (LLMs) have shown the capability to generalize across various tasks. Thus, they represent a promising approach to address the generalizability problem of previous approaches. In this study, we evaluated three LLMs (two general-purpose models, one code-specific model) using three prompting techniques on two benchmark datasets from prior studies on flaky test classification. Furthermore, we manually investigated 50 samples from the given datasets to determine whether classifying flaky tests based only on test code is feasible for humans. Our findings indicate that LLMs struggle to classify flaky tests given only the test code. The results of our best prompt-model combination were only marginally better than random guessing. In our manual analysis, we found that the test code does not necessarily contain sufficient information for a flakiness classification. Our findings motivate future work to evaluate LLMs for flakiness classification with additional context, for example, using retrieval-augmented generation or agentic AI.", "published": "2026-02-05T09:15:09Z", "updated": "2026-02-05T09:15:09Z", "authors": ["Alexander Berndt", "Vekil Bekmyradov", "Rainer Gemulla", "Marcus Kessel", "Thomas Bach", "Sebastian Baltes"], "pdf_url": "https://arxiv.org/pdf/2602.05465v1"}
{"id": "http://arxiv.org/abs/2602.05458v1", "title": "Emergence-as-Code for Self-Governing Reliable Systems", "summary": "SLO-as-code has made per-service} reliability declarative, but user experience is defined by journeys whose reliability is an emergent property of microservice topology, routing, redundancy, timeouts/fallbacks, shared failure domains, and tail amplification. As a result, journey objectives (e.g., \"checkout p99 < 400 ms\") are often maintained outside code and drift as the system evolves, forcing teams to either miss user expectations or over-provision and gate releases with ad-hoc heuristics. We propose Emergence-as-Code (EmaC), a vision for making journey reliability computable and governable via intent plus evidence. An EmaC spec declares journey intent (objective, control-flow operators, allowed actions) and binds it to atomic SLOs and telemetry. A runtime inference component consumes operational artifacts (e.g., tracing and traffic configuration) to synthesize a candidate journey model with provenance and confidence. From the last accepted model, the EmaC compiler/controller derives bounded journey SLOs and budgets under explicit correlation assumptions (optimistic independence vs. pessimistic shared fate), and emits control-plane artifacts (burn-rate alerts, rollout gates, action guards) that are reviewable in a Git workflow. An anonymized artifact repository provides a runnable example specification and generated outputs.", "published": "2026-02-05T09:04:33Z", "updated": "2026-02-05T09:04:33Z", "authors": ["Anatoly A. Krasnovsky"], "pdf_url": "https://arxiv.org/pdf/2602.05458v1"}
{"id": "http://arxiv.org/abs/2507.15226v2", "title": "Code Clone Detection via an AlphaFold-Inspired Framework", "summary": "Code clone detection plays a critical role in software maintenance and vulnerability analysis. Substantial methods have been proposed to detect code clones. However, they struggle to extract high-level program semantics directly from a single linear token sequence, leading to unsatisfactory detection performance. A similar single-sequence challenge has been successfully addressed in protein structure prediction by AlphaFold. Motivated by the successful resolution of the shared single-sequence challenge by AlphaFold, as well as the sequential similarities between proteins and code, we leverage AlphaFold for code clone detection. In particular, we propose AlphaCC, which represents code fragments as token sequences and adapts AlphaFold's sequence-to-structure modeling capability to infer code semantics. The pipeline of AlphaCC goes through three steps. First, AlphaCC transforms each input code fragment into a token sequence and, motivated by AlphaFold's use of multiple sequence alignment (MSA), novelly uses a retrieval-augmentation strategy to construct an MSA from lexically similar token sequences. Second, AlphaCC adopts a modified attention-based encoder based on AlphaFold to model dependencies within and across token sequences. Finally, unlike AlphaFold's protein structure prediction task, AlphaCC computes similarity scores between token sequences through a late interaction strategy and performs binary classification to determine code clone pairs. Comprehensive evaluations on three datasets, particularly two semantic clone detection datasets, show that AlphaCC consistently outperforms all baselines, demonstrating strong semantic understanding. AlphaCC further achieves strong performance on instances where tool-dependent methods fail, highlighting its tool-independence. Moreover, AlphaCC maintains competitive efficiency, enabling practical usage in large-scale clone detection tasks.", "published": "2025-07-21T03:57:59Z", "updated": "2026-02-05T08:41:13Z", "authors": ["Changguo Jia", "Yi Zhan", "Tianqi Zhao", "Hengzhi Ye", "Minghui Zhou"], "pdf_url": "https://arxiv.org/pdf/2507.15226v2"}
{"id": "http://arxiv.org/abs/2512.13515v3", "title": "Fine-tuned LLM-based Code Migration Framework", "summary": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.", "published": "2025-12-15T16:42:51Z", "updated": "2026-02-05T08:30:36Z", "authors": ["Oleg Grynets", "Vasyl Lyashkevych", "Dmytro Baran", "Maksym Orliansky", "Taras Zelenyy", "Markiian Leshchyshyn"], "pdf_url": "https://arxiv.org/pdf/2512.13515v3"}
{"id": "http://arxiv.org/abs/2506.17208v3", "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems", "summary": "The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards -- SWE-Bench Lite and SWE-Bench Verified -- have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (79 entries) and Verified (99 entries) leaderboards, analyzing 80 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.", "published": "2025-06-20T17:57:08Z", "updated": "2026-02-05T08:04:43Z", "authors": ["Matias Martinez", "Xavier Franch"], "pdf_url": "https://arxiv.org/pdf/2506.17208v3"}
{"id": "http://arxiv.org/abs/2602.05312v1", "title": "Does Programming Language Matter? An Empirical Study of Fuzzing Bug Detection", "summary": "Fuzzing has become a popular technique for automatically detecting vulnerabilities and bugs by generating unexpected inputs. In recent years, the fuzzing process has been integrated into continuous integration workflows (i.e., continuous fuzzing), enabling short and frequent testing cycles. Despite its widespread adoption, prior research has not examined whether the effectiveness of continuous fuzzing varies across programming languages. This study conducts a large-scale cross-language analysis to examine how fuzzing bug characteristics and detection efficiency differ among languages. We analyze 61,444 fuzzing bugs and 999,248 builds from 559 OSS-Fuzz projects categorized by primary language. Our findings reveal that (i) C++ and Rust exhibit higher fuzzing bug detection frequencies, (ii) Rust and Python show low vulnerability ratios but tend to expose more critical vulnerabilities, (iii) crash types vary across languages and unreproducible bugs are more frequent in Go but rare in Rust, and (iv) Python attains higher patch coverage but suffers from longer time-to-detection. These results demonstrate that fuzzing behavior and effectiveness are strongly shaped by language design, providing insights for language-aware fuzzing strategies and tool development.", "published": "2026-02-05T05:08:51Z", "updated": "2026-02-05T05:08:51Z", "authors": ["Tatsuya Shirai", "Olivier Nourry", "Yutaro Kashiwa", "Kenji Fujiwara", "Hajimu Iida"], "pdf_url": "https://arxiv.org/pdf/2602.05312v1"}
{"id": "http://arxiv.org/abs/2505.14759v4", "title": "LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models", "summary": "Large Language Models for code often entail significant computational complexity, which grows significantly with the length of the input code sequence. We propose LeanCode for code simplification to reduce training and prediction time, leveraging code contexts in utilizing attention scores to represent the tokens' importance. We advocate for the selective removal of tokens based on the average context-aware attention scores rather than average scores across all inputs. LeanCode uses the attention scores of `CLS' tokens within the encoder for classification tasks, such as code search. It also employs the encoder-decoder attention scores to determine token significance for sequence-to-sequence tasks like code summarization. Our evaluation shows LeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements of 60% and 16% for code search, and 29% and 27% for code summarization, respectively.", "published": "2025-05-20T16:38:53Z", "updated": "2026-02-05T04:30:19Z", "authors": ["Yan Wang", "Ling Ding", "Tien N Nguyen", "Shaohua Wang", "Yanan Zheng"], "pdf_url": "https://arxiv.org/pdf/2505.14759v4"}
{"id": "http://arxiv.org/abs/2602.05270v1", "title": "PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models", "summary": "As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.", "published": "2026-02-05T03:48:17Z", "updated": "2026-02-05T03:48:17Z", "authors": ["Thanh Le-Cong", "Bach Le", "Toby Murray", "Michael Pradel", "Cristian Cadar"], "pdf_url": "https://arxiv.org/pdf/2602.05270v1"}
{"id": "http://arxiv.org/abs/2601.22129v2", "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents", "summary": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.", "published": "2026-01-29T18:50:29Z", "updated": "2026-02-05T03:46:22Z", "authors": ["Yifeng Ding", "Lingming Zhang"], "pdf_url": "https://arxiv.org/pdf/2601.22129v2"}
{"id": "http://arxiv.org/abs/2602.05242v1", "title": "EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering", "summary": "Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.", "published": "2026-02-05T03:02:54Z", "updated": "2026-02-05T03:02:54Z", "authors": ["Chenhui Mao", "Yuanting Lei", "Zhixiang Wei", "Ming Liang", "Zhixiang Wang", "Jingxuan Xu", "Dajun Chen", "Wei Jiang", "Yong Li"], "pdf_url": "https://arxiv.org/pdf/2602.05242v1"}
{"id": "http://arxiv.org/abs/2510.00031v2", "title": "VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs", "summary": "We propose VibeCodeHPC, an automatic tuning system for HPC programs based on multi-agent LLMs for code generation. VibeCodeHPC tunes programs through multi-agent role allocation and iterative prompt refinement. We describe the system configuration with four roles: Project Manager (PM), System Engineer (SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent deployment and activity monitoring functions to facilitate effective multi-agent collaboration. In our case study, we convert and optimize CPU-based matrix-matrix multiplication code written in C to GPU code using CUDA. The multi-agent configuration of VibeCodeHPC achieved higher-quality code generation per unit time compared to a solo-agent configuration. Additionally, the dynamic agent deployment and activity monitoring capabilities facilitated more effective identification of requirement violations and other issues.", "published": "2025-09-26T04:54:13Z", "updated": "2026-02-05T02:24:20Z", "authors": ["Shun-ichiro Hayashi", "Koki Morita", "Daichi Mukunoki", "Tetsuya Hoshino", "Takahiro Katagiri"], "pdf_url": "https://arxiv.org/pdf/2510.00031v2"}
{"id": "http://arxiv.org/abs/2602.05157v1", "title": "The Necessity of a Holistic Safety Evaluation Framework for AI-Based Automation Features", "summary": "The intersection of Safety of Intended Functionality (SOTIF) and Functional Safety (FuSa) analysis of driving automation features has traditionally excluded Quality Management (QM) components from rigorous safety impact evaluations. While QM components are not typically classified as safety-relevant, recent developments in artificial intelligence (AI) integration reveal that such components can contribute to SOTIF-related hazardous risks. Compliance with emerging AI safety standards, such as ISO/PAS 8800, necessitates re-evaluating safety considerations for these components. This paper examines the necessity of conducting holistic safety analysis and risk assessment on AI components, emphasizing their potential to introduce hazards with the capacity to violate risk acceptance criteria when deployed in safety-critical driving systems, particularly in perception algorithms. Using case studies, we demonstrate how deficiencies in AI-driven perception systems can emerge even in QM-classified components, leading to unintended functional behaviors with critical safety implications. By bridging theoretical analysis with practical examples, this paper argues for the adoption of comprehensive FuSa, SOTIF, and AI standards-driven methodologies to identify and mitigate risks in AI components. The findings demonstrate the importance of revising existing safety frameworks to address the evolving challenges posed by AI, ensuring comprehensive safety assurance across all component classifications spanning multiple safety standards.", "published": "2026-02-05T00:22:24Z", "updated": "2026-02-05T00:22:24Z", "authors": ["Alireza Abbaspour", "Shabin Mahadevan", "Kilian Zwirglmaier", "Jeff Stafford"], "pdf_url": "https://arxiv.org/pdf/2602.05157v1"}
