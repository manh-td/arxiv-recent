{"id": "http://arxiv.org/abs/2602.16708v1", "title": "Policy Compiler for Secure Agentic Systems", "summary": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.\n  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.", "published": "2026-02-18T18:57:12Z", "updated": "2026-02-18T18:57:12Z", "authors": ["Nils Palumbo", "Sarthak Choudhary", "Jihye Choi", "Prasad Chalasani", "Mihai Christodorescu", "Somesh Jha"], "pdf_url": "https://arxiv.org/pdf/2602.16708v1"}
{"id": "http://arxiv.org/abs/2602.16700v1", "title": "The Role of Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems", "summary": "In symmetric private information retrieval (SPIR), a user communicates with multiple servers to retrieve from them a message in a database, while not revealing the message index to any individual server (user privacy), and learning no additional information about the database (database privacy). We study the problem of SPIR on graph-replicated database systems, where each node of the graph represents a server and each link represents a message. Each message is replicated at exactly two servers; those at which the link representing the message is incident. To ensure database privacy, the servers share a set of common randomness, independent of the database and the user's desired message index. We study two cases of common randomness distribution to the servers: i) graph-replicated common randomness, and ii) fully-replicated common randomness. Given a graph-replicated database system, in i), we assign one randomness variable independently to every pair of servers sharing a message, while in ii), we assign an identical set of randomness variable to all servers, irrespective of the underlying graph. In both settings, our goal is to characterize the SPIR capacity, i.e., the maximum number of desired message symbols retrieved per downloaded symbol, and quantify the minimum amount of common randomness required to achieve the capacity. To this goal, in setting i), we derive a general lower bound on the SPIR capacity, and show it to be tight for path and regular graphs through a matching converse. Moreover, we establish that the minimum size of common randomness required for SPIR is equal to the message size. In setting ii), the SPIR capacity improves over the first, more restrictive setting. We show this through capacity lower bounds for a class of graphs, by constructing SPIR schemes from PIR schemes.", "published": "2026-02-18T18:46:58Z", "updated": "2026-02-18T18:46:58Z", "authors": ["Shreya Meel", "Sennur Ulukus"], "pdf_url": "https://arxiv.org/pdf/2602.16700v1"}
{"id": "http://arxiv.org/abs/2505.12393v2", "title": "Protocol as Poetry: A Case Study of Pak's Smart Contract-Based Protocol Art", "summary": "Protocol art has recently proliferated through blockchain-based smart contracts, building on a century-long lineage of conceptual, participatory, interactive, systematic, algorithmic, and generative art practices. Few studies have examined the characteristics and appreciation of this emerging art form. To address this gap, this paper presents an annotated portfolio analysis of protocol artworks by Pak, a pioneering and influential pseudonymous artist who treats smart contracts as medium and collective participation through protocol as message. Tracing the evolution from early open-edition releases of The Fungible (2021) and the dynamic mechanics of Merge (2021) to the soul-bound messaging of Censored (2022) and the reflective absence of Not Found (2023), we examine how Pak choreographs distributed agency across collectors and autonomous code, demonstrating how programmable protocols become a social fabric in artistic meaning-making. Through thematic analysis of Pak's works, we identify seven core characteristics distinguishing protocol art from other art forms: (1) system-centric rather than object-centric composition, (2) autonomous governance enabling open-ended control, (3) distributed agency and communal authorship, (4) temporal dynamism and lifecycle aesthetics, (5) economy-driven engagement, (6) poetic message embedded in interaction rituals, and (7) interoperability enabling composability for emergent complexity. We then discuss how these features set protocol art apart from adjacent movements such as conceptual, generative, participatory, interactive, and performance art. By analyzing principles grounded in Pak's practice, we contribute to the emerging literature on protocol art (or \"protocolism\") and offer design implications for future artists exploring this evolving form.", "published": "2025-05-18T12:43:10Z", "updated": "2026-02-18T17:58:03Z", "authors": ["Botao Amber Hu"], "pdf_url": "https://arxiv.org/pdf/2505.12393v2"}
{"id": "http://arxiv.org/abs/2602.15238v2", "title": "Closing the Distribution Gap in Adversarial Training for LLMs", "summary": "Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.", "published": "2026-02-16T22:34:52Z", "updated": "2026-02-18T17:57:10Z", "authors": ["Chengzhi Hu", "Jonas Dornbusch", "David Lüdke", "Stephan Günnemann", "Leo Schwinn"], "pdf_url": "https://arxiv.org/pdf/2602.15238v2"}
{"id": "http://arxiv.org/abs/2508.10836v2", "title": "SoK: Data Minimization in Machine Learning", "summary": "Data minimization (DM) describes the principle of collecting only the data strictly necessary for a given task. It is a foundational principle across major data protection regulations like GDPR and CPRA. Violations of this principle have substantial real-world consequences, with regulatory actions resulting in fines reaching hundreds of millions of dollars. Notably, the relevance of data minimization is particularly pronounced in machine learning (ML) applications, which typically rely on large datasets, resulting in an emerging research area known as Data Minimization in Machine Learning (DMML). At the same time, existing work on other ML privacy and security topics often addresses concerns relevant to DMML without explicitly acknowledging the connection. This disconnect leads to confusion among practitioners, complicating their efforts to implement DM principles and interpret the terminology, metrics, and evaluation criteria used across different research communities. To address this gap, we present the first systematization of knowledge (SoK) for DMML. We introduce a general framework for DMML, encompassing a unified data pipeline, adversarial models, and points of minimization. This framework allows us to systematically review data minimization literature as well as DM-adjacent methodologies whose link to DM was often overlooked. Our structured overview is designed to help practitioners and researchers effectively adopt and apply DM principles in ML, by helping them identify relevant techniques and understand underlying assumptions and trade-offs through a DM-centric lens.", "published": "2025-08-14T17:00:13Z", "updated": "2026-02-18T17:46:15Z", "authors": ["Robin Staab", "Nikola Jovanović", "Kimberly Mai", "Prakhar Ganesh", "Martin Vechev", "Ferdinando Fioretto", "Matthew Jagielski"], "pdf_url": "https://arxiv.org/pdf/2508.10836v2"}
{"id": "http://arxiv.org/abs/2602.16596v1", "title": "Sequential Membership Inference Attacks", "summary": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.", "published": "2026-02-18T16:51:13Z", "updated": "2026-02-18T16:51:13Z", "authors": ["Thomas Michel", "Debabrota Basu", "Emilie Kaufmann"], "pdf_url": "https://arxiv.org/pdf/2602.16596v1"}
{"id": "http://arxiv.org/abs/2602.15689v2", "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models", "summary": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.", "published": "2026-02-17T16:12:21Z", "updated": "2026-02-18T16:42:07Z", "authors": ["Noa Linder", "Meirav Segal", "Omer Antverg", "Gil Gekker", "Tomer Fichman", "Omri Bodenheimer", "Edan Maor", "Omer Nevo"], "pdf_url": "https://arxiv.org/pdf/2602.15689v2"}
{"id": "http://arxiv.org/abs/2602.16569v1", "title": "Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face", "summary": "Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.", "published": "2026-02-18T16:11:11Z", "updated": "2026-02-18T16:11:11Z", "authors": ["Nicolò Di Domenico", "Annalisa Franco", "Matteo Ferrara", "Davide Maltoni"], "pdf_url": "https://arxiv.org/pdf/2602.16569v1"}
{"id": "http://arxiv.org/abs/2602.16564v1", "title": "A Scalable Approach to Solving Simulation-Based Network Security Games", "summary": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.", "published": "2026-02-18T16:07:01Z", "updated": "2026-02-18T16:07:01Z", "authors": ["Michael Lanier", "Yevgeniy Vorobeychik"], "pdf_url": "https://arxiv.org/pdf/2602.16564v1"}
{"id": "http://arxiv.org/abs/2602.16520v1", "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents", "summary": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.", "published": "2026-02-18T15:07:09Z", "updated": "2026-02-18T15:07:09Z", "authors": ["Doron Shavit"], "pdf_url": "https://arxiv.org/pdf/2602.16520v1"}
{"id": "http://arxiv.org/abs/2602.16489v1", "title": "Phase-Based Bit Commitment Protocol", "summary": "With the rise of artificial intelligence and machine learning, a new wave of private information is being flushed into applications. This development raises privacy concerns, as private datasets can be stolen or abused for non-authorized purposes. Secure function computation aims to solve such problems by allowing a service provider to compute functions of datasets in the possession of a a data provider without reading the data itself. A foundational primitive for such tasks is Bit Commitment (BC), which is known to be impossible to realize without added assumptions. Given the pressing nature of the topic, it is thus important to develop BC systems and prove their security under reasonable assumptions. In this work, we provide a novel quantum optical BC protocol that uses the added assumption that the network provider will secure transmission lines against eavesdropping. Under this added assumption, we prove security of our protocol in the honest but curious setting and discuss the hardness of Mayer's attack in the context of our protocol.", "published": "2026-02-18T14:22:22Z", "updated": "2026-02-18T14:22:22Z", "authors": ["Janis Nötzel", "Anshul Singhal", "Peter van Loock"], "pdf_url": "https://arxiv.org/pdf/2602.16489v1"}
{"id": "http://arxiv.org/abs/2602.16480v1", "title": "SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data", "summary": "Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency.", "published": "2026-02-18T14:14:38Z", "updated": "2026-02-18T14:14:38Z", "authors": ["Yiwen Lu"], "pdf_url": "https://arxiv.org/pdf/2602.16480v1"}
{"id": "http://arxiv.org/abs/2511.14406v2", "title": "Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation", "summary": "Large models adaptation through Federated Learning (FL) addresses a wide range of use cases and is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA). However, this distributed learning paradigm faces several security threats, particularly to its integrity, such as backdoor attacks that aim to inject malicious behavior during the local training steps of certain clients. We present the first analysis of the influence of LoRA on state-of-the-art backdoor attacks targeting model adaptation in FL. Specifically, we focus on backdoor lifespan, a critical characteristic in FL, that can vary depending on the attack scenario and the attacker's ability to effectively inject the backdoor. A key finding in our experiments is that for an optimally injected backdoor, the backdoor persistence after the attack is longer when the LoRA's rank is lower. Importantly, our work highlights evaluation issues of backdoor attacks against FL and contributes to the development of more robust and fair evaluations of backdoor attacks, enhancing the reliability of risk assessments for critical FL systems. Our code is publicly available.", "published": "2025-11-18T12:13:59Z", "updated": "2026-02-18T13:50:01Z", "authors": ["Bastien Vuillod", "Pierre-Alain Moellic", "Jean-Max Dutertre"], "pdf_url": "https://arxiv.org/pdf/2511.14406v2"}
{"id": "http://arxiv.org/abs/2602.16436v1", "title": "Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent", "summary": "Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.", "published": "2026-02-18T13:13:43Z", "updated": "2026-02-18T13:13:43Z", "authors": ["Jean Dufraiche", "Paul Mangold", "Michaël Perrot", "Marc Tommasi"], "pdf_url": "https://arxiv.org/pdf/2602.16436v1"}
{"id": "http://arxiv.org/abs/2602.14135v2", "title": "ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI", "summary": "Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the \"ForesightSafety Bench\" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.", "published": "2026-02-15T13:12:44Z", "updated": "2026-02-18T11:09:42Z", "authors": ["Haibo Tong", "Feifei Zhao", "Linghao Feng", "Ruoyu Wu", "Ruolin Chen", "Lu Jia", "Zhou Zhao", "Jindong Li", "Tenglong Li", "Erliang Lin", "Shuai Yang", "Enmeng Lu", "Yinqian Sun", "Qian Zhang", "Zizhe Ruan", "Jinyu Fan", "Zeyang Yue", "Ping Wu", "Huangrui Li", "Chengyi Sun", "Yi Zeng"], "pdf_url": "https://arxiv.org/pdf/2602.14135v2"}
{"id": "http://arxiv.org/abs/2602.16338v1", "title": "push0: Scalable and Fault-Tolerant Orchestration for Zero-Knowledge Proof Generation", "summary": "Zero-knowledge proof generation imposes stringent timing and reliability constraints on blockchain systems. For ZK-rollups, delayed proofs cause finality lag and economic loss; for Ethereum's emerging L1 zkEVM, proofs must complete within the 12-second slot window to enable stateless validation. The Ethereum Foundation's Ethproofs initiative coordinates multiple independent zkVMs across proving clusters to achieve real-time block proving, yet no principled orchestration framework addresses the joint challenges of (i) strict head-of-chain ordering, (ii) sub-slot latency bounds, (iii) fault-tolerant task reassignment, and (iv) prover-agnostic workflow composition. We present push0, a cloud-native proof orchestration system that decouples prover binaries from scheduling infrastructure. push0 employs an event-driven dispatcher--collector architecture over persistent priority queues, enforcing block-sequential proving while exploiting intra-block parallelism. We formalize requirements drawn from production ZK-rollup operations and the Ethereum real-time proving specification, then demonstrate via production Kubernetes cluster experiments that push0 achieves 5 ms median orchestration overhead with 99--100% scaling efficiency at 32 dispatchers for realistic workloads--overhead negligible (less than 0.1%) relative to typical proof computation times of 7+ seconds. Controlled Docker experiments validate these results, showing comparable performance (3--10 ms P50) when network variance is eliminated. Production deployment on the Zircuit zkrollup (14+ million mainnet blocks since March 2025) provides ecological validity for these controlled experiments. Our design enables seamless integration of heterogeneous zkVMs, supports automatic task recovery via message persistence, and provides the scheduling primitives necessary for both centralized rollup operators and decentralized multi-prover networks.", "published": "2026-02-18T10:22:33Z", "updated": "2026-02-18T10:22:33Z", "authors": ["Mohsen Ahmadvand", "Rok Pajnič", "Ching-Lun Chiu"], "pdf_url": "https://arxiv.org/pdf/2602.16338v1"}
{"id": "http://arxiv.org/abs/2510.21190v2", "title": "The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning", "summary": "As Large Language Models (LLMs) become integral to computing infrastructure, safety alignment serves as the primary security control preventing the generation of harmful payloads. However, this defense remains brittle. Existing jailbreak attacks typically bifurcate into white-box methods, which are inapplicable to commercial APIs due to lack of gradient access, and black-box optimization techniques, which often yield unnatural (e.g., syntactically rigid) or non-transferable (e.g., lacking cross-model generalization) prompts. In this work, we introduce TrojFill, a black-box exploitation framework that bypasses safety filters by targeting a fundamental logic flaw in current alignment paradigms: the decoupling of unsafety reasoning from content generation. TrojFill structurally reframes malicious instructions as a template-filling task required for safety analysis. By embedding obfuscated payloads (e.g., via placeholder substitution) into a \"Trojan\" structure, the attack induces the model to generate prohibited content as a \"demonstrative example\" ostensibly required for a subsequent sentence-by-sentence safety critique. This approach effectively masks the malicious intent from standard intent classifiers. We evaluate TrojFill against representative commercial systems, including GPT-4o, Gemini-2.5, DeepSeek-3.1, and Qwen-Max. Our results demonstrate that TrojFill achieves near-universal bypass rates: reaching 100% Attack Success Rate (ASR) on Gemini-flash-2.5 and DeepSeek-3.1, and 97% on GPT-4o, significantly outperforming existing black-box baselines. Furthermore, unlike optimization-based adversarial prompts, TrojFill generates highly interpretable and transferable attack vectors, exposing a systematic vulnerability inaligned LLMs.", "published": "2025-10-24T06:43:10Z", "updated": "2026-02-18T09:55:23Z", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long", "Kwok Yan Lam"], "pdf_url": "https://arxiv.org/pdf/2510.21190v2"}
{"id": "http://arxiv.org/abs/2602.16309v1", "title": "The Weight of a Bit: EMFI Sensitivity Analysis of Embedded Deep Learning Models", "summary": "Fault injection attacks on embedded neural network models have been shown as a potent threat. Numerous works studied resilience of models from various points of view. As of now, there is no comprehensive study that would evaluate the influence of number representations used for model parameters against electromagnetic fault injection (EMFI) attacks.\n  In this paper, we investigate how four different number representations influence the success of an EMFI attack on embedded neural network models. We chose two common floating-point representations (32-bit, and 16-bit), and two integer representations (8-bit, and 4-bit). We deployed four common image classifiers, ResNet-18, ResNet-34, ResNet-50, and VGG-11, on an embedded memory chip, and utilized a low-cost EMFI platform to trigger faults. Our results show that while floating-point representations exhibit almost a complete degradation in accuracy (Top-1 and Top-5) after a single fault injection, integer representations offer better resistance overall. Especially, when considering the the 8-bit representation on a relatively large network (VGG-11), the Top-1 accuracies stay at around 70% and the Top-5 at around 90%.", "published": "2026-02-18T09:40:29Z", "updated": "2026-02-18T09:40:29Z", "authors": ["Jakub Breier", "Štefan Kučerák", "Xiaolu Hou"], "pdf_url": "https://arxiv.org/pdf/2602.16309v1"}
{"id": "http://arxiv.org/abs/2602.16304v1", "title": "Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification", "summary": "The prevalence of malicious packages in open-source repositories, such as PyPI, poses a critical threat to the software supply chain. While Large Language Models (LLMs) have emerged as a promising tool for automated security tasks, their effectiveness in detecting malicious packages and indicators remains underexplored. This paper presents a systematic evaluation of 13 LLMs for detecting malicious software packages. Using a curated dataset of 4,070 packages (3,700 benign and 370 malicious), we evaluate model performance across two tasks: binary classification (package detection) and multi-label classification (identification of specific malicious indicators). We further investigate the impact of prompting strategies, temperature settings, and model specifications on detection accuracy. We find a significant \"granularity gap\" in LLMs' capabilities. While GPT-4.1 achieves near-perfect performance in binary detection (F1 $\\approx$ 0.99), performance degrades by approximately 41\\% when the task shifts to identifying specific malicious indicators. We observe that general models are best for filtering out the majority of threats, while specialized coder models are better at detecting attacks that follow a strict, predictable code structure. Our correlation analysis indicates that parameter size and context width have negligible explanatory power regarding detection accuracy. We conclude that while LLMs are powerful detectors at the package level, they lack the semantic depth required for precise identification at the granular indicator level.", "published": "2026-02-18T09:36:46Z", "updated": "2026-02-18T09:36:46Z", "authors": ["Ahmed Ryan", "Ibrahim Khalil", "Abdullah Al Jahid", "Md Erfan", "Akond Ashfaque Ur Rahman", "Md Rayhanur Rahman"], "pdf_url": "https://arxiv.org/pdf/2602.16304v1"}
{"id": "http://arxiv.org/abs/2602.16268v1", "title": "Quantum Oracle Distribution Switching and its Applications to Fully Anonymous Ring Signatures", "summary": "Ring signatures are a powerful primitive that allows a member to sign on behalf of a group, without revealing their identity. Recently, ring signatures have received additional attention as an ingredient for post-quantum deniable authenticated key exchange, e.g., for a post-quantum version of the Signal protocol, employed by virtually all end-to-end-encrypted messenger services. While several ring signature constructions from post-quantum assumptions offer suitable security and efficiency for use in deniable key exchange, they are currently proven secure in the random oracle model (ROM) only, which is insufficient for post-quantum security.\n  In this work, we provide four security reductions in the quantum-accessible random oracle model (QROM) for two generic ring signature constructions: two for the AOS framework and two for a construction paradigm based on ring trapdoors, whose generic backbone we formalize. The two security proofs for AOS ring signatures differ in their requirements on the underlying sigma protocol and their tightness. The two reductions for the ring-trapdoor-based ring signatures exhibit various differences in requirements and the security they provide. We employ the measure-and-reprogram technique, QROM straightline extraction tools based on the compressed oracle, history-free reductions and QROM reprogramming tools. To make use of Rényi divergence properties in the QROM, we study the behavior of quantum algorithms that interact with an oracle whose distribution is based on one of two different distributions over the set of outputs. We provide tight bounds for the statistical distance, show that the Rényi divergence can not be used to replace the entire oracle and provide a workaround.", "published": "2026-02-18T08:41:04Z", "updated": "2026-02-18T08:41:04Z", "authors": ["Marvin Beckmann", "Christian Majenz"], "pdf_url": "https://arxiv.org/pdf/2602.16268v1"}
{"id": "http://arxiv.org/abs/2602.15485v2", "title": "SecCodeBench-V2 Technical Report", "summary": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and JavaScript. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.", "published": "2026-02-17T10:47:06Z", "updated": "2026-02-18T08:08:18Z", "authors": ["Longfei Chen", "Ji Zhao", "Lanxiao Cui", "Tong Su", "Xingbo Pan", "Ziyang Li", "Yongxing Wu", "Qijiang Cao", "Qiyao Cai", "Jing Zhang", "Yuandong Ni", "Junyao He", "Zeyu Zhang", "Chao Ge", "Xuhuai Lu", "Zeyu Gao", "Yuxin Cui", "Weisen Chen", "Yuxuan Peng", "Shengping Wang", "Qi Li", "Yukai Huang", "Yukun Liu", "Tuo Zhou", "Terry Yue Zhuo", "Junyang Lin", "Chao Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.15485v2"}
{"id": "http://arxiv.org/abs/2602.15195v2", "title": "Weight space Detection of Backdoors in LoRA Adapters", "summary": "LoRA adapters let users fine-tune large language models (LLMs) efficiently. However, LoRA adapters are shared through open repositories like Hugging Face Hub \\citep{huggingface_hub_docs}, making them vulnerable to backdoor attacks. Current detection methods require running the model with test input data -- making them impractical for screening thousands of adapters where the trigger for backdoor behavior is unknown. We detect poisoned adapters by analyzing their weight matrices directly, without running the model -- making our method data-agnostic. Our method extracts simple statistics -- how concentrated the singular values are, their entropy, and the distribution shape -- and flags adapters that deviate from normal patterns. We evaluate the method on 500 LoRA adapters -- 400 clean, and 100 poisoned for Llama-3.2-3B on instruction and reasoning datasets: Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval, and GLUE dataset. We achieve 97\\% detection accuracy with less than 2\\% false positives.", "published": "2026-02-16T21:20:47Z", "updated": "2026-02-18T07:52:06Z", "authors": ["David Puertolas Merenciano", "Ekaterina Vasyagina", "Raghav Dixit", "Kevin Zhu", "Ruizhe Li", "Javier Ferrando", "Maheep Chaudhary"], "pdf_url": "https://arxiv.org/pdf/2602.15195v2"}
{"id": "http://arxiv.org/abs/2509.00770v2", "title": "Bayesian and Multi-Objective Decision Support for Real-Time Incident Mitigation in Critical Infrastructure", "summary": "Critical infrastructure increasingly relies on interconnected cyber-physical systems whose security incidents can escalate rapidly into safety and operational failures. Existing decision-support approaches struggle to support real-time incident response because they rely on static assumptions, incomplete vulnerability data, and single-objective risk models that do not adequately capture trade-offs between attack likelihood, impact severity, and system availability. This paper proposes a real-time, adaptive decision-support framework for incident mitigation in critical infrastructure that combines hierarchical system modelling with Bayesian probabilistic reasoning. The framework leverages probabilistic graphical models (Bayesian Networks) constructed from system architecture and vulnerability data, and employs confidence-calibrated exposure estimation to integrate complementary vulnerability scoring metrics under epistemic uncertainty. Mitigation strategies are explored as countermeasure portfolios and refined using multi-objective optimisation to identify Pareto-optimal trade-offs suitable for time- and resource-constrained response scenarios. Frequency-based heuristics are applied to prioritise robust mitigation actions across optimisation runs. The framework is evaluated on three representative cyber-physical attack scenarios, demonstrating its ability to adapt to evolving threats and provide actionable decision support under real-time constraints, thereby enhancing the operational resilience of critical infrastructure.", "published": "2025-08-31T09:47:38Z", "updated": "2026-02-18T07:30:03Z", "authors": ["Shaofei Huang", "Christopher M. Poskitt", "Lwin Khin Shar"], "pdf_url": "https://arxiv.org/pdf/2509.00770v2"}
{"id": "http://arxiv.org/abs/2501.03544v4", "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models", "summary": "Recent text-to-image (T2I) models have exhibited remarkable performance in generating high-quality images from text descriptions. However, these models are vulnerable to misuse, particularly generating not-safe-for-work (NSFW) content, such as sexually explicit, violent, political, and disturbing images, raising serious ethical concerns. In this work, we present PromptGuard, a novel content moderation technique that draws inspiration from the system prompt mechanism in large language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack a direct interface for enforcing behavioral guidelines. Our key idea is to optimize a safety soft prompt that functions as an implicit system prompt within the T2I model's textual embedding space. This universal soft prompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image generation without altering the inference efficiency or requiring proxy models. We further enhance its reliability and helpfulness through a divide-and-conquer strategy, which optimizes category-specific soft prompts and combines them into holistic safety guidance. Extensive experiments across five datasets demonstrate that PromptGuard effectively mitigates NSFW content generation while preserving high-quality benign outputs. PromptGuard achieves 3.8 times faster than prior content moderation methods, surpassing eight state-of-the-art defenses with an optimal unsafe ratio down to 5.84%.", "published": "2025-01-07T05:39:21Z", "updated": "2026-02-18T05:55:40Z", "authors": ["Lingzhi Yuan", "Xinfeng Li", "Chejian Xu", "Guanhong Tao", "Xiaojun Jia", "Yihao Huang", "Wei Dong", "Yang Liu", "Xiaofeng Wang", "Bo Li"], "pdf_url": "https://arxiv.org/pdf/2501.03544v4"}
{"id": "http://arxiv.org/abs/2506.17047v2", "title": "Navigating the Deep: End-to-End Extraction on Deep Neural Networks", "summary": "Neural network model extraction has recently emerged as an important security concern, as adversaries attempt to recover a network's parameters via black-box queries. Carlini et al. proposed in CRYPTO'20 a model extraction approach, consisting of two steps: signature extraction and sign extraction. However, in practice this signature-extraction method is limited to very shallow networks only, and the proposed sign-extraction method is exponential in time. Recently, Canales-Martinez et al. (Eurocrypt'24) proposed a polynomial-time sign-extraction method, but it assumes the corresponding signatures have already been successfully extracted and can fail on so-called low-confidence neurons.\n  In this work, we first revisit and refine the signature extraction process by systematically identifying and addressing for the first time critical limitations of Carlini et al.'s signature-extraction method. These limitations include rank deficiency and noise propagation from deeper layers. To overcome these challenges, we propose efficient algorithmic solutions for each of the identified issues. Our approach permits the extraction of much deeper networks than previously possible. In addition, we propose new methods to improve numerical precision in signature extraction, and enhance the sign extraction part by combining two polynomial methods to avoid exponential exhaustive search in the case of low-confidence neurons. This leads to the very first end-to-end model extraction method that runs in polynomial time.\n  We validate our attack through extensive experiments on ReLU-based neural networks, demonstrating significant improvements in extraction depth. For instance, our attack extracts consistently at least eight layers of neural networks trained on either the MNIST or CIFAR-10 datasets, while previous works could barely extract the first three layers of networks of similar width.", "published": "2025-06-20T14:59:47Z", "updated": "2026-02-18T05:28:19Z", "authors": ["Haolin Liu", "Adrien Siproudhis", "Samuel Experton", "Peter Lorenz", "Christina Boura", "Thomas Peyrin"], "pdf_url": "https://arxiv.org/pdf/2506.17047v2"}
{"id": "http://arxiv.org/abs/2501.16534v5", "title": "Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs", "summary": "Alignment in large language models (LLMs) is used to enforce guidelines such as safety. Yet, alignment fails in the face of jailbreak attacks that modify inputs to induce unsafe outputs. In this paper, we introduce and evaluate a new technique for jailbreak attacks. We observe that alignment embeds a safety classifier in the LLM responsible for deciding between refusal and compliance, and seek to extract an approximation of this classifier: a surrogate classifier. To this end, we build candidate classifiers from subsets of the LLM. We first evaluate the degree to which candidate classifiers approximate the LLM's safety classifier in benign and adversarial settings. Then, we attack the candidates and measure how well the resulting adversarial inputs transfer to the LLM. Our evaluation shows that the best candidates achieve accurate agreement (an F1 score above 80%) using as little as 20% of the model architecture. Further, we find that attacks mounted on the surrogate classifiers can be transferred to the LLM with high success. For example, a surrogate using only 50% of the Llama 2 model achieved an attack success rate (ASR) of 70% with half the memory footprint and runtime -- a substantial improvement over attacking the LLM directly, where we only observed a 22% ASR. These results show that extracting surrogate classifiers is an effective and efficient means for modeling (and therein addressing) the vulnerability of aligned models to jailbreaking attacks. The code is available at https://github.com/jcnf0/targeting-alignment.", "published": "2025-01-27T22:13:05Z", "updated": "2026-02-18T03:35:12Z", "authors": ["Jean-Charles Noirot Ferrand", "Yohan Beugin", "Eric Pauley", "Ryan Sheatsley", "Patrick McDaniel"], "pdf_url": "https://arxiv.org/pdf/2501.16534v5"}
{"id": "http://arxiv.org/abs/2602.16156v1", "title": "Weak Zero-Knowledge and One-Way Functions", "summary": "We study the implications of the existence of weak Zero-Knowledge (ZK) protocols for worst-case hard languages. These are protocols that have completeness, soundness, and zero-knowledge errors (denoted $ε_c$, $ε_s$, and $ε_z$, respectively) that might not be negligible. Under the assumption that there are worst-case hard languages in NP, we show the following:\n  1. If all languages in NP have NIZK proofs or arguments satisfying $ ε_c+ε_s+ ε_z < 1 $, then One-Way Functions (OWFs) exist.\n  This covers all possible non-trivial values for these error rates. It additionally implies that if all languages in NP have such NIZK proofs and $ε_c$ is negligible, then they also have NIZK proofs where all errors are negligible. Previously, these results were known under the more restrictive condition $ ε_c+\\sqrt{ε_s}+ε_z < 1 $ [Chakraborty et al., CRYPTO 2025].\n  2. If all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+(2k-1).ε_z < 1 $, then OWFs exist.\n  3. If, for some constant $k$, all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+k.ε_z < 1 $, then infinitely-often OWFs exist.", "published": "2026-02-18T03:09:48Z", "updated": "2026-02-18T03:09:48Z", "authors": ["Rohit Chatterjee", "Yunqi Li", "Prashant Nalini Vasudevan"], "pdf_url": "https://arxiv.org/pdf/2602.16156v1"}
{"id": "http://arxiv.org/abs/2512.03310v3", "title": "Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs", "summary": "The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.", "published": "2025-12-02T23:46:42Z", "updated": "2026-02-18T01:49:39Z", "authors": ["Kunj Joshi", "David A. Smith"], "pdf_url": "https://arxiv.org/pdf/2512.03310v3"}
{"id": "http://arxiv.org/abs/2602.16130v1", "title": "Managing Credible Anonymous Identities in Web 3.0 Services: A Scalable On-Chain Admission Framework with Recursive Proof Aggregation", "summary": "Open Web 3.0 platforms increasingly operate as \\emph{service ecosystems} (e.g., DeFi, DAOs, and decentralized social applications) where \\emph{admission control} and \\emph{account provisioning} must be delivered as an always-on service under bursty demand. Service operators face a fundamental tension: enforcing Sybil resistance (one-person-one-account) while preserving user privacy, yet keeping on-chain verification cost and admission latency predictable at scale. Existing credential-based ZK admission approaches typically require per-request on-chain verification, making the provisioning cost grow with the number of concurrent joiners. We present \\textbf{ZK-AMS}, a scalable admission and provisioning layer that bridges real-world \\emph{Personhood Credentials} to anonymous on-chain service accounts. ZK-AMS combines (i) zero-knowledge credential validation, (ii) a \\emph{permissionless} batch submitter model, and (iii) a decentralized, privacy-preserving folding pipeline that uses Nova-style recursive aggregation together with multi-key homomorphic encryption, enabling batch settlement with \\emph{constant} on-chain verification per batch. We implement ZK-AMS end-to-end on an Ethereum testbed and evaluate admission throughput, end-to-end latency, and gas consumption. Results show stable verification cost across batch sizes and substantially improved admission efficiency over non-recursive baselines, providing a practical and cost-predictable admission service for large-scale Web 3.0 communities.", "published": "2026-02-18T01:43:17Z", "updated": "2026-02-18T01:43:17Z", "authors": ["Zibin Lin", "Taotao Wang", "Shengli Zhang", "Long Shi", "Shui Yu"], "pdf_url": "https://arxiv.org/pdf/2602.16130v1"}
{"id": "http://arxiv.org/abs/2602.16109v1", "title": "Federated Graph AGI for Cross-Border Insider Threat Intelligence in Government Financial Schemes", "summary": "Cross-border insider threats pose a critical challenge to government financial schemes, particularly when dealing with distributed, privacy-sensitive data across multiple jurisdictions. Existing approaches face fundamental limitations: they cannot effectively share intelligence across borders due to privacy constraints, lack reasoning capabilities to understand complex multi-step attack patterns, and fail to capture intricate graph-structured relationships in financial networks. We introduce FedGraph-AGI, a novel federated learning framework integrating Artificial General Intelligence (AGI) reasoning with graph neural networks for privacy-preserving cross-border insider threat detection. Our approach combines: (1) federated graph neural networks preserving data sovereignty; (2) Mixture-of-Experts (MoE) aggregation for heterogeneous jurisdictions; and (3) AGI-powered reasoning via Large Action Models (LAM) performing causal inference over graph data. Through experiments on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves 92.3% accuracy, significantly outperforming federated baselines (86.1%) and centralized approaches (84.7%). Our ablation studies reveal AGI reasoning contributes 6.8% improvement, while MoE adds 4.4%. The system maintains epsilon = 1.0 differential privacy while achieving near-optimal performance and scales efficiently to 50+ clients. This represents the first integration of AGI reasoning with federated graph learning for insider threat detection, opening new directions for privacy-preserving cross-border intelligence sharing.", "published": "2026-02-18T00:39:54Z", "updated": "2026-02-18T00:39:54Z", "authors": ["Srikumar Nayak", "James Walmesley"], "pdf_url": "https://arxiv.org/pdf/2602.16109v1"}
{"id": "http://arxiv.org/abs/2602.16098v1", "title": "Collaborative Zone-Adaptive Zero-Day Intrusion Detection for IoBT", "summary": "The Internet of Battlefield Things (IoBT) relies on heterogeneous, bandwidth-constrained, and intermittently connected tactical networks that face rapidly evolving cyber threats. In this setting, intrusion detection cannot depend on continuous central collection of raw traffic due to disrupted links, latency, operational security limits, and non-IID traffic across zones. We present Zone-Adaptive Intrusion Detection (ZAID), a collaborative detection and model-improvement framework for unseen attack types, where \"zero-day\" refers to previously unobserved attack families and behaviours (not vulnerability disclosure timing). ZAID combines a universal convolutional model for generalisable traffic representations, an autoencoder-based reconstruction signal as an auxiliary anomaly score, and lightweight adapter modules for parameter-efficient zone adaptation. To support cross-zone generalisation under constrained connectivity, ZAID uses federated aggregation and pseudo-labelling to leverage locally observed, weakly labelled behaviours. We evaluate ZAID on ToN_IoT using a zero-day protocol that excludes MITM, DDoS, and DoS from supervised training and introduces them during zone-level deployment and adaptation. ZAID achieves up to 83.16% accuracy on unseen attack traffic and transfers to UNSW-NB15 under the same procedure, with a best accuracy of 71.64%. These results indicate that parameter-efficient, zone-personalised collaboration can improve the detection of previously unseen attacks in contested IoBT environments.", "published": "2026-02-18T00:02:15Z", "updated": "2026-02-18T00:02:15Z", "authors": ["Amirmohammad Pasdar", "Shabnam Kasra Kermanshahi", "Nour Moustafa", "Van-Thuan Pham"], "pdf_url": "https://arxiv.org/pdf/2602.16098v1"}
