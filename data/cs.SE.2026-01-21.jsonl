{"id": "http://arxiv.org/abs/2601.15232v1", "title": "When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling", "summary": "Large Language Models (LLMs) have revolutionized intelligent application development. While standalone LLMs cannot perform any actions, LLM agents address the limitation by integrating tools. However, debugging LLM agents is difficult and costly as the field is still in it's early stage and the community is underdeveloped. To understand the bugs encountered during agent development, we present the first comprehensive study of bug types, root causes, and effects in LLM agent-based software. We collected and analyzed 1,187 bug-related posts and code snippets from Stack Overflow, GitHub, and Hugging Face forums, focused on LLM agents built with seven widely used LLM frameworks as well as custom implementations. For a deeper analysis, we have also studied the component where the bug occurred, along with the programming language and framework. This study also investigates the feasibility of automating bug identification. For that, we have built a ReAct agent named BugReAct, equipped with adequate external tools to determine whether it can detect and annotate the bugs in our dataset. According to our study, we found that BugReAct equipped with Gemini 2.5 Flash achieved a remarkable performance in annotating bug characteristics with an average cost of 0.01 USD per post/code snippet.", "published": "2026-01-21T18:13:10Z", "updated": "2026-01-21T18:13:10Z", "authors": ["Niful Islam", "Ragib Shahriar Ayon", "Deepak George Thomas", "Shibbir Ahmed", "Mohammad Wardat"], "pdf_url": "https://arxiv.org/pdf/2601.15232v1"}
{"id": "http://arxiv.org/abs/2601.15195v1", "title": "Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub", "summary": "AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.", "published": "2026-01-21T17:12:46Z", "updated": "2026-01-21T17:12:46Z", "authors": ["Ramtin Ehsani", "Sakshi Pathak", "Shriya Rawal", "Abdullah Al Mujahid", "Mia Mohammad Imran", "Preetha Chatterjee"], "pdf_url": "https://arxiv.org/pdf/2601.15195v1"}
{"id": "http://arxiv.org/abs/2601.15188v1", "title": "Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback", "summary": "This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.", "published": "2026-01-21T17:06:41Z", "updated": "2026-01-21T17:06:41Z", "authors": ["Stephan Wallraven", "Tim Köhne", "Hartmut Westenberger", "Andreas Moser"], "pdf_url": "https://arxiv.org/pdf/2601.15188v1"}
{"id": "http://arxiv.org/abs/2601.01219v2", "title": "HD-GEN: A High-Performance Software System for Human Mobility Data Generation Based on Patterns of Life", "summary": "Understanding individual-level human mobility is critical for a wide range of applications. As such, real-world trajectory datasets provide valuable insights into actual movement behaviors and patterns of life but are often constrained by data sparsity and participant bias. Synthetic data, by contrast, offers scalability and flexibility but frequently lacks realism. To address this gap, we introduce a comprehensive software pipeline for, generating, calibrating, processing, and visualizing large-scale individual-level human mobility datasets that combine the realism of empirical data with the control and extensibility of Patterns-of-Life simulations. Our system consists of four integrated components. (1) a data generation engine which constructs geographically grounded simulations using OpenStreetMap data to produce diverse mobility logs. (2) a genetic algorithm-based calibration module that fine-tunes simulation parameters to align with real-world mobility characteristics, such as daily trip counts and radius of gyration, enabling realistic behavioral modeling. (3) a data processing suite which transforms raw simulation logs into structured formats suitable for downstream applications, including model training and benchmarking, and (4) a visualization module that extracts key mobility patterns and insights from the processed datasets and presents them through intuitive visual analytics for improved interpretability.", "published": "2026-01-03T16:01:00Z", "updated": "2026-01-21T16:37:40Z", "authors": ["Hossein Amiri", "Joon-Seok Kim", "Hamdi Kavak", "Andrew Crooks", "Dieter Pfoser", "Carola Wenk", "Andreas Züfle"], "pdf_url": "https://arxiv.org/pdf/2601.01219v2"}
{"id": "http://arxiv.org/abs/2601.15154v1", "title": "SAGA: Detecting Security Vulnerabilities Using Static Aspect Analysis", "summary": "Python is one of the most popular programming languages; as such, projects written in Python involve an increasing number of diverse security vulnerabilities. However, existing state-of-the-art analysis tools for Python only support a few vulnerability types. Hence, there is a need to detect a large variety of vulnerabilities in Python projects.\n  In this paper, we propose the SAGA approach to detect and locate vulnerabilities in Python source code in a versatile way. SAGA includes a source code parser able to extract control- and data-flow information and to represent it as a symbolic control-flow graph, as well as a domain-specific language defining static aspects of the source code and their evolution during graph traversals. We have leveraged this language to define a library of static aspects for integrity, confidentiality, and other security-related properties.\n  We have evaluated SAGA on a dataset of 108 vulnerabilities, obtaining 100% sensitivity and 99.15% specificity, with only one false positive, while outperforming four common security analysis tools. This analysis was performed in less than 31 seconds, i.e., between 2.5 and 512.1 times faster than the baseline tools.", "published": "2026-01-21T16:26:26Z", "updated": "2026-01-21T16:26:26Z", "authors": ["Yoann Marquer", "Domenico Bianculli", "Lionel C. Briand"], "pdf_url": "https://arxiv.org/pdf/2601.15154v1"}
{"id": "http://arxiv.org/abs/2510.17932v2", "title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models", "summary": "We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code.", "published": "2025-10-20T15:11:56Z", "updated": "2026-01-21T16:16:29Z", "authors": ["Jiahao Tang", "Henry Hengyuan Zhao", "Lijian Wu", "Yifei Tao", "Dongxing Mao", "Yang Wan", "Jingru Tan", "Min Zeng", "Min Li", "Alex Jinpeng Wang"], "pdf_url": "https://arxiv.org/pdf/2510.17932v2"}
{"id": "http://arxiv.org/abs/2601.15139v1", "title": "Why Authors and Maintainers Link (or Don't Link) Their PyPI Libraries to Code Repositories and Donation Platforms", "summary": "Metadata of libraries on the Python Package Index (PyPI)-including links to source code repositories and donation platforms-plays a critical role in supporting the transparency, trust, and sustainability of open-source libraries. Yet, many packages lack such metadata, and little is known about the underlying reasons. This paper presents a large-scale empirical study combining two targeted surveys sent to 50,000 PyPI authors and maintainers. We analyze more than 1,400 responses using large language model (LLM)-based topic modeling to uncover key motivations and barriers related to linking repositories and donation platforms. While repository URLs are often linked to foster collaboration, increase transparency, and enable issue tracking, some maintainers omit them due to oversight, laziness, or the perceived irrelevance to their project. Donation platform links are reported to support open source work or receive financial contributions, but are hindered by skepticism, technical friction, and organizational constraints. Cross-cutting challenges-such as outdated links, lack of awareness, and unclear guidance-affect both types of metadata. We further assess the robustness of our topic modeling pipeline across 30 runs (84% lexical and 89% semantic similarity) and validate topic quality with 23 expert raters (Randolph's kappa = 0.55). The study contributes empirical insights into PyPI's metadata practices and provides recommendations for improving them, while also demonstrating the effectiveness of our topic modeling approach for analyzing short-text survey responses.", "published": "2026-01-21T16:13:57Z", "updated": "2026-01-21T16:13:57Z", "authors": ["Alexandros Tsakpinis", "Nicolas Raube", "Alexander Pretschner"], "pdf_url": "https://arxiv.org/pdf/2601.15139v1"}
{"id": "http://arxiv.org/abs/2601.15094v1", "title": "Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks", "summary": "Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis--an aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues.", "published": "2026-01-21T15:33:16Z", "updated": "2026-01-21T15:33:16Z", "authors": ["Md Zahidul Haque", "Saima Afrin", "Antonio Mastropaolo"], "pdf_url": "https://arxiv.org/pdf/2601.15094v1"}
{"id": "http://arxiv.org/abs/2601.15084v1", "title": "DeLog: An Efficient Log Compression Framework with Pattern Signature Synthesis", "summary": "Parser-based log compression, which separates static tem- plates from dynamic variables, is a promising approach to exploit the unique structure of log data. However, its perfor- mance on complex production logs is often unsatisfactory. This performance gap coincides with a known degradation in the accuracy of its core log parsing component on such data, motivating our investigation into a foundational yet unverified question: does higher parsing accuracy necessarily lead to better compression ratio?\n  To answer this, we conduct the first empirical study quanti- fying this relationship and find that a higher parsing accuracy does not guarantee a better compression ratio. Instead, our findings reveal that compression ratio is dictated by achiev- ing effective pattern-based grouping and encoding, i.e., the partitioning of tokens into low entropy, highly compressible groups.\n  Guided by this insight, we design DeLog, a novel log com- pressor that implements a Pattern Signature Synthesis mecha- nism to achieve efficient pattern-based grouping. On 16 public and 10 production datasets, DeLog achieves state-of-the-art compression ratio and speed.", "published": "2026-01-21T15:26:09Z", "updated": "2026-01-21T15:26:09Z", "authors": ["Siyu Yu", "Yifan Wu", "Junjielong Xu", "Ying Fu", "Ning Wang", "Maoyin Liu", "Pancheng Jiang", "Xiang Zhang", "Tong Jia", "Pinjia He", "Ying Li"], "pdf_url": "https://arxiv.org/pdf/2601.15084v1"}
{"id": "http://arxiv.org/abs/2601.15074v1", "title": "SmartOracle -- An Agentic Approach to Mitigate Noise in Differential Oracles", "summary": "Differential fuzzers detect bugs by executing identical inputs across distinct implementations of the same specification, such as JavaScript interpreters. Validating the outputs requires an oracle and for differential testing of JavaScript, these are constructed manually, making them expensive, time-consuming, and prone to false positives. Worse, when the specification evolves, this manual effort must be repeated.\n  Inspired by the success of agentic systems in other SE domains, this paper introduces SmartOracle. SmartOracle decomposes the manual triage workflow into specialized Large Language Model (LLM) sub-agents. These agents synthesize independently gathered evidence from terminal runs and targeted specification queries to reach a final verdict.\n  For historical benchmarks, SmartOracle achieves 0.84 recall with an 18% false positive rate. Compared to a sequential Gemini 2.5 Pro baseline, it improves triage accuracy while reducing analysis time by 4$\\times$ and API costs by 10$\\times$. In active fuzzing campaigns, SmartOracle successfully identified and reported previously unknown specification-level issues across major engines, including bugs in V8, JavaScriptCore, and GraalJS.\n  The success of SmartOracle's agentic architecture on Javascript suggests it might be useful other software systems- a research direction we will explore in future work.", "published": "2026-01-21T15:20:53Z", "updated": "2026-01-21T15:20:53Z", "authors": ["Srinath Srinivasan", "Tim Menzies", "Marcelo D'Amorim"], "pdf_url": "https://arxiv.org/pdf/2601.15074v1"}
{"id": "http://arxiv.org/abs/2510.25692v3", "title": "A Configuration-First Framework for Reproducible, Low-Code Localization", "summary": "Machine learning is increasingly permeating radio-based localization services. To keep results credible and comparable, everyday workflows should make rigorous experiment specification and exact repeatability the default, without blocking advanced experimentation. However, in practice, researchers face a three-way gap that could be filled by a framework that offers (i) low coding effort for end-to-end studies, (ii) reproducibility by default, including versioned code, data, and configurations, controlled randomness, isolated runs, and recorded artifacts, and (iii) built-in extensibility so new models, metrics, and stages can be added with minimal integration effort. Existing tools rarely deliver all three for machine learning in general and localization workflows, supporting location-based services, in particular. In this paper, we introduce a low-code, configuration-first framework in which experiments are declared in human-readable configuration files, a workflow orchestrator executes standardized pipelines from data preparation to reporting, and all artifacts, such as datasets, models, metrics, and reports, are versioned. We instantiate the framework as LOCALIZE with preconfigured, versioned datasets that reduce initial setup effort and boilerplate, thereby accelerating model development and evaluation. The design, with explicit extension points, allows experts to add components without reworking the underlying infrastructure. Through a qualitative comparison and a head-to-head study against a plain Jupyter notebook baseline, we show that the framework reduces authoring effort while maintaining comparable runtime and memory behavior. Furthermore, using a example dataset, we demonstrate that scaling the training data from 1x to 10x keeps orchestration overheads bounded as data grows.", "published": "2025-10-29T16:57:33Z", "updated": "2026-01-21T15:14:36Z", "authors": ["Tim Strnad", "Blaž Bertalanič", "Carolina Fortuna"], "pdf_url": "https://arxiv.org/pdf/2510.25692v3"}
{"id": "http://arxiv.org/abs/2601.11430v2", "title": "A Practical Guide to Establishing Technical Debt Management", "summary": "This white paper provides an overview of the topic of \"technical debt\" and presents an approach for managing technical debt in teams. The white paper is based on the results of my dissertation, which aimed to translate scientific findings into practical guidance. To this end, I collaborated with other researchers to support three teams from different companies in adapting and establishing a technical debt management system tailored to their specific needs. Research findings were supplemented with details or additional approaches. Research results that were less practical were discarded. The result is a guide on establishing technical debt management within a team. The guide is intended to provide orientation and not be a rigid framework. We distinguish between \"best practices\" and \"nice-to-haves.\" \"Best practices\" are understood to be all approaches that were adopted by all three teams. \"Nice-to-haves\" were used by at least one team. In many places, it is explicitly mentioned that the team should decide together how to design the process. This also applies, of course, to all areas where this was not explicitly mentioned. This white paper explicitly does not cover the establishment of technical debt management across the entire company, but provides suggestions for this at the end.", "published": "2026-01-16T16:52:18Z", "updated": "2026-01-21T14:45:42Z", "authors": ["Marion Wiese"], "pdf_url": "https://arxiv.org/pdf/2601.11430v2"}
{"id": "http://arxiv.org/abs/2601.15041v1", "title": "HyperNet-Adaptation for Diffusion-Based Test Case Generation", "summary": "The increasing deployment of deep learning systems requires systematic evaluation of their reliability in real-world scenarios. Traditional gradient-based adversarial attacks introduce small perturbations that rarely correspond to realistic failures and mainly assess robustness rather than functional behavior. Generative test generation methods offer an alternative but are often limited to simple datasets or constrained input domains. Although diffusion models enable high-fidelity image synthesis, their computational cost and limited controllability restrict their applicability to large-scale testing. We present HyNeA, a generative testing method that enables direct and efficient control over diffusion-based generation. HyNeA provides dataset-free controllability through hypernetworks, allowing targeted manipulation of the generative process without relying on architecture-specific conditioning mechanisms or dataset-driven adaptations such as fine-tuning. HyNeA employs a distinct training strategy that supports instance-level tuning to identify failure-inducing test cases without requiring datasets that explicitly contain examples of similar failures. This approach enables the targeted generation of realistic failure cases at substantially lower computational cost than search-based methods. Experimental results show that HyNeA improves controllability and test diversity compared to existing generative test generators and generalizes to domains where failure-labeled training data is unavailable.", "published": "2026-01-21T14:45:15Z", "updated": "2026-01-21T14:45:15Z", "authors": ["Oliver Weißl", "Vincenzo Riccio", "Severin Kacianka", "Andrea Stocco"], "pdf_url": "https://arxiv.org/pdf/2601.15041v1"}
{"id": "http://arxiv.org/abs/2601.13996v2", "title": "Software Testing in the Quantum World", "summary": "Quantum computing offers significant speedups for simulating physical, chemical, and biological systems, and for optimization and machine learning. As quantum software grows in complexity, the classical simulation of quantum computers, which has long been essential for quality assurance, becomes infeasible. This shift requires new quality-assurance methods that operate directly on real quantum computers. This paper presents the key challenges in testing large-scale quantum software and offers software engineering perspectives for addressing them.", "published": "2026-01-20T14:09:24Z", "updated": "2026-01-21T13:21:36Z", "authors": ["Rui Abreu", "Shaukat Ali", "Paolo Arcaini", "Jose Campos", "Michael Felderer", "Claude Gravel", "Fuyuki Ishikawa", "Stefan Klikovits", "Andriy Miranskyy", "Anila Mjeda", "Mohammad Reza Mousavi", "Masaomi Yamaguchi", "Lei Zhang", "Jianjun Zhao"], "pdf_url": "https://arxiv.org/pdf/2601.13996v2"}
{"id": "http://arxiv.org/abs/2601.14936v1", "title": "LLM-Based Repair of C++ Implicit Data Loss Compiler Warnings: An Industrial Case Study", "summary": "This paper presents a method to automatically fix implicit data loss warnings in large C++ projects using Large Language Models (LLMs). Our approach uses the Language Server Protocol (LSP) to gather context, Tree-sitter to extract relevant code, and LLMs to make decisions and generate fixes. The method evaluates the necessity of range checks concerning performance implications and generates appropriate fixes. We tested this method in a large C++ project, resulting in a 92.73% acceptance rate of the fixes by human developers during the code review. Our LLM-generated fixes reduced the number of warning fix changes that introduced additional instructions due to range checks and exception handling by 39.09% compared to a baseline fix strategy. This result was 13.56% behind the optimal solutions created by human developers. These findings demonstrate that our LLM-based approach can reduce the manual effort to address compiler warnings while maintaining code quality and performance in a real-world scenario. Our automated approach shows promise for integration into existing development workflows, potentially improving code maintenance practices in complex C++ software projects.", "published": "2026-01-21T12:30:32Z", "updated": "2026-01-21T12:30:32Z", "authors": ["Chansong You", "Hyun Deok Choi", "Jingun Hong"], "pdf_url": "https://arxiv.org/pdf/2601.14936v1"}
{"id": "http://arxiv.org/abs/2512.07404v3", "title": "On LLMs' Internal Representation of Code Correctness", "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.", "published": "2025-12-08T10:38:03Z", "updated": "2026-01-21T12:24:23Z", "authors": ["Francisco Ribeiro", "Claudio Spiess", "Prem Devanbu", "Sarah Nadi"], "pdf_url": "https://arxiv.org/pdf/2512.07404v3"}
{"id": "http://arxiv.org/abs/2601.14912v1", "title": "AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems", "summary": "Alerts are critical for detecting anomalies in large-scale cloud systems, ensuring reliability and user experience. However, current systems generate overwhelming volumes of alerts, degrading operational efficiency due to ineffective alert life-cycle management. This paper details the efforts of Company-X to optimize alert life-cycle management, addressing alert fatigue in cloud systems. We propose AlertGuardian, a framework collaborating large language models (LLMs) and lightweight graph models to optimize the alert life-cycle through three phases: Alert Denoise uses graph learning model with virtual noise to filter noise, Alert Summary employs Retrieval Augmented Generation (RAG) with LLMs to create actionable summary, and Alert Rule Refinement leverages multi-agent iterative feedbacks to improve alert rule quality. Evaluated on four real-world datasets from Company-X's services, AlertGuardian significantly mitigates alert fatigue (94.8\\% alert reduction ratios) and accelerates fault diagnosis (90.5\\% diagnosis accuracy). Moreover, AlertGuardian improves 1,174 alert rules, with 375 accepted by SREs (32% acceptance rate). Finally, we share success stories and lessons learned about alert life-cycle management after the deployment of AlertGuardian in Company-X.", "published": "2026-01-21T11:51:59Z", "updated": "2026-01-21T11:51:59Z", "authors": ["Guangba Yu", "Genting Mai", "Rui Wang", "Ruipeng Li", "Pengfei Chen", "Long Pan", "Ruijie Xu"], "pdf_url": "https://arxiv.org/pdf/2601.14912v1"}
{"id": "http://arxiv.org/abs/2404.00971v3", "title": "Beyond Functional Correctness: Exploring Hallucinations in LLM-Generated Code", "summary": "The rise of Large Language Models (LLMs) has significantly advanced various applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misaligned with the real-world knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investigating the hallucination in the domain of Natural Language Generation (NLG), leaving a gap in comprehensively understanding the types, causes, and impacts of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations, as well as their causes and impacts. Our study established a comprehensive taxonomy of code hallucinations, encompassing 3 primary categories and 12 specific categories. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and benchmarks. Moreover, we perform an in-depth analysis on the causes and impacts of various hallucinations, aiming to provide valuable insights into hallucination mitigation. Finally, to enhance the correctness and reliability of LLM-generated code in a lightweight manner, we explore training-free hallucination mitigation approaches by prompt enhancing techniques. We believe our findings will shed light on future research about code hallucination evaluation and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future. The replication package is available at https://github.com/Lorien1128/code_hallucination", "published": "2024-04-01T07:31:45Z", "updated": "2026-01-21T11:08:06Z", "authors": ["Fang Liu", "Yang Liu", "Lin Shi", "Zhen Yang", "Li Zhang", "Xiaoli Lian", "Zhongqi Li", "Yuchi Ma"], "pdf_url": "https://arxiv.org/pdf/2404.00971v3"}
{"id": "http://arxiv.org/abs/2601.14865v1", "title": "Understanding Usefulness in Developer Explanations on Stack Overflow", "summary": "Explanations are essential in software engineering (SE) and requirements communication, helping stakeholders clarify ambiguities, justify design choices, and build shared understanding. Online Q&A forums such as Stack Overflow provide large-scale settings where such explanations are produced and evaluated, offering valuable insights into what makes them effective. While prior work has explored answer acceptance and voting behavior, little is known about which specific features make explanations genuinely useful. The relative influence of structural, contextual, and linguistic factors, such as content richness, timing, and sentiment, remains unclear. We analyzed 3,323 questions and 59,398 answers from Stack Overflow, combining text analysis and statistical modeling to examine how explanation attributes relate to perceived usefulness (normalized upvotes). Structural and contextual factors, especially explanation length, code inclusion, timing, and author reputation, show small to moderate positive effects. Sentiment polarity has negligible influence, suggesting that clarity and substance outweigh tone in technical communication. This study provides an empirical account of what drives perceived usefulness in developer explanations. It contributes methodological transparency through open data and replication materials, and conceptual insight by relating observed communication patterns to principles of requirements communication. The findings offer evidence-based implications for how developers and RE practitioners can craft clearer and more effective explanations, potentially supporting fairer communication in both open and organizational contexts. From an RE perspective, these determinants can be interpreted as practical signals for ambiguity reduction and rationale articulation in day-to-day requirements communication.", "published": "2026-01-21T10:50:43Z", "updated": "2026-01-21T10:50:43Z", "authors": ["Martin Obaidi", "Kushtrim Qengaj", "Hannah Deters", "Jakob Droste", "Marc Herrmann", "Kurt Schneider", "Jil Klünder"], "pdf_url": "https://arxiv.org/pdf/2601.14865v1"}
{"id": "http://arxiv.org/abs/2601.14861v1", "title": "Reclaiming Software Engineering as the Enabling Technology for the Digital Age", "summary": "Software engineering is the invisible infrastructure of the digital age. Every breakthrough in artificial intelligence, quantum computing, photonics, and cybersecurity relies on advances in software engineering, yet the field is too often treated as a supportive digital component rather than as a strategic, enabling discipline. In policy frameworks, including major European programmes, software appears primarily as a building block within other technologies, while the scientific discipline of software engineering remains largely absent. This position paper argues that the long-term sustainability, dependability, and sovereignty of digital technologies depend on investment in software engineering research. It is a call to reclaim the identity of software engineering.", "published": "2026-01-21T10:44:35Z", "updated": "2026-01-21T10:44:35Z", "authors": ["Tanja E. J. Vos", "Tijs van der Storm", "Alexander Serebrenik", "Lionel Briand", "Roberto Di Cosmo", "J. -M Bruel", "Benoît Combemale"], "pdf_url": "https://arxiv.org/pdf/2601.14861v1"}
{"id": "http://arxiv.org/abs/2601.14840v1", "title": "Implementing Knowledge Representation and Reasoning with Object Oriented Design", "summary": "This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation & Reasoning (KR&R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.", "published": "2026-01-21T10:14:29Z", "updated": "2026-01-21T10:14:29Z", "authors": ["Abdelrhman Bassiouny", "Tom Schierenbeck", "Sorin Arion", "Benjamin Alt", "Naren Vasantakumaar", "Giang Nguyen", "Michael Beetz"], "pdf_url": "https://arxiv.org/pdf/2601.14840v1"}
{"id": "http://arxiv.org/abs/2601.14800v1", "title": "FastFI: Enhancing API Call-Site Robustness in Microservice-Based Systems with Fault Injection", "summary": "Fault injection is a key technique for assessing software reliability, enabling proactive detection of system defects before they manifest in production. However, the increasing complexity of microservice architectures leads to exponential growth in the fault-injection space, rendering traditional random injection inefficient. Recent lineage-driven approaches mitigate this problem through heuristic pruning, but they face two limitations. First, combinatorial-fault discovery remains bottlenecked by general-purpose SAT solvers, which fail to exploit the monotone and low-overlap structure of derived CNF formulas and typically rely on a static upper bound on fault size. Second, existing techniques provide limited post-injection guidance beyond reporting detected faults. To address these challenges, we propose FastFI, a fault-injection-guided framework to enhance the robustness of API call sites in microservice-based systems. FastFI features a DFS-based solver with dynamic fault injection to discover all valid combinatorial faults, and it leverages fault-injection results to identify critical APIs whose call sites should be hardened for robustness. Experiments on four representative microservice benchmarks show that FastFI reduces end-to-end fault-injection time by an average of 76.12\\% compared to state-of-the-art baselines while maintaining acceptable resource overhead. Moreover, FastFI accurately identifies high-impact APIs and provides actionable guidance for call-site hardening.", "published": "2026-01-21T09:24:54Z", "updated": "2026-01-21T09:24:54Z", "authors": ["Yuzhen Tan", "Jian Wang", "Shuaiyu Xie", "Bing Li", "Yunqing Yong", "Neng Zhang", "Shaolin Tan"], "pdf_url": "https://arxiv.org/pdf/2601.14800v1"}
{"id": "http://arxiv.org/abs/2601.14743v1", "title": "ARISE -- Adaptive Refinement and Iterative Scenario Engineering", "summary": "The effectiveness of collision-free trajectory planners depends on the quality and diversity of training data, especially for rare scenarios. A widely used approach to improve dataset diversity involves generating realistic synthetic traffic scenarios. However, producing such scenarios remains difficult due to the precision required when scripting them manually or generating them in a single pass. Natural language offers a flexible way to describe scenarios, but existing text-to-simulation pipelines often rely on static snippet retrieval, limited grammar, single-pass decoding, or lack robust executability checks. Moreover, they depend heavily on constrained LLM prompting with minimal post-processing. To address these limitations, we introduce ARISE - Adaptive Refinement and Iterative Scenario Engineering, a multi-stage tool that converts natural language prompts into executable Scenic scripts through iterative LLM-guided refinement. After each generation, ARISE tests script executability in simulation software, feeding structured diagnostics back to the LLM until both syntactic and functional requirements are met. This process significantly reduces the need for manual intervention. Through extensive evaluation, ARISE outperforms the baseline in generating semantically accurate and executable traffic scenarios with greater reliability and robustness.", "published": "2026-01-21T07:57:24Z", "updated": "2026-01-21T07:57:24Z", "authors": ["Konstantin Poddubnyy", "Igor Vozniak", "Nils Lipp", "Ivan Burmistrov", "Davit Hovhannisyan", "Christian Mueller", "Philipp Slusallek"], "pdf_url": "https://arxiv.org/pdf/2601.14743v1"}
{"id": "http://arxiv.org/abs/2510.17795v2", "title": "What Makes AI Research Replicable? Executable Knowledge Graphs as Scientific Knowledge Representations", "summary": "Replicating AI research is a crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code, primarily due to insufficient background knowledge and the limitations of retrieval-augmented generation (RAG) methods, which fail to capture latent technical details hidden in referenced papers. Furthermore, previous approaches tend to overlook valuable implementation-level code signals and lack structured knowledge representations that support multi-granular retrieval and reuse. To overcome these challenges, we propose Executable Knowledge Graphs (xKG), a pluggable, paper-centric knowledge base that automatically integrates code snippets and technical insights extracted from scientific literature. When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench, demonstrating its effectiveness as a general and extensible solution for automated AI research replication. Code is available at https://github.com/zjunlp/xKG.", "published": "2025-10-20T17:53:23Z", "updated": "2026-01-21T07:42:57Z", "authors": ["Yujie Luo", "Zhuoyun Yu", "Xuehai Wang", "Yuqi Zhu", "Ningyu Zhang", "Lanning Wei", "Lun Du", "Da Zheng", "Huajun Chen"], "pdf_url": "https://arxiv.org/pdf/2510.17795v2"}
{"id": "http://arxiv.org/abs/2601.14731v1", "title": "ARFT-Transformer: Modeling Metric Dependencies for Cross-Project Aging-Related Bug Prediction", "summary": "Software systems that run for long periods often suffer from software aging, which is typically caused by Aging-Related Bugs (ARBs). To mitigate the risk of ARBs early in the development phase, ARB prediction has been introduced into software aging research. However, due to the difficulty of collecting ARBs, within-project ARB prediction faces the challenge of data scarcity, leading to the proposal of cross-project ARB prediction. This task faces two major challenges: 1) domain adaptation issue caused by distribution difference between source and target projects; and 2) severe class imbalance between ARB-prone and ARB-free samples. Although various methods have been proposed for cross-project ARB prediction, existing approaches treat the input metrics independently and often neglect the rich inter-metric dependencies, which can lead to overlapping information and misjudgment of metric importance, potentially affecting the model's performance. Moreover, they typically use cross-entropy as the loss function during training, which cannot distinguish the difficulty of sample classification. To overcome these limitations, we propose ARFT-Transformer, a transformer-based cross-project ARB prediction framework that introduces a metric-level multi-head attention mechanism to capture metric interactions and incorporates Focal Loss function to effectively handle class imbalance. Experiments conducted on three large-scale open-source projects demonstrate that ARFT-Transformer on average outperforms state-of-the-art cross-project ARB prediction methods in both single-source and multi-source cases, achieving up to a 29.54% and 19.92% improvement in Balance metric.", "published": "2026-01-21T07:41:55Z", "updated": "2026-01-21T07:41:55Z", "authors": ["Shuning Ge", "Fangyun Qin", "Xiaohui Wan", "Yang Liu", "Qian Dai", "Zheng Zheng"], "pdf_url": "https://arxiv.org/pdf/2601.14731v1"}
{"id": "http://arxiv.org/abs/2601.14617v1", "title": "UniCon: A Unified System for Efficient Robot Learning Transfers", "summary": "Deploying learning-based controllers across heterogeneous robots is challenging due to platform differences, inconsistent interfaces, and inefficient middleware. To address these issues, we present UniCon, a lightweight framework that standardizes states, control flow, and instrumentation across platforms. It decomposes workflows into execution graphs with reusable components, separating system states from control logic to enable plug-and-play deployment across various robot morphologies. Unlike traditional middleware, it prioritizes efficiency through batched, vectorized data flow, minimizing communication overhead and improving inference latency. This modular, data-oriented approach enables seamless sim-to-real transfer with minimal re-engineering. We demonstrate that UniCon reduces code redundancy when transferring workflows and achieves higher inference efficiency compared to ROS-based systems. Deployed on over 12 robot models from 7 manufacturers, it has been successfully integrated into ongoing research projects, proving its effectiveness in real-world scenarios.", "published": "2026-01-21T03:19:32Z", "updated": "2026-01-21T03:19:32Z", "authors": ["Yunfeng Lin", "Li Xu", "Yong Yu", "Jiangmiao Pang", "Weinan Zhang"], "pdf_url": "https://arxiv.org/pdf/2601.14617v1"}
{"id": "http://arxiv.org/abs/2509.25247v2", "title": "Protocode: Prototype-Driven Interpretability for Code Generation in LLMs", "summary": "Since the introduction of Large Language Models (LLMs), they have been widely adopted for various tasks such as text summarization, question answering, speech-to-text translation, and more. In recent times, the use of LLMs for code generation has gained significant attention, with tools such as Cursor and Windsurf demonstrating the ability to analyze massive code repositories and recommend relevant changes. Big tech companies have also acknowledged the growing reliance on LLMs for code generation within their codebases. Although these advances significantly improve developer productivity, increasing reliance on automated code generation can proportionally increase the risk of suboptimal solutions and insecure code. Our work focuses on automatically sampling In-Context Learning (ICL) demonstrations which can improve model performance and enhance the interpretability of the generated code. Using AST-based analysis on outputs from the MBPP test set, we identify regions of code most influenced by the chosen demonstrations. In our experiments, we show that high-quality ICL demonstrations not only make outputs easier to interpret but also yield a positive performance improvement on the pass@10 metric. Conversely, poorly chosen ICL demonstrations affected the LLM performance on the pass@10 metric negatively compared to the base model. Overall, our approach highlights the importance of efficient sampling strategies for ICL, which can affect the performance of the model on any given task.", "published": "2025-09-27T00:32:45Z", "updated": "2026-01-21T03:18:16Z", "authors": ["Krishna Vamshi Bodla", "Haizhao Yang"], "pdf_url": "https://arxiv.org/pdf/2509.25247v2"}
{"id": "http://arxiv.org/abs/2601.14598v1", "title": "HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation", "summary": "Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \\textsc{HELIOS}, a framework that reframes LLM-based decompilation as a structured reasoning task. \\textsc{HELIOS} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build.\n  On HumanEval-Decompile for \\texttt{x86\\_64}, \\textsc{HELIOS} raises average object file compilability from 45.0\\% to 85.2\\% for Gemini~2.0 and from 71.4\\% to 89.6\\% for GPT-4.1~Mini. With compiler feedback, compilability exceeds 94\\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \\textsc{HELIOS} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \\textsc{HELIOS} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.", "published": "2026-01-21T02:37:33Z", "updated": "2026-01-21T02:37:33Z", "authors": ["Yonatan Gizachew Achamyeleh", "Harsh Thomare", "Mohammad Abdullah Al Faruque"], "pdf_url": "https://arxiv.org/pdf/2601.14598v1"}
