{"id": "http://arxiv.org/abs/2510.22815v2", "title": "On the Freshness of Pinned Dependencies in Maven", "summary": "Library dependencies in software ecosystems play a crucial role in the development of software. As newer releases of these libraries are published, developers may opt to pin their dependencies to a particular version. While pinning may have benefits in ensuring reproducible builds and avoiding breaking changes, it bears larger risks in using outdated dependencies that may contain bugs and security vulnerabilities. To understand the frequency and consequences of dependency pinning, we first define the concepts of stale and fresh pins, which are distinguished based on how outdated the dependency is relative to the release date of the project. We conduct an empirical study to show that over 60% of consumers of popular Maven libraries contain stale pins to their dependencies, with some outdated versions over a year old. These pinned versions often miss out on security fixes; we find that 10% of all dependency upgrades in our dataset to the latest minor or patch version would reduce security vulnerabilities.\n  We prototype an approach called Pin-Freshener that can encourage developers to freshen their pins by leveraging the insight that crowdsourced tests of peer projects can provide additional signal for the safety of an upgrade. Running Pin-Freshener on dependency upgrades shows that just 1-5 additional test suites can provide 35-100% more coverage of a dependency, compared to that of a single consumer test suite. Our evaluation on real-world pins to the top 500 popular libraries in Maven shows that Pin-Freshener can provide an additional signal of at least 5 passing crowdsourced test suites to over 3,000 consumers to safely perform an upgrade that reduces security vulnerabilities. Pin-Freshener can provide practical confidence to developers by offering additional signal beyond their own test suites, representing an improvement over current practices.", "published": "2025-10-26T20:02:49Z", "updated": "2025-12-19T17:54:40Z", "authors": ["Vasudev Vikram", "Yuvraj Agarwal", "Rohan Padhye"], "pdf_url": "https://arxiv.org/pdf/2510.22815v2"}
{"id": "http://arxiv.org/abs/2511.15817v4", "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code", "summary": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.\n  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.", "published": "2025-11-19T19:18:28Z", "updated": "2025-12-19T17:21:22Z", "authors": ["Alejandro Velasco", "Daniel Rodriguez-Cardenas", "Dipin Khati", "David N. Palacio", "Luftar Rahman Alif", "Denys Poshyvanyk"], "pdf_url": "https://arxiv.org/pdf/2511.15817v4"}
{"id": "http://arxiv.org/abs/2512.17814v1", "title": "LLM-based Behaviour Driven Development for Hardware Design", "summary": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.\n  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.", "published": "2025-12-19T17:19:08Z", "updated": "2025-12-19T17:19:08Z", "authors": ["Rolf Drechsler", "Qian Liu"], "pdf_url": "https://arxiv.org/pdf/2512.17814v1"}
{"id": "http://arxiv.org/abs/2512.17710v1", "title": "A Practical Solution to Systematically Monitor Inconsistencies in SBOM-based Vulnerability Scanners", "summary": "Software Bill of Materials (SBOM) provides new opportunities for automated vulnerability identification in software products. While the industry is adopting SBOM-based Vulnerability Scanning (SVS) to identify vulnerabilities, we increasingly observe inconsistencies and unexpected behavior, that result in false negatives and silent failures. In this work, we present the background necessary to understand the underlying complexity of SVS and introduce SVS-TEST, a method and tool to analyze the capability, maturity, and failure conditions of SVS-tools in real-world scenarios. We showcase the utility of SVS-TEST in a case study evaluating seven real-world SVS-tools using 16 precisely crafted SBOMs and their respective ground truth. Our results unveil significant differences in the reliability and error handling of SVS-tools; multiple SVS-tools silently fail on valid input SBOMs, creating a false sense of security. We conclude our work by highlighting implications for researchers and practitioners, including how organizations and developers of SVS-tools can utilize SVS-TEST to monitor SVS capability and maturity. All results and research artifacts are made publicly available and all findings were disclosed to the SVS-tool developers ahead of time.", "published": "2025-12-19T15:42:22Z", "updated": "2025-12-19T15:42:22Z", "authors": ["Martin Rosso", "Muhammad Asad Jahangir Jaffar", "Alessandro Brighente", "Mauro Conti"], "pdf_url": "https://arxiv.org/pdf/2512.17710v1"}
{"id": "http://arxiv.org/abs/2507.22659v2", "title": "A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models", "summary": "The increasing adoption of Large Language Models (LLMs) in software engineering has sparked interest in their use for software vulnerability detection. However, the rapid development of this field has resulted in a fragmented research landscape, with diverse studies that are difficult to compare due to differences in, e.g., system designs and dataset usage. This fragmentation makes it difficult to obtain a clear overview of the state-of-the-art or compare and categorize studies meaningfully. In this work, we present a comprehensive systematic literature review (SLR) of LLM-based software vulnerability detection. We analyze 263 studies published between January 2020 and November 2025, categorizing them by task formulation, input representation, system architecture, and techniques. Further, we analyze the datasets used, including their characteristics, vulnerability coverage, and diversity. We present a fine-grained taxonomy of vulnerability detection approaches, identify key limitations, and outline actionable future research opportunities. By providing a structured overview of the field, this review improves transparency and serves as a practical guide for researchers and practitioners aiming to conduct more comparable and reproducible research. We publicly release all artifacts and maintain a living repository of LLM-based software vulnerability detection studies at https://github.com/hs-esslingen-it-security/Awesome-LLM4SVD.", "published": "2025-07-30T13:17:16Z", "updated": "2025-12-19T15:41:06Z", "authors": ["Sabrina Kaniewski", "Fabian Schmidt", "Markus Enzweiler", "Michael Menth", "Tobias Heer"], "pdf_url": "https://arxiv.org/pdf/2507.22659v2"}
{"id": "http://arxiv.org/abs/2506.11023v2", "title": "OntoGSN: An Ontology-Based Framework for Semantic Management and Extension of Assurance Cases", "summary": "Assurance cases (ACs) are a common artifact for building and maintaining confidence in system properties such as safety or robustness. Constructing an AC can be challenging, although existing tools provide support in static, document-centric applications and methods for dynamic contexts (e.g., autonomous driving) are emerging. Unfortunately, managing ACs remains a challenge, since maintaining the embedded knowledge in the face of changes requires substantial effort, in the process deterring developers - or worse, producing poorly managed cases that instill false confidence. To address this, we present OntoGSN: an ontology and supporting middleware for managing ACs in the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge representation and a queryable graph that can be automatically populated, evaluated, and updated. Our contributions include: a 1:1 formalization of the GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology and parser for integration with a widely used AC tool; a repository and documentation of design decisions for OntoGSN maintenance; a SPARQL query library with automation patterns; and a prototypical interface. The ontology strictly adheres to the standard's text and has been evaluated according to FAIR principles, the OOPS framework, competency questions, and community feedback. The development of other middleware elements is guided by the community needs and subject to ongoing evaluations. To demonstrate the utility of our contributions, we illustrate dynamic AC management in an example involving assurance of adversarial robustness in large language models.", "published": "2025-05-20T08:15:16Z", "updated": "2025-12-19T15:34:46Z", "authors": ["Tomas Bueno Momcilovic", "Barbara Gallina", "Ingmar Kessler", "Jule Hendricks", "Dian Balta"], "pdf_url": "https://arxiv.org/pdf/2506.11023v2"}
{"id": "http://arxiv.org/abs/2512.17540v1", "title": "SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review", "summary": "Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.", "published": "2025-12-19T13:02:22Z", "updated": "2025-12-19T13:02:22Z", "authors": ["Kai Wang", "Bingcheng Mao", "Shuai Jia", "Yujie Ding", "Dongming Han", "Tianyi Ma", "Bin Cao"], "pdf_url": "https://arxiv.org/pdf/2512.17540v1"}
{"id": "http://arxiv.org/abs/2512.17517v1", "title": "PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology", "summary": "We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL", "published": "2025-12-19T12:35:57Z", "updated": "2025-12-19T12:35:57Z", "authors": ["Siemen Brussee", "Pieter A. Valkema", "Jurre A. J. Weijer", "Thom Doeleman", "Anne M. R. Schrader", "Jesper Kers"], "pdf_url": "https://arxiv.org/pdf/2512.17517v1"}
{"id": "http://arxiv.org/abs/2508.12436v2", "title": "Feature Request Analysis and Processing: Tasks, Techniques, and Trends", "summary": "Feature requests are proposed by users to request new features or enhancements of existing features of software products, which represent users' wishes and demands. Satisfying users' demands can benefit the product from both competitiveness and user satisfaction. Feature requests have seen a rise in interest in the past few years and the amount of research has been growing. However, the diversity in the research topics suggests the need for their collective analysis to identify the challenges and opportunities so as to promote new advances in the future. In this work, following a defined process and a search protocol, we provide a systematic overview of the research area by searching and categorizing relevant studies. We select and analyze 131 primary studies using descriptive statistics and qualitative analysis methods. We classify the studies into different topics and group them from the perspective of requirements engineering activities. We investigate open tools as well as datasets for future research. In addition, we identify several key challenges and opportunities, such as: (1) ensuring the quality of feature requests, (2) improving their specification and validation, and (3) developing high-quality benchmarks for large language model-driven tasks.", "published": "2025-08-17T17:09:16Z", "updated": "2025-12-19T12:24:06Z", "authors": ["Feifei Niu", "Chuanyi Li", "Haosheng Zuo", "Jionghan Wu", "Xin Xia"], "pdf_url": "https://arxiv.org/pdf/2508.12436v2"}
{"id": "http://arxiv.org/abs/2512.17500v1", "title": "Why Is My Transaction Risky? Understanding Smart Contract Semantics and Interactions in the NFT Ecosystem", "summary": "The NFT ecosystem represents an interconnected, decentralized environment that encompasses the creation, distribution, and trading of Non-Fungible Tokens (NFTs), where key actors, such as marketplaces, sellers, and buyers, utilize smart contracts to facilitate secure, transparent, and trustless transactions. Scam tokens are deliberately created to mislead users and facilitate financial exploitation, posing significant risks in the NFT ecosystem. Prior work has explored the NFT ecosystem from various perspectives, including security challenges, actor behaviors, and risks from scams and wash trading, leaving a gap in understanding the semantics and interactions of smart contracts during transactions, and how the risks associated with scam tokens manifest in relation to the semantics and interactions of contracts. To bridge this gap, we conducted a large-scale empirical study on smart contract semantics and interactions in the NFT ecosystem, using a curated dataset of nearly 100 million transactions across 20 million blocks on Ethereum. We observe a limited semantic diversity among smart contracts in the NFT ecosystem, dominated by proxy, token, and DeFi contracts. Marketplace and proxy registry contracts are the most frequently involved in smart contract interactions during transactions, engaging with a broad spectrum of contracts in the ecosystem. Token contracts exhibit bytecode-level diversity, whereas scam tokens exhibit bytecode convergence. Certain interaction patterns between smart contracts are common to both risky and non-risky transactions, while others are predominantly associated with risky transactions. Based on our findings, we provide recommendations to mitigate risks in the blockchain ecosystem, and outline future research directions.", "published": "2025-12-19T12:09:29Z", "updated": "2025-12-19T12:09:29Z", "authors": ["Yujing Chen", "Xuanming Liu", "Zhiyuan Wan", "Zuobin Wang", "David Lo", "Difan Xie", "Xiaohu Yang"], "pdf_url": "https://arxiv.org/pdf/2512.17500v1"}
{"id": "http://arxiv.org/abs/2510.26538v2", "title": "Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models: A Reflection", "summary": "Software Engineering (SE) research involving the use of Large Language Models (LLMs) has introduced several new challenges related to rigour in benchmarking, contamination, replicability, and sustainability. In this paper, we invite the research community to reflect on how these challenges are addressed in SE. Our results provide a structured overview of current LLM-based SE research at ICSE, highlighting both encouraging practices and persistent shortcomings. We conclude with recommendations to strengthen benchmarking rigour, improve replicability, and address the financial and environmental costs of LLM-based SE.", "published": "2025-10-30T14:27:51Z", "updated": "2025-12-19T11:30:54Z", "authors": ["David Williams", "Max Hort", "Maria Kechagia", "Aldeida Aleti", "Justyna Petke", "Federica Sarro"], "pdf_url": "https://arxiv.org/pdf/2510.26538v2"}
{"id": "http://arxiv.org/abs/2512.17460v1", "title": "When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction", "summary": "Software Defect Prediction (SDP) models are central to proactive software quality assurance, yet their effectiveness is often constrained by the quality of available datasets. Prior research has typically examined single issues such as class imbalance or feature irrelevance in isolation, overlooking that real-world data problems frequently co-occur and interact. This study presents, to our knowledge, the first large-scale empirical analysis in SDP that simultaneously examines five co-occurring data quality issues (class imbalance, class overlap, irrelevant features, attribute noise, and outliers) across 374 datasets and five classifiers. We employ Explainable Boosting Machines together with stratified interaction analysis to quantify both direct and conditional effects under default hyperparameter settings, reflecting practical baseline usage.\n  Our results show that co-occurrence is nearly universal: even the least frequent issue (attribute noise) appears alongside others in more than 93% of datasets. Irrelevant features and imbalance are nearly ubiquitous, while class overlap is the most consistently harmful issue. We identify stable tipping points around 0.20 for class overlap, 0.65-0.70 for imbalance, and 0.94 for irrelevance, beyond which most models begin to degrade. We also uncover counterintuitive patterns, such as outliers improving performance when irrelevant features are low, underscoring the importance of context-aware evaluation. Finally, we expose a performance-robustness trade-off: no single learner dominates under all conditions.\n  By jointly analyzing prevalence, co-occurrence, thresholds, and conditional effects, our study directly addresses a persistent gap in SDP research. Hence, moving beyond isolated analyses to provide a holistic, data-aware understanding of how quality issues shape model performance in real-world settings.", "published": "2025-12-19T11:21:12Z", "updated": "2025-12-19T11:21:12Z", "authors": ["Emmanuel Charleson Dapaah", "Jens Grabowski"], "pdf_url": "https://arxiv.org/pdf/2512.17460v1"}
{"id": "http://arxiv.org/abs/2512.17455v1", "title": "An Investigation on How AI-Generated Responses Affect SoftwareEngineering Surveys", "summary": "Survey research is a fundamental empirical method in software engineering, enabling the systematic collection of data on professional practices, perceptions, and experiences. However, recent advances in large language models (LLMs) have introduced new risks to survey integrity, as participants can use generative tools to fabricate or manipulate their responses. This study explores how LLMs are being misused in software engineering surveys and investigates the methodological implications of such behavior for data authenticity, validity, and research integrity. We collected data from two survey deployments conducted in 2025 through the Prolific platform and analyzed the content of participants' answers to identify irregular or falsified responses. A subset of responses suspected of being AI generated was examined through qualitative pattern inspection, narrative characterization, and automated detection using the Scribbr AI Detector. The analysis revealed recurring structural patterns in 49 survey responses indicating synthetic authorship, including repetitive sequencing, uniform phrasing, and superficial personalization. These false narratives mimicked coherent reasoning while concealing fabricated content, undermining construct, internal, and external validity. Our study identifies data authenticity as an emerging dimension of validity in software engineering surveys. We emphasize that reliable evidence now requires combining automated and interpretive verification procedures, transparent reporting, and community standards to detect and prevent AI generated responses, thereby protecting the credibility of surveys in software engineering.", "published": "2025-12-19T11:17:05Z", "updated": "2025-12-19T11:17:05Z", "authors": ["Ronnie de Souza Santos", "Italo Santos", "Maria Teresa Baldassarre", "Cleyton Magalhaes", "Mairieli Wessel"], "pdf_url": "https://arxiv.org/pdf/2512.17455v1"}
{"id": "http://arxiv.org/abs/2512.17419v1", "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories", "summary": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.", "published": "2025-12-19T10:16:51Z", "updated": "2025-12-19T10:16:51Z", "authors": ["Lilin Wang", "Lucas Ramalho", "Alan Celestino", "Phuc Anthony Pham", "Yu Liu", "Umang Kumar Sinha", "Andres Portillo", "Onassis Osunwa", "Gabriel Maduekwe"], "pdf_url": "https://arxiv.org/pdf/2512.17419v1"}
{"id": "http://arxiv.org/abs/2504.17287v3", "title": "RBCTest: Leveraging LLMs to Mine and Verify Oracles of API Response Bodies for RESTful API Testing", "summary": "In API testing, deriving logical constraints on API response bodies to be used as oracles is crucial for generating test cases and performing automated testing of RESTful APIs. However, existing approaches are restricted to dynamic analysis, in which oracles are extracted via the execution of APIs as part of the system under test. In this paper, we propose a complementary LLM-based static approach in which constraints for API response bodies are mined from API specifications. We leverage large language models (LLMs) to comprehend API specifications, mine constraints for response bodies, and generate test cases. To reduce LLM hallucination, we apply an Observation-Confirmation (OC) scheme that uses initial prompts to contextualize constraints, allowing subsequent prompts to more accurately confirm their presence. Our empirical results show that RBCTest with OC prompting achieves high precision in constraint mining, with averages ranging from 85.1% to 93.6%. It also performs well in generating test cases from mined constraints, with precision ranging from 86.4% to 91.7%. We further use test cases generated by RBCTest to detect 46 mismatches between API specifications and actual response data across 19 real-world APIs. Four of these mismatches were reported in developers' forums.", "published": "2025-04-24T06:28:18Z", "updated": "2025-12-19T10:00:40Z", "authors": ["Hieu Huynh", "Tri Le", "Tu Nguyen", "Viet Nguyen", "Vu Nguyen", "Tien N. Nguyen"], "pdf_url": "https://arxiv.org/pdf/2504.17287v3"}
{"id": "http://arxiv.org/abs/2512.17387v1", "title": "CIFE: Code Instruction-Following Evaluation", "summary": "Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.", "published": "2025-12-19T09:43:20Z", "updated": "2025-12-19T09:43:20Z", "authors": ["Sravani Gunnu", "Shanmukha Guttula", "Hima Patel"], "pdf_url": "https://arxiv.org/pdf/2512.17387v1"}
{"id": "http://arxiv.org/abs/2512.17371v1", "title": "GraphCue for SDN Configuration Code Synthesis", "summary": "We present GraphCue, a topology-grounded retrieval and agent-in-the-loop framework for automated SDN configuration. Each case is abstracted into a JSON graph and embedded using a lightweight three-layer GCN trained with contrastive learning. The nearest validated reference is injected into a structured prompt that constrains code generation, while a verifier closes the loop by executing the candidate configuration and feeding failures back to the agent. On 628 validation cases, GraphCue achieves an 88.2 percent pass rate within 20 iterations and completes 95 percent of verification loops within 9 seconds. Ablation studies without retrieval or structured prompting perform substantially worse, indicating that topology-aware retrieval and constraint-based conditioning are key drivers of performance.", "published": "2025-12-19T09:13:51Z", "updated": "2025-12-19T09:13:51Z", "authors": ["Haomin Qi", "Fengfei Yu", "Chengbo Huang"], "pdf_url": "https://arxiv.org/pdf/2512.17371v1"}
{"id": "http://arxiv.org/abs/2512.16529v2", "title": "ParamExplorer: A framework for exploring parameters in generative art", "summary": "Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.", "published": "2025-12-18T13:37:50Z", "updated": "2025-12-19T09:09:13Z", "authors": ["Julien Gachadoat", "Guillaume Lagarde"], "pdf_url": "https://arxiv.org/pdf/2512.16529v2"}
{"id": "http://arxiv.org/abs/2512.17363v1", "title": "What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice", "summary": "Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.", "published": "2025-12-19T09:02:58Z", "updated": "2025-12-19T09:02:58Z", "authors": ["Yuqing Niu", "Jieke Shi", "Ruidong Han", "Ye Liu", "Chengyan Ma", "Yunbo Lyu", "David Lo"], "pdf_url": "https://arxiv.org/pdf/2512.17363v1"}
{"id": "http://arxiv.org/abs/2512.17334v1", "title": "Bridging Natural Language and Formal Specification--Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs", "summary": "Automating the translation of natural language (NL) software requirements into formal specifications remains a critical challenge in scaling formal verification practices to industrial settings, particularly in safety-critical domains. Existing approaches, both rule-based and learning-based, face significant limitations. While large language models (LLMs) like GPT-4o demonstrate proficiency in semantic extraction, they still encounter difficulties in addressing the complexity, ambiguity, and logical depth of real-world industrial requirements. In this paper, we propose Req2LTL, a modular framework that bridges NL and Linear Temporal Logic (LTL) through a hierarchical intermediate representation called OnionL. Req2LTL leverages LLMs for semantic decomposition and combines them with deterministic rule-based synthesis to ensure both syntactic validity and semantic fidelity. Our comprehensive evaluation demonstrates that Req2LTL achieves 88.4% semantic accuracy and 100% syntactic correctness on real-world aerospace requirements, significantly outperforming existing methods.", "published": "2025-12-19T08:25:54Z", "updated": "2025-12-19T08:25:54Z", "authors": ["Zhi Ma", "Cheng Wen", "Zhexin Su", "Xiao Liang", "Cong Tian", "Shengchao Qin", "Mengfei Yang"], "pdf_url": "https://arxiv.org/pdf/2512.17334v1"}
{"id": "http://arxiv.org/abs/2512.16070v2", "title": "LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling", "summary": "The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.", "published": "2025-12-18T01:35:30Z", "updated": "2025-12-19T08:24:20Z", "authors": ["Xin Wang", "Zhenhao Li", "Zishuo Ding"], "pdf_url": "https://arxiv.org/pdf/2512.16070v2"}
{"id": "http://arxiv.org/abs/2512.14806v3", "title": "Let the Barbarians In: How AI Can Accelerate Systems Performance Research", "summary": "Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight.\n  Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.", "published": "2025-12-16T18:51:23Z", "updated": "2025-12-19T07:14:17Z", "authors": ["Audrey Cheng", "Shu Liu", "Melissa Pan", "Zhifei Li", "Shubham Agarwal", "Mert Cemri", "Bowen Wang", "Alexander Krentsel", "Tian Xia", "Jongseok Park", "Shuo Yang", "Jeff Chen", "Lakshya Agrawal", "Ashwin Naren", "Shulu Li", "Ruiying Ma", "Aditya Desai", "Jiarong Xing", "Koushik Sen", "Matei Zaharia", "Ion Stoica"], "pdf_url": "https://arxiv.org/pdf/2512.14806v3"}
{"id": "http://arxiv.org/abs/2512.17280v1", "title": "Sensor Management System (SMS): Open-source software for FAIR sensor metadata management in Earth system sciences", "summary": "Deriving reliable conclusions and insights from environmental observational data urgently requires the enrichment with consistent and comprehensive metadata, including time-resolved context such as changing deployments, configurations, and maintenance actions. We have therefore developed the Sensor Management System (SMS), which provides a user-friendly and feature-rich platform for modeling even the most complex sensor systems and managing all sensor-related information across their life cycle. Each entity is described via well-defined terms like Devices, Platforms and Configurations, as well as Sites that are further enhanced with attributes for, e.g., instrument manufacturers, contact information or measured quantities and complemented by a continuous history of system-related actions. By further linking the SMS to sub-sequent systems and services like PID-registration or controlled vocabularies and establishing a community of end-users, the SMS provides the central element of a digital ecosystem, that fosters a more consistent, sustainable and FAIR provision of sensor-related metadata.", "published": "2025-12-19T06:55:44Z", "updated": "2025-12-19T06:55:44Z", "authors": ["Christof Lorenza", "Nils Brinckmann", "Jan Bumberger", "Marc Hanisch", "Tobias Kuhnert", "Ulrich Loup", "Rubankumar Moorthy", "Florian Obsersteiner", "David Schäfer", "Thomas Schnicke"], "pdf_url": "https://arxiv.org/pdf/2512.17280v1"}
{"id": "http://arxiv.org/abs/2405.04994v2", "title": "SPVR: syntax-to-prompt vulnerability repair based on large language models", "summary": "Purpose: In the field of vulnerability repair, previous research has leveraged pretrained models and LLM-based prompt engineering, among which LLM-based approaches show better generalizability and achieve the best performance. However, the LLM-based approaches generally regard vulnerability repair as a sequence-to-sequence task, and do not explicitly capture the syntax patterns for different vulnerability types, leading to limited accuracy. We aim to create a method that ensures the specificity of prompts targeting vulnerable code while also leveraging the generative capabilities of Large Language Models. Methods: We propose SPVR (Syntax-to-Prompt Vulnerability Repair), a novel framework that collects information from syntax trees, and generates corresponding prompts. Our method consists of three steps: rule design, prompt generation, and patch generation. In the rule design step, our method parses code patches and designs rules to extract relevant contextual information. These rules aid in identifying vulnerability-related issues. In the prompt generation step, our method extracts information from vulnerable code with pre-defined rules, automatically converting them into prompts. We also incorporate the description of CWE (Common Weakness Enumeration) as known information into the prompts. Finally, in the patch generation step, this prompt will serve as input to any conversational LLM to obtain code patches. Results: Extensive experiments validate that our method achieves excellent results in assisting LLMs to fix vulnerabilities accurately. We utilize multiple Large Language Models to validate the effectiveness of our work, repairing 143 of 547 vulnerable code using ChatGPT-4. We conducted a comparison of our approach against several existing vulnerability repair approaches (including fine-tuning-based and prompt-based), across multiple metrics.", "published": "2024-05-08T11:58:55Z", "updated": "2025-12-19T01:26:30Z", "authors": ["Ruoke Wang", "Zongjie Li", "Cuiyun Gao", "Chaozheng Wang", "Yang Xiao", "Xuan Wang"], "pdf_url": "https://arxiv.org/pdf/2405.04994v2"}
{"id": "http://arxiv.org/abs/2504.20196v2", "title": "Understanding and supporting how developers prompt for LLM-powered code editing in practice", "summary": "Large Language Models (LLMs) are rapidly transforming software engineering, with coding assistants embedded in an IDE becoming increasingly prevalent. While research has focused on improving the tools and understanding developer perceptions, a critical gap exists in understanding how developers actually use these tools in their daily workflows, and, crucially, where they struggle. This paper addresses part of this gap through a multi-phased investigation of developer interactions with an LLM-powered code editing feature, Transform Code, in an IDE widely used at Google. First, we analyze telemetry logs of the feature usage, revealing that frequent re-prompting can be an indicator of developer struggles with using Transform Code. Second, we conduct a qualitative analysis of unsatisfactory requests, identifying five key categories of information often missing from developer prompts. Finally, based on these findings, we propose and evaluate a tool, AutoPrompter, for automatically improving prompts by inferring missing information from the surrounding code context, leading to a 27% improvement in edit correctness on our test set.", "published": "2025-04-28T18:59:28Z", "updated": "2025-12-19T00:21:29Z", "authors": ["Daye Nam", "Ahmed Omran", "Ambar Murillo", "Saksham Thakur", "Abner Araujo", "Marcel Blistein", "Alexander Frömmgen", "Vincent Hellendoorn", "Satish Chandra"], "pdf_url": "https://arxiv.org/pdf/2504.20196v2"}
