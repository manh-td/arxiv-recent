{"id": "http://arxiv.org/abs/2602.03470v1", "title": "Reading Between the Code Lines: On the Use of Self-Admitted Technical Debt for Security Analysis", "summary": "Static Analysis Tools (SATs) are central to security engineering activities, as they enable early identification of code weaknesses without requiring execution. However, their effectiveness is often limited by high false-positive rates and incomplete coverage of vulnerability classes. At the same time, developers frequently document security-related shortcuts and compromises as Self-Admitted Technical Debt (SATD) in software artifacts, such as code comments. While prior work has recognized SATD as a rich source of security information, it remains unclear whether -and in what ways- it is utilized during SAT-aided security analysis. OBJECTIVE: This work investigates the extent to which security-related SATD complements the output produced by SATs and helps bridge some of their well-known limitations. METHOD: We followed a mixed-methods approach consisting of (i) the analysis of a SATD-annotated vulnerability dataset using three state-of-the-art SATs and (ii) an online survey with 72 security practitioners. RESULTS: The combined use of all SATs flagged 114 of the 135 security-related SATD instances, spanning 24 distinct Common Weakness Enumeration (CWE) identifiers. A manual mapping of the SATD comments revealed 33 unique CWE types, 6 of which correspond to categories that SATs commonly overlook or struggle to detect (e.g., race conditions). Survey responses further suggest that developers frequently pair SAT outputs with SATD insights to better understand the impact and root causes of security weaknesses and to identify suitable fixes. IMPLICATIONS: Our findings show that such SATD-encoded information can be a meaningful complement to SAT-driven security analysis, while helping to overcome some of SATs' practical shortcomings.", "published": "2026-02-03T12:43:16Z", "updated": "2026-02-03T12:43:16Z", "authors": ["Nicolás E. Díaz Ferreyra", "Moritz Mock", "Max Kretschmann", "Barbara Russo", "Mojtaba Shahin", "Mansooreh Zahedi", "Riccardo Scandariato"], "pdf_url": "https://arxiv.org/pdf/2602.03470v1"}
{"id": "http://arxiv.org/abs/2602.03462v1", "title": "RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes", "summary": "Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .", "published": "2026-02-03T12:35:09Z", "updated": "2026-02-03T12:35:09Z", "authors": ["Ruwei Pan", "Yakun Zhang", "Qingyuan Liang", "Yueheng Zhu", "Chao Liu", "Lu Zhang", "Hongyu Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.03462v1"}
{"id": "http://arxiv.org/abs/2510.26275v2", "title": "A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI", "summary": "Generative AI (GenAI) is rapidly transforming software engineering (SE) practices, influencing how SE processes are executed, as well as how software systems are developed, operated, and evolved. This paper applies design science research to build a roadmap for GenAI-augmented SE. The process consists of three cycles that incrementally integrate multiple sources of evidence, including collaborative discussions from the FSE 2025 \"Software Engineering 2030\" workshop, rapid literature reviews, and external feedback sessions involving peers. McLuhan's tetrads were used as a conceptual instrument to systematically capture the transforming effects of GenAI on SE processes and software products.The resulting roadmap identifies four fundamental forms of GenAI augmentation in SE and systematically characterizes their related research challenges and opportunities. These insights are then consolidated into a set of future research directions. By grounding the roadmap in a rigorous multi-cycle process and cross-validating it among independent author teams and peers, the study provides a transparent and reproducible foundation for analyzing how GenAI affects SE processes, methods and tools, and for framing future research within this rapidly evolving area. Based on these findings, the article finally makes ten predictions for SE in the year 2030.", "published": "2025-10-30T08:59:01Z", "updated": "2026-02-03T12:22:58Z", "authors": ["Domenico Amalfitano", "Andreas Metzger", "Marco Autili", "Tommaso Fulcini", "Tobias Hey", "Jan Keim", "Patrizio Pelliccione", "Vincenzo Scotti", "Anne Koziolek", "Raffaela Mirandola", "Andreas Vogelsang"], "pdf_url": "https://arxiv.org/pdf/2510.26275v2"}
{"id": "http://arxiv.org/abs/2508.07468v3", "title": "CP-Agent: Agentic Constraint Programming", "summary": "The translation of natural language to formal constraint models requires expertise in the problem domain and modeling frameworks. To explore the effectiveness of agentic workflows, we propose CP-Agent, a Python coding agent that uses the ReAct framework with a persistent IPython kernel. We provide the relevant domain knowledge as a project prompt of under 50 lines. The algorithm works by iteratively executing code, observing the solver's feedback, and refining constraint models based on execution results.\n  We evaluate CP-Agent on 101 constraint programming problems from CP-Bench. We made minor changes to the benchmark to address systematic ambiguities in the problem specifications and errors in the ground-truth models. On the clarified benchmark, CP-Agent achieves perfect accuracy on all 101 problems. Our experiments show that minimal guidance outperforms detailed procedural scaffolding. Our experiments also show that explicit task management tools can have both positive and negative effects on focused modeling tasks.", "published": "2025-08-10T19:59:01Z", "updated": "2026-02-03T12:13:41Z", "authors": ["Stefan Szeider"], "pdf_url": "https://arxiv.org/pdf/2508.07468v3"}
{"id": "http://arxiv.org/abs/2602.03419v1", "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments", "summary": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World", "published": "2026-02-03T11:44:39Z", "updated": "2026-02-03T11:44:39Z", "authors": ["Shuang Sun", "Huatong Song", "Lisheng Huang", "Jinhao Jiang", "Ran Le", "Zhihao Lv", "Zongchao Chen", "Yiwen Hu", "Wenyang Luo", "Wayne Xin Zhao", "Yang Song", "Hongteng Xu", "Tao Zhang", "Ji-Rong Wen"], "pdf_url": "https://arxiv.org/pdf/2602.03419v1"}
{"id": "http://arxiv.org/abs/2602.03411v1", "title": "SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training", "summary": "In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.", "published": "2026-02-03T11:38:48Z", "updated": "2026-02-03T11:38:48Z", "authors": ["Huatong Song", "Lisheng Huang", "Shuang Sun", "Jinhao Jiang", "Ran Le", "Daixuan Cheng", "Guoxin Chen", "Yiwen Hu", "Zongchao Chen", "Wayne Xin Zhao", "Yang Song", "Tao Zhang", "Ji-Rong Wen"], "pdf_url": "https://arxiv.org/pdf/2602.03411v1"}
{"id": "http://arxiv.org/abs/2602.03400v1", "title": "Precision in Practice: Knowledge Guided Code Summarizing Grounded in Industrial Expectations", "summary": "Code summaries are essential for helping developers understand code functionality and reducing maintenance and collaboration costs. Although recent advances in large language models (LLMs) have significantly improved automatic code summarization, the practical usefulness of generated summaries in industrial settings remains insufficiently explored. In collaboration with documentation experts from the industrial HarmonyOS project, we conducted a questionnaire study showing that over 57.4% of code summaries produced by state-of-the-art approaches were rejected due to violations of developers' expectations for industrial documentation. Beyond semantic similarity to reference summaries, developers emphasize additional requirements, including the use of appropriate domain terminology, explicit function categorization, and the avoidance of redundant implementation details.\n  To address these expectations, we propose ExpSum, an expectation-aware code summarization approach that integrates function metadata abstraction, informative metadata filtering, context-aware domain knowledge retrieval, and constraint-driven prompting to guide LLMs in generating structured, expectation-aligned summaries. We evaluate ExpSum on the HarmonyOS project and widely used code summarization benchmarks. Experimental results show that ExpSum consistently outperforms all baselines, achieving improvements of up to 26.71% in BLEU-4 and 20.10% in ROUGE-L on HarmonyOS. Furthermore, LLM-based evaluations indicate that ExpSum-generated summaries better align with developer expectations across other projects, demonstrating its effectiveness for industrial code documentation.", "published": "2026-02-03T11:22:28Z", "updated": "2026-02-03T11:22:28Z", "authors": ["Jintai Li", "Songqiang Chen", "Shuo Jin", "Xiaoyuan Xie"], "pdf_url": "https://arxiv.org/pdf/2602.03400v1"}
{"id": "http://arxiv.org/abs/2602.01155v2", "title": "Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles", "summary": "Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.", "published": "2026-02-01T11:06:03Z", "updated": "2026-02-03T11:15:44Z", "authors": ["Hugo Math", "Julian Lorenz", "Stefan Oelsner", "Rainer Lienhart"], "pdf_url": "https://arxiv.org/pdf/2602.01155v2"}
{"id": "http://arxiv.org/abs/2602.03311v1", "title": "Multi-Level Testing of Conversational AI Systems", "summary": "Conversational AI systems combine AI-based solutions with the flexibility of conversational interfaces. However, most existing testing solutions do not straightforwardly adapt to the characteristics of conversational interaction or to the behavior of AI components. To address this limitation, this Ph.D. thesis investigates a new family of testing approaches for conversational AI systems, focusing on the validation of their constituent elements at different levels of granularity, from the integration between the language and the AI components, to individual conversational agents, up to multi-agent implementations of conversational AI systems", "published": "2026-02-03T09:38:59Z", "updated": "2026-02-03T09:38:59Z", "authors": ["Elena Masserini"], "pdf_url": "https://arxiv.org/pdf/2602.03311v1"}
{"id": "http://arxiv.org/abs/2601.17482v2", "title": "LogPrism: Unifying Structure and Variable Encoding for Effective Log Compression", "summary": "In the field of log compression, the prevailing \"parse-then-compress\" paradigm fundamentally limits effectiveness by treating log parsing and compression as isolated objectives. While parsers prioritize semantic accuracy (i.e., event identification), they often obscure deep correlations between static templates and dynamic variables that are critical for storage efficiency. In this paper, we investigate this misalignment through a comprehensive empirical study and propose LogPrism, a framework that bridges the gap via unified redundancy encoding. Rather than relying on a rigid pre-parsing step, LogPrism dynamically integrates structural extraction with variable encoding by constructing a Unified Redundancy Tree (URT). This hierarchical approach effectively mines \"structure+variable\" co-occurrence patterns, capturing deep contextual redundancies while accelerating processing through pre-emptive pattern encoding. Extensive experiments on 16 benchmark datasets confirm that LogPrism establishes a new state-of-the-art. It achieves the highest compression ratio on 14 datasets, surpassing existing baselines by margins of 6.12% to 83.34%, while delivering superior throughput at 29.87 MB/s (1.68$\\times$~43.04$\\times$ faster than competitors). Moreover, when configured in single-archive mode to maximize global pattern discovery, LogPrism boosts its compression ratio by 273.27%, outperforming the best baseline by 19.39% with a 2.62$\\times$ speed advantage.", "published": "2026-01-24T15:12:31Z", "updated": "2026-02-03T09:31:17Z", "authors": ["Yang Liu", "Kaiming Zhang", "Zhuangbin Chen", "Zibin Zheng"], "pdf_url": "https://arxiv.org/pdf/2601.17482v2"}
{"id": "http://arxiv.org/abs/2602.02419v2", "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration", "summary": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38% percentage points over Gemini-only inference.", "published": "2026-02-02T18:22:45Z", "updated": "2026-02-03T08:31:29Z", "authors": ["Qingni Wang", "Yue Fan", "Xin Eric Wang"], "pdf_url": "https://arxiv.org/pdf/2602.02419v2"}
{"id": "http://arxiv.org/abs/2602.03181v1", "title": "Synthesizing File-Level Data for Unit Test Generation with Chain-of-Thoughts via Self-Debugging", "summary": "Automatic unit test (UT) generation is essential for software quality assurance, but existing approaches--including symbolic execution, search-based approaches, and recent LLM-based generators--struggle to produce human-quality tests with correct, meaningful assertions and reliable chain-of-thought (CoT) explanations. We identify a gap in UT training data: repository-mined tests lack developer CoTs, while LLM-distilled CoTs are often incorrect or incomplete. To address this issue, we propose a novel data-distillation approach that uses self-debugging to produce high-quality UT training examples paired with faithful CoTs. Our approach combines (1) guided test repair, a heuristic loop (error-, failure-, and coverage-focused steps) that asks the used model to diagnose and iteratively fix generated tests, and (2) CoT compression, which compacts original and debugging CoTs into concise explanations that directly justify correct tests. We apply this pipeline to a large corpus of open-source projects to construct a dataset of 74,518 high-quality <focal method, test, CoT> examples, and then use it for supervised fine-tuning of a base model. An empirical evaluation shows that the fine-tuned model achieves high UT generation effectiveness: it attains a pass rate of 36.17% on test assertions, a branch coverage of 43.90%, and a mutation score of 88.66%, substantially higher than state-of-the-art commercial models like o4-mini.", "published": "2026-02-03T06:52:54Z", "updated": "2026-02-03T06:52:54Z", "authors": ["Ziyue Hua", "Tianyu Chen", "Yeyun Gong", "Shuai Lu", "Peng Cheng", "Qinglin Zhu", "Yibo He", "Yingjie Fu", "Wenpin Jiao", "Wei Yang", "Tao Xie"], "pdf_url": "https://arxiv.org/pdf/2602.03181v1"}
{"id": "http://arxiv.org/abs/2602.03154v1", "title": "Intelligent Front-End Personalization: AI-Driven UI Adaptation", "summary": "Front-end personalization has traditionally relied on static designs or rule-based adaptations, which fail to fully capture user behavior patterns. This paper presents an AI driven approach for dynamic front-end personalization, where UI layouts, content, and features adapt in real-time based on predicted user behavior. We propose three strategies: dynamic layout adaptation using user path prediction, content prioritization through reinforcement learning, and a comparative analysis of AI-driven vs. rule-based personalization. Technical implementation details, algorithms, system architecture, and evaluation methods are provided to illustrate feasibility and performance gains.", "published": "2026-02-03T06:10:10Z", "updated": "2026-02-03T06:10:10Z", "authors": ["Mona Rajhans"], "pdf_url": "https://arxiv.org/pdf/2602.03154v1"}
{"id": "http://arxiv.org/abs/2512.22043v3", "title": "HALF: Hollowing Analysis Framework for Binary Programs with Kernel Module Assistance", "summary": "Binary program analysis represents a fundamental pillar of modern system security. Fine-grained methodologies like dynamic taint analysis still suffer from deployment complexity and performance overhead despite significant progress. Traditional in-process analysis tools trigger severe \\textbf{address-space conflicts} that inevitably disrupt the native memory layout of the target. These conflicts frequently cause layout-sensitive exploits and evasive malware to deviate from their intended execution paths or fail entirely. This paper introduces \\textbf{HALF} as a novel framework that resolves this fundamental tension while ensuring both analysis fidelity and practical performance. HALF achieves high-fidelity address-space transparency by leveraging a kernel-assisted process hollowing mechanism. This design effectively eliminates the observation artifacts that characterize traditional instrumentation tools. We further mitigate the synchronization latency of decoupled execution by implementing an exception-driven strategy via a lightweight kernel monitor. Extensive evaluation of a Windows-based prototype demonstrates that HALF maintains superior performance compared to conventional in-process baselines. HALF also provides unique capabilities for deconstructing complex, stealthy threats where existing frameworks fail to maintain execution integrity.", "published": "2025-12-26T14:34:30Z", "updated": "2026-02-03T05:19:18Z", "authors": ["Zhangbo Long", "Letian Sha", "Jiaye Pan", "Haiping Huang", "Dongpeng Xu", "Yifei Huang", "Fu Xiao"], "pdf_url": "https://arxiv.org/pdf/2512.22043v3"}
{"id": "http://arxiv.org/abs/2512.10398v6", "title": "Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases", "summary": "Real-world software engineering tasks require coding agents that can operate on massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade coding agents offer transparency but struggle when scaled to heavier, production-level workloads, while production-grade systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK supports a unified orchestrator with advanced context management for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the construction, evaluation, and refinement of agents through a build-test-improve cycle, enabling rapid agent development on new tasks and tool stacks. Instantiated on the Confucius SDK using the meta-agent, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a Resolve@1 of 59%, exceeding prior research baselines as well as commercial results, under identical repositories, model backends, and tool access.", "published": "2025-12-11T08:05:58Z", "updated": "2026-02-03T05:01:47Z", "authors": ["Sherman Wong", "Zhenting Qi", "Zhaodong Wang", "Nathan Hu", "Samuel Lin", "Jun Ge", "Erwin Gao", "Wenlin Chen", "Yilun Du", "Minlan Yu", "Ying Zhang"], "pdf_url": "https://arxiv.org/pdf/2512.10398v6"}
{"id": "http://arxiv.org/abs/2602.03093v1", "title": "Maintaining the Heterogeneity in the Organization of Software Engineering Research", "summary": "The heterogeneity in the organization of software engineering (SE) research historically exists, i.e., funded research model and hands-on model, which makes software engineering become a thriving interdisciplinary field in the last 50 years. However, the funded research model is becoming dominant in SE research recently, indicating such heterogeneity has been seriously and systematically threatened. In this essay, we first explain why the heterogeneity is needed in the organization of SE research, then present the current trend of SE research nowadays, as well as the consequences and potential futures. The choice is at our hands, and we urge our community to seriously consider maintaining the heterogeneity in the organization of software engineering research.", "published": "2026-02-03T04:34:59Z", "updated": "2026-02-03T04:34:59Z", "authors": ["Yang Yue", "Zheng Jiang", "Yi Wang"], "pdf_url": "https://arxiv.org/pdf/2602.03093v1"}
{"id": "http://arxiv.org/abs/2602.03070v1", "title": "ProOPF: Benchmarking and Improving LLMs for Professional-Grade Power Systems Optimization Modeling", "summary": "Growing renewable penetration introduces substantial uncertainty into power system operations, necessitating frequent adaptation of dispatch objectives and constraints and challenging expertise-intensive, near-real-time modeling workflows. Large Language Models (LLMs) provide a promising avenue for automating this process by translating natural-language (NL) operational requirements into executable optimization models via semantic reasoning and code synthesis. Yet existing LLM datasets and benchmarks for optimization modeling primarily target coarse-grained cross-domain generalization, offering limited, rigorous evaluation in power-system settings, particularly for Optimal Power Flow (OPF). We therefore introduce \\textbf{ProOPF-D} and \\textbf{ProOPF-B}, a dataset and benchmark for professional-grade OPF modeling: ProOPF-D contains 12K instances pairing NL requests with parameter adjustments and structural extensions to a canonical OPF, together with executable implementations; ProOPF-B provides 121 expert-annotated test cases with ground-truth code, enabling end-to-end evaluation under both concrete and abstract OPF modeling regimes.", "published": "2026-02-03T03:59:03Z", "updated": "2026-02-03T03:59:03Z", "authors": ["Chao Shen", "Zihan Guo", "Xu Wan", "Zhenghao Yang", "Yifan Zhang", "Wengi Huang", "Jie Song", "Zongyan Zhang", "Mingyang Sun"], "pdf_url": "https://arxiv.org/pdf/2602.03070v1"}
{"id": "http://arxiv.org/abs/2602.01044v2", "title": "Morphis: SLO-Aware Resource Scheduling for Microservices with Time-Varying Call Graphs", "summary": "Modern microservice systems exhibit continuous structural evolution in their runtime call graphs due to workload fluctuations, fault responses, and deployment activities. Despite this complexity, our analysis of over 500,000 production traces from ByteDance reveals a latent regularity: execution paths concentrate around a small set of recurring invocation patterns. However, existing resource management approaches fail to exploit this structure. Industrial autoscalers like Kubernetes HPA ignore inter-service dependencies, while recent academic methods often assume static topologies, rendering them ineffective under dynamic execution contexts. In this work, we propose Morphis, a dependency-aware provisioning framework that unifies pattern-aware trace analysis with global optimization. It introduces structural fingerprinting that decomposes traces into a stable execution backbone and interpretable deviation subgraphs. Then, resource allocation is formulated as a constrained optimization problem over predicted pattern distributions, jointly minimizing aggregate CPU usage while satisfying end-to-end tail-latency SLOs. Our extensive evaluations on the TrainTicket benchmark demonstrate that Morphis reduces CPU consumption by 35-38% compared to state-of-the-art baselines while maintaining 98.8% SLO compliance.", "published": "2026-02-01T06:04:19Z", "updated": "2026-02-03T03:56:21Z", "authors": ["Yu Tang", "Hailiang Zhao", "Chuansheng Lu", "Yifei Zhang", "Kingsum Chow", "Shuiguang Deng", "Rui Shi"], "pdf_url": "https://arxiv.org/pdf/2602.01044v2"}
{"id": "http://arxiv.org/abs/2601.19065v2", "title": "The Opaque Pointer Design Pattern in Python: Towards a Pythonic PIMPL for Modularity, Encapsulation, and Stability", "summary": "Python libraries often need to maintain a stable public API even as internal implementations evolve, gain new backends, or depend on heavy optional libraries. In Python, where internal objects are easy to inspect and import, users can come to rely on \"reachable internals\" that were never intended to be public, making refactoring risky and slowing long-term maintenance. This paper revisits the pointer-to-implementation (PIMPL) idiom from C++ and reinterprets it as a Pythonic pattern of opaque delegation: a small public object (or module) that delegates its behavior to a separate implementation object treated as internal. We situate this pattern within a broader taxonomy of encapsulation techniques in Python, relate it to existing practices such as module-level indirection, facade objects, and backend dispatch, and identify PIMPL-like structures already used in the standard library and the scientific Python ecosystem. We then show how a Pythonic PIMPL can be used in existing codebases to isolate heavy dependencies, support lazy imports, and enable runtime selection of alternative backends without changing the public API. Finally, we discuss the benefits and trade-offs of the approach and offer practical guidance on when the pattern is appropriate and how to apply it in large, long-lived Python libraries.", "published": "2026-01-27T01:02:26Z", "updated": "2026-02-03T02:40:48Z", "authors": ["Antonios Saravanos", "John Pazarzis", "Stavros Zervoudakis", "Dongnanzi Zheng"], "pdf_url": "https://arxiv.org/pdf/2601.19065v2"}
{"id": "http://arxiv.org/abs/2512.12553v2", "title": "Cargo Sherlock: An SMT-Based Checker for Software Trust Costs", "summary": "Supply chain attacks threaten open-source software ecosystems. This paper proposes a formal framework for quantifying trust in third-party software dependencies that is both formally checkable - formalized in satisfiability modulo theories (SMT) - while at the same time incorporating human factors, like the number of downloads, authors, and other metadata that are commonly used to identify trustworthy software in practice. We use data from both software analysis tools and metadata to build a first-order relational model of software dependencies; to obtain an overall \"trust cost\" combining these factors, we propose a formalization based on the minimum trust problem which asks for the minimum cost of a set of assumptions which can be used to prove that the code is safe. We implement these ideas in Cargo Sherlock, targeted for Rust libraries (crates), incorporating a list of candidate assumptions motivated by quantifiable trust metrics identified in prior work. Our evaluation shows that Cargo Sherlock can be used to identify synthetically generated supply chain attacks and known incidents involving typosquatted and poorly AI-maintained crates, and that its performance scales to Rust crates with many dependencies.", "published": "2025-12-14T04:59:10Z", "updated": "2026-02-03T01:07:40Z", "authors": ["Muhammad Hassnain", "Anirudh Basu", "Ethan Ng", "Caleb Stanford"], "pdf_url": "https://arxiv.org/pdf/2512.12553v2"}
{"id": "http://arxiv.org/abs/2602.02966v1", "title": "What Do Contribution Guidelines Say About Software Testing?", "summary": "Software testing plays a crucial role in the contribution process of open-source projects. For example, contributions introducing new features are expected to include tests, and contributions with tests are more likely to be accepted. Although most real-world projects require contributors to write tests, the specific testing practices communicated to contributors remain unclear. In this paper, we present an empirical study to understand better how software testing is approached in contribution guidelines. We analyze the guidelines of 200 Python and JavaScript open-source software projects. We find that 78\\% of the projects include some form of test documentation for contributors. Test documentation is located in multiple sources, including \\texttt{CONTRIBUTING} files (58\\%), external documentation (24\\%), and \\texttt{README} files (8\\%). Furthermore, test documentation commonly explains how to run tests (83.5\\%), but less often provides guidance on how to write tests (37\\%). It frequently covers unit tests (71\\%), but rarely addresses integration (20.5\\%) and end-to-end tests (15.5\\%). Other key testing aspects are also less frequently discussed: test coverage (25.5\\%) and mocking (9.5\\%). We conclude by discussing implications and future research.", "published": "2026-02-03T01:05:15Z", "updated": "2026-02-03T01:05:15Z", "authors": ["Bruna Falcucci", "Felipe Gomide", "Andre Hora"], "pdf_url": "https://arxiv.org/pdf/2602.02966v1"}
{"id": "http://arxiv.org/abs/2602.02965v1", "title": "Understanding Bug-Reproducing Tests: A First Empirical Study", "summary": "Developers create bug-reproducing tests that support debugging by failing as long as the bug is present, and passing once the bug has been fixed. These tests are usually integrated into existing test suites and executed regularly alongside all other tests to ensure that future regressions are caught. Despite this co-existence with other types of tests, the properties of bug-reproducing tests are scarcely researched, and it remains unclear whether they differ fundamentally. In this short paper, we provide an initial empirical study to understand bug-reproducing tests better. We analyze 642 bug-reproducing tests of 15 real-world Python systems. Overall, we find that bug-reproducing tests are not (statistically significantly) different from other tests regarding LOC, number of assertions, and complexity. However, bug-reproducing tests contain slightly more try/except blocks and ``weak assertions'' (e.g.,~\\texttt{assertNotEqual}). Lastly, we detect that the majority (95%) of the bug-reproducing tests reproduce a single bug, while 5% reproduce multiple bugs. We conclude by discussing implications and future research directions.", "published": "2026-02-03T01:04:18Z", "updated": "2026-02-03T01:04:18Z", "authors": ["Andre Hora", "Gordon Fraser"], "pdf_url": "https://arxiv.org/pdf/2602.02965v1"}
{"id": "http://arxiv.org/abs/2602.02964v1", "title": "Testing Framework Migration with Large Language Models", "summary": "Python developers rely on two major testing frameworks: \\texttt{unittest} and \\texttt{Pytest}. While \\texttt{Pytest} offers simpler assertions, reusable fixtures, and better interoperability, migrating existing suites from \\texttt{unittest} remains a manual and time-consuming process. Automating this migration could substantially reduce effort and accelerate test modernization. In this paper, we investigate the capability of Large Language Models (LLMs) to automate test framework migrations from \\texttt{unittest} to \\texttt{Pytest}. We evaluate GPT 4o and Claude Sonnet 4 under three prompting strategies (Zero-shot, One-shot, and Chain-of-Thought) and two temperature settings (0.0 and 1.0). To support this analysis, we first introduce a curated dataset of real-world migrations extracted from the top 100 Python open-source projects. Next, we actually execute the LLM-generated test migrations in their respective test suites. Overall, we find that 51.5% of the LLM-generated test migrations failed, while 48.5% passed. The results suggest that LLMs can accelerate test migration, but there are often caveats. For example, Claude Sonnet 4 exhibited more conservative migrations (e.g., preserving class-based tests and legacy \\texttt{unittest} references), while GPT-4o favored more transformations (e.g., to function-based tests). We conclude by discussing multiple implications for practitioners and researchers.", "published": "2026-02-03T01:04:09Z", "updated": "2026-02-03T01:04:09Z", "authors": ["Altino Alves", "João Eduardo Montandon", "Andre Hora"], "pdf_url": "https://arxiv.org/pdf/2602.02964v1"}
{"id": "http://arxiv.org/abs/2602.02934v1", "title": "Beyond Blame: Rethinking SZZ with Knowledge Graph Search", "summary": "Identifying Bug-Inducing Commits (BICs) is fundamental for understanding software defects and enabling downstream tasks such as defect prediction and automated program repair. Yet existing SZZ-based approaches are limited by their reliance on git blame, which restricts the search space to commits that directly modified the fixed lines. Our preliminary study on 2,102 validated bug-fixing commits reveals that this limitation is significant: over 40% of cases cannot be solved by blame alone, as 28% of BICs require traversing commit history beyond blame results and 14% are blameless.\n  We present AgenticSZZ, the first approach to apply Temporal Knowledge Graphs (TKGs) to software evolution analysis. AgenticSZZ reframes BIC identification from a ranking problem over blame commits into a graph search problem, where temporal ordering is fundamental to causal reasoning about bug introduction. The approach operates in two phases: (1) constructing a TKG that encodes commits with temporal and structural relationships, expanding the search space by traversing file history backward from two reference points (blame commits and the BFC); and (2) leveraging an LLM agent to navigate the graph using specialized tools for candidate exploration and causal analysis.\n  Evaluation on three datasets shows that AgenticSZZ achieves F1-scores of 0.48 to 0.74, with statistically significant improvements over state-of-the-art by up to 27%. Our ablation study confirms that both components are essential, reflecting a classic exploration-exploitation trade-off: the TKG expands the search space while the agent provides intelligent selection. By transforming BIC identification into a graph search problem, we open a new research direction for temporal and causal reasoning in software evolution analysis.", "published": "2026-02-03T00:10:48Z", "updated": "2026-02-03T00:10:48Z", "authors": ["Yu Shi", "Hao Li", "Bram Adams", "Ahmed E. Hassan"], "pdf_url": "https://arxiv.org/pdf/2602.02934v1"}
