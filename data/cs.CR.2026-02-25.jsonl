{"id": "http://arxiv.org/abs/2602.22037v1", "title": "A Critical Look into Threshold Homomorphic Encryption for Private Average Aggregation", "summary": "Threshold Homomorphic Encryption (Threshold HE) is a good fit for implementing private federated average aggregation, a key operation in Federated Learning (FL). Despite its potential, recent studies have shown that threshold schemes available in mainstream HE libraries can introduce unexpected security vulnerabilities if an adversary has access to a restricted decryption oracle. This oracle reflects the FL clients' capacity to collaboratively decrypt the aggregated result without knowing the secret key. This work surveys the use of threshold RLWE-based HE for federated average aggregation and examines the performance impact of using smudging noise with a large variance as a countermeasure. We provide a detailed comparison of threshold variants of BFV and CKKS, finding that CKKS-based aggregations perform comparably to BFV-based solutions.", "published": "2026-02-25T15:45:27Z", "updated": "2026-02-25T15:45:27Z", "authors": ["Miguel Morona-Mínguez", "Alberto Pedrouzo-Ulloa", "Fernando Pérez-González"], "pdf_url": "https://arxiv.org/pdf/2602.22037v1"}
{"id": "http://arxiv.org/abs/2602.05674v2", "title": "Fast Private Adaptive Query Answering for Large Data Domains", "summary": "Privately releasing marginals of a tabular dataset is a foundational problem in differential privacy. However, state-of-the-art mechanisms suffer from a computational bottleneck when marginal estimates are reconstructed from noisy measurements. Recently, residual queries were introduced and shown to lead to highly efficient reconstruction in the batch query answering setting. We introduce new techniques to integrate residual queries into state-of-the-art adaptive mechanisms such as AIM. Our contributions include a novel conceptual framework for residual queries using multi-dimensional arrays, lazy updating strategies, and adaptive optimization of the per-round privacy budget allocation. Together these contributions reduce error, improve speed, and simplify residual query operations. We integrate these innovations into a new mechanism (AIM+GReM), which improves AIM by using fast residual-based reconstruction instead of a graphical model approach. Our mechanism is orders of magnitude faster than the original framework and demonstrates competitive error and greatly improved scalability.", "published": "2026-02-05T13:57:56Z", "updated": "2026-02-25T14:10:20Z", "authors": ["Miguel Fuentes", "Brett Mullins", "Yingtai Xiao", "Daniel Kifer", "Cameron Musco", "Daniel Sheldon"], "pdf_url": "https://arxiv.org/pdf/2602.05674v2"}
{"id": "http://arxiv.org/abs/2602.21892v1", "title": "APFuzz: Towards Automatic Greybox Protocol Fuzzing", "summary": "Greybox protocol fuzzing is a random testing approach for stateful protocol implementations, where the input is protocol messages generated from mutations of seeds, and the search in the input space is driven by the feedback on coverage of both code and state. State model and message model are the core components of communication protocols, which also have significant impacts on protocol fuzzing. In this work, we propose APFuzz (Automatic greybox Protocol Fuzzer) with novel designs to increase the smartness of greybox protocol fuzzers from the perspectives of both the state model and the message model. On the one hand, APFuzz employs a two-stage process of static and dynamic analysis to automatically identify state variables, which are then used to infer an accurate state model during fuzzing. On the other hand, APFuzz introduces field-level mutation operations for binary protocols, leveraging message structure awareness enabled by Large Language Models. We conduct extensive experiments on a public protocol fuzzing benchmark, comparing APFuzz with the baseline fuzzer AFLNET as well as several state-of-the-art greybox protocol fuzzers.", "published": "2026-02-25T13:21:06Z", "updated": "2026-02-25T13:21:06Z", "authors": ["Yu Wang", "Yang Xiang", "Chandra Thapa", "Hajime Suzuki"], "pdf_url": "https://arxiv.org/pdf/2602.21892v1"}
{"id": "http://arxiv.org/abs/2602.21841v1", "title": "Resilient Federated Chain: Transforming Blockchain Consensus into an Active Defense Layer for Federated Learning", "summary": "Federated Learning (FL) has emerged as a key paradigm for building Trustworthy AI systems by enabling privacy-preserving, decentralized model training. However, FL is highly susceptible to adversarial attacks that compromise model integrity and data confidentiality, a vulnerability exacerbated by the fact that conventional data inspection methods are incompatible with its decentralized design. While integrating FL with Blockchain technology has been proposed to address some limitations, its potential for mitigating adversarial attacks remains largely unexplored. This paper introduces Resilient Federated Chain (RFC), a novel blockchain-enabled FL framework designed specifically to enhance resilience against such threats. RFC builds upon the existing Proof of Federated Learning architecture by repurposing the redundancy of its Pooled Mining mechanism as an active defense layer that can be combined with robust aggregation rules. Furthermore, the framework introduces a flexible evaluation function in its consensus mechanism, allowing for adaptive defense against different attack strategies. Extensive experimental evaluation on image classification tasks under various adversarial scenarios, demonstrates that RFC significantly improves robustness compared to baseline methods, providing a viable solution for securing decentralized learning environments.", "published": "2026-02-25T12:20:47Z", "updated": "2026-02-25T12:20:47Z", "authors": ["Mario García-Márquez", "Nuria Rodríguez-Barroso", "M. Victoria Luzón", "Francisco Herrera"], "pdf_url": "https://arxiv.org/pdf/2602.21841v1"}
{"id": "http://arxiv.org/abs/2602.21826v1", "title": "The Silent Spill: Measuring Sensitive Data Leaks Across Public URL Repositories", "summary": "A large number of URLs are made public by various platforms for security analysis, archiving, and paste sharing -- such as VirusTotal, URLScan.io, Hybrid Analysis, the Wayback Machine, and RedHunt. These services may unintentionally expose links containing sensitive information, as reported in some news articles and blog posts. However, no large-scale measurement has quantified the extent of such exposures. We present an automated system that detects and analyzes potential sensitive information leaked through publicly accessible URLs. The system combines lexical URL filtering, dynamic rendering, OCR-based extraction, and content classification to identify potential leaks. We apply it to 6,094,475 URLs collected from public scanning platforms, paste sites, and web archives, identifying 12,331 potential exposures across authentication, financial, personal, and document-related domains. These findings show that sensitive information remains exposed, underscoring the importance of automated detection to identify accidental leaks.", "published": "2026-02-25T11:54:46Z", "updated": "2026-02-25T11:54:46Z", "authors": ["Tarek Ramadan", "AbdelRahman Abdou", "Mohammad Mannan", "Amr Youssef"], "pdf_url": "https://arxiv.org/pdf/2602.21826v1"}
{"id": "http://arxiv.org/abs/2602.21794v1", "title": "MulCovFuzz: A Multi-Component Coverage-Guided Greybox Fuzzer for 5G Protocol Testing", "summary": "As mobile networks transition to 5G infrastructure, ensuring robust security becomes more important due to the complex architecture and expanded attack surface. Traditional security testing approaches for 5G networks rely on black-box fuzzing techniques, which are limited by their inability to observe internal program state and coverage information. This paper presents MulCovFuzz, a novel coverage-guided greybox fuzzing tool for 5G network testing. Unlike existing tools that depend solely on system response, MulCovFuzz implements a multi-component coverage collection mechanism that dynamically monitors code coverage across different components of the 5G system architecture. Our approach introduces a novel testing paradigm that includes a scoring function combining coverage rewards with efficiency metrics to guide test case generation. We evaluate MulCovFuzz on open-source 5G implementation OpenAirInterface. Our experimental results demonstrate that MulCovFuzz significantly outperforms traditional fuzzing approaches, achieving a 5.85\\% increase in branch coverage, 7.17\\% increase in line coverage, and 16\\% improvement in unique crash discovery during 24h fuzzing testing. MulCovFuzz uncovered three zero-day vulnerabilities, two of which were not identified by any other fuzzing technique. This work contributes to the advancement of security testing tools for next-generation mobile networks.", "published": "2026-02-25T11:17:42Z", "updated": "2026-02-25T11:17:42Z", "authors": ["Yu Wang", "Yang Xiang", "Chandra Thapa", "Hajime Suzuki"], "pdf_url": "https://arxiv.org/pdf/2602.21794v1"}
{"id": "http://arxiv.org/abs/2501.08449v2", "title": "A Refreshment Stirred, Not Shaken: Invariant-Preserving Deployments of Differential Privacy for the U.S. Decennial Census", "summary": "Protecting an individual's privacy when releasing their data is inherently an exercise in relativity, regardless of how privacy is qualified or quantified. This is because we can only limit the gain in information about an individual relative to what could be derived from other sources. This framing is the essence of differential privacy (DP), through which this article examines two statistical disclosure control (SDC) methods for the United States Decennial Census: the Permutation Swapping Algorithm (PSA), which resembles the 2010 Census's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA), which was used in the 2020 DAS. To varying degrees, both methods leave unaltered certain statistics of the confidential data (their invariants) and hence neither can be readily reconciled with DP, at least as originally conceived. Nevertheless, we show how invariants can naturally be integrated into DP and use this to establish that the PSA satisfies pure DP subject to the invariants it necessarily induces, thereby proving that this traditional SDC method can, in fact, be understood from the perspective of DP. By a similar modification to zero-concentrated DP, we also provide a DP specification for the TDA. Finally, as a point of comparison, we consider a counterfactual scenario in which the PSA was adopted for the 2020 Census, resulting in a reduction in the nominal protection loss budget but at the cost of releasing many more invariants. This highlights the pervasive danger of comparing budgets without accounting for the other dimensions on which DP formulations vary (such as the invariants they permit). Therefore, while our results articulate the mathematical guarantees of SDC provided by the PSA, the TDA, and the 2020 DAS in general, care must be taken in translating these guarantees into actual privacy protection$\\unicode{x2014}$just as is the case for any DP deployment.", "published": "2025-01-14T21:38:01Z", "updated": "2026-02-25T10:50:54Z", "authors": ["James Bailie", "Ruobin Gong", "Xiao-Li Meng"], "pdf_url": "https://arxiv.org/pdf/2501.08449v2"}
{"id": "http://arxiv.org/abs/2602.05066v2", "title": "Bypassing AI Control Protocols via Agent-as-a-Proxy Attacks", "summary": "As AI agents automate critical workloads, they remain vulnerable to indirect prompt injection (IPI) attacks. Current defenses rely on monitoring protocols that jointly evaluate an agent's Chain-of-Thought (CoT) and tool-use actions to ensure alignment with user intent. We demonstrate that these monitoring-based defenses can be bypassed via a novel Agent-as-a-Proxy attack, where prompt injection attacks treat the agent as a delivery mechanism, bypassing both agent and monitor simultaneously. While prior work on scalable oversight has focused on whether small monitors can supervise large agents, we show that even frontier-scale monitors are vulnerable. Large-scale monitoring models like Qwen2.5-72B can be bypassed by agents with similar capabilities, such as GPT-4o mini and Llama-3.1-70B. On the AgentDojo benchmark, we achieve a high attack success rate against AlignmentCheck and Extract-and-Evaluate monitors under diverse monitoring LLMs. Our findings suggest current monitoring-based agentic defenses are fundamentally fragile regardless of model scale.", "published": "2026-02-04T21:38:38Z", "updated": "2026-02-25T10:09:46Z", "authors": ["Jafar Isbarov", "Murat Kantarcioglu"], "pdf_url": "https://arxiv.org/pdf/2602.05066v2"}
{"id": "http://arxiv.org/abs/2602.21737v1", "title": "Implementation and transition to post-quantum cryptography of the Minimal IKE protocol", "summary": "This paper concerns the Minimal Internet Key Exchange (IKE) protocol, which has received little attention to date, despite its potential to make the best-known IKE protocol sufficiently lightweight to be also applied in contexts where it is currently prohibitive, due to its large footprint. First, we introduce and describe Colibri, an efficient, open-source implementation of the Minimal IKE protocol, which allows us to quantitatively assess its real advantages in terms of lightness. Then we introduce a post-quantum variant of the Minimal IKE protocol, which is essential to make it contemporary, and assess it through Colibri. We demonstrate that the protocol performance remains excellent even in such a more challenging context, making it suitable for deploying pervasive and quantum-resistant virtual private networks.", "published": "2026-02-25T09:46:57Z", "updated": "2026-02-25T09:46:57Z", "authors": ["Davide De Zuane", "Paolo Santini", "Marco Baldi"], "pdf_url": "https://arxiv.org/pdf/2602.21737v1"}
{"id": "http://arxiv.org/abs/2602.21721v1", "title": "Private and Robust Contribution Evaluation in Federated Learning", "summary": "Cross-silo federated learning allows multiple organizations to collaboratively train machine learning models without sharing raw data, but client updates can still leak sensitive information through inference attacks. Secure aggregation protects privacy by hiding individual updates, yet it complicates contribution evaluation, which is critical for fair rewards and detecting low-quality or malicious participants. Existing marginal-contribution methods, such as the Shapley value, are incompatible with secure aggregation, and practical alternatives, such as Leave-One-Out, are crude and rely on self-evaluation.\n  We introduce two marginal-difference contribution scores compatible with secure aggregation. Fair-Private satisfies standard fairness axioms, while Everybody-Else eliminates self-evaluation and provides resistance to manipulation, addressing a largely overlooked vulnerability. We provide theoretical guarantees for fairness, privacy, robustness, and computational efficiency, and evaluate our methods on multiple medical image datasets and CIFAR10 in cross-silo settings. Our scores consistently outperform existing baselines, better approximate Shapley-induced client rankings, and improve downstream model performance as well as misbehavior detection. These results demonstrate that fairness, privacy, robustness, and practical utility can be achieved jointly in federated contribution evaluation, offering a principled solution for real-world cross-silo deployments.", "published": "2026-02-25T09:27:40Z", "updated": "2026-02-25T09:27:40Z", "authors": ["Delio Jaramillo Velez", "Gergely Biczok", "Alexandre Graell i Amat", "Johan Ostman", "Balazs Pejo"], "pdf_url": "https://arxiv.org/pdf/2602.21721v1"}
{"id": "http://arxiv.org/abs/2507.21540v2", "title": "PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking", "summary": "The increasing sophistication of large vision-language models (LVLMs) has been accompanied by advances in safety alignment mechanisms designed to prevent harmful content generation. However, these defenses remain vulnerable to sophisticated adversarial attacks. Existing jailbreak methods typically rely on direct and semantically explicit prompts, overlooking subtle vulnerabilities in how LVLMs compose information over multiple reasoning steps. In this paper, we propose a novel and effective jailbreak framework inspired by Return-Oriented Programming (ROP) techniques from software security. Our approach decomposes a harmful instruction into a sequence of individually benign visual gadgets. A carefully engineered textual prompt directs the sequence of inputs, prompting the model to integrate the benign visual gadgets through its reasoning process to produce a coherent and harmful output. This makes the malicious intent emergent and difficult to detect from any single component. We validate our method through extensive experiments on established benchmarks including SafeBench and MM-SafetyBench, targeting popular LVLMs. Results show that our approach consistently and substantially outperforms existing baselines on state-of-the-art models, achieving near-perfect attack success rates (over 0.90 on SafeBench) and improving ASR by up to 0.39. Our findings reveal a critical and underexplored vulnerability that exploits the compositional reasoning abilities of LVLMs, highlighting the urgent need for defenses that secure the entire reasoning process.", "published": "2025-07-29T07:13:56Z", "updated": "2026-02-25T08:16:45Z", "authors": ["Quanchen Zou", "Zonghao Ying", "Moyang Chen", "Wenzhuo Xu", "Yisong Xiao", "Yakai Li", "Deyue Zhang", "Dongdong Yang", "Zhao Liu", "Xiangzheng Zhang"], "pdf_url": "https://arxiv.org/pdf/2507.21540v2"}
{"id": "http://arxiv.org/abs/2506.06226v2", "title": "No Data? No Problem: Synthesizing Security Graphs for Better Intrusion Detection", "summary": "Provenance graph analysis plays a vital role in intrusion detection, particularly against Advanced Persistent Threats (APTs), by exposing complex attack patterns. While recent systems combine graph neural networks (GNNs) with natural language processing (NLP) to capture structural and semantic features, their effectiveness is limited by class imbalance in real-world data. To address this, we introduce PROVSYN, a novel hybrid provenance graph synthesis framework, which comprises three components: (1) graph structure synthesis via heterogeneous graph generation models, (2) textual attribute synthesis via fine-tuned Large Language Models (LLMs), and (3) five-dimensional fidelity evaluation. Experiments on six benchmark datasets demonstrate that PROVSYN consistently produces higher-fidelity graphs across the five evaluation dimensions compared to four strong baselines. To further demonstrate the practical utility of PROVSYN, we utilize the synthesized graphs to augment training datasets for downstream APT detection models. The results show that PROVSYN effectively mitigates data imbalance, improving normalized entropy by up to 35%, and enhances the generalizability of downstream detection models, achieving an accuracy improvement of up to 38%.", "published": "2025-06-06T16:41:17Z", "updated": "2026-02-25T07:54:34Z", "authors": ["Yi Huang", "Shaofei Li", "Yao Guo", "Xiangqun Chen", "Ding Li", "Wajih Ul Hassan"], "pdf_url": "https://arxiv.org/pdf/2506.06226v2"}
{"id": "http://arxiv.org/abs/2407.15524v9", "title": "Minimal Cascade Gradient Smoothing for Fast Transferable Preemptive Adversarial Defense", "summary": "Adversarial attacks persist as a major challenge in deep learning. While training- and test-time defenses are well-studied, they often reduce clean accuracy, incur high cost, or fail under adaptive threats. In contrast, preemptive defenses, which perturb media before release, offer a practical alternative but remain slow, model-coupled, and brittle. We propose the Minimal Sufficient Preemptive Defense (MSPD), a fast, transferable framework that defends against future attacks without access to the target model or gradients. MSPD is driven by Minimal Cascade Gradient Smoothing (MCGS), a two-epoch optimization paradigm executed on a surrogate backbone. This defines a minimal yet effective regime for robust generalization across unseen models and attacks. MSPD runs at 0.02s/image (CIFAR-10) and 0.26s/image (ImageNet), 28--1696 times faster than prior preemptive methods, while improving robust accuracy by +5% and clean accuracy by +3.7% across 11 models and 7 attacks. To evaluate adaptive robustness, we introduce Preemptive Reversion, the first white-box diagnostic attack that cancels preemptive perturbations under full gradient access. Even in this setting, MSPD retains a +2.2% robustness margin over the baseline. In practice, when gradients are unavailable, MSPD remains reliable and efficient. MSPD, MCGS, and Preemptive Reversion are each supported by formal theoretical proofs. The implementation is available at https://github.com/azrealwang/MSPD.", "published": "2024-07-22T10:23:44Z", "updated": "2026-02-25T07:50:07Z", "authors": ["Hanrui Wang", "Ching-Chun Chang", "Chun-Shien Lu", "Ching-Chia Kao", "Shuo Wang", "Isao Echizen"], "pdf_url": "https://arxiv.org/pdf/2407.15524v9"}
{"id": "http://arxiv.org/abs/2602.21630v1", "title": "Type-Based Enforcement of Non-Interference for Choreographic Programming", "summary": "Choreographies describe distributed protocols from a global viewpoint, enabling correct-by-construction synthesis of local behaviours. We develop a policy-parametric type system that prevents information leaks from high-security data to low-security observers, handling both explicit and implicit flows through a program-counter discipline. The system supports recursive procedures via a procedure context that we reconstruct through constraint generation. We prove termination-insensitive non-interference with respect to a standard small-step semantics.", "published": "2026-02-25T06:50:51Z", "updated": "2026-02-25T06:50:51Z", "authors": ["Marco Bertoni", "Saverio Giallorenzo", "Marco Peressotti"], "pdf_url": "https://arxiv.org/pdf/2602.21630v1"}
{"id": "http://arxiv.org/abs/2601.20917v5", "title": "FIPS 204-Compatible Threshold ML-DSA via Shamir Nonce DKG", "summary": "We present the first threshold ML-DSA (FIPS 204) scheme achieving statistical share privacy (no computational assumptions) with arbitrary thresholds, while producing standard 3.3 KB signatures verifiable by unmodified implementations. Our primary technique, Shamir nonce DKG, jointly generates the signing nonce so that both the nonce and the long-term secret are degree-(T-1) Shamir sharings. This gives the honest party's nonce share conditional min-entropy exceeding 5x the secret-key entropy for signing sets of size at most 17. In coordinator-based profiles (P1, P3+), this removes the two-honest requirement (it suffices that the signing set size is at least T); in the fully distributed profile (P2), we additionally require at least two non-coordinator honest parties for mask-hiding. Key privacy of the aggregate signature relies on the same lattice hardness as single-signer ML-DSA (an open problem in the literature). As a secondary technique, pairwise-canceling masks handle three challenges unique to lattice-based threshold signing: the infinity-norm rejection check on z, secure r0-check evaluation without leaking cs2, and EUF-CMA security under the resulting Irwin-Hall nonce distribution. A direct shift-invariance analysis gives per-session loss below 0.013 bits (below 0.007 bits when the signing set size is at most 17); over qs signing sessions the total loss is below 0.013qs bits, eliminating the scalability gap in prior work. We give three deployment profiles with complete UC proofs: P1 (TEE, 5.8 ms for 3-of-5), P2 (MPC, 5 rounds, 22 ms), and P3+ (2PC semi-async, 22 ms). Our Rust implementation supports thresholds from 2-of-3 to 32-of-45 with sub-100 ms latency and about 21-45 percent success rates.", "published": "2026-01-28T18:13:47Z", "updated": "2026-02-25T05:54:48Z", "authors": ["Leo Kao"], "pdf_url": "https://arxiv.org/pdf/2601.20917v5"}
{"id": "http://arxiv.org/abs/2602.21593v1", "title": "Breaking Semantic-Aware Watermarks via LLM-Guided Coherence-Preserving Semantic Injection", "summary": "Generative images have proliferated on Web platforms in social media and online copyright distribution scenarios, and semantic watermarking has increasingly been integrated into diffusion models to support reliable provenance tracking and forgery prevention for web content. Traditional noise-layer-based watermarking, however, remains vulnerable to inversion attacks that can recover embedded signals. To mitigate this, recent content-aware semantic watermarking schemes bind watermark signals to high-level image semantics, constraining local edits that would otherwise disrupt global coherence. Yet, large language models (LLMs) possess structured reasoning capabilities that enable targeted exploration of semantic spaces, allowing locally fine-grained but globally coherent semantic alterations that invalidate such bindings. To expose this overlooked vulnerability, we introduce a Coherence-Preserving Semantic Injection (CSI) attack that leverages LLM-guided semantic manipulation under embedding-space similarity constraints. This alignment enforces visual-semantic consistency while selectively perturbing watermark-relevant semantics, ultimately inducing detector misclassification. Extensive empirical results show that CSI consistently outperforms prevailing attack baselines against content-aware semantic watermarking, revealing a fundamental security weakness of current semantic watermark designs when confronted with LLM-driven semantic perturbations.", "published": "2026-02-25T05:38:08Z", "updated": "2026-02-25T05:38:08Z", "authors": ["Zheng Gao", "Xiaoyu Li", "Zhicheng Bao", "Xiaoyan Feng", "Jiaojiao Jiang"], "pdf_url": "https://arxiv.org/pdf/2602.21593v1"}
{"id": "http://arxiv.org/abs/2507.17691v2", "title": "CASCADE: LLM-Powered JavaScript Deobfuscator at Google", "summary": "Software obfuscation, particularly prevalent in JavaScript, hinders code comprehension and analysis, posing significant challenges to software testing, static analysis, and malware detection. This paper introduces CASCADE, a novel hybrid approach that integrates the advanced coding capabilities of Gemini with the deterministic transformation capabilities of a compiler Intermediate Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to identify critical prelude functions, the foundational components underlying the most prevalent obfuscation techniques, and leveraging JSIR for subsequent code transformations, CASCADE effectively recovers semantic elements like original strings and API names, and reveals original program behaviors. This method overcomes limitations of existing static and dynamic deobfuscation techniques, eliminating hundreds to thousands of hardcoded rules while achieving reliability and flexibility. CASCADE is already deployed in Google's production environment, demonstrating substantial improvements in JavaScript deobfuscation efficiency and reducing reverse engineering efforts.", "published": "2025-07-23T16:57:32Z", "updated": "2026-02-25T04:00:38Z", "authors": ["Shan Jiang", "Pranoy Kovuri", "David Tao", "Zhixun Tan"], "pdf_url": "https://arxiv.org/pdf/2507.17691v2"}
{"id": "http://arxiv.org/abs/2602.21529v1", "title": "TM-RUGPULL: A Temporary Sound, Multimodal Dataset for Early Detection of RUG Pulls Across the Tokenized Ecosystem", "summary": "Rug-pull attacks pose a systemic threat across the blockchain ecosystem, yet research into early detection is hindered by the lack of scientific-grade datasets. Existing resources often suffer from temporal data leakage, narrow modality, and ambiguous labeling, particularly outside DeFi contexts. To address these limitations, we present TM-RugPull, a rigorously curated, leakage-resistant dataset of 1,028 token projects spanning DeFi, meme coins, NFTs, and celebrity-themed tokens. RugPull enforces strict temporal hygiene by extracting all features on chain behavior, smart contract metadata, and OSINT signals strictly from the first half of each project's lifespan. Labels are grounded in forensic reports and longevity criteria, verified through multi-expert consensus. This dataset enables causally valid, multimodal analysis of rug-pull dynamics and establishes a new benchmark for reproducible fraud detection research.", "published": "2026-02-25T03:32:28Z", "updated": "2026-02-25T03:32:28Z", "authors": ["Fatemeh Shoaei", "Mohammad Pishdar", "Mozafar Bag-Mohammadi", "Mojtaba Karami"], "pdf_url": "https://arxiv.org/pdf/2602.21529v1"}
{"id": "http://arxiv.org/abs/2602.21524v1", "title": "Quantum Attacks Targeting Nuclear Power Plants: Threat Analysis, Defense and Mitigation Strategies", "summary": "The advent of Cryptographically Relevant Quantum Computers (CRQCs) presents a fundamental and existential threat to the forensic integrity and operational safety of Industrial Control Systems (ICS) and Operational Technology (OT) in critical infrastructure. This paper introduces a novel, forensics-first framework for achieving quantum resilience in high-consequence environments, with a specific focus on nuclear power plants. We systematically analyze the quantum threat landscape across the Purdue architecture (L0-L5), detailing how Harvest-Now, Decrypt-Later (HNDL) campaigns, enabled by algorithms like Shor's, can retroactively compromise cryptographic foundations, undermine evidence admissibility, and facilitate sophisticated sabotage. Through two detailed case studies, \\textsc{Quantum~Scar} and \\textsc{Quantum~Dawn}, we demonstrate multi-phase attack methodologies where state-level adversaries exploit cryptographic monoculture and extended OT lifecycles to degrade safety systems while creating unsolvable forensic paradoxes. Our probabilistic risk modeling reveals alarming success probabilities (up to 78\\% for targeted facilities under current defenses), underscoring the criticality of immediate action. In response, we propose and validate a phased, defense-in-depth migration path to Post-Quantum Cryptography (PQC), integrating hybrid key exchange, cryptographic diversity, secure time synchronization, and side-channel resistant implementations aligned with ISA/IEC 62443 and NIST standards. The paper concludes that without urgent adoption of quantum-resilient controls, the integrity of both physical safety systems and digital forensic evidence remains at severe and irreversible risk.", "published": "2026-02-25T03:26:09Z", "updated": "2026-02-25T03:26:09Z", "authors": ["Yaser Baseri", "Edward Waller"], "pdf_url": "https://arxiv.org/pdf/2602.21524v1"}
{"id": "http://arxiv.org/abs/2602.21508v1", "title": "WaterVIB: Learning Minimal Sufficient Watermark Representations via Variational Information Bottleneck", "summary": "Robust watermarking is critical for intellectual property protection, whereas existing methods face a severe vulnerability against regeneration-based AIGC attacks. We identify that existing methods fail because they entangle the watermark with high-frequency cover texture, which is susceptible to being rewritten during generative purification. To address this, we propose WaterVIB, a theoretically grounded framework that reformulates the encoder as an information sieve via the Variational Information Bottleneck. Instead of overfitting to fragile cover details, our approach forces the model to learn a Minimal Sufficient Statistic of the message. This effectively filters out redundant cover nuances prone to generative shifts, retaining only the essential signal invariant to regeneration. We theoretically prove that optimizing this bottleneck is a necessary condition for robustness against distribution-shifting attacks. Extensive experiments demonstrate that WaterVIB significantly outperforms state-of-the-art methods, achieving superior zero-shot resilience against unknown diffusion-based editing.", "published": "2026-02-25T02:38:17Z", "updated": "2026-02-25T02:38:17Z", "authors": ["Haoyuan He", "Yu Zheng", "Jie Zhou", "Jiwen Lu"], "pdf_url": "https://arxiv.org/pdf/2602.21508v1"}
{"id": "http://arxiv.org/abs/2602.21459v1", "title": "Regular Expression Denial of Service Induced by Backreferences", "summary": "This paper presents the first systematic study of denial-of-service vulnerabilities in Regular Expressions with Backreferences (REwB). We introduce the Two-Phase Memory Automaton (2PMFA), an automaton model that precisely captures REwB semantics. Using this model, we derive necessary conditions under which backreferences induce super-linear backtracking runtime, even when sink ambiguity is linear -- a regime where existing detectors report no vulnerability. Based on these conditions, we identify three vulnerability patterns, develop detection and attack-construction algorithms, and validate them in practice. Using the Snort intrusion detection ruleset, our evaluation identifies 45 previously unknown REwB vulnerabilities with quadratic or worse runtime. We further demonstrate practical exploits against Snort, including slowing rule evaluation by 0.6-1.2 seconds and bypassing alerts by triggering PCRE's matching limit.", "published": "2026-02-25T00:23:50Z", "updated": "2026-02-25T00:23:50Z", "authors": ["Yichen Liu", "Berk Çakar", "Aman Agrawal", "Minseok Seo", "James C. Davis", "Dongyoon Lee"], "pdf_url": "https://arxiv.org/pdf/2602.21459v1"}
