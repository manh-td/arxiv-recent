{"id": "http://arxiv.org/abs/2510.23483v2", "title": "Towards a Functionally Complete and Parameterizable TFHE Processor", "summary": "Fully homomorphic encryption allows the evaluation of arbitrary functions on encrypted data. It can be leveraged to secure outsourced and multiparty computation. TFHE is a fast torus-based fully homomorphic encryption scheme that allows both linear operations, as well as the evaluation of arbitrary non-linear functions. It currently provides the fastest bootstrapping operation performance of any other FHE scheme. Despite its fast performance, TFHE suffers from a considerably higher computational overhead for the evaluation of homomorphic circuits. Computations in the encrypted domain are orders of magnitude slower than their unencrypted equivalents. This bottleneck hinders the widespread adoption of (T)FHE for the protection of sensitive data. While state-of-the-art implementations focused on accelerating and outsourcing single operations, their scalability and practicality are constrained by high memory bandwidth costs. In order to overcome this, we propose an FPGA-based hardware accelerator for the evaluation of homomorphic circuits. Specifically, we design a functionally complete TFHE processor for FPGA hardware capable of processing instructions on the data completely on the FPGA. In order to achieve a higher throughput from our TFHE processor, we implement an improved programmable bootstrapping module, which outperforms the current state-of-the-art by 240% to 480% more bootstrappings per second. Our efficient, compact, and scalable design lays the foundation for implementing complete FPGA-based TFHE processor architectures.", "published": "2025-10-27T16:16:40Z", "updated": "2025-12-26T17:47:40Z", "authors": ["Valentin Reyes Häusler", "Gabriel Ott", "Aruna Jayasena", "Andreas Peter"], "pdf_url": "https://arxiv.org/pdf/2510.23483v2"}
{"id": "http://arxiv.org/abs/2512.22090v1", "title": "Abstraction of Trusted Execution Environments as the Missing Layer for Broad Confidential Computing Adoption: A Systematization of Knowledge", "summary": "Trusted Execution Environments (TEEs) protect sensitive code and data from the operating system, hypervisor, or other untrusted software. Different solutions exist, each proposing different features. Abstraction layers aim to unify the ecosystem, allowing application developers and system administrators to leverage confidential computing as broadly and efficiently as possible. We start with an overview of representative available TEE technologies. We describe and summarize each TEE ecosystem, classifying them in different categories depending on their main design choices. Then, we propose a systematization of knowledge focusing on different abstraction layers around each design choice. We describe the underlying technologies of each design, as well as the inner workings and features of each abstraction layer. Our study reveals opportunities for improving existing abstraction layer solutions. It also highlights WebAssembly, a promising approach that supports the largest set of features. We close with a discussion on future directions for research, such as how future abstraction layers may evolve and integrate with the confidential computing ecosystem.", "published": "2025-12-26T17:28:32Z", "updated": "2025-12-26T17:28:32Z", "authors": ["Quentin Michaud", "Sara Ramezanian", "Dhouha Ayed", "Olivier Levillain", "Joaquin Garcia-Alfaro"], "pdf_url": "https://arxiv.org/pdf/2512.22090v1"}
{"id": "http://arxiv.org/abs/2512.20323v2", "title": "Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms", "summary": "Wi-Fi/RF-based human sensing has achieved remarkable progress with deep learning, yet practical deployments increasingly require feature sharing for cloud analytics, collaborative training, or benchmark evaluation. Releasing intermediate representations such as CSI spectrograms can inadvertently expose sensitive information, including user identity, location, and membership, motivating formal privacy guarantees. In this paper, we study differentially private (DP) feature release for wireless sensing and propose an adaptive privacy budget allocation mechanism tailored to the highly non-uniform structure of CSI time-frequency representations. Our pipeline converts CSI to bounded spectrogram features, applies sensitivity control via clipping, estimates task-relevant importance over the time-frequency plane, and allocates a global privacy budget across spectrogram blocks before injecting calibrated Gaussian noise. Experiments on multi-user activity sensing (WiMANS), multi-person 3D pose estimation (Person-in-WiFi 3D), and respiration monitoring (Resp-CSI) show that adaptive allocation consistently improves the privacy-utility frontier over uniform perturbation under the same privacy budget. Our method yields higher accuracy and lower error while substantially reducing empirical leakage in identity and membership inference attacks.", "published": "2025-12-23T12:45:49Z", "updated": "2025-12-26T17:21:55Z", "authors": ["Ipek Sena Yilmaz", "Onur G. Tuncer", "Zeynep E. Aksoy", "Zeynep Yağmur Baydemir"], "pdf_url": "https://arxiv.org/pdf/2512.20323v2"}
{"id": "http://arxiv.org/abs/2512.22076v1", "title": "ReSMT: An SMT-Based Tool for Reverse Engineering", "summary": "Software obfuscation techniques make code more difficult\n  to understand, without changing its functionality. Such techniques\n  are often used by authors of malicious software to avoid\n  detection. Reverse Engineering\n  of obfuscated code, i.e., the process of overcoming obfuscation and\n  answering questions about the functionality of the code, is\n  notoriously difficult; and while various tools and methods exist for\n  this purpose, the process remains complex and slow, especially when\n  dealing with layered or customized obfuscation techniques.\n  Here, we present a novel, automated tool for addressing some of the\n  challenges in reverse engineering of obfuscated code. Our tool,\n  called ReSMT, converts the obfuscated assembly code into a complex\n  system of logical assertions that represent the code functionality,\n  and then applies SMT solving and simulation tools to inspect the\n  obfuscated code's execution. The approach is mostly automatic,\n  alleviating the need for highly specialized deobfuscation skills.\n  In an elaborate case study that we conducted, ReSMT successfully\n  tackled complex obfuscated code, and was able to solve reverse-engineering\n  queries about it. We believe that these results showcase the potential\n  and usefulness of our proposed approach.", "published": "2025-12-26T16:29:31Z", "updated": "2025-12-26T16:29:31Z", "authors": ["Nir Somech", "Guy Katz"], "pdf_url": "https://arxiv.org/pdf/2512.22076v1"}
{"id": "http://arxiv.org/abs/2502.20565v3", "title": "Communication-Efficient and Differentially Private Vertical Federated Learning with Zeroth-Order Optimization", "summary": "Vertical Federated Learning (VFL) enables collaborative model training across feature-partitioned devices, yet its reliance on device-server information exchange introduces significant communication overhead and privacy risks. Downlink communication from the server to devices in VFL exposes gradient-related signals of the global loss that can be leveraged in inference attacks. Existing privacy-preserving VFL approaches that inject differential privacy (DP) noise on the downlink have the natural repercussion of degraded gradient quality, slowed convergence, and excessive communication rounds. In this work, we propose DPZV, a communication-efficient and differentially private ZO-VFL framework with tunable privacy guarantees. Based on zeroth-order (ZO) optimization, DPZV injects calibrated scalar-valued DP noise on the downlink, significantly reducing variance amplification while providing equivalent protection against targeted inference attacks. Through rigorous theoretical analysis, we establish convergence guarantees comparable to first-order DP-SGD, despite relying solely on ZO estimators, and prove that DPZV satisfies $(ε, δ)$-DP. Extensive experiments demonstrate that DPZV consistently achieves a superior privacy-utility tradeoff and requires fewer communication rounds than existing DP-VFL baselines under strict privacy constraints ($ε\\leq 10$).", "published": "2025-02-27T22:07:16Z", "updated": "2025-12-26T16:03:55Z", "authors": ["Jianing Zhang", "Evan Chen", "Dong-Jun Han", "Chaoyue Liu", "Christopher G. Brinton"], "pdf_url": "https://arxiv.org/pdf/2502.20565v3"}
{"id": "http://arxiv.org/abs/2512.22060v1", "title": "Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management", "summary": "Natural Language Processing (NLP) systems are increasingly used in sensitive domains such as healthcare, finance, and government, where they handle large volumes of personal and regulated data. However, these systems introduce distinct risks related to security, privacy, and regulatory compliance that are not fully addressed by existing AI governance frameworks. This paper introduces the Secure and Compliant NLP Lifecycle Management Framework (SC-NLP-LMF), a comprehensive six-phase model designed to ensure the secure operation of NLP systems from development to retirement. The framework, developed through a systematic PRISMA-based review of 45 peer-reviewed and regulatory sources, aligns with leading standards, including NIST AI RMF, ISO/IEC 42001:2023, the EU AI Act, and MITRE ATLAS. It integrates established methods for bias detection, privacy protection (differential privacy, federated learning), secure deployment, explainability, and secure model decommissioning. A healthcare case study illustrates how SC-NLP-LMF detects emerging terminology drift (e.g., COVID-related language) and guides compliant model updates. The framework offers organizations a practical, lifecycle-wide structure for developing, deploying, and maintaining secure and accountable NLP systems in high-risk environments.", "published": "2025-12-26T15:28:20Z", "updated": "2025-12-26T15:28:20Z", "authors": ["Sunil Arora", "John Hastings"], "pdf_url": "https://arxiv.org/pdf/2512.22060v1"}
{"id": "http://arxiv.org/abs/2512.22046v1", "title": "Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models", "summary": "Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.", "published": "2025-12-26T14:48:58Z", "updated": "2025-12-26T14:48:58Z", "authors": ["Zongmin Zhang", "Zhen Sun", "Yifan Liao", "Wenhan Dong", "Xinlei He", "Xingshuo Han", "Shengmin Xu", "Xinyi Huang"], "pdf_url": "https://arxiv.org/pdf/2512.22046v1"}
{"id": "http://arxiv.org/abs/2508.01595v2", "title": "BeDKD: Backdoor Defense based on Dynamic Knowledge Distillation and Directional Mapping Modulator", "summary": "Although existing backdoor defenses have gained success in mitigating backdoor attacks, they still face substantial challenges. In particular, most of them rely on large amounts of clean data to weaken the backdoor mapping but generally struggle with residual trigger effects, resulting in persistently high attack success rates (ASR). Therefore, in this paper, we propose a novel Backdoor defense method based on Directional mapping module and adversarial Knowledge Distillation (BeDKD), which balances the trade-off between defense effectiveness and model performance using a small amount of clean and poisoned data. We first introduce a directional mapping module to identify poisoned data, which destroys clean mapping while keeping backdoor mapping on a small set of flipped clean data. Then, the adversarial knowledge distillation is designed to reinforce clean mapping and suppress backdoor mapping through a cycle iteration mechanism between trust and punish distillations using clean and identified poisoned data. We conduct experiments to mitigate mainstream attacks on three datasets, and experimental results demonstrate that BeDKD surpasses the state-of-the-art defenses and reduces the ASR by 98% without significantly reducing the CACC. Our code are available in https://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/BeDKD.", "published": "2025-08-03T05:28:01Z", "updated": "2025-12-26T12:28:29Z", "authors": ["Zhengxian Wu", "Juan Wen", "Wanli Peng", "Yinghan Zhou", "Changtong dou", "Yiming Xue"], "pdf_url": "https://arxiv.org/pdf/2508.01595v2"}
{"id": "http://arxiv.org/abs/2404.00287v2", "title": "Evaluating Large Language Models for Line-Level Vulnerability Localization", "summary": "Recently, Automated Vulnerability Localization (AVL) has attracted growing attention, aiming to facilitate diagnosis by pinpointing the specific lines of code responsible for vulnerabilities. Large Language Models (LLMs) have shown potential in various domains, yet their effectiveness in line-level vulnerability localization remains underexplored.\n  In this work, we present the first comprehensive empirical evaluation of LLMs for AVL. Our study examines 19 leading LLMs suitable for code analysis, including ChatGPT and multiple open-source models, spanning encoder-only, encoder-decoder, and decoder-only architectures, with model sizes from 60M to 70B parameters. We evaluate three paradigms including few-shot prompting, discriminative fine-tuning, and generative fine-tuning with and without Low-Rank Adaptation (LoRA), on both a BigVul-derived dataset for C/C++ and a smart contract vulnerability dataset.}\n  Our results show that discriminative fine-tuning achieves substantial performance gains over existing learning-based AVL methods when sufficient training data is available. In low-data settings, prompting advanced LLMs such as ChatGPT proves more effective. We also identify challenges related to input length and unidirectional context during fine-tuning, and propose two remedial strategies: a sliding window approach and right-forward embedding, both of which yield significant improvements. Moreover, we provide the first assessment of LLM generalizability in AVL, showing that certain models can transfer effectively across Common Weakness Enumerations (CWEs) and projects. However, performance degrades notably for newly discovered vulnerabilities containing unfamiliar lexical or structural patterns, underscoring the need for continual adaptation.", "published": "2024-03-30T08:42:10Z", "updated": "2025-12-26T08:03:04Z", "authors": ["Jian Zhang", "Chong Wang", "Anran Li", "Weisong Sun", "Cen Zhang", "Wei Ma", "Yang Liu"], "pdf_url": "https://arxiv.org/pdf/2404.00287v2"}
{"id": "http://arxiv.org/abs/2512.15144v3", "title": "MCPZoo: A Large-Scale Dataset of Runnable Model Context Protocol Servers for AI Agent", "summary": "Model Context Protocol (MCP) enables agents to interact with external tools, yet empirical research on MCP is hindered by the lack of large-scale, accessible datasets. We present MCPZoo, the largest and most comprehensive dataset of MCP servers collected from multiple public sources, comprising 129,059 servers (56,053 distinct). MCPZoo includes 16,356 server instances that have been deployed and verified as runnable and interactable, supporting realistic experimentation beyond static analysis. The dataset provides unified metadata and access interfaces, enabling systematic exploration and interaction without manual deployment effort. MCPZoo is released as an open and accessible resource to support research on MCP-based systems and security analysis.", "published": "2025-12-17T07:13:08Z", "updated": "2025-12-26T07:55:37Z", "authors": ["Mengying Wu", "Pei Chen", "Geng Hong", "Baichao An", "Jinsong Chen", "Binwang Wan", "Xudong Pan", "Jiarun Dai", "Min Yang"], "pdf_url": "https://arxiv.org/pdf/2512.15144v3"}
{"id": "http://arxiv.org/abs/2502.09990v3", "title": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability", "summary": "Despite the rapid development of safety alignment techniques for LLMs, defending against multi-turn jailbreaks is still a challenging task. In this paper, we conduct a comprehensive comparison, revealing that some existing defense methods can improve the robustness of LLMs against multi-turn jailbreaks but compromise usability, i.e., reducing general capabilities or causing the over-refusal problem. From the perspective of mechanism interpretability of LLMs, we discover that these methods fail to establish a boundary that exactly distinguishes safe and harmful feature representations. Therefore, boundary-safe representations close to harmful representations are inevitably disrupted, leading to a decline in usability. To address this issue, we propose X-Boundary to push harmful representations away from boundary-safe representations and obtain an exact distinction boundary. In this way, harmful representations can be precisely erased without disrupting safe ones. Experimental results show that X-Boundary achieves state-of-the-art defense performance against multi-turn jailbreaks, while reducing the over-refusal rate by about 20% and maintaining nearly complete general capability. Furthermore, we theoretically prove and empirically verify that X-Boundary can accelerate the convergence process during training. Please see our code at: https://github.com/AI45Lab/X-Boundary.", "published": "2025-02-14T08:22:51Z", "updated": "2025-12-26T07:50:47Z", "authors": ["Xiaoya Lu", "Dongrui Liu", "Yi Yu", "Luxin Xu", "Jing Shao"], "pdf_url": "https://arxiv.org/pdf/2502.09990v3"}
{"id": "http://arxiv.org/abs/2512.12324v2", "title": "UniMark: Artificial Intelligence Generated Content Identification Toolkit", "summary": "The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \\textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \\emph{Hidden Watermarking} for copyright protection and \\emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.", "published": "2025-12-13T13:30:48Z", "updated": "2025-12-26T07:22:58Z", "authors": ["Meilin Li", "Ji He", "Yi Yu", "Jia Xu", "Shanzhe Lei", "Yan Teng", "Yingchun Wang", "Xuhong Wang"], "pdf_url": "https://arxiv.org/pdf/2512.12324v2"}
{"id": "http://arxiv.org/abs/2512.21871v1", "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?", "summary": "Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.", "published": "2025-12-26T05:09:55Z", "updated": "2025-12-26T05:09:55Z", "authors": ["Naen Xu", "Jinghuai Zhang", "Changjiang Li", "Hengyu An", "Chunyi Zhou", "Jun Wang", "Boyu Xu", "Yuyuan Li", "Tianyu Du", "Shouling Ji"], "pdf_url": "https://arxiv.org/pdf/2512.21871v1"}
{"id": "http://arxiv.org/abs/2503.02176v3", "title": "Client-Aided Secure Two-Party Computation of Dynamic Controllers", "summary": "In this paper, we propose a secure two-party computation protocol for dynamic controllers using a secret sharing scheme. The proposed protocol realizes outsourcing of controller computation to two servers, while controller parameters, states, inputs, and outputs are kept secret against the servers. Unlike previous encrypted controls in a single-server setting, the proposed method can operate a dynamic controller for an infinite time horizon without controller state decryption or input re-encryption. We show that the control performance achievable by the proposed protocol can be made arbitrarily close to that attained by the unencrypted controller. Furthermore, system-theoretic and cryptographic modifications of the protocol are presented to improve the communication complexity. The feasibility of the protocol is demonstrated through numerical examples of PID and observer-based controls.", "published": "2025-03-04T01:35:47Z", "updated": "2025-12-26T03:28:15Z", "authors": ["Kaoru Teranishi", "Takashi Tanaka"], "pdf_url": "https://arxiv.org/pdf/2503.02176v3"}
{"id": "http://arxiv.org/abs/2512.21827v1", "title": "Securing Cross-Domain Internet of Drones: An RFF-PUF Allied Authenticated Key Exchange Protocol With Over-the-Air Enrollment", "summary": "The Internet of Drones (IoD) is an emerging and crucial paradigm enabling advanced applications that require seamless, secure communication across heterogeneous and untrusted domains. In such environments, access control and the transmission of sensitive data pose significant security challenges for IoD systems, necessitating the design of lightweight mutual authentication and key exchange protocols. Existing solutions are often hampered by high computation overhead, reliance on third parties, the requirement for secret storage in resource-constrained drones, and the need for a strictly controlled enrollment environment. These limitations make them impractical for dynamic cross-domain deployment. To address these limitations, we propose a lightweight mutual authentication mechanism that integrates Radio Frequency Fingerprint (RFF) and Physical Unclonable Function (PUF) technologies for secure drone-to-drone (D2D) and drone-to-ground station server (D2G) communication. RFF-based device identification is used to achieve over-the-air (OTA) enrollment, while the PUF serves as the root of trust for establishing mutual authentication among communication parties. Additionally, the on-the-fly key generation capability of the PUF is co-designed with One-Time-Pad (OTP) encryption to realize ephemeral keying and eliminate the need for storing secrets within drones. Both informal security analysis and ProVerif-based formal security verification comprehensively demonstrate the resilience of our protocol against common security attacks. The proposed protocol also outperforms existing IoD authentication schemes in terms of security features, as well as computation, communication, and storage overhead.", "published": "2025-12-26T02:04:24Z", "updated": "2025-12-26T02:04:24Z", "authors": ["Xuanyu Chen", "Yue Zheng", "Junqing Zhang", "Guanxiong Shen", "Chip-Hong Chang"], "pdf_url": "https://arxiv.org/pdf/2512.21827v1"}
{"id": "http://arxiv.org/abs/2512.21813v1", "title": "Organizational Learning in Industry 4.0: Applying Crossan's 4I Framework with Double Loop Learning", "summary": "The Advanced Dynamic Security Learning (DSL) Process Model is an Industry 4.0 cybersecurity incident response architecture proposed in this paper. This model addresses proactive and reflective cybersecurity governance across complex cyber-physical systems by combining Argyris and Schön's double-loop learning theory with Crossan's 4I organizational learning framework. Given that 65% of industrial companies suffer ransomware attacks annually and many of them lack cybersecurity awareness, this reveals the gravity of cyber threats. Feedforward and feedback learning loops in this paradigm help promote strategic transformation and ongoing growth. The DSL model helps Industry 4.0 organizations adapt to growing challenges posed by the projected 18.8 billion IoT devices by bridging operational obstacles and promoting systemic resilience. This research presents a scalable, methodical cybersecurity maturity approach based on a comprehensive analysis of the literature and a qualitative study.", "published": "2025-12-26T00:54:01Z", "updated": "2025-12-26T00:54:01Z", "authors": ["Nimra Akram", "Atif Ahmad", "Sean B Maynard"], "pdf_url": "https://arxiv.org/pdf/2512.21813v1"}
