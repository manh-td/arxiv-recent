{"id": "http://arxiv.org/abs/2602.09015v2", "title": "CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection", "summary": "Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The flexibility of malicious email attachments makes them stand out as a preferred vector for attackers as they can embed harmful content such as malware or malicious URLs inside standard document formats. Although phishing email defenses have improved a lot, attackers continue to abuse attachments, enabling malicious content to bypass security measures. Moreover, another challenge that researches face in training advance models, is lack of an unified and comprehensive dataset that covers the most prevalent data types. To address this gap, we generated CIC-Trap4Phish, a multi-format dataset containing both malicious and benign samples across five categories commonly used in phishing campaigns: Microsoft Word documents, Excel spreadsheets, PDF files, HTML pages, and QR code images. For the first four file types, a set of execution-free static feature pipeline was proposed, designed to capture structural, lexical, and metadata-based indicators without the need to open or execute files. Feature selection was performed using a combination of SHAP analysis and feature importance, yielding compact, discriminative feature subsets for each file type. The selected features were evaluated by using lightweight machine learning models, including Random Forest, XGBoost, and Decision Tree. All models demonstrate high detection accuracy across formats. For QR code-based phishing (quishing), two complementary methods were implemented: image-based detection by employing Convolutional Neural Networks (CNNs) and lexical analysis of decoded URLs using recent lightweight language models.", "published": "2026-02-09T18:57:00Z", "updated": "2026-02-10T15:11:35Z", "authors": ["Fatemeh Nejati", "Mahdi Rabbani", "Morteza Eskandarian", "Mansur Mirani", "Gunjan Piya", "Igor Opushnyev", "Ali A. Ghorbani", "Sajjad Dadkhah"], "pdf_url": "https://arxiv.org/pdf/2602.09015v2"}
{"id": "http://arxiv.org/abs/2505.15935v3", "title": "MAPS: A Multilingual Benchmark for Agent Performance and Security", "summary": "Agentic AI systems, which build on Large Language Models (LLMs) and interact with tools and memory, have rapidly advanced in capability and scope. Yet, since LLMs have been shown to struggle in multilingual settings, typically resulting in lower performance and reduced safety, agentic systems risk inheriting these limitations. This raises concerns about the accessibility of such systems, as users interacting in languages other than English may encounter unreliable or security-critical agent behavior. Despite growing interest in evaluating agentic AI and recent initial efforts toward multilingual interaction, existing benchmarks do not yet provide a comprehensive, multi-domain, security-aware evaluation of multilingual agentic systems. To address this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate agentic AI systems across diverse languages and tasks. MAPS builds on four widely used agentic benchmarks - GAIA (real-world tasks), SWE-Bench (code generation), MATH (mathematical reasoning), and the Agent Security Benchmark (security). We translate each dataset into eleven diverse languages, resulting in 805 unique tasks and 9,660 total language-specific instances - enabling a systematic analysis of the Multilingual Effect on AI agents' performance and robustness. Empirically, we observe a degradation in both performance and security when transitioning from English to other languages, with severity varying by task and correlating with the amount of translated input. This work establishes the first standardized evaluation framework for multilingual agentic AI, encouraging future research towards equitable, reliable, and accessible agentic AI. MAPS benchmark suite is publicly available at https://huggingface.co/datasets/Fujitsu-FRE/MAPS", "published": "2025-05-21T18:42:00Z", "updated": "2026-02-10T15:07:11Z", "authors": ["Omer Hofman", "Jonathan Brokman", "Oren Rachmil", "Shamik Bose", "Vikas Pahuja", "Toshiya Shimizu", "Trisha Starostina", "Kelly Marchisio", "Seraphina Goldfarb-Tarrant", "Roman Vainshtein"], "pdf_url": "https://arxiv.org/pdf/2505.15935v3"}
{"id": "http://arxiv.org/abs/2512.08493v2", "title": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?", "summary": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented vulnerability types. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance. Code repository is available here : https://github.com/DynaSoumhaneOuchebara/LLM-based-code-augmentation-Generate-or-Refactor-", "published": "2025-12-09T11:15:13Z", "updated": "2026-02-10T14:48:37Z", "authors": ["Dyna Soumhane Ouchebara", "Stéphane Dupont"], "pdf_url": "https://arxiv.org/pdf/2512.08493v2"}
{"id": "http://arxiv.org/abs/2602.09822v1", "title": "From Multi-sig to DLCs: Modern Oracle Designs on Bitcoin", "summary": "Unlike Ethereum, which was conceived as a general-purpose smart-contract platform, Bitcoin was designed primarily as a transaction ledger for its native currency, which limits programmability for conditional applications. This constraint is particularly evident when considering oracles, mechanisms that enable Bitcoin contracts to depend on exogenous events. This paper investigates whether new oracle designs have emerged for Bitcoin Layer 1 since the 2015 transition to the Ethereum smart contracts era and whether subsequent Bitcoin improvement proposals have expanded oracles' implementability. Using Scopus and Web of Science searches, complemented by Google Scholar to capture protocol proposals, we observe that the indexed academic coverage remains limited, and many contributions circulate outside journal venues. Within the retrieved corpus, the main post-2015 shift is from multisig-style, which envisioned oracles as co-signers, toward attestation-based designs, mainly represented by Discreet Log Contracts (DLCs), which show stronger Bitcoin community compliance, tool support, and evidence of practical implementations in real-world scenarios such as betting and prediction-market mechanisms.", "published": "2026-02-10T14:31:03Z", "updated": "2026-02-10T14:31:03Z", "authors": ["Giulio Caldarelli"], "pdf_url": "https://arxiv.org/pdf/2602.09822v1"}
{"id": "http://arxiv.org/abs/2602.09774v1", "title": "QRS: A Rule-Synthesizing Neuro-Symbolic Triad for Autonomous Vulnerability Discovery", "summary": "Static Application Security Testing (SAST) tools are integral to modern DevSecOps pipelines, yet tools like CodeQL, Semgrep, and SonarQube remain fundamentally constrained: they require expert-crafted queries, generate excessive false positives, and detect only predefined vulnerability patterns. Recent work has explored augmenting SAST with Large Language Models (LLMs), but these approaches typically use LLMs to triage existing tool outputs rather than to reason about vulnerability semantics directly. We introduce QRS (Query, Review, Sanitize), a neuro-symbolic framework that inverts this paradigm. Rather than filtering results from static rules, QRS employs three autonomous agents that generate CodeQL queries from a structured schema definition and few-shot examples, then validate findings through semantic reasoning and automated exploit synthesis. This architecture enables QRS to discover vulnerability classes beyond predefined patterns while substantially reducing false positives. We evaluate QRS on full Python packages rather than isolated snippets. In 20 historical CVEs in popular PyPI libraries, QRS achieves 90.6% detection accuracy. Applied to the 100 most-downloaded PyPI packages, QRS identified 39 medium-to-high-severity vulnerabilities, 5 of which were assigned new CVEs, 5 received documentation updates, while the remaining 29 were independently discovered by concurrent researchers, validating both the severity and discoverability of these findings. QRS accomplishes this with low time overhead and manageable token costs, demonstrating that LLM-driven query synthesis and code review can complement manually curated rule sets and uncover vulnerability patterns that evade existing industry tools.", "published": "2026-02-10T13:35:24Z", "updated": "2026-02-10T13:35:24Z", "authors": ["George Tsigkourakos", "Constantinos Patsakis"], "pdf_url": "https://arxiv.org/pdf/2602.09774v1"}
{"id": "http://arxiv.org/abs/2602.08668v2", "title": "Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion", "summary": "Hybrid Retrieval-Augmented Generation (RAG) pipelines combine vector similarity search with knowledge graph expansion for multi-hop reasoning. We show that this composition introduces a distinct security failure mode: a vector-retrieved \"seed\" chunk can pivot via entity links into sensitive graph neighborhoods, causing cross-tenant data leakage that does not occur in vector-only retrieval. We formalize this risk as Retrieval Pivot Risk (RPR) and introduce companion metrics Leakage@k, Amplification Factor, and Pivot Depth (PD) to quantify leakage magnitude and traversal structure.\n  We present seven Retrieval Pivot Attacks that exploit the vector-to-graph boundary and show that adversarial injection is not required: naturally shared entities create cross-tenant pivot paths organically. Across a synthetic multi-tenant enterprise corpus and the Enron email corpus, the undefended hybrid pipeline exhibits high pivot risk (RPR up to 0.95) with multiple unauthorized items returned per query. Leakage consistently appears at PD=2, which we attribute to the bipartite chunk-entity topology and formalize as a proposition.\n  We then show that enforcing authorization at a single location, the graph expansion boundary, eliminates measured leakage (RPR near 0) across both corpora, all attack variants, and label forgery rates up to 10 percent, with minimal overhead. Our results indicate the root cause is boundary enforcement, not inherently complex defenses: two individually secure retrieval components can compose into an insecure system unless authorization is re-checked at the transition point.", "published": "2026-02-09T13:55:04Z", "updated": "2026-02-10T13:19:20Z", "authors": ["Scott Thornton"], "pdf_url": "https://arxiv.org/pdf/2602.08668v2"}
{"id": "http://arxiv.org/abs/2112.06841v2", "title": "Information-Theoretic Limits of Quantum Learning via Data Compression", "summary": "Understanding the power of quantum data in machine learning is central to many proposed applications of quantum technologies. While access to quantum data can offer exponential advantages for carefully designed learning tasks and often under strong assumptions on the data distribution, it remains an open question whether such advantages persist in less structured settings and under more realistic, naturally occurring distributions. Motivated by these practical concerns, we introduce a systematic framework based on quantum lossy data compression to bound the power of quantum data in the context of probably approximately correct (PAC) learning. Specifically, we provide lower bounds on the sample complexity of quantum learners for arbitrary functions when data is drawn from Zipf's distribution, a widely used model for the empirical distributions of real-world data. We also establish lower bounds on the size of quantum input data required to learn linear functions, thereby proving the optimality of previous positive results. Beyond learning theory, we show that our framework has applications in secure delegated quantum computation within the measurement-based quantum computation (MBQC) model. In particular, we constrain the amount of private information the server can infer, strengthening the security guarantees of the delegation protocol proposed in (Mantri et al., PRX, 2017).", "published": "2021-12-13T17:57:30Z", "updated": "2026-02-10T12:15:05Z", "authors": ["Armando Angrisani", "Brian Coyle", "Elham Kashefi"], "pdf_url": "https://arxiv.org/pdf/2112.06841v2"}
{"id": "http://arxiv.org/abs/2602.09707v1", "title": "PiTPM: Partially Interactive Signatures for Multi-Device TPM Operations", "summary": "Trusted Platform Module (TPM) 2.0 devices provide efficient hardware-based cryptographic security through tamper-resistant key storage and computation, making them ideal building blocks for multi-party signature schemes in distributed systems. However, existing TPM-based multi-signature constructions suffer from a fundamental limitation, they require interactive protocols where all participants must coordinate during the commitment phase, before any signature can be computed. This interactive requirement creates several critical problems, such as synchronization bottlenecks, quadratic communication complexity, and aborted protocols as a result of participant failure. These limitations become particularly heightened for applications that require cross-device cryptographic operations. This paper presents PiTPM, an Aggregator Framework built upon Schnorr's digital signature. Our protocol eliminates the interactive requirement using a hybrid trust architecture. The proposed framework uses pre-shared randomness seeds stored securely in an Aggregator, enabling deterministic computation of global commitments without inter-participant communication. The resulting signatures of the proposed framework are of constant size regardless of signer count. Our experimental results show a possible paradigm shift in TPM-based cryptographic system design, demonstrating that hybrid trust architectures can achieve significant performance improvements while maintaining rigorous security guarantees. We provide a comprehensive formal security analysis proving EU-CMA security under the discrete logarithm assumption in the random oracle model.", "published": "2026-02-10T12:09:05Z", "updated": "2026-02-10T12:09:05Z", "authors": ["Yunusa Simpa Abdulsalam", "Mustapha Hedabou"], "pdf_url": "https://arxiv.org/pdf/2602.09707v1"}
{"id": "http://arxiv.org/abs/2602.09634v1", "title": "LLM-FS: Zero-Shot Feature Selection for Effective and Interpretable Malware Detection", "summary": "Feature selection (FS) remains essential for building accurate and interpretable detection models, particularly in high-dimensional malware datasets. Conventional FS methods such as Extra Trees, Variance Threshold, Tree-based models, Chi-Squared tests, ANOVA, Random Selection, and Sequential Attention rely primarily on statistical heuristics or model-driven importance scores, often overlooking the semantic context of features. Motivated by recent progress in LLM-driven FS, we investigate whether large language models (LLMs) can guide feature selection in a zero-shot setting, using only feature names and task descriptions, as a viable alternative to traditional approaches. We evaluate multiple LLMs (GPT-5.0, GPT-4.0, Gemini-2.5 etc.) on the EMBOD dataset (a fusion of EMBER and BODMAS benchmark datasets), comparing them against established FS methods across several classifiers, including Random Forest, Extra Trees, MLP, and KNN. Performance is assessed using accuracy, precision, recall, F1, AUC, MCC, and runtime. Our results demonstrate that LLM-guided zero-shot feature selection achieves competitive performance with traditional FS methods while offering additional advantages in interpretability, stability, and reduced dependence on labeled data. These findings position zero-shot LLM-based FS as a promising alternative strategy for effective and interpretable malware detection, paving the way for knowledge-guided feature selection in security-critical applications", "published": "2026-02-10T10:29:34Z", "updated": "2026-02-10T10:29:34Z", "authors": ["Naveen Gill", "Ajvad Haneef K", "Madhu Kumar S D"], "pdf_url": "https://arxiv.org/pdf/2602.09634v1"}
{"id": "http://arxiv.org/abs/2602.09629v1", "title": "Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks", "summary": "Large Language Models (LLMs) deploy safety mechanisms to prevent harmful outputs, yet these defenses remain vulnerable to adversarial prompts. While existing research demonstrates that jailbreak attacks succeed, it does not explain \\textit{where} defenses fail or \\textit{why}.\n  To address this gap, we propose that LLM safety operates as a sequential pipeline with distinct checkpoints. We introduce the \\textbf{Four-Checkpoint Framework}, which organizes safety mechanisms along two dimensions: processing stage (input vs.\\ output) and detection level (literal vs.\\ intent). This creates four checkpoints, CP1 through CP4, each representing a defensive layer that can be independently evaluated. We design 13 evasion techniques, each targeting a specific checkpoint, enabling controlled testing of individual defensive layers.\n  Using this framework, we evaluate GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across 3,312 single-turn, black-box test cases. We employ an LLM-as-judge approach for response classification and introduce Weighted Attack Success Rate (WASR), a severity-adjusted metric that captures partial information leakage overlooked by binary evaluation.\n  Our evaluation reveals clear patterns. Traditional Binary ASR reports 22.6\\% attack success. However, WASR reveals 52.7\\%, a 2.3$\\times$ higher vulnerability. Output-stage defenses (CP3, CP4) prove weakest at 72--79\\% WASR, while input-literal defenses (CP1) are strongest at 13\\% WASR. Claude achieves the strongest safety (42.8\\% WASR), followed by GPT-5 (55.9\\%) and Gemini (59.5\\%).\n  These findings suggest that current defenses are strongest at input-literal checkpoints but remain vulnerable to intent-level manipulation and output-stage techniques. The Four-Checkpoint Framework provides a structured approach for identifying and addressing safety vulnerabilities in deployed systems.", "published": "2026-02-10T10:17:25Z", "updated": "2026-02-10T10:17:25Z", "authors": ["Hayfa Dhabhi", "Kashyap Thimmaraju"], "pdf_url": "https://arxiv.org/pdf/2602.09629v1"}
{"id": "http://arxiv.org/abs/2602.09627v1", "title": "Parallel Composition for Statistical Privacy", "summary": "Differential Privacy (DP) considers a scenario in which an adversary has almost complete information about the entries of a database. This worst-case assumption is likely to overestimate the privacy threat faced by an individual in practice. In contrast, Statistical Privacy (SP), as well as related notions such as noiseless privacy or limited background knowledge privacy, describe a setting in which the adversary knows the distribution of the database entries, but not their exact realizations. In this case, privacy analysis must account for the interaction between uncertainty induced by the entropy of the underlying distributions and privacy mechanisms that distort query answers, which can be highly non-trivial.\n  This paper investigates this problem for multiple queries (composition). A privacy mechanism is proposed that is based on subsampling and randomly partitioning the database to bound the dependency among queries. This way for the first time, to the best of our knowledge, upper privacy bounds against limited adversaries are obtained without any further restriction on the database.\n  These bounds show that in realistic application scenarios taking the entropy of distributions into account yields improvements of privacy and precision guarantees. We illustrate examples where for fixed privacy parameters and utility loss SP allows significantly more queries than DP.", "published": "2026-02-10T10:13:44Z", "updated": "2026-02-10T10:13:44Z", "authors": ["Dennis Breutigam", "Rüdiger Reischuk"], "pdf_url": "https://arxiv.org/pdf/2602.09627v1"}
{"id": "http://arxiv.org/abs/2504.11730v2", "title": "Blockchain Application in Metaverse: A Review", "summary": "In recent years, the term Metaverse emerged as one of the most compelling concepts, captivating the interest of international companies such as Tencent, ByteDance, Microsoft, and Facebook. These company recognized the Metaverse as a pivotal element for future success and have since made significant investments in this area. The Metaverse is still in its developmental stages, requiring the integration and advancement of various technologies to bring its vision to life. One of the key technologies associated with the Metaverse is blockchain, known for its decentralization, security, trustworthiness, and ability to manage time-series data. These characteristics align perfectly with the ecosystem of the Metaverse, making blockchain foundational for its security and infrastructure. This paper introduces both blockchain and the Metaverse ecosystem while exploring the application of the blockchain within the Metaverse, including decentralization, consensus mechanisms, hash algorithms, timestamping, smart contracts, distributed storage, distributed ledgers, and non-fungible tokens (NFTs) to provide insights for researchers investigating these topics.", "published": "2025-04-16T03:07:35Z", "updated": "2026-02-10T10:04:39Z", "authors": ["Bingquan Jin", "Hailu Kuang", "Xiaoqi Li"], "pdf_url": "https://arxiv.org/pdf/2504.11730v2"}
{"id": "http://arxiv.org/abs/2602.09611v1", "title": "AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models", "summary": "Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\\% AUC) and robust attack resilience (at least 88.61\\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.", "published": "2026-02-10T10:02:29Z", "updated": "2026-02-10T10:02:29Z", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Yongyi Cui", "Gerard de Melo", "Linlin Wang"], "pdf_url": "https://arxiv.org/pdf/2602.09611v1"}
{"id": "http://arxiv.org/abs/2505.16439v2", "title": "Password Strength Detection via Machine Learning: Analysis, Modeling, and Evaluation", "summary": "As network security issues continue gaining prominence, password security has become crucial in safeguarding personal information and network systems. This study first introduces various methods for system password cracking, outlines password defense strategies, and discusses the application of machine learning in the realm of password security. Subsequently, we conduct a detailed public password database analysis, uncovering standard features and patterns among passwords. We extract multiple characteristics of passwords, including length, the number of digits, the number of uppercase and lowercase letters, and the number of special characters. We then experiment with six different machine learning algorithms: support vector machines, logistic regression, neural networks, decision trees, random forests, and stacked models, evaluating each model's performance based on various metrics, including accuracy, recall, and F1 score through model validation and hyperparameter tuning. The evaluation results on the test set indicate that decision trees and stacked models excel in accuracy, recall, and F1 score, making them a practical option for the strong and weak password classification task.", "published": "2025-05-22T09:27:40Z", "updated": "2026-02-10T10:02:18Z", "authors": ["Jiazhi Mo", "Hailu Kuang", "Xiaoqi Li"], "pdf_url": "https://arxiv.org/pdf/2505.16439v2"}
{"id": "http://arxiv.org/abs/2602.09606v1", "title": "When Handshakes Tell the Truth: Detecting Web Bad Bots via TLS Fingerprints", "summary": "Automated traffic continued to surpass human-generated traffic on the web, and a rising proportion of this automation was explicitly malicious. Evasive bots could pretend to be real users, even solve Captchas and mimic human interaction patterns. This work explores a less intrusive, protocol-level method: using TLS fingerprinting with the JA4 technique to tell apart bots from real users. Two gradient-boosted machine learning classifiers (XGBoost and CatBoost) were trained and evaluated on a dataset of real TLS fingerprints (JA4DB) after feature extraction, which derived informative signals from JA4 fingerprints that describe TLS handshake parameters. The CatBoost model performed better, achieving an AUC of 0.998 and an F1 score of 0.9734. It was accurate 0.9863 of the time on the test set. The XGBoost model showed almost similar results. Feature significance analyses identified JA4 components, especially ja4\\_b, cipher\\_count, and ext\\_count, as the most influential on model effectiveness. Future research will extend this method to new protocols, such as HTTP/3, and add additional device-fingerprinting features to test how well the system resists advanced bot evasion tactics.", "published": "2026-02-10T09:56:20Z", "updated": "2026-02-10T09:56:20Z", "authors": ["Ghalia Jarad", "Kemal Bicakci"], "pdf_url": "https://arxiv.org/pdf/2602.09606v1"}
{"id": "http://arxiv.org/abs/2602.09548v1", "title": "ReSIM: Re-ranking Binary Similarity Embeddings to Improve Function Search Performance", "summary": "Binary Function Similarity (BFS), the problem of determining whether two binary functions originate from the same source code, has been extensively studied in recent research across security, software engineering, and machine learning communities. This interest arises from its central role in developing vulnerability detection systems, copyright infringement analysis, and malware phylogeny tools. Nearly all binary function similarity systems embed assembly functions into real-valued vectors, where similar functions map to points that lie close to each other in the metric space. These embeddings enable function search: a query function is embedded and compared against a database of candidate embeddings to retrieve the most similar matches.\n  Despite their effectiveness, such systems rely on bi-encoder architectures that embed functions independently, limiting their ability to capture cross-function relationships and similarities. To address this limitation, we introduce ReSIM, a novel and enhanced function search system that complements embedding-based search with a neural re-ranker. Unlike traditional embedding models, our reranking module jointly processes query-candidate pairs to compute ranking scores based on their mutual representation, allowing for more accurate similarity assessment. By re-ranking the top results from embedding-based retrieval, ReSIM leverages fine-grained relation information that bi-encoders cannot capture.\n  We evaluate ReSIM across seven embedding models on two benchmark datasets, demonstrating consistent improvements in search effectiveness, with average gains of 21.7% in terms of nDCG and 27.8% in terms of Recall.", "published": "2026-02-10T08:57:49Z", "updated": "2026-02-10T08:57:49Z", "authors": ["Gianluca Capozzi", "Anna Paola Giancaspro", "Fabio Petroni", "Leonardo Querzoni", "Giuseppe Antonio Di Luna"], "pdf_url": "https://arxiv.org/pdf/2602.09548v1"}
{"id": "http://arxiv.org/abs/2602.09499v1", "title": "Computationally Efficient Replicable Learning of Parities", "summary": "We study the computational relationship between replicability (Impagliazzo et al. [STOC `22], Ghazi et al. [NeurIPS `21]) and other stability notions. Specifically, we focus on replicable PAC learning and its connections to differential privacy (Dwork et al. [TCC 2006]) and to the statistical query (SQ) model (Kearns [JACM `98]). Statistically, it was known that differentially private learning and replicable learning are equivalent and strictly more powerful than SQ-learning. Yet, computationally, all previously known efficient (i.e., polynomial-time) replicable learning algorithms were confined to SQ-learnable tasks or restricted distributions, in contrast to differentially private learning.\n  Our main contribution is the first computationally efficient replicable algorithm for realizable learning of parities over arbitrary distributions, a task that is known to be hard in the SQ-model, but possible under differential privacy. This result provides the first evidence that efficient replicable learning over general distributions strictly extends efficient SQ-learning, and is closer in power to efficient differentially private learning, despite computational separations between replicability and privacy. Our main building block is a new, efficient, and replicable algorithm that, given a set of vectors, outputs a subspace of their linear span that covers most of them.", "published": "2026-02-10T07:53:46Z", "updated": "2026-02-10T07:53:46Z", "authors": ["Moshe Noivirt", "Jessica Sorrell", "Eliad Tsfadia"], "pdf_url": "https://arxiv.org/pdf/2602.09499v1"}
{"id": "http://arxiv.org/abs/2602.07513v2", "title": "SPECA: Specification-to-Checklist Agentic Auditing for Multi-Implementation Systems -- A Case Study on Ethereum Clients", "summary": "Multi-implementation systems are increasingly audited against natural-language specifications. Differential testing scales well when implementations disagree, but it provides little signal when all implementations converge on the same incorrect interpretation of an ambiguous requirement. We present SPECA, a Specification-to-Checklist Auditing framework that turns normative requirements into checklists, maps them to implementation locations, and supports cross-implementation reuse.\n  We instantiate SPECA in an in-the-wild security audit contest for the Ethereum Fusaka upgrade, covering 11 production clients. Across 54 submissions, 17 were judged valid by the contest organizers. Cross-implementation checks account for 76.5 percent (13 of 17) of valid findings, suggesting that checklist-derived one-to-many reuse is a practical scaling mechanism in multi-implementation audits. To understand false positives, we manually coded the 37 invalid submissions and find that threat model misalignment explains 56.8 percent (21 of 37): reports that rely on assumptions about trust boundaries or scope that contradict the audit's rules. We detected no High or Medium findings in the V1 deployment; misses concentrated in specification details and implicit assumptions (57.1 percent), timing and concurrency issues (28.6 percent), and external library dependencies (14.3 percent). Our improved agent, evaluated against the ground truth of a competitive audit, achieved a strict recall of 27.3 percent on high-impact vulnerabilities, placing it in the top 4 percent of human auditors and outperforming 49 of 51 contestants on critical issues. These results, though from a single deployment, suggest that early, explicit threat modeling is essential for reducing false positives and focusing agentic auditing effort. The agent-driven process enables expert validation and submission in about 40 minutes on average.", "published": "2026-02-07T12:19:00Z", "updated": "2026-02-10T07:04:48Z", "authors": ["Masato Kamba", "Akiyoshi Sannai"], "pdf_url": "https://arxiv.org/pdf/2602.07513v2"}
{"id": "http://arxiv.org/abs/2602.09434v1", "title": "A Behavioral Fingerprint for Large Language Models: Provenance Tracking via Refusal Vectors", "summary": "Protecting the intellectual property of large language models (LLMs) is a critical challenge due to the proliferation of unauthorized derivative models. We introduce a novel fingerprinting framework that leverages the behavioral patterns induced by safety alignment, applying the concept of refusal vectors for LLM provenance tracking. These vectors, extracted from directional patterns in a model's internal representations when processing harmful versus harmless prompts, serve as robust behavioral fingerprints. Our contribution lies in developing a fingerprinting system around this concept and conducting extensive validation of its effectiveness for IP protection. We demonstrate that these behavioral fingerprints are highly robust against common modifications, including finetunes, merges, and quantization. Our experiments show that the fingerprint is unique to each model family, with low cosine similarity between independently trained models. In a large-scale identification task across 76 offspring models, our method achieves 100\\% accuracy in identifying the correct base model family. Furthermore, we analyze the fingerprint's behavior under alignment-breaking attacks, finding that while performance degrades significantly, detectable traces remain. Finally, we propose a theoretical framework to transform this private fingerprint into a publicly verifiable, privacy-preserving artifact using locality-sensitive hashing and zero-knowledge proofs.", "published": "2026-02-10T05:57:35Z", "updated": "2026-02-10T05:57:35Z", "authors": ["Zhenyu Xu", "Victor S. Sheng"], "pdf_url": "https://arxiv.org/pdf/2602.09434v1"}
{"id": "http://arxiv.org/abs/2602.09433v1", "title": "Autonomous Action Runtime Management(AARM):A System Specification for Securing AI-Driven Actions at Runtime", "summary": "As artificial intelligence systems evolve from passive assistants into autonomous agents capable of executing consequential actions, the security boundary shifts from model outputs to tool execution. Traditional security paradigms - log aggregation, perimeter defense, and post-hoc forensics - cannot protect systems where AI-driven actions are irreversible, execute at machine speed, and originate from potentially compromised orchestration layers. This paper introduces Autonomous Action Runtime Management (AARM), an open specification for securing AI-driven actions at runtime. AARM defines a runtime security system that intercepts actions before execution, accumulates session context, evaluates against policy and intent alignment, enforces authorization decisions, and records tamper-evident receipts for forensic reconstruction. We formalize a threat model addressing prompt injection, confused deputy attacks, data exfiltration, and intent drift. We introduce an action classification framework distinguishing forbidden, context-dependent deny, and context-dependent allow actions. We propose four implementation architectures - protocol gateway, SDK instrumentation, kernel eBPF, and vendor integration - with distinct trust properties, and specify minimum conformance requirements for AARM-compliant systems. AARM is model-agnostic, framework-agnostic, and vendor-neutral, treating action execution as the stable security boundary. This specification aims to establish industry-wide requirements before proprietary fragmentation forecloses interoperability.", "published": "2026-02-10T05:57:30Z", "updated": "2026-02-10T05:57:30Z", "authors": ["Herman Errico"], "pdf_url": "https://arxiv.org/pdf/2602.09433v1"}
{"id": "http://arxiv.org/abs/2602.09431v1", "title": "Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models", "summary": "Large vision-language models (LVLMs) have achieved impressive success across multimodal tasks, but their reliance on visual inputs exposes them to significant adversarial threats. Existing encoder-based attacks perturb the input image by optimizing solely on the vision encoder, rather than the entire LVLM, offering a computationally efficient alternative to end-to-end optimization. However, their transferability across different LVLM architectures in realistic black-box scenarios remains poorly understood. To address this gap, we present the first systematic study towards encoder-based adversarial transferability in LVLMs. Our contributions are threefold. First, through large-scale benchmarking over eight diverse LVLMs, we reveal that existing attacks exhibit severely limited transferability. Second, we perform in-depth analysis, disclosing two root causes that hinder the transferability: (1) inconsistent visual grounding across models, where different models focus their attention on distinct regions; (2) redundant semantic alignment within models, where a single object is dispersed across multiple overlapping token representations. Third, we propose Semantic-Guided Multimodal Attack (SGMA), a novel framework to enhance the transferability. Inspired by the discovered causes in our analysis, SGMA directs perturbations toward semantically critical regions and disrupts cross-modal grounding at both global and local levels. Extensive experiments across different victim models and tasks show that SGMA achieves higher transferability than existing attacks. These results expose critical security risks in LVLM deployment and underscore the urgent need for robust multimodal defenses.", "published": "2026-02-10T05:51:02Z", "updated": "2026-02-10T05:51:02Z", "authors": ["Xinwei Zhang", "Li Bai", "Tianwei Zhang", "Youqian Zhang", "Qingqing Ye", "Yingnan Zhao", "Ruochen Du", "Haibo Hu"], "pdf_url": "https://arxiv.org/pdf/2602.09431v1"}
{"id": "http://arxiv.org/abs/2411.16598v5", "title": "DiffBreak: Is Diffusion-Based Purification Robust?", "summary": "Diffusion-based purification (DBP) has become a cornerstone defense against adversarial examples (AEs), regarded as robust due to its use of diffusion models (DMs) that project AEs onto the natural data manifold. We refute this core claim, theoretically proving that gradient-based attacks effectively target the DM rather than the classifier, causing DBP's outputs to align with adversarial distributions. This prompts a reassessment of DBP's robustness, attributing it to two critical flaws: incorrect gradients and inappropriate evaluation protocols that test only a single random purification of the AE. We show that with proper accounting for stochasticity and resubmission risk, DBP collapses. To support this, we introduce DiffBreak, the first reliable toolkit for differentiation through DBP, eliminating gradient flaws that previously further inflated robustness estimates. We also analyze the current defense scheme used for DBP where classification relies on a single purification, pinpointing its inherent invalidity. We provide a statistically grounded majority-vote (MV) alternative that aggregates predictions across multiple purified copies, showing partial but meaningful robustness gain. We then propose a novel adaptation of an optimization method against deepfake watermarking, crafting systemic perturbations that defeat DBP even under MV, challenging DBP's viability.", "published": "2024-11-25T17:30:32Z", "updated": "2026-02-10T05:30:35Z", "authors": ["Andre Kassis", "Urs Hengartner", "Yaoliang Yu"], "pdf_url": "https://arxiv.org/pdf/2411.16598v5"}
{"id": "http://arxiv.org/abs/2601.20917v3", "title": "FIPS 204-Compatible Threshold ML-DSA via Masked Lagrange Reconstruction", "summary": "We present masked Lagrange reconstruction, a technique enabling threshold ML-DSA (FIPS 204) with arbitrary thresholds while producing standard 3.3 KB signatures verifiable by unmodified implementations. Existing approaches face fundamental limitations: noise flooding yields 5x larger signatures breaking FIPS compatibility, while compact schemes are restricted to small thresholds (T <= 8) due to the Ball-Cakan-Malkin bound on binary-coefficient secret sharing.\n  Our technique overcomes the barrier that Lagrange coefficients grow as Theta(q) for moderate T, making individual contributions too large for rejection sampling. We introduce pairwise-canceling masks that hide these coefficients while preserving correctness. Unlike ECDSA threshold schemes, ML-DSA presents three additional challenges: the ||z||_inf check must pass after masking, the r0-check must not leak cs_2, and the resulting Irwin-Hall nonce distribution must preserve EUF-CMA security. We solve all three with complete UC proofs.\n  We instantiate this technique in three deployment profiles. Profile P1 uses a TEE coordinator (3 rounds, 6ms). Profile P2 eliminates hardware trust via MPC (5 rounds, 22ms for small thresholds, up to 194ms for T=16). Profile P3+ uses lightweight 2PC with semi-asynchronous signing where signers precompute nonces offline and respond within a time window (2 logical rounds, 22ms), similar to FROST.\n  Our Rust implementation scales from 2-of-3 to 32-of-45 thresholds. P1 and P3+ achieve sub-100ms latency for all configurations; P2 trades higher latency for eliminating hardware trust. Success rates of approximately 21-45% are comparable to single-signer ML-DSA's approximately 20-25%.", "published": "2026-01-28T18:13:47Z", "updated": "2026-02-10T05:00:49Z", "authors": ["Leo Kao"], "pdf_url": "https://arxiv.org/pdf/2601.20917v3"}
{"id": "http://arxiv.org/abs/2602.09392v1", "title": "LLMAC: A Global and Explainable Access Control Framework with Large Language Model", "summary": "Today's business organizations need access control systems that can handle complex, changing security requirements that go beyond what traditional methods can manage. Current approaches, such as Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC), and Discretionary Access Control (DAC), were designed for specific purposes. They cannot effectively manage the dynamic, situation-dependent workflows that modern systems require. In this research, we introduce LLMAC, a new unified approach using Large Language Models (LLMs) to combine these different access control methods into one comprehensive, understandable system. We used an extensive synthetic dataset that represents complex real-world scenarios, including policies for ownership verification, version management, workflow processes, and dynamic role separation. Using Mistral 7B, our trained LLM model achieved outstanding results with 98.5% accuracy, significantly outperforming traditional methods (RBAC: 14.5%, ABAC: 58.5%, DAC: 27.5%) while providing clear, human readable explanations for each decision. Performance testing shows that the system can be practically deployed with reasonable response times and computing resources.", "published": "2026-02-10T04:02:11Z", "updated": "2026-02-10T04:02:11Z", "authors": ["Sharif Noor Zisad", "Ragib Hasan"], "pdf_url": "https://arxiv.org/pdf/2602.09392v1"}
{"id": "http://arxiv.org/abs/2602.09369v1", "title": "Timing and Memory Telemetry on GPUs for AI Governance", "summary": "The rapid expansion of GPU-accelerated computing has enabled major advances in large-scale artificial intelligence (AI), while heightening concerns about how accelerators are observed or governed once deployed. Governance is essential to ensure that large-scale compute infrastructure is not silently repurposed for training models, circumventing usage policies, or operating outside legal oversight. Because current GPUs expose limited trusted telemetry and can be modified or virtualized by adversaries, we explore whether compute-based measurements can provide actionable signals of utilization when host and device are untrusted. We introduce a measurement framework that leverages architectural characteristics of modern GPUs to generate timing- and memory-based observables that correlate with compute activity. Our design draws on four complementary primitives: (1) a probabilistic, workload-driven mechanism inspired by Proof-of-Work (PoW) to expose parallel effort, (2) sequential, latency-sensitive workloads derived via Verifiable Delay Functions (VDFs) to characterize scalar execution pressure, (3) General Matrix Multiplication (GEMM)-based tensor-core measurements that reflect dense linear-algebra throughput, and (4) a VRAM-residency test that distinguishes on-device memory locality from off-chip access through bandwidth-dependent hashing. These primitives provide statistical and behavioral indicators of GPU engagement that remain observable even without trusted firmware, enclaves, or vendor-controlled counters. We evaluate their responses to contention, architectural alignment, memory pressure, and power overhead, showing that timing shifts and residency latencies reveal meaningful utilization patterns. Our results illustrate why compute-based telemetry can complement future accountability mechanisms by exposing architectural signals relevant to post-deployment GPU governance.", "published": "2026-02-10T03:20:06Z", "updated": "2026-02-10T03:20:06Z", "authors": ["Saleh K. Monfared", "Fatemeh Ganji", "Dan Holcomb", "Shahin Tajik"], "pdf_url": "https://arxiv.org/pdf/2602.09369v1"}
{"id": "http://arxiv.org/abs/2602.09357v1", "title": "Data Sharing with Endogenous Choices over Differential Privacy Levels", "summary": "We study coalition formation for data sharing under differential privacy when agents have heterogeneous privacy costs. Each agent holds a sensitive data point and decides whether to participate in a data-sharing coalition and how much noise to add to their data. Privacy choices induce a fundamental trade-off: higher privacy reduces individual data-sharing costs but degrades data utility and statistical accuracy for the coalition. These choices generate externalities across agents, making both participation and privacy levels strategic. Our goal is to understand which coalitions are stable, how privacy choices shape equilibrium outcomes, and how decentralized data sharing compares to a centralized, socially optimal benchmark.\n  We provide a comprehensive equilibrium analysis across a broad range of privacy-cost regimes, from decreasing costs (e.g., privacy amplification from pooling data) to increasing costs (e.g., greater exposure to privacy attacks in larger coalitions). We first characterize Nash equilibrium coalitions with endogenous privacy levels and show that equilibria may fail to exist and can be non-monotonic in problem parameters. We also introduce a weaker equilibrium notion called robust equilibrium (that allows more widespread equilibrium existence by equipping existing players in the coalition with the power to prevent or veto external players from joining) and fully characterize such equilibria. Finally, we analyze, for both Nash and robust equilibria, the efficiency relative to the social optimum in terms of social welfare and estimator accuracy. We derive bounds that depend sharply on the number of players, properties of the cost profile and how privacy costs scale with coalition size.", "published": "2026-02-10T03:01:14Z", "updated": "2026-02-10T03:01:14Z", "authors": ["Raef Bassily", "Kate Donahue", "Diptangshu Sen", "Annuo Zhao", "Juba Ziani"], "pdf_url": "https://arxiv.org/pdf/2602.09357v1"}
{"id": "http://arxiv.org/abs/2602.09338v1", "title": "Privacy Amplification for BandMF via $b$-Min-Sep Subsampling", "summary": "We study privacy amplification for BandMF, i.e., DP-SGD with correlated noise across iterations via a banded correlation matrix. We propose $b$-min-sep subsampling, a new subsampling scheme that generalizes Poisson and balls-in-bins subsampling, extends prior practical batching strategies for BandMF, and enables stronger privacy amplification than cyclic Poisson while preserving the structural properties needed for analysis. We give a near-exact privacy analysis using Monte Carlo accounting, based on a dynamic program that leverages the Markovian structure in the subsampling procedure. We show that $b$-min-sep matches cyclic Poisson subsampling in the high noise regime and achieves strictly better guarantees in the mid-to-low noise regime, with experimental results that bolster our claims. We further show that unlike previous BandMF subsampling schemes, our $b$-min-sep subsampling naturally extends to the multi-attribution user-level privacy setting.", "published": "2026-02-10T02:10:38Z", "updated": "2026-02-10T02:10:38Z", "authors": ["Andy Dong", "Arun Ganesh"], "pdf_url": "https://arxiv.org/pdf/2602.09338v1"}
{"id": "http://arxiv.org/abs/2602.09333v1", "title": "XMap: Fast Internet-wide IPv4 and IPv6 Network Scanner", "summary": "XMap is an open-source network scanner designed for performing fast Internet-wide IPv4 and IPv6 network research scanning. XMap was initially developed as the research artifact of a paper published at 2021 IEEE/IFIP International Conference on Dependable Systems and Networks (DSN '21) and then made available on GitHub. XMap is the first tool to support fast Internet-wide IPv6 network scanning in 2020. During the last five years, XMap has made substantial impact in academia, industry, and government. It has been referenced in 52 research papers (15 published at top-tier security venues and 11 in leading networking societies), received over 450 GitHub stars, featured in multiple news outlets, and deployed or recommended by international companies up to date. Additionally, XMap has contributed to the implementation of RFC documents and the discovery of various vulnerabilities. This paper provides fundamental details about XMap, its architecture, and its impact.", "published": "2026-02-10T02:06:48Z", "updated": "2026-02-10T02:06:48Z", "authors": ["Xiang Li", "Zixuan Xie", "Lu Sun", "Yuqi Qiu", "Zuyao Xu", "Zheli Liu"], "pdf_url": "https://arxiv.org/pdf/2602.09333v1"}
{"id": "http://arxiv.org/abs/2602.09319v1", "title": "Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation", "summary": "Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.", "published": "2026-02-10T01:27:46Z", "updated": "2026-02-10T01:27:46Z", "authors": ["Zhisheng Qi", "Utkarsh Sahu", "Li Ma", "Haoyu Han", "Ryan Rossi", "Franck Dernoncourt", "Mahantesh Halappanavar", "Nesreen Ahmed", "Yushun Dong", "Yue Zhao", "Yu Zhang", "Yu Wang"], "pdf_url": "https://arxiv.org/pdf/2602.09319v1"}
