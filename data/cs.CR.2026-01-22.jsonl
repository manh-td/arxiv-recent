{"id": "http://arxiv.org/abs/2601.16199v1", "title": "PAL*M: Property Attestation for Large Generative Models", "summary": "Machine learning property attestations allow provers (e.g., model providers or owners) to attest properties of their models/datasets to verifiers (e.g., regulators, customers), enabling accountability towards regulations and policies. But, current approaches do not support generative models or large datasets. We present PAL*M, a property attestation framework for large generative models, illustrated using large language models. PAL*M defines properties across training and inference, leverages confidential virtual machines with security-aware GPUs for coverage of CPU-GPU operations, and proposes using incremental multiset hashing over memory-mapped datasets to efficiently track their integrity. We implement PAL*M on Intel TDX and NVIDIA H100, showing it is efficient, scalable, versatile, and secure.", "published": "2026-01-22T18:51:13Z", "updated": "2026-01-22T18:51:13Z", "authors": ["Prach Chantasantitam", "Adam Ilyas Caulfield", "Vasisht Duddu", "Lachlan J. Gunn", "N. Asokan"], "pdf_url": "https://arxiv.org/pdf/2601.16199v1"}
{"id": "http://arxiv.org/abs/2601.16160v1", "title": "CONTEX-T: Contextual Privacy Exploitation via Transformer Spectral Analysis for IoT Device Fingerprinting", "summary": "The rapid expansion of internet of things (IoT) devices have created a pervasive ecosystem where encrypted wireless communications serve as the primary privacy and security protection mechanism. While encryption effectively protects message content, packet metadata and statistics inadvertently expose device identities and user contexts. Various studies have exploited raw packet statistics and their visual representations for device fingerprinting and identification. However, these approaches remain confined to the spatial domain with limited feature representation. Therefore, this paper presents CONTEX-T, a novel framework that exploits contextual privacy vulnerabilities using spectral representation of encrypted wireless traffic for IoT device characterization. The experiments show that spectral analysis provides new and rich feature representation for covert reconnaissance attacks, revealing a complex and expanding threat landscape that would require robust countermeasures for IoT security management. CONTEXT-T first transforms raw packet length sequences into time-frequency spectral representations and then utilizes transformer-based spectral analysis for the device identification. We systematically evaluated multiple spectral representation techniques and transformer-based models across encrypted traffic samples from various IoT devices. CONTEXT-T effectively exploited privacy vulnerabilities and achieved device classification accuracy exceeding 99% across all devices while remaining completely passive and undetectable.", "published": "2026-01-22T18:03:34Z", "updated": "2026-01-22T18:03:34Z", "authors": ["Nazmul Islam", "Mohammad Zulkernine"], "pdf_url": "https://arxiv.org/pdf/2601.16160v1"}
{"id": "http://arxiv.org/abs/2510.05766v2", "title": "New Insights into Involutory and Orthogonal MDS Matrices", "summary": "MDS matrices play a critical role in the design of diffusion layers for block ciphers and hash functions due to their optimal branch number. Involutory and orthogonal MDS matrices offer additional benefits by allowing identical or nearly identical circuitry for both encryption and decryption, leading to equivalent implementation costs for both processes. These properties have been further generalized through the notions of semi-involutory and semi-orthogonal matrices. While much of the existing literature focuses on identifying efficiently implementable MDS candidates or proposing new constructions for MDS matrices of various orders, this work takes a different direction. Rather than introducing novel constructions or prioritizing implementation efficiency, we investigate structural relationships between the generalized variants and their conventional counterparts. Specifically, we establish nontrivial interconnections between semi-involutory and involutory matrices, as well as between semi-orthogonal and orthogonal matrices. Exploiting these relationships, we show that the number of semi-involutory MDS matrices can be directly derived from the number of involutory MDS matrices, and vice versa. A similar correspondence holds for semi-orthogonal and orthogonal MDS matrices. We also examine the intersection of these classes and show that the number of $3 \\times 3$ MDS matrices that are both semi-involutory and semi-orthogonal coincides with the number of semi-involutory MDS matrices over $\\mathbb{F}_{2^m}$. Furthermore, we derive the general structure of orthogonal matrices of arbitrary order $n$ over $\\mathbb{F}_{2^m}$. Finally, leveraging the aforementioned interconnections, we present an alternative and direct derivation of the explicit formulae for counting $3 \\times 3$ semi-involutory MDS matrices and $3 \\times 3$ semi-orthogonal MDS matrices.", "published": "2025-10-07T10:35:20Z", "updated": "2026-01-22T17:51:37Z", "authors": ["Yogesh Kumar", "Susanta Samanta", "Atul Gaur"], "pdf_url": "https://arxiv.org/pdf/2510.05766v2"}
{"id": "http://arxiv.org/abs/2601.16140v1", "title": "Learning to Watermark in the Latent Space of Generative Models", "summary": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.", "published": "2026-01-22T17:34:30Z", "updated": "2026-01-22T17:34:30Z", "authors": ["Sylvestre-Alvise Rebuffi", "Tuan Tran", "Valeriu Lacatusu", "Pierre Fernandez", "Tomáš Souček", "Nikola Jovanović", "Tom Sander", "Hady Elsahar", "Alexandre Mourachko"], "pdf_url": "https://arxiv.org/pdf/2601.16140v1"}
{"id": "http://arxiv.org/abs/2601.05180v2", "title": "The Adverse Effects of Omitting Records in Differential Privacy: How Sampling and Suppression Degrade the Privacy--Utility Tradeoff (Long Version)", "summary": "Sampling is renowned for its privacy amplification in differential privacy (DP), and is often assumed to improve the utility of a DP mechanism by allowing a noise reduction. In this paper, we further show that this last assumption is flawed: When measuring utility at equal privacy levels, sampling as preprocessing consistently yields penalties due to utility loss from omitting records over all canonical DP mechanisms -- Laplace, Gaussian, exponential, and report noisy max -- , as well as recent applications of sampling, such as clustering.\n  Extending this analysis, we investigate suppression as a generalized method of choosing, or omitting, records. Developing a theoretical analysis of this technique, we derive privacy bounds for arbitrary suppression strategies under unbounded approximate DP. We find that our tested suppression strategy also fails to improve the privacy--utility tradeoff. Surprisingly, uniform sampling emerges as one of the best suppression methods -- despite its still degrading effect. Our results call into question common preprocessing assumptions in DP practice.", "published": "2026-01-08T18:03:57Z", "updated": "2026-01-22T17:15:02Z", "authors": ["Àlex Miranda-Pascual", "Javier Parra-Arnau", "Thorsten Strufe"], "pdf_url": "https://arxiv.org/pdf/2601.05180v2"}
{"id": "http://arxiv.org/abs/2410.06692v4", "title": "How hard can it be? Quantifying MITRE attack campaigns with attack trees and cATM logic", "summary": "The landscape of cyber threats grows more complex by the day. Advanced Persistent Threats carry out attack campaigns - e.g. operations Dream Job, Wocao, and WannaCry - against which cybersecurity practitioners must defend. To prioritise which of these to defend against, cybersecurity experts must be equipped with the right toolbox to evaluate the most threatening ones. In particular, they would strongly benefit from (a) an estimation of the likelihood values for each attack recorded in the wild, and (b) transparently operationalising these values to compare campaigns quantitatively. Security experts could then perform transparent and accountable quantitatively-informed decisions. Here we construct such a framework: (1) quantifying the likelihood of attack campaigns via data-driven procedures on the MITRE knowledge-base, (2) introducing a methodology for automatic modelling of MITRE intelligence data, that captures any attack campaign via template attack tree models, and (3) proposing an open-source tool to perform these comparisons based on the cATM logic. Finally, we quantify the likelihood of all MITRE Enterprise campaigns, and compare the likelihood of the Wocao and Dream Job MITRE campaigns - generated with our proposed approach - against manually-built attack tree models. We demonstrate how our methodology is substantially lighter in modelling effort, and capable of capturing all the quantitative relevant data.", "published": "2024-10-09T08:52:58Z", "updated": "2026-01-22T14:57:23Z", "authors": ["Stefano M. Nicoletti", "Milan Lopuhaä-Zwakenberg", "Mariëlle Stoelinga", "Fabio Massacci", "Carlos E. Budde"], "pdf_url": "https://arxiv.org/pdf/2410.06692v4"}
{"id": "http://arxiv.org/abs/2508.20424v3", "title": "Attacks on Approximate Caches in Text-to-Image Diffusion Models", "summary": "Diffusion models are a powerful class of generative models that produce images and other content from user prompts, but they are computationally intensive. To mitigate this cost, recent academic and industry work has adopted approximate caching, which reuses intermediate states from similar prompts in a cache. While efficient, this optimization introduces new security risks by breaking isolation among users. This paper provides a comprehensive assessment of the security vulnerabilities introduced by approximate caching. First, we demonstrate a remote covert channel established with the approximate cache, where a sender injects prompts with special keywords into the cache system and a receiver can recover that even after days, to exchange information. Second, we introduce a prompt stealing attack using the approximate cache, where an attacker can recover existing cached prompts from hits. Finally, we introduce a poisoning attack that embeds the attacker's logos into the previously stolen prompt, leading to unexpected logo rendering for the requests that hit the poisoned cache prompts. These attacks are all performed remotely through the serving system, demonstrating severe security vulnerabilities in approximate caching. The code for this work is available.", "published": "2025-08-28T04:46:44Z", "updated": "2026-01-22T14:44:17Z", "authors": ["Desen Sun", "Shuncheng Jie", "Sihang Liu"], "pdf_url": "https://arxiv.org/pdf/2508.20424v3"}
{"id": "http://arxiv.org/abs/2512.20712v2", "title": "Real-World Adversarial Attacks on RF-Based Drone Detectors", "summary": "Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.", "published": "2025-12-23T19:19:45Z", "updated": "2026-01-22T12:36:23Z", "authors": ["Omer Gazit", "Yael Itzhakev", "Yuval Elovici", "Asaf Shabtai"], "pdf_url": "https://arxiv.org/pdf/2512.20712v2"}
{"id": "http://arxiv.org/abs/2509.21772v2", "title": "PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation", "summary": "Phishing attacks are a significant societal threat, disproportionately harming vulnerable populations and eroding trust in essential digital services. Current defenses are often reactive, failing against modern evasive tactics like cloaking that conceal malicious content. To address this, we introduce PhishLumos, an adaptive multi-agent system that proactively mitigates entire attack campaigns. It confronts a core cybersecurity imbalance: attackers can easily scale operations, while defense remains an intensive expert task. Instead of being blocked by evasion, PhishLumos treats it as a critical signal to investigate the underlying infrastructure. Its Large Language Model (LLM)-powered agents uncover shared hosting, certificates, and domain registration patterns. On real-world data, our system identified 100% of campaigns in the median case, over a week before their confirmation by cybersecurity experts. PhishLumos demonstrates a practical shift from reactive URL blocking to proactive campaign mitigation, protecting users before they are harmed and making the digital world safer for all.", "published": "2025-09-26T02:21:04Z", "updated": "2026-01-22T12:32:23Z", "authors": ["Daiki Chiba", "Hiroki Nakano", "Takashi Koide"], "pdf_url": "https://arxiv.org/pdf/2509.21772v2"}
{"id": "http://arxiv.org/abs/2502.03365v4", "title": "A Match Made in Heaven? AI-driven Matching of Vulnerabilities and Security Unit Tests", "summary": "Software vulnerabilities are often detected via taint analysis, penetration testing, or fuzzing. They are also found via unit tests that exercise security-sensitive behavior with specific inputs, called vulnerability-witnessing tests. Generative AI models could help developers in writing them, but they require many examples to learn from, which are currently scarce. This paper introduces VuTeCo, an AI-driven framework for collecting examples of vulnerability-witnessing tests from Java repositories. VuTeCo carries out two tasks: (1) The \"Finding\" task to determine whether a unit test case is security-related, and (2) the \"Matching\" task to relate a test case to the vulnerability it witnesses. VuTeCo addresses the Finding task with UniXcoder, achieving an F0.5 score of 0.73 and a precision of 0.83 on a test set of unit tests from Vul4J. The Matching task is addressed using DeepSeek Coder, achieving an F0.5 score of 0.65 and a precision of 0.75 on a test set of pairs of unit tests and vulnerabilities from Vul4J. VuTeCo has been used in the wild on 427 Java projects and 1,238 vulnerabilities, obtaining 224 test cases confirmed to be security-related and 35 tests correctly matched to 29 vulnerabilities. The validated tests were collected in a new dataset called Test4Vul. VuTeCo lays the foundation for large-scale retrieval of vulnerability-witnessing tests, enabling future AI models to better understand and generate security unit tests.", "published": "2025-02-05T17:02:42Z", "updated": "2026-01-22T11:23:52Z", "authors": ["Emanuele Iannone", "Quang-Cuong Bui", "Riccardo Scandariato"], "pdf_url": "https://arxiv.org/pdf/2502.03365v4"}
{"id": "http://arxiv.org/abs/2508.16150v4", "title": "Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks", "summary": "Membership Inference Attacks (MIAs) pose a significant privacy risk by enabling adversaries to determine if a specific data point was part of a model's training set. This work empirically investigates whether MU algorithms can function as a targeted, active defense mechanism, in scenarios where a privacy audit identifies specific classes or individuals as highly susceptible to MIAs post-training. By 'dulling' the model's categorical memory of these samples, the process effectively mitigates the membership signal and reduces the MIA success rate for the most vulnerable users. We evaluate the defense potential of three MU algorithms, Negative Gradient (neg grad), SCalable Remembering and Unlearning unBound (SCRUB), and Selective Fine-tuning and Targeted Confusion (SFTC), across four diverse datasets and three complexity-based model groups. Our findings reveal that MU can function as a countermeasure against MIAs, though its success is critically contingent on algorithm choice, model capacity, and a profound sensitivity to learning rates. While Negative Gradient often induces a generalized degradation of membership signals across both forget and retain set, SFTC identifies a critical ``divergence effect'' where targeted forgetting reinforces the membership signal of retained data. Conversely, SCRUB provides a more balanced defense with minimal collateral impact on MIA perspective.", "published": "2025-08-22T07:19:33Z", "updated": "2026-01-22T10:28:23Z", "authors": ["Theodoros Tsiolakis", "Vasilis Perifanis", "Nikolaos Pavlidis", "Christos Chrysanthos Nikolaidis", "Aristeidis Sidiropoulos", "Pavlos S. Efraimidis"], "pdf_url": "https://arxiv.org/pdf/2508.16150v4"}
{"id": "http://arxiv.org/abs/2601.15824v1", "title": "Introducing the Generative Application Firewall (GAF)", "summary": "This paper introduces the Generative Application Firewall (GAF), a new architectural layer for securing LLM applications. Existing defenses -- prompt filters, guardrails, and data-masking -- remain fragmented; GAF unifies them into a single enforcement point, much like a WAF coordinates defenses for web traffic, while also covering autonomous agents and their tool interactions.", "published": "2026-01-22T10:19:24Z", "updated": "2026-01-22T10:19:24Z", "authors": ["Joan Vendrell Farreny", "Martí Jordà Roca", "Miquel Cornudella Gaya", "Rodrigo Fernández Baón", "Víctor García Martínez", "Eduard Camacho Sucarrat", "Alessandro Pignati"], "pdf_url": "https://arxiv.org/pdf/2601.15824v1"}
{"id": "http://arxiv.org/abs/2601.01747v4", "title": "Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization", "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs", "published": "2026-01-05T02:49:33Z", "updated": "2026-01-22T09:09:47Z", "authors": ["Jiwei Guan", "Haibo Jin", "Haohan Wang"], "pdf_url": "https://arxiv.org/pdf/2601.01747v4"}
{"id": "http://arxiv.org/abs/2408.06219v3", "title": "120 Domain-Specific Languages for Security", "summary": "Security engineering, from security requirements engineering to the implementation of cryptographic protocols, is often supported by domain-specific languages (DSLs). Unfortunately, a lack of knowledge about these DSLs, such as which security aspects are addressed and when, hinders their effective use and further research. This systematic literature review examines 120 security-oriented DSLs based on six research questions concerning security aspects and goals, language-specific characteristics, integration into the software development lifecycle (SDLC), and effectiveness of the DSLs. We observe a high degree of fragmentation, which leads to opportunities for integration. We also need to improve the usability and evaluation of security DSLs.", "published": "2024-08-12T15:19:54Z", "updated": "2026-01-22T09:03:35Z", "authors": ["Markus Krausz", "Sven Peldszus", "Francesco Regazzoni", "Thorsten Berger", "Tim Güneysu"], "pdf_url": "https://arxiv.org/pdf/2408.06219v3"}
{"id": "http://arxiv.org/abs/2601.15774v1", "title": "FirmReBugger: A Benchmark Framework for Monolithic Firmware Fuzzers", "summary": "Monolithic Firmware is widespread. Unsurprisingly, fuzz testing firmware is an active research field with new advances addressing the unique challenges in the domain. However, understanding and evaluating improvements by deriving metrics such as code coverage and unique crashes are problematic, leading to a desire for a reliable bug-based benchmark. To address the need, we design and build FirmReBugger, a holistic framework for fairly assessing monolithic firmware fuzzers with a realistic, diverse, bug-based benchmark. FirmReBugger proposes using bug oracles--C syntax expressions of bug descriptors--with an interpreter to automate analysis and accurately report on bugs discovered, discriminating between states of detected, triggered, reached and not reached. Importantly, our idea of benchmarking does not modify the target binary and simply replays fuzzing seeds to isolate the benchmark implementation from the fuzzer while providing a simple means to extend with new bug oracles. Further, analyzing fuzzing roadblocks, we created FirmBench, a set of diverse, real-world binary targets with 313 software bug oracles. Incorporating our analysis of roadblocks challenging monolithic firmware fuzzing, the bench provides for rapid evaluation of future advances. We implement FirmReBugger in a FuzzBench-for-Firmware type service and use FirmBench to evaluate 9 state-of-the art monolithic firmware fuzzers in the style of a reproducibility study, using a 10 CPU-year effort, to report our findings.", "published": "2026-01-22T09:02:35Z", "updated": "2026-01-22T09:02:35Z", "authors": ["Mathew Duong", "Michael Chesser", "Guy Farrelly", "Surya Nepal", "Damith C. Ranasinghe"], "pdf_url": "https://arxiv.org/pdf/2601.15774v1"}
{"id": "http://arxiv.org/abs/2601.15754v1", "title": "CAFE-GB: Scalable and Stable Feature Selection for Malware Detection via Chunk-wise Aggregated Gradient Boosting", "summary": "High-dimensional malware datasets often exhibit feature redundancy, instability, and scalability limitations, which hinder the effectiveness and interpretability of machine learning-based malware detection systems. Although feature selection is commonly employed to mitigate these issues, many existing approaches lack robustness when applied to large-scale and heterogeneous malware data. To address this gap, this paper proposes CAFE-GB (Chunk-wise Aggregated Feature Estimation using Gradient Boosting), a scalable feature selection framework designed to produce stable and globally consistent feature rankings for high-dimensional malware detection. CAFE-GB partitions training data into overlapping chunks, estimates local feature importance using gradient boosting models, and aggregates these estimates to derive a robust global ranking. Feature budget selection is performed separately through a systematic k-selection and stability analysis to balance detection performance and robustness. The proposed framework is evaluated on two large-scale malware datasets: BODMAS and CIC-AndMal2020, representing large and diverse malware feature spaces. Experimental results show that classifiers trained on CAFE-GB -selected features achieve performance parity with full-feature baselines across multiple metrics, including Accuracy, F1-score, MCC, ROC-AUC, and PR-AUC, while reducing feature dimensionality by more than 95\\%. Paired Wilcoxon signed-rank tests confirm that this reduction does not introduce statistically significant performance degradation. Additional analyses demonstrate low inter-feature redundancy and improved interpretability through SHAP-based explanations. Runtime and memory profiling further indicate reduced downstream classification overhead. Overall, CAFE-GB provides a stable, interpretable, and scalable feature selection strategy for large-scale malware detection.", "published": "2026-01-22T08:43:15Z", "updated": "2026-01-22T08:43:15Z", "authors": ["Ajvad Haneef K", "Karan Kuwar Singh", "Madhu Kumar S D"], "pdf_url": "https://arxiv.org/pdf/2601.15754v1"}
{"id": "http://arxiv.org/abs/2507.14629v2", "title": "VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking", "summary": "Though vertical federated learning (VFL) is generally considered to be privacy-preserving, recent studies have shown that VFL system is vulnerable to label inference attacks originating from various attack surfaces. Among these attacks, the model completion (MC) attack is currently the most powerful one. Existing defense methods against it either sacrifice model accuracy or incur impractical computational overhead. In this paper, we propose VMask, a novel label privacy protection framework designed to defend against MC attack from the perspective of layer masking. Our key insight is to disrupt the strong correlation between input data and intermediate outputs by applying the secret sharing (SS) technique to mask layer parameters in the attacker's model. We devise a strategy for selecting critical layers to mask, reducing the overhead that would arise from naively applying SS to the entire model. Moreover, VMask is the first framework to offer a tunable privacy budget to defenders, allowing for flexible control over the levels of label privacy according to actual requirements. We built a VFL system, implemented VMask on it, and extensively evaluated it using five model architectures and 13 datasets with different modalities, comparing it to 12 other defense methods. The results demonstrate that VMask achieves the best privacy-utility trade-off, successfully thwarting the MC attack (reducing the label inference accuracy to a random guessing level) while preserving model performance (e.g., in Transformer-based model, the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up to 60,846 times faster than cryptography-based methods, and it only marginally exceeds that of standard VFL by 1.8 times in a large Transformer-based model, which is generally acceptable.", "published": "2025-07-19T13:51:09Z", "updated": "2026-01-22T08:34:21Z", "authors": ["Juntao Tan", "Lan Zhang", "Zhonghao Hu", "Kai Yang", "Peng Ran", "Bo Li"], "pdf_url": "https://arxiv.org/pdf/2507.14629v2"}
{"id": "http://arxiv.org/abs/2507.14625v2", "title": "VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning", "summary": "Vertical federated learning (VFL) enables multiple parties with disjoint features to collaboratively train models without sharing raw data. While privacy vulnerabilities of VFL are extensively-studied, its security threats-particularly targeted label attacks-remain underexplored. In such attacks, a passive party perturbs inputs at inference to force misclassification into adversary-chosen labels. Existing methods rely on unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore anomaly detectors deployed in real-world systems. To bridge this gap, we introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly designed to evade detector-enhanced VFL inference. During the preparation stage, the attacker selects a minimal set of high-expressiveness samples (via maximum mean discrepancy), submits them through VFL protocol to collect predicted labels, and uses these pseudo-labels to train estimated detector and surrogate model on local features. In attack stage, these models guide gradient-based perturbations of remaining samples, crafting adversarial instances that induce targeted misclassifications and evade detection. We implement VTarbel and evaluate it against four model architectures, seven multimodal datasets, and two anomaly detectors. Across all settings, VTarbel outperforms four state-of-the-art baselines, evades detection, and retains effective against three representative privacy-preserving defenses. These results reveal critical security blind spots in current VFL deployments and underscore urgent need for robust, attack-aware defenses.", "published": "2025-07-19T13:43:50Z", "updated": "2026-01-22T08:30:25Z", "authors": ["Juntao Tan", "Anran Li", "Quanchao Liu", "Peng Ran", "Lan Zhang"], "pdf_url": "https://arxiv.org/pdf/2507.14625v2"}
{"id": "http://arxiv.org/abs/2504.00035v3", "title": "Is Your Writing Being Mimicked by AI? Unveiling Imitation with Invisible Watermarks in Creative Writing", "summary": "Efficient knowledge injection methods for Large Language Models (LLMs), such as In-Context Learning, knowledge editing, and efficient parameter fine-tuning, significantly enhance model utility on downstream tasks. However, they also pose substantial risks of unauthorized imitation and compromised data provenance for high-value unstructured data assets like creative works. Current copyright protection methods for creative works predominantly focus on visual arts, leaving a critical and unaddressed data engineering challenge in the safeguarding of creative writing. In this paper, we propose WIND (Watermarking via Implicit and Non-disruptive Disentanglement), a novel zero-watermarking, verifiable and implicit scheme that safeguards creative writing databases by providing verifiable copyright protection. Specifically, we decompose creative essence into five key elements, which are extracted utilizing LLMs through a designed instance delimitation mechanism and consolidated into condensed-lists. These lists enable WIND to convert core copyright attributes into verifiable watermarks via implicit encoding within a disentanglement creative space, where 'disentanglement' refers to the separation of creative-specific and creative-irrelevant features. This approach, utilizing implicit encoding, avoids distorting fragile textual content. Extensive experiments demonstrate that WIND effectively verifies creative writing copyright ownership against AI imitation, achieving F1 scores above 98% and maintaining robust performance under stringent low false-positive rates where existing state-of-the-art text watermarking methods struggle.", "published": "2025-03-30T08:19:12Z", "updated": "2026-01-22T07:56:15Z", "authors": ["Ziwei Zhang", "Juan Wen", "Wanli Peng", "Zhengxian Wu", "Yinghan Zhou", "Yiming Xue"], "pdf_url": "https://arxiv.org/pdf/2504.00035v3"}
{"id": "http://arxiv.org/abs/2601.15716v1", "title": "zkFinGPT: Zero-Knowledge Proofs for Financial Generative Pre-trained Transformers", "summary": "Financial Generative Pre-trained Transformers (FinGPT) with multimodal capabilities are now being increasingly adopted in various financial applications. However, due to the intellectual property of model weights and the copyright of training corpus and benchmarking questions, verifying the legitimacy of GPT's model weights and the credibility of model outputs is a pressing challenge. In this paper, we introduce a novel zkFinGPT scheme that applies zero-knowledge proofs (ZKPs) to high-value financial use cases, enabling verification while protecting data privacy. We describe how zkFinGPT will be applied to three financial use cases. Our experiments on two existing packages reveal that zkFinGPT introduces substantial computational overhead that hinders its real-world adoption. E.g., for LLama3-8B model, it generates a commitment file of $7.97$MB using $531$ seconds, and takes $620$ seconds to prove and $2.36$ seconds to verify.", "published": "2026-01-22T07:37:33Z", "updated": "2026-01-22T07:37:33Z", "authors": ["Xiao-Yang Liu", "Ningjie Li", "Keyi Wang", "Xiaoli Zhi", "Weiqin Tong"], "pdf_url": "https://arxiv.org/pdf/2601.15716v1"}
{"id": "http://arxiv.org/abs/2601.15697v1", "title": "Balancing Security and Privacy: The Pivotal Role of AI in Modern Healthcare Systems", "summary": "As digital threats continue to grow, organizations must find ways to enhance security while protecting user privacy. This paper explores how artificial intelligence (AI) plays a crucial role in achieving this balance. AI technologies can improve security by detecting threats, monitoring systems, and automating responses. However, using AI also raises privacy concerns that need careful consideration.We examine real-world examples from the healthcare sector to illustrate how organizations can implement AI solutions that strengthen security without compromising patient privacy. Additionally, we discuss the importance of creating transparent AI systems and adhering to privacy regulations.Ultimately, this paper provides insights and recommendations for integrating AI into healthcare security practices, helping organizations navigate the challenges of modern management while keeping patient data safe.", "published": "2026-01-22T06:51:45Z", "updated": "2026-01-22T06:51:45Z", "authors": ["Binu V P", "Deepthy K Bhaskar", "Minimol B"], "pdf_url": "https://arxiv.org/pdf/2601.15697v1"}
{"id": "http://arxiv.org/abs/2601.15678v1", "title": "Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems", "summary": "Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.", "published": "2026-01-22T05:59:42Z", "updated": "2026-01-22T05:59:42Z", "authors": ["Mengyu Yao", "Ziqi Zhang", "Ning Luo", "Shaofei Li", "Yifeng Cai", "Xiangqun Chen", "Yao Guo", "Ding Li"], "pdf_url": "https://arxiv.org/pdf/2601.15678v1"}
{"id": "http://arxiv.org/abs/2601.15663v1", "title": "TempoNet: Learning Realistic Communication and Timing Patterns for Network Traffic Simulation", "summary": "Realistic network traffic simulation is critical for evaluating intrusion detection systems, stress-testing network protocols, and constructing high-fidelity environments for cybersecurity training. While attack traffic can often be layered into training environments using red-teaming or replay methods, generating authentic benign background traffic remains a core challenge -- particularly in simulating the complex temporal and communication dynamics of real-world networks. This paper introduces TempoNet, a novel generative model that combines multi-task learning with multi-mark temporal point processes to jointly model inter-arrival times and all packet- and flow-header fields. TempoNet captures fine-grained timing patterns and higher-order correlations such as host-pair behavior and seasonal trends, addressing key limitations of GAN-, LLM-, and Bayesian-based methods that fail to reproduce structured temporal variation. TempoNet produces temporally consistent, high-fidelity traces, validated on real-world datasets. Furthermore, we show that intrusion detection models trained on TempoNet-generated background traffic perform comparably to those trained on real data, validating its utility for real-world security applications.", "published": "2026-01-22T05:23:19Z", "updated": "2026-01-22T05:23:19Z", "authors": ["Kristen Moore", "Diksha Goel", "Cody James Christopher", "Zhen Wang", "Minjune Kim", "Ahmed Ibrahim", "Ahmad Mohsin", "Seyit Camtepe"], "pdf_url": "https://arxiv.org/pdf/2601.15663v1"}
{"id": "http://arxiv.org/abs/2601.15652v1", "title": "Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models", "summary": "Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).\n  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises (\"Sycophancy\").\n  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.", "published": "2026-01-22T05:00:21Z", "updated": "2026-01-22T05:00:21Z", "authors": ["Manish Bhatt"], "pdf_url": "https://arxiv.org/pdf/2601.15652v1"}
{"id": "http://arxiv.org/abs/2509.17302v5", "title": "TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion", "summary": "Text embedding inversion attacks reconstruct original sentences from latent representations, posing severe privacy threats in collaborative inference and edge computing. We propose TextCrafter, an optimization-based adversarial perturbation mechanism that combines RL learned, geometry aware noise injection orthogonal to user embeddings with cluster priors and PII signal guidance to suppress inversion while preserving task utility. Unlike prior defenses either non learnable or agnostic to perturbation direction, TextCrafter provides a directional protective policy that balances privacy and utility. Under strong privacy setting, TextCrafter maintains 70 percentage classification accuracy on four datasets and consistently outperforms Gaussian/LDP baselines across lower privacy budgets, demonstrating a superior privacy utility trade off.", "published": "2025-09-22T00:51:20Z", "updated": "2026-01-22T04:16:46Z", "authors": ["Duoxun Tang", "Xinhang Jiang", "Jiajun Niu"], "pdf_url": "https://arxiv.org/pdf/2509.17302v5"}
{"id": "http://arxiv.org/abs/2601.15632v1", "title": "Side-Channel Attacks on Open vSwitch", "summary": "Virtualization is widely adopted in cloud systems to manage resource sharing among users. A virtualized environment usually deploys a virtual switch within the host system to enable virtual machines to communicate with each other and with the physical network. The Open vSwitch (OVS) is one of the most popular software-based virtual switches. It maintains a cache hierarchy to accelerate packet forwarding from the host to virtual machines. We characterize the caching system inside OVS from a security perspective and identify three attack primitives. Based on the attack primitives, we present three remote attacks via OVS, breaking the isolation in virtualized environments. First, we identify remote covert channels using different caches. Second, we present a novel header recovery attack that leaks a remote user's packet header fields, breaking the confidentiality guarantees from the system. Third, we demonstrate a remote packet rate monitoring attack that recovers the packet rate of a remote victim. To defend against these attacks, we also discuss and evaluate mitigation solutions.", "published": "2026-01-22T04:12:03Z", "updated": "2026-01-22T04:12:03Z", "authors": ["Daewoo Kim", "Sihang Liu"], "pdf_url": "https://arxiv.org/pdf/2601.15632v1"}
{"id": "http://arxiv.org/abs/2601.14582v2", "title": "Automatically Tightening Access Control Policies with Restricter", "summary": "Robust access control is a cornerstone of secure software, systems, and networks. An access control mechanism is as effective as the policy it enforces. However, authoring effective policies that satisfy desired properties such as the principle of least privilege is a challenging task even for experienced administrators, as evidenced by many real instances of policy misconfiguration. In this paper, we set out to address this pain point by proposing Restricter, which automatically tightens each (permit) policy rule of a policy with respect to an access log, which captures some already exercised access requests and their corresponding access decisions (i.e., allow or deny). Restricter achieves policy tightening by reducing the number of access requests permitted by a policy rule without sacrificing the functionality of the underlying system it is regulating. We implement Restricter for Amazon's Cedar policy language and demonstrate its effectiveness through two realistic case studies.", "published": "2026-01-21T01:42:05Z", "updated": "2026-01-22T03:16:34Z", "authors": ["Ka Lok Wu", "Christa Jenkins", "Scott D. Stoller", "Omar Chowdhury"], "pdf_url": "https://arxiv.org/pdf/2601.14582v2"}
{"id": "http://arxiv.org/abs/2601.15595v1", "title": "Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning", "summary": "Large language models (LLMs) exhibit powerful capabilities but risk memorizing sensitive personally identifiable information (PII) from their training data, posing significant privacy concerns. While machine unlearning techniques aim to remove such data, they predominantly depend on access to the training data. This requirement is often impractical, as training data in real-world deployments is commonly proprietary or inaccessible. To address this limitation, we propose Data-Free Selective Unlearning (DFSU), a novel privacy-preserving framework that removes sensitive PII from an LLM without requiring its training data. Our approach first synthesizes pseudo-PII through language model inversion, then constructs token-level privacy masks for these synthetic samples, and finally performs token-level selective unlearning via a contrastive mask loss within a low-rank adaptation (LoRA) subspace. Extensive experiments on the AI4Privacy PII-Masking dataset using Pythia models demonstrate that our method effectively removes target PII while maintaining model utility.", "published": "2026-01-22T02:43:12Z", "updated": "2026-01-22T02:43:12Z", "authors": ["Xinjie Zhou", "Zhihui Yang", "Lechao Cheng", "Sai Wu", "Gang Chen"], "pdf_url": "https://arxiv.org/pdf/2601.15595v1"}
{"id": "http://arxiv.org/abs/2510.11137v3", "title": "CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense", "summary": "Large language models have gained widespread attention recently, but their potential security vulnerabilities, especially privacy leakage, are also becoming apparent. To test and evaluate for data extraction risks in LLM, we proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and Defense. We introduce several innovative components, including Dynamic Loss, Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested to enhance the consistency of the soft prompt tuning process. Through extensive experimentation with various combinations, we achieved an extraction rate of 65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other reference works confirm our superior extraction rates. We evaluate CoSPED on more scenarios, achieving Pythia model extraction rate of 51.7% and introducing cross-model comparison. Finally, we explore defense through Rank-One Model Editing and achieve a reduction in the extraction rate to 1.6%, which proves that our analysis of extraction mechanisms can directly inform effective mitigation strategies against soft prompt-based attacks.", "published": "2025-10-13T08:26:47Z", "updated": "2026-01-22T02:42:09Z", "authors": ["Zhuochen Yang", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "pdf_url": "https://arxiv.org/pdf/2510.11137v3"}
{"id": "http://arxiv.org/abs/2508.18665v5", "title": "Membership Inference Attacks on LLM-based Recommender Systems", "summary": "Large language models (LLMs) based recommender systems (RecSys) can adapt to different domains flexibly. It utilizes in-context learning (ICL), i.e., prompts, to customize the recommendation functions, which include sensitive historical user-specific item interactions, encompassing implicit feedback such as clicked items and explicit product reviews. Such private information may be exposed by novel privacy attacks. However, no study has been conducted on this important issue. We design several membership inference attacks (MIAs) aimed to revealing whether system prompts include victims' historical interactions. The attacks are \\emph{Similarity, Memorization, Inquiry, and Poisoning attacks}, each utilizing unique features of LLMs or RecSys. We have carefully evaluated them on five of the latest open-source LLMs and three well-known RecSys benchmark datasets. The results confirm that the MIA threat to LLM RecSys is realistic: inquiry and poisoning attacks show significantly high attack advantages. We also discussed possible methods to mitigate such MIA threats. We have also analyzed the factors affecting these attacks, such as the number of shots in system prompts, the position of the victim in the shots, the number of poisoning items in the prompt,etc.", "published": "2025-08-26T04:14:39Z", "updated": "2026-01-22T02:01:51Z", "authors": ["Jiajie He", "Min-Chun Chen", "Xintong Chen", "Xinyang Fang", "Yuechun Gu", "Keke Chen"], "pdf_url": "https://arxiv.org/pdf/2508.18665v5"}
{"id": "http://arxiv.org/abs/2509.24257v4", "title": "VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference", "summary": "Decentralized inference provides a scalable and resilient paradigm for serving large language models (LLMs), enabling fragmented global resource utilization and reducing reliance on centralized providers. However, in a permissionless environment without trusted nodes, ensuring the correctness of model outputs remains a core challenge. We introduce VeriLLM, a publicly verifiable protocol for decentralized LLM inference that achieves security with incentive guarantees while maintaining practical efficiency. VeriLLM combines lightweight empirical rerunning with minimal on-chain checks to preclude free-riding, allowing verifiers to validate results at approximately 1% of the underlying inference cost by exploiting the structural separation between prefill and autoregressive decoding. To prevent verification bottlenecks, we design an isomorphic inference--verification architecture that multiplexes both inference and verification roles across the same GPU workers. This design (i) improves GPU utilization and overall throughput, (ii) enlarges the effective validator set, enhancing robustness and liveness, and (iii) enforces task indistinguishability to prevent node-specific optimizations or selective behavior. Through theoretical analysis and system-level evaluation, we show that VeriLLM achieves reliable public verifiability with minimal overhead, offering a practical foundation for trustworthy and scalable decentralized LLM inference.", "published": "2025-09-29T04:07:32Z", "updated": "2026-01-22T01:57:41Z", "authors": ["Ke Wang", "Zishuo Zhao", "Xinyuan Song", "Zelin Li", "Libin Xia", "Chris Tong", "Bill Shi", "Wenjie Qu", "Eric Yang", "Lynn Ai"], "pdf_url": "https://arxiv.org/pdf/2509.24257v4"}
{"id": "http://arxiv.org/abs/2511.05867v3", "title": "Can LLM Infer Risk Information From MCP Server System Logs?", "summary": "Large Language Models (LLMs) demonstrate strong capabilities in solving complex tasks when integrated with external tools. The Model Context Protocol (MCP) has become a standard interface for enabling such tool-based interactions. However, these interactions introduce substantial security concerns, particularly when the MCP server is compromised or untrustworthy. While prior benchmarks primarily focus on prompt injection attacks or analyze the vulnerabilities of LLM-MCP interaction trajectories, limited attention has been given to the underlying system logs associated with malicious MCP servers. To address this gap, we present the first synthetic benchmark for evaluating LLMs' ability to identify security risks from system logs. We define nine categories of MCP server risks and generate 1,800 synthetic system logs using ten state-of-the-art LLMs. These logs are embedded in the return values of 243 curated MCP servers, yielding a dataset of 2,421 chat histories for training and 471 queries for evaluation. Our pilot experiments reveal that smaller models often fail to detect risky system logs, leading to high false negatives. While models trained with supervised fine-tuning (SFT) tend to over-flag benign logs, resulting in elevated false positives, Reinforcement Learning with Verifiable Reward (RLVR) offers a better precision-recall balance. In particular, after training with Group Relative Policy Optimization (GRPO), Llama3.1-8B-Instruct achieves 83 percent accuracy, surpassing the best-performing large remote model by 9 percentage points. Fine-grained, per-category analysis further underscores the effectiveness of reinforcement learning in enhancing LLM safety within the MCP framework. Code and data are available at https://github.com/PorUna-byte/MCP-RiskCue.", "published": "2025-11-08T05:52:53Z", "updated": "2026-01-22T00:04:40Z", "authors": ["Jiayi Fu", "Yuansen Zhang", "Yinggui Wang"], "pdf_url": "https://arxiv.org/pdf/2511.05867v3"}
