{"id": "http://arxiv.org/abs/2601.07786v1", "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt", "summary": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.", "published": "2026-01-12T17:59:34Z", "updated": "2026-01-12T17:59:34Z", "authors": ["Abdullah Al Mujahid", "Mia Mohammad Imran"], "pdf_url": "https://arxiv.org/pdf/2601.07786v1"}
{"id": "http://arxiv.org/abs/2505.23516v3", "title": "The CASE Framework -- A New Architecture for Participatory Research and Digital Health Surveillance", "summary": "We present CASE, an open-source framework for adaptive participatory research and disease surveillance. Unlike traditional survey platforms with static branching logic, CASE uses an event-driven architecture that adjusts survey workflows in real time based on participant responses, external data, temporal conditions, and evolving participant state. This design supports everything from simple one-time questionnaires to complex longitudinal studies with sophisticated conditional logic.\n  Built on over a decade of practical experience, CASE underwent major architectural changes in 2024. We replaced a complex microservice design with a streamlined monolithic architecture, significantly improving maintainability and deployment accessibility, particularly for institutions with limited technical resources.\n  CASE has been successfully deployed across diverse domains, powering national disease surveillance platforms, supporting post-COVID cohort studies, and enabling real-time sentiment analysis during political events. These applications, involving tens of thousands of participants, demonstrate the framework's scalability, versatility, and practical value.\n  This paper describes the foundations of CASE, documents its architectural evolution, and shares lessons learned from real-world deployments across diverse research domains and regulatory environments. We position CASE as a mature research infrastructure that balances sophisticated functionality with practical deployment needs for sustainable and institutionally controlled data collection systems.", "published": "2025-05-29T14:56:26Z", "updated": "2026-01-12T16:06:36Z", "authors": ["Marco Hirsch", "Peter Hevesi", "Paul Lukowicz"], "pdf_url": "https://arxiv.org/pdf/2505.23516v3"}
{"id": "http://arxiv.org/abs/2504.17977v2", "title": "From Bugs to Benchmarks: A Comprehensive Survey of Software Defect Datasets", "summary": "Software defect datasets, which are collections of software bugs and their associated information, are essential resources for researchers and practitioners in software engineering and beyond. Such datasets facilitate empirical research and enable standardized benchmarking for a wide range of techniques, including fault detection, fault localization, test generation, test prioritization, automated program repair, and emerging areas like agentic AI-based software development. Over the years, numerous software defect datasets with diverse characteristics have been developed, providing rich resources for the community, yet making it increasingly difficult to navigate the landscape. To address this challenge, this article provides a comprehensive survey of 151 software defect datasets. The survey discusses the scope of existing datasets, e.g., regarding the application domain of the buggy software, the types of defects, and the programming languages used. We also examine the construction of these datasets, including the data sources and construction methods employed. Furthermore, we assess the availability and usability of the datasets, validating their availability and examining how defects are presented. To better understand the practical uses of these datasets, we analyze the publications that cite them, revealing that the primary use cases are evaluations of new techniques and empirical research. Based on our comprehensive review of the existing datasets, this paper suggests potential opportunities for future research, including addressing underrepresented kinds of defects, enhancing availability and usability through better dataset organization, and developing more efficient strategies for dataset construction and maintenance. All surveyed datasets and their classifications are available at https://defect-datasets.github.io/.", "published": "2025-04-24T23:07:04Z", "updated": "2026-01-12T15:07:01Z", "authors": ["Hao-Nan Zhu", "Robert M. Furth", "Michael Pradel", "Cindy Rubio-González"], "pdf_url": "https://arxiv.org/pdf/2504.17977v2"}
{"id": "http://arxiv.org/abs/2510.23010v2", "title": "TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation", "summary": "Agentic code generation requires large language models (LLMs) capable of complex context management and multi-step reasoning. Prior multi-agent frameworks attempt to address these challenges through collaboration, yet they often suffer from rigid workflows and high reasoning recovery costs. To overcome these limitations, we propose TALM (Tree-Structured Multi-Agent Framework with Long-Term Memory), a dynamic framework that integrates structured task decomposition, localized re-reasoning, and long-term memory mechanisms. TALM employs an extensible tree-based collaboration structure. The parent-child relationships, when combined with a divide-and-conquer strategy, enhance reasoning flexibility and enable efficient error correction across diverse task scopes. Furthermore, a long-term memory module enables semantic querying and integration of prior knowledge, supporting implicit self-improvement through experience reuse. Experimental results on HumanEval, BigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently delivers strong reasoning performance and high token efficiency, highlighting its robustness and practical utility in complex code generation tasks.", "published": "2025-10-27T05:07:36Z", "updated": "2026-01-12T15:04:16Z", "authors": ["Ming-Tung Shen", "Yuh-Jzer Joung"], "pdf_url": "https://arxiv.org/pdf/2510.23010v2"}
{"id": "http://arxiv.org/abs/2601.07602v1", "title": "OODEval: Evaluating Large Language Models on Object-Oriented Design", "summary": "Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.", "published": "2026-01-12T14:51:31Z", "updated": "2026-01-12T14:51:31Z", "authors": ["Bingxu Xiao", "Yunwei Dong", "Yiqi Tang", "Manqing Zhang", "Yifan Zhou", "Chunyan Ma", "Yepang Liu"], "pdf_url": "https://arxiv.org/pdf/2601.07602v1"}
{"id": "http://arxiv.org/abs/2601.07537v1", "title": "FairRF: Multi-Objective Search for Single and Intersectional Software Fairness", "summary": "Background: The wide adoption of AI- and ML-based systems in sensitive domains raises severe concerns about their fairness. Many methods have been proposed in the literature to enhance software fairness. However, the majority behave as a black-box, not allowing stakeholders to prioritise fairness or effectiveness (i.e., prediction correctness) based on their needs. Aims: In this paper, we introduce FairRF, a novel approach based on multi-objective evolutionary search to optimise fairness and effectiveness in classification tasks. FairRF uses a Random Forest (RF) model as a base classifier and searches for the best hyperparameter configurations and data mutation to maximise fairness and effectiveness. Eventually, it returns a set of Pareto optimal solutions, allowing the final stakeholders to choose the best one based on their needs. Method: We conduct an extensive empirical evaluation of FairRF against 26 different baselines in 11 different scenarios using five effectiveness and three fairness metrics. Additionally, we also include two variations of the fairness metrics for intersectional bias for a total of six definitions analysed. Result: Our results show that FairRF can significantly improve the fairness of base classifiers, while maintaining consistent prediction effectiveness. Additionally, FairRF provides a more consistent optimisation under all fairness definitions compared to state-of-the-art bias mitigation methods and overcomes the existing state-of-the-art approach for intersectional bias mitigation. Conclusions: FairRF is an effective approach for bias mitigation also allowing stakeholders to adapt the development of fair software systems based on their specific needs.", "published": "2026-01-12T13:42:45Z", "updated": "2026-01-12T13:42:45Z", "authors": ["Giordano d'Alosio", "Max Hort", "Rebecca Moussa", "Federica Sarro"], "pdf_url": "https://arxiv.org/pdf/2601.07537v1"}
{"id": "http://arxiv.org/abs/2507.02533v2", "title": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models", "summary": "Fairness--the absence of unjustified bias--is a core principle in the development of Artificial Intelligence (AI) systems, yet it remains difficult to assess and enforce. Current approaches to fairness testing in large language models (LLMs) often rely on manual evaluation, fixed templates, deterministic heuristics, and curated datasets, making them resource-intensive and difficult to scale. This work aims to lay the groundwork for a novel, automated method for testing fairness in LLMs, reducing the dependence on domain-specific resources and broadening the applicability of current approaches. Our approach, Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to uncover bias by examining how model outputs vary in response to controlled modifications of input prompts, defined by metamorphic relations (MRs). Second, we propose exploiting the potential of LLMs for both test case generation and output evaluation, leveraging their capability to generate diverse inputs and classify outputs effectively. The proposal is complemented by three open-source tools supporting LLM-driven generation, execution, and evaluation of test cases. We report the findings of several experiments involving 12 pre-trained LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases. The results show that Meta-Fair is effective in uncovering bias in LLMs, achieving an average precision of 92% and revealing biased behaviour in 29% of executions. Additionally, LLMs prove to be reliable and consistent evaluators, with the best-performing models achieving F1-scores of up to 0.79. Although non-determinism affects consistency, these effects can be mitigated through careful MR design. While challenges remain to ensure broader applicability, the results indicate a promising path towards an unprecedented level of automation in LLM testing.", "published": "2025-07-03T11:20:59Z", "updated": "2026-01-12T13:30:36Z", "authors": ["Miguel Romero-Arjona", "José A. Parejo", "Juan C. Alonso", "Ana B. Sánchez", "Aitor Arrieta", "Sergio Segura"], "pdf_url": "https://arxiv.org/pdf/2507.02533v2"}
{"id": "http://arxiv.org/abs/2601.07526v1", "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era", "summary": "The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.", "published": "2026-01-12T13:25:33Z", "updated": "2026-01-12T13:25:33Z", "authors": ["Lei Zhang", "Mouxiang Chen", "Ruisheng Cao", "Jiawei Chen", "Fan Zhou", "Yiheng Xu", "Jiaxi Yang", "Liang Chen", "Changwei Luo", "Kai Zhang", "Fan Yan", "KaShun Shum", "Jiajun Zhang", "Zeyu Cui", "Hu Feng", "Junyang Lin", "Binyuan Hui", "Min Yang"], "pdf_url": "https://arxiv.org/pdf/2601.07526v1"}
{"id": "http://arxiv.org/abs/2601.07504v1", "title": "FROAV: A Framework for RAG Observation and Agent Verification -- Lowering the Barrier to LLM Agent Research", "summary": "The rapid advancement of Large Language Models (LLMs) and their integration into autonomous agent systems has created unprecedented opportunities for document analysis, decision support, and knowledge retrieval. However, the complexity of developing, evaluating, and iterating on LLM-based agent workflows presents significant barriers to researchers, particularly those without extensive software engineering expertise. We present FROAV (Framework for RAG Observation and Agent Verification), an open-source research platform that democratizes LLM agent research by providing a plug-and-play architecture combining visual workflow orchestration, a comprehensive evaluation framework, and extensible Python integration. FROAV implements a multi-stage Retrieval-Augmented Generation (RAG) pipeline coupled with a rigorous \"LLM-as-a-Judge\" evaluation system, all accessible through intuitive graphical interfaces. Our framework integrates n8n for no-code workflow design, PostgreSQL for granular data management, FastAPI for flexible backend logic, and Streamlit for human-in-the-loop interaction. Through this integrated ecosystem, researchers can rapidly prototype RAG strategies, conduct prompt engineering experiments, validate agent performance against human judgments, and collect structured feedback-all without writing infrastructure code. We demonstrate the framework's utility through its application to financial document analysis, while emphasizing its material-agnostic architecture that adapts to any domain requiring semantic analysis. FROAV represents a significant step toward making LLM agent research accessible to a broader scientific community, enabling researchers to focus on hypothesis testing and algorithmic innovation rather than system integration challenges.", "published": "2026-01-12T13:02:32Z", "updated": "2026-01-12T13:02:32Z", "authors": ["Tzu-Hsuan Lin", "Chih-Hsuan Kao"], "pdf_url": "https://arxiv.org/pdf/2601.07504v1"}
{"id": "http://arxiv.org/abs/2601.07476v1", "title": "NanoCockpit: Performance-optimized Application Framework for AI-based Autonomous Nanorobotics", "summary": "Autonomous nano-drones, powered by vision-based tiny machine learning (TinyML) models, are a novel technology gaining momentum thanks to their broad applicability and pushing scientific advancement on resource-limited embedded systems. Their small form factor, i.e., a few 10s grams, severely limits their onboard computational resources to sub-\\SI{100}{\\milli\\watt} microcontroller units (MCUs). The Bitcraze Crazyflie nano-drone is the \\textit{de facto} standard, offering a rich set of programmable MCUs for low-level control, multi-core processing, and radio transmission. However, roboticists very often underutilize these onboard precious resources due to the absence of a simple yet efficient software layer capable of time-optimal pipelining of multi-buffer image acquisition, multi-core computation, intra-MCUs data exchange, and Wi-Fi streaming, leading to sub-optimal control performances. Our \\textit{NanoCockpit} framework aims to fill this gap, increasing the throughput and minimizing the system's latency, while simplifying the developer experience through coroutine-based multi-tasking. In-field experiments on three real-world TinyML nanorobotics applications show our framework achieves ideal end-to-end latency, i.e. zero overhead due to serialized tasks, delivering quantifiable improvements in closed-loop control performance ($-$30\\% mean position error, mission success rate increased from 40\\% to 100\\%).", "published": "2026-01-12T12:29:38Z", "updated": "2026-01-12T12:29:38Z", "authors": ["Elia Cereda", "Alessandro Giusti", "Daniele Palossi"], "pdf_url": "https://arxiv.org/pdf/2601.07476v1"}
{"id": "http://arxiv.org/abs/2504.12443v2", "title": "Bridging the Gap: A Comparative Study of Academic and Developer Approaches to Smart Contract Vulnerabilities", "summary": "In this paper, we investigate the strategies adopted by Solidity developers to fix security vulnerabilities in smart contracts. Vulnerabilities are categorized using the DASP TOP 10 taxonomy, and fixing strategies are extracted from GitHub commits in open-source Solidity projects. Each commit was selected through a two-phase process: an initial filter using natural language processing techniques, followed by manual validation by the authors. We analyzed these commits to evaluate adherence to academic best practices. Our results show that developers often follow established guidelines for well-known vulnerability types such as Reentrancy and Arithmetic. However, in less-documented categories like Denial of Service, Bad Randomness, and Time Manipulation, adherence is significantly lower, suggesting gaps between academic literature and practical development. From non-aligned commits, we identified 27 novel fixing strategies not previously discussed in the literature. These emerging patterns offer actionable solutions for securing smart contracts in underexplored areas. To evaluate the quality of these new fixes, we conducted a questionnaire with academic and industry experts, who assessed each strategy based on Generalizability, Long-term Sustainability, and Effectiveness. Additionally, we performed a post-fix analysis by tracking subsequent commits to the fixed files, assessing the persistence and evolution of the fixes over time. Our findings offer an empirically grounded view of how vulnerabilities are addressed in practice, bridging theoretical knowledge and real-world solutions in the domain of smart contract security.", "published": "2025-04-16T19:20:00Z", "updated": "2026-01-12T12:03:11Z", "authors": ["Francesco Salzano", "Lodovica Marchesi", "Cosmo Kevin Antenucci", "Simone Scalabrino", "Roberto Tonelli", "Rocco Oliveto", "Remo Pareschi"], "pdf_url": "https://arxiv.org/pdf/2504.12443v2"}
{"id": "http://arxiv.org/abs/2412.07817v2", "title": "Modern Middlewares for Automated Vehicles: A Tutorial", "summary": "This paper offers a tutorial on current middlewares in automated vehicles. Our aim is to provide the reader with an overview of current middlewares and to identify open challenges in this field. We start by explaining the fundamentals of software architecture in distributed systems and the distinguishing requirements of Automated Vehicles. We then distinguish between communication middlewares and architecture platforms and highlight their key principles and differences. Next, we present five state-of-the-art middlewares as well as their capabilities and functions. We explore how these middlewares could be applied in the design of future vehicle software and their role in the automotive domain. Finally, we compare the five middlewares presented and discuss open research challenges.", "published": "2024-12-10T09:52:52Z", "updated": "2026-01-12T09:09:28Z", "authors": ["David Philipp Klüner", "Marius Molz", "Alexandru Kampmann", "Stefan Kowalewski", "Bassam Alrifaee"], "pdf_url": "https://arxiv.org/pdf/2412.07817v2"}
{"id": "http://arxiv.org/abs/2601.04124v2", "title": "Smells Depend on the Context: An Interview Study of Issue Tracking Problems and Smells in Practice", "summary": "Issue Tracking Systems (ITSs) enable software developers and managers to collect and resolve issues collaboratively. While researchers have extensively analysed ITS data to automate or assist specific activities such as issue assignments, duplicate detection, or priority prediction, developer studies on ITSs remain rare. Particularly, little is known about the challenges Software Engineering (SE) teams encounter in ITSs and when certain practices and workarounds (such as leaving issue fields like \"priority\" empty) are considered problematic. To fill this gap, we conducted an in-depth interview study with 26 experienced SE practitioners from different organisations and industries. We asked them about general problems encountered, as well as the relevance of 31 ITS smells (aka potentially problematic practices) discussed in the literature. By applying Thematic Analysis to the interview notes, we identified 14 common problems including issue findability, zombie issues, workflow bloat, and lack of workflow enforcement. Participants also stated that many of the ITS smells do not occur or are not problematic. Our results suggest that ITS problems and smells are highly dependent on context factors such as ITS configuration, workflow stage, and team size. We also discuss potential tooling solutions to configure, monitor, and visualise ITS smells to cope with these challenges.", "published": "2026-01-07T17:38:52Z", "updated": "2026-01-12T09:03:20Z", "authors": ["Lloyd Montgomery", "Clara Lüders", "Christian Rahe", "Walid Maalej"], "pdf_url": "https://arxiv.org/pdf/2601.04124v2"}
{"id": "http://arxiv.org/abs/2601.07301v1", "title": "Engineering Decisions in MBSE: Insights for a Decision Capture Framework Development", "summary": "Decision-making is a core engineering design activity that conveys the engineer's knowledge and translates it into courses of action. Capturing this form of knowledge can reap potential benefits for the engineering teams and enhance development efficiency. Despite its clear value, traditional decision capture often requires a significant amount of effort and still falls short of capturing the necessary context for reuse. Model-based systems engineering (MBSE) can be a promising solution to address these challenges by embedding decisions directly within system models, which can reduce the capture workload while maintaining explicit links to requirements, behaviors, and architectural elements. This article discusses a lightweight framework for integrating decision capture into MBSE workflows by representing decision alternatives as system model slices. Using a simplified industry example from aircraft architecture, we discuss the main challenges associated with decision capture and propose preliminary solutions to address these challenges.", "published": "2026-01-12T08:22:45Z", "updated": "2026-01-12T08:22:45Z", "authors": ["Nidhal Selmi", "Jean-michel Bruel", "Sébastien Mosser", "Matthieu Crespo", "Alain Kerbrat"], "pdf_url": "https://arxiv.org/pdf/2601.07301v1"}
{"id": "http://arxiv.org/abs/2601.07136v1", "title": "A Large-Scale Study on the Development and Issues of Multi-Agent AI Systems", "summary": "The rapid emergence of multi-agent AI systems (MAS), including LangChain, CrewAI, and AutoGen, has shaped how large language model (LLM) applications are developed and orchestrated. However, little is known about how these systems evolve and are maintained in practice. This paper presents the first large-scale empirical study of open-source MAS, analyzing over 42K unique commits and over 4.7K resolved issues across eight leading systems. Our analysis identifies three distinct development profiles: sustained, steady, and burst-driven. These profiles reflect substantial variation in ecosystem maturity. Perfective commits constitute 40.8% of all changes, suggesting that feature enhancement is prioritized over corrective maintenance (27.4%) and adaptive updates (24.3%). Data about issues shows that the most frequent concerns involve bugs (22%), infrastructure (14%), and agent coordination challenges (10%). Issue reporting also increased sharply across all frameworks starting in 2023. Median resolution times range from under one day to about two weeks, with distributions skewed toward fast responses but a minority of issues requiring extended attention. These results highlight both the momentum and the fragility of the current ecosystem, emphasizing the need for improved testing infrastructure, documentation quality, and maintenance practices to ensure long-term reliability and sustainability.", "published": "2026-01-12T02:07:15Z", "updated": "2026-01-12T02:07:15Z", "authors": ["Daniel Liu", "Krishna Upadhyay", "Vinaik Chhetri", "A. B. Siddique", "Umar Farooq"], "pdf_url": "https://arxiv.org/pdf/2601.07136v1"}
