{"id": "http://arxiv.org/abs/2601.03242v1", "title": "SLIM: Stealthy Low-Coverage Black-Box Watermarking via Latent-Space Confusion Zones", "summary": "Training data is a critical and often proprietary asset in Large Language Model (LLM) development, motivating the use of data watermarking to embed model-transferable signals for usage verification. We identify low coverage as a vital yet largely overlooked requirement for practicality, as individual data owners typically contribute only a minute fraction of massive training corpora. Prior methods fail to maintain stealthiness, verification feasibility, or robustness when only one or a few sequences can be modified. To address these limitations, we introduce SLIM, a framework enabling per-user data provenance verification under strict black-box access. SLIM leverages intrinsic LLM properties to induce a Latent-Space Confusion Zone by training the model to map semantically similar prefixes to divergent continuations. This manifests as localized generation instability, which can be reliably detected via hypothesis testing. Experiments demonstrate that SLIM achieves ultra-low coverage capability, strong black-box verification performance, and great scalability while preserving both stealthiness and model utility, offering a robust solution for protecting training data in modern LLM pipelines.", "published": "2026-01-06T18:37:45Z", "updated": "2026-01-06T18:37:45Z", "authors": ["Hengyu Wu", "Yang Cao"], "pdf_url": "https://arxiv.org/pdf/2601.03242v1"}
{"id": "http://arxiv.org/abs/2601.03241v1", "title": "On the Capacity Region of Individual Key Rates in Vector Linear Secure Aggregation", "summary": "We provide new insights into an open problem recently posed by Yuan-Sun [ISIT 2025], concerning the minimum individual key rate required in the vector linear secure aggregation problem. Consider a distributed system with $K$ users, where each user $k\\in [K]$ holds a data stream $W_k$ and an individual key $Z_k$. A server aims to compute a linear function $\\mathbf{F}[W_1;\\ldots;W_K]$ without learning any information about another linear function $\\mathbf{G}[W_1;\\ldots;W_K]$, where $[W_1;\\ldots;W_K]$ denotes the row stack of $W_1,\\ldots,W_K$. The open problem is to determine the minimum required length of $Z_k$, denoted as $R_k$, $k\\in [K]$. In this paper, we characterize a new achievable region for the rate tuple $(R_1,\\ldots,R_K)$. The region is polyhedral, with vertices characterized by a binary rate assignment $(R_1,\\ldots,R_K) = (\\mathbf{1}(1 \\in \\mathcal{I}),\\ldots,\\mathbf{1}(K\\in \\mathcal{I}))$, where $\\mathcal{I}\\subseteq [K]$ satisfies the \\textit{rank-increment condition}: $\\mathrm{rank}\\left(\\bigl[\\mathbf{F}_{\\mathcal{I}};\\mathbf{G}_{\\mathcal{I}}\\bigr]\\right) =\\mathrm{rank}\\bigl(\\mathbf{F}_{\\mathcal{I}}\\bigr)+N$. Here, $\\mathbf{F}_\\mathcal{I}$ and $\\mathbf{G}_\\mathcal{I}$ are the submatrices formed by the columns indexed by $\\mathcal{I}$. Our results uncover the novel fact that it is not necessary for every user to hold a key, thereby strictly enlarging the best-known achievable region in the literature. Furthermore, we provide a converse analysis to demonstrate its optimality when minimizing the number of users that hold keys.", "published": "2026-01-06T18:34:07Z", "updated": "2026-01-06T18:34:07Z", "authors": ["Lei Hu", "Sennur Ulukus"], "pdf_url": "https://arxiv.org/pdf/2601.03241v1"}
{"id": "http://arxiv.org/abs/2601.00042v2", "title": "Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing", "summary": "Production LLM agents with tool-using capabilities require security testing despite their safety training. We adapt Go-Explore to evaluate GPT-4o-mini across 28 experimental runs spanning six research questions. We find that random-seed variance dominates algorithmic parameters, yielding an 8x spread in outcomes; single-seed comparisons are unreliable, while multi-seed averaging materially reduces variance in our setup. Reward shaping consistently harms performance, causing exploration collapse in 94% of runs or producing 18 false positives with zero verified attacks. In our environment, simple state signatures outperform complex ones. For comprehensive security testing, ensembles provide attack-type diversity, whereas single agents optimize coverage within a given attack type. Overall, these results suggest that seed variance and targeted domain knowledge can outweigh algorithmic sophistication when testing safety-trained models.", "published": "2025-12-31T03:38:38Z", "updated": "2026-01-06T16:35:24Z", "authors": ["Manish Bhatt", "Adrian Wood", "Idan Habler", "Ammar Al-Kahfah"], "pdf_url": "https://arxiv.org/pdf/2601.00042v2"}
{"id": "http://arxiv.org/abs/2501.11052v3", "title": "SLVC-DIDA: Signature-less Verifiable Credential-based Issuer-hiding and Multi-party Authentication for Decentralized Identity", "summary": "As an emerging paradigm in digital identity, Decentralized Identity (DID) appears advantages over traditional identity management methods in a variety of aspects, e.g., enhancing user-centric online services and ensuring complete user autonomy and control. Verifiable Credential (VC) techniques are used to facilitate decentralized DID-based access control across multiple entities. However, existing DID schemes generally rely on a distributed public key infrastructure that also causes challenges, such as context information deduction, key exposure, and issuer data leakage. To address the issues above, this paper proposes a issuer-hiding and privacy-preserving DID multi-party authentication model with a signature-less VC scheme, named SLVC-DIDA, for the first time. Our proposed scheme avoids the dependence on signing keys by employing hashing and issuer membership proofs, which supports universal zero-knowledge multi-party DID authentications, eliminating additional technical integrations. We adopt a novel zero-knowledge circuit to maintain the anonymity of the issuer set, thereby enabling public verification while safeguarding the privacy of identity attributes via a Merkle tree-based VC list. Furthermore, by eliminating reliance on a Public Key Infrastructure (PKI), SLVC-DIDA enables decentralized and self-sovereign DID authentication. Our experiments further evaluate the effectiveness and practicality of SLVC-DIDA.", "published": "2025-01-19T13:58:01Z", "updated": "2026-01-06T15:49:35Z", "authors": ["Tianxiu Xie", "Keke Gai", "Jing Yu", "Liehuang Zhu", "Bin Xiao"], "pdf_url": "https://arxiv.org/pdf/2501.11052v3"}
{"id": "http://arxiv.org/abs/2502.03203v6", "title": "FSLH: Flexible Mechanized Speculative Load Hardening", "summary": "The Spectre speculative side-channel attacks pose formidable threats for security. Research has shown that code following the cryptographic constant-time discipline can be efficiently protected against Spectre v1 using a selective variant of Speculative Load Hardening (SLH). SLH was, however, not strong enough for protecting non-cryptographic code, leading to the introduction of Ultimate SLH, which provides protection for arbitrary programs, but has too large overhead for general use, since it conservatively assumes that all data is secret. In this paper we introduce a flexible SLH notion that achieves the best of both worlds by generalizing both Selective and Ultimate SLH. We give a suitable security definition for such transformations protecting arbitrary programs: any transformed program running with speculation should not leak more than what the source program leaks sequentially. We formally prove using the Rocq prover that two flexible SLH variants enforce this relative security guarantee. As easy corollaries we also obtain that, in our setting, Ultimate SLH enforces our relative security notion, and two selective SLH variants enforce speculative constant-time security.", "published": "2025-02-05T14:23:43Z", "updated": "2026-01-06T14:20:59Z", "authors": ["Jonathan Baumann", "Roberto Blanco", "Léon Ducruet", "Sebastian Harwig", "Catalin Hritcu"], "pdf_url": "https://arxiv.org/pdf/2502.03203v6"}
{"id": "http://arxiv.org/abs/2601.03031v1", "title": "FlexProofs: A Vector Commitment with Flexible Linear Time for Computing All Proofs", "summary": "In this paper, we introduce FlexProofs, a new vector commitment (VC) scheme that achieves two key properties: (1) the prover can generate all individual opening proofs for a vector of size $N$ in optimal time ${\\cal O}(N)$, and there is a flexible batch size parameter $b$ that can be increased to further reduce the time to generate all proofs; and (2) the scheme is directly compatible with a family of zkSNARKs that encode their input as a multi-linear polynomial. As a critical building block, we propose the first functional commitment (FC) scheme for multi-exponentiations with batch opening. Compared with HydraProofs, the only existing VC scheme that computes all proofs in optimal time ${\\cal O}(N)$ and is directly compatible with zkSNARKs, FlexProofs may speed up the process of generating all proofs, if the parameter $b$ is properly chosen. Our experiments show that for $N=2^{16}$ and $b=\\log^2 N$, FlexProofs can be $6\\times$ faster than HydraProofs. Moreover, when combined with suitable zkSNARKs, FlexProofs enable practical applications such as verifiable secret sharing and verifiable robust aggregation.", "published": "2026-01-06T14:05:16Z", "updated": "2026-01-06T14:05:16Z", "authors": ["Jing Liu", "Liang Feng Zhang"], "pdf_url": "https://arxiv.org/pdf/2601.03031v1"}
{"id": "http://arxiv.org/abs/2601.03013v1", "title": "LLMs, You Can Evaluate It! Design of Multi-perspective Report Evaluation for Security Operation Centers", "summary": "Security operation centers (SOCs) often produce analysis reports on security incidents, and large language models (LLMs) will likely be used for this task in the near future. We postulate that a better understanding of how veteran analysts evaluate reports, including their feedback, can help produce analysis reports in SOCs. In this paper, we aim to leverage LLMs for analysis reports. To this end, we first construct a Analyst-wise checklist to reflect SOC practitioners' opinions for analysis report evaluation through literature review and user study with SOC practitioners. Next, we design a novel LLM-based conceptual framework, named MESSALA, by further introducing two new techniques, granularization guideline and multi-perspective evaluation. MESSALA can maximize report evaluation and provide feedback on veteran SOC practitioners' perceptions. When we conduct extensive experiments with MESSALA, the evaluation results by MESSALA are the closest to those of veteran SOC practitioners compared with the existing LLM-based methods. We then show two key insights. We also conduct qualitative analysis with MESSALA, and then identify that MESSALA can provide actionable items that are necessary for improving analysis reports.", "published": "2026-01-06T13:37:50Z", "updated": "2026-01-06T13:37:50Z", "authors": ["Hiroyuki Okada", "Tatsumi Oba", "Naoto Yanai"], "pdf_url": "https://arxiv.org/pdf/2601.03013v1"}
{"id": "http://arxiv.org/abs/2601.03005v1", "title": "JPU: Bridging Jailbreak Defense and Unlearning via On-Policy Path Rectification", "summary": "Despite extensive safety alignment, Large Language Models (LLMs) often fail against jailbreak attacks. While machine unlearning has emerged as a promising defense by erasing specific harmful parameters, current methods remain vulnerable to diverse jailbreaks. We first conduct an empirical study and discover that this failure mechanism is caused by jailbreaks primarily activating non-erased parameters in the intermediate layers. Further, by probing the underlying mechanism through which these circumvented parameters reassemble into the prohibited output, we verify the persistent existence of dynamic $\\textbf{jailbreak paths}$ and show that the inability to rectify them constitutes the fundamental gap in existing unlearning defenses. To bridge this gap, we propose $\\textbf{J}$ailbreak $\\textbf{P}$ath $\\textbf{U}$nlearning (JPU), which is the first to rectify dynamic jailbreak paths towards safety anchors by dynamically mining on-policy adversarial samples to expose vulnerabilities and identify jailbreak paths. Extensive experiments demonstrate that JPU significantly enhances jailbreak resistance against dynamic attacks while preserving the model's utility.", "published": "2026-01-06T13:30:10Z", "updated": "2026-01-06T13:30:10Z", "authors": ["Xi Wang", "Songlei Jian", "Shasha Li", "Xiaopeng Li", "Zhaoye Li", "Bin Ji", "Baosheng Wang", "Jie Yu"], "pdf_url": "https://arxiv.org/pdf/2601.03005v1"}
{"id": "http://arxiv.org/abs/2506.12846v5", "title": "VFEFL: Privacy-Preserving Federated Learning against Malicious Clients via Verifiable Functional Encryption", "summary": "Federated learning is a promising distributed learning paradigm that enables collaborative model training without exposing local client data, thereby protecting data privacy. However, it also brings new threats and challenges. The advancement of model inversion attacks has rendered the plaintext transmission of local models insecure, while the distributed nature of federated learning makes it particularly vulnerable to attacks raised by malicious clients. To protect data privacy and prevent malicious client attacks, this paper proposes a privacy-preserving Federated Learning framework based on Verifiable Functional Encryption (VFEFL), without a non-colluding dual-server assumption or additional trusted third-party. Specifically, we propose a novel Cross-Ciphertext Decentralized Verifiable Functional Encryption (CC-DVFE) scheme that enables the verification of specific relationships over multi-dimensional ciphertexts. This scheme is formally treated, in terms of definition, security model and security proof. Furthermore, based on the proposed CC-DVFE scheme, we design a privacy-preserving federated learning framework that incorporates a novel robust aggregation rule to detect malicious clients, enabling the effective training of high-accuracy models under adversarial settings. Finally, we provide the formal analysis and empirical evaluation of VFEFL. The results demonstrate that our approach achieves the desired privacy protection, robustness, verifiability and fidelity, while eliminating the reliance on non-colluding dual-server assumption or trusted third parties required by most existing methods.", "published": "2025-06-15T13:38:40Z", "updated": "2026-01-06T13:23:15Z", "authors": ["Nina Cai", "Jinguang Han", "Weizhi Meng"], "pdf_url": "https://arxiv.org/pdf/2506.12846v5"}
{"id": "http://arxiv.org/abs/2601.02984v1", "title": "Selfish Mining in Multi-Attacker Scenarios: An Empirical Evaluation of Nakamoto, Fruitchain, and Strongchain", "summary": "The aim of this work is to enhance blockchain security by deepening the understanding of selfish mining attacks in various consensus protocols, especially the ones that have the potential to mitigate selfish mining. Previous research was mainly focused on a particular protocol with a single selfish miner, while only limited studies have been conducted on two or more attackers. To address this gap, we proposed a stochastic simulation framework that enables analysis of selfish mining with multiple attackers across various consensus protocols. We created the model of Proof-of-Work (PoW) Nakamoto consensus (serving as the baseline) as well as models of two additional consensus protocols designed to mitigate selfish mining: Fruitchain and Strongchain. Using our framework, thresholds reported in the literature were verified, and several novel thresholds were discovered for 2 and more attackers. We made the source code of our framework available, enabling researchers to evaluate any newly added protocol with one or more selfish miners and cross-compare it with already modeled protocols.", "published": "2026-01-06T12:51:35Z", "updated": "2026-01-06T12:51:35Z", "authors": ["Martin Perešíni", "Tomáš Hladký", "Jakub Kubík", "Ivan Homoliak"], "pdf_url": "https://arxiv.org/pdf/2601.02984v1"}
{"id": "http://arxiv.org/abs/2601.02981v1", "title": "Developing and Evaluating Lightweight Cryptographic Algorithms for Secure Embedded Systems in IoT Devices", "summary": "The high rate of development of Internet of Things (IoT) devices has brought to attention new challenges in the area of data security, especially within the resource-limited realm of RFID tags, sensors, and embedded systems. Traditional cryptographic implementations can be of inappropriate computational complexity and energy usage and hence are not suitable on these platforms. This paper examines the design, implementation, and testing of lightweight cryptographic algorithms that have been specifically designed to be used in secure embedded systems. A comparison of some of the state-of-the-art lightweight encryption algorithms, that is PRESENT, SPECK, and SIMON, focuses on the main performance indicators, i.e., throughput, use of memory, and energy utilization. The study presents novel lightweight algorithms that are founded upon the Feistel-network architecture and their safety under cryptanalytic attacks, e.g., differential and linear cryptanalysis. The proposed solutions are proven through hardware implementation on the FPGA platform. The results have shown that lightweight cryptography is an effective strategy that could be used to establish security and maintain performance in the IoT and other resource-limited settings.", "published": "2026-01-06T12:45:12Z", "updated": "2026-01-06T12:45:12Z", "authors": ["Brahim Khalil Sedraoui", "Abdelmadjid Benmachiche", "Amina Makhlouf"], "pdf_url": "https://arxiv.org/pdf/2601.02981v1"}
{"id": "http://arxiv.org/abs/2512.10449v3", "title": "When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection", "summary": "Driven by surging submission volumes, scientific peer review has catalyzed two parallel trends: individual over-reliance on LLMs and institutional AI-powered assessment systems. This study investigates the robustness of \"LLM-as-a-Judge\" systems to adversarial PDF manipulation via invisible text injections and layout aware encoding attacks. We specifically target the distinct incentive of flipping \"Reject\" decisions to \"Accept,\" a vulnerability that fundamentally compromises scientific integrity. To measure this, we introduce the Weighted Adversarial Vulnerability Score (WAVS), a novel metric that quantifies susceptibility by weighting score inflation against the severity of decision shifts relative to ground truth. We adapt 15 domain-specific attack strategies, ranging from semantic persuasion to cognitive obfuscation, and evaluate them across 13 diverse language models (including GPT-5 and DeepSeek) using a curated dataset of 200 official and real-world accepted and rejected submissions (e.g., ICLR OpenReview). Our results demonstrate that obfuscation techniques like \"Maximum Mark Magyk\" and \"Symbolic Masking & Context Redirection\" successfully manipulate scores, achieving decision flip rates of up to 86.26% in open-source models, while exposing distinct \"reasoning traps\" in proprietary systems. We release our complete dataset and injection framework to facilitate further research on the topic (https://anonymous.4open.sciencer/llm-jailbreak-FC9E/).", "published": "2025-12-11T09:13:36Z", "updated": "2026-01-06T12:04:46Z", "authors": ["Devanshu Sahoo", "Manish Prasad", "Vasudev Majhi", "Jahnvi Singh", "Vinay Chamola", "Yash Sinha", "Murari Mandal", "Dhruv Kumar"], "pdf_url": "https://arxiv.org/pdf/2512.10449v3"}
{"id": "http://arxiv.org/abs/2601.02949v1", "title": "Exploring Blockchain Interoperability: Frameworks, Use Cases, and Future Challenges", "summary": "Trust between entities in any scenario without a trusted third party is very difficult, and trust is exactly what blockchain aims to bring into the digital world with its basic features. Many applications are moving to blockchain adoption, enabling users to work in a trustworthy manner. The early generations of blockchain have a problem; they cannot share information with other blockchains. As more and more entities move their applications to the blockchain, they generate large volumes of data, and as applications have become more complex, sharing information between different blockchains has become a necessity. This has led to the research and development of interoperable solutions allowing blockchains to connect together. This paper discusses a few blockchain platforms that provide interoperable solutions, emphasising their ability to connect heterogeneous blockchains. It also discusses a case study scenario to illustrate the importance and benefits of using interoperable solutions. We also present a few topics that need to be solved in the realm of interoperability.", "published": "2026-01-06T11:46:29Z", "updated": "2026-01-06T11:46:29Z", "authors": ["Stanly Wilson", "Kwabena Adu-Duodu", "Yinhao Li", "Ellis Solaiman", "Omer Rana", "Rajiv Ranjan"], "pdf_url": "https://arxiv.org/pdf/2601.02949v1"}
{"id": "http://arxiv.org/abs/2601.02947v1", "title": "Quality Degradation Attack in Synthetic Data", "summary": "Synthetic Data Generation (SDG) can be used to facilitate privacy-preserving data sharing. However, most existing research focuses on privacy attacks where the adversary is the recipient of the released synthetic data and attempts to infer sensitive information from it. This study investigates quality degradation attacks initiated by adversaries who possess access to the real dataset or control over the generation process, such as the data owner, the synthetic data provider, or potential intruders. We formalize a corresponding threat model and empirically evaluate the effectiveness of targeted manipulations of real data (e.g., label flipping and feature-importance-based interventions) on the quality of generated synthetic data. The results show that even small perturbations can substantially reduce downstream predictive performance and increase statistical divergence, exposing vulnerabilities within SDG pipelines. This study highlights the need to integrate integrity verification and robustness mechanisms, alongside privacy protection, to ensure the reliability and trustworthiness of synthetic data sharing frameworks.", "published": "2026-01-06T11:43:31Z", "updated": "2026-01-06T11:43:31Z", "authors": ["Qinyi Liu", "Dong Liu", "Farhad Vadiee", "Mohammad Khalil", "Pedro P. Vergara Barrios"], "pdf_url": "https://arxiv.org/pdf/2601.02947v1"}
{"id": "http://arxiv.org/abs/2601.02941v1", "title": "SastBench: A Benchmark for Testing Agentic SAST Triage", "summary": "SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity, employed by commercial and non-commercial organizations to identify potential vulnerabilities in software. Despite their great utility, they generate numerous false positives, requiring costly manual filtering (aka triage). While LLM-powered agents show promise for automating cybersecurity tasks, existing benchmarks fail to emulate real-world SAST finding distributions. We introduce SastBench, a benchmark for evaluating SAST triage agents that combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SastBench features an agent-agnostic design. We evaluate different agents on the benchmark and present a comparative analysis of their performance, provide a detailed analysis of the dataset, and discuss the implications for future development.", "published": "2026-01-06T11:36:30Z", "updated": "2026-01-06T11:36:30Z", "authors": ["Jake Feiglin", "Guy Dar"], "pdf_url": "https://arxiv.org/pdf/2601.02941v1"}
{"id": "http://arxiv.org/abs/2509.08248v3", "title": "EFPIX: A zero-trust encrypted flood protocol", "summary": "We propose EFPIX (Encrypted Flood Protocol for Information eXchange), a flood-based relay communication protocol that achieves end-to-end encryption, plausible deniability for users, and untraceable messages while hiding metadata, such as sender and receiver, from those not involved. It also has built-in spam resistance and multiple optional enhancements. It can be used in privacy-critical communication, infrastructure-loss scenarios, space/research/military communication, where central servers are infeasible, or general-purpose messaging.", "published": "2025-09-10T03:06:42Z", "updated": "2026-01-06T11:09:03Z", "authors": ["Arin Upadhyay"], "pdf_url": "https://arxiv.org/pdf/2509.08248v3"}
{"id": "http://arxiv.org/abs/2601.02914v1", "title": "Vulnerabilities of Audio-Based Biometric Authentication Systems Against Deepfake Speech Synthesis", "summary": "As audio deepfakes transition from research artifacts to widely available commercial tools, robust biometric authentication faces pressing security threats in high-stakes industries. This paper presents a systematic empirical evaluation of state-of-the-art speaker authentication systems based on a large-scale speech synthesis dataset, revealing two major security vulnerabilities: 1) modern voice cloning models trained on very small samples can easily bypass commercial speaker verification systems; and 2) anti-spoofing detectors struggle to generalize across different methods of audio synthesis, leading to a significant gap between in-domain performance and real-world robustness. These findings call for a reconsideration of security measures and stress the need for architectural innovations, adaptive defenses, and the transition towards multi-factor authentication.", "published": "2026-01-06T10:55:32Z", "updated": "2026-01-06T10:55:32Z", "authors": ["Mengze Hong", "Di Jiang", "Zeying Xie", "Weiwei Zhao", "Guan Wang", "Chen Jason Zhang"], "pdf_url": "https://arxiv.org/pdf/2601.02914v1"}
{"id": "http://arxiv.org/abs/2601.02855v1", "title": "Context-aware Privacy Bounds for Linear Queries", "summary": "Linear queries, as the basis of broad analysis tasks, are often released through privacy mechanisms based on differential privacy (DP), the most popular framework for privacy protection. However, DP adopts a context-free definition that operates independently of the data-generating distribution. In this paper, we revisit the privacy analysis of the Laplace mechanism through the lens of pointwise maximal leakage (PML). We demonstrate that the distribution-agnostic definition of the DP framework often mandates excessive noise. To address this, we incorporate an assumption about the prior distribution by lower-bounding the probability of any single record belonging to any specific class. With this assumption, we derive a tight, context-aware leakage bound for general linear queries, and prove that our derived bound is strictly tighter than the standard DP guarantee and converges to the DP guarantee as this probability lower bound approaches zero. Numerical evaluations demonstrate that by exploiting this prior knowledge, the required noise scale can be reduced while maintaining privacy guarantees.", "published": "2026-01-06T09:34:04Z", "updated": "2026-01-06T09:34:04Z", "authors": ["Heng Zhao", "Sara Saeidian", "Tobias J. Oechtering"], "pdf_url": "https://arxiv.org/pdf/2601.02855v1"}
{"id": "http://arxiv.org/abs/2412.07261v3", "title": "MemHunter: Automated and Verifiable Memorization Detection at Dataset-scale in LLMs", "summary": "Large language models (LLMs) have been shown to memorize and reproduce content from their training data, raising significant privacy concerns, especially with web-scale datasets. Existing methods for detecting memorization are primarily sample-specific, relying on manually crafted or discretely optimized memory-inducing prompts generated on a per-sample basis, which become impractical for dataset-level detection due to the prohibitive computational cost of iterating through all samples. In real-world scenarios, data owners may need to verify whether a susceptible LLM has memorized their dataset, particularly if the LLM may have collected the data from the web without authorization. To address this, we introduce MemHunter, which trains a memory-inducing LLM and employs hypothesis testing to efficiently detect memorization at the dataset level, without requiring sample-specific memory inducing. Experiments on models like Pythia and Llama demonstrate that MemHunter can extract up to 40% more training data than existing methods under constrained time resources and reduce search time by up to 80% when integrated as a plug-in. Crucially, MemHunter is the first method capable of dataset-level memorization detection, providing a critical tool for assessing privacy risks in LLMs powered by large-scale datasets.", "published": "2024-12-10T07:42:46Z", "updated": "2026-01-06T07:27:23Z", "authors": ["Zhenpeng Wu", "Jian Lou", "Zibin Zheng", "Chuan Chen"], "pdf_url": "https://arxiv.org/pdf/2412.07261v3"}
{"id": "http://arxiv.org/abs/2506.11486v2", "title": "On Differential and Boomerang Properties of a Class of Binomials over Finite Fields of Odd Characteristic", "summary": "In this paper, we investigate the differential and boomerang properties of a class of binomial $F_{r,u}(x) = x^r(1 + uχ(x))$ over the finite field $\\mathbb{F}_{p^n}$, where $r = \\frac{p^n+1}{4}$, $p^n \\equiv 3 \\pmod{4}$, and $χ(x) = x^{\\frac{p^n -1}{2}}$ is the quadratic character in $\\mathbb{F}_{p^n}$. We show that $F_{r,\\pm1}$ is locally-PN with boomerang uniformity $0$ when $p^n \\equiv 3 \\pmod{8}$. To the best of our knowledge, it is the second known non-PN function class with boomerang uniformity $0$, and the first such example over odd characteristic fields with $p > 3$. Moreover, we show that $F_{r,\\pm1}$ is locally-APN with boomerang uniformity at most $2$ when $p^n \\equiv 7 \\pmod{8}$. We also provide complete classifications of the differential and boomerang spectra of $F_{r,\\pm1}$. Furthermore, we thoroughly investigate the differential uniformity of $F_{r,u}$ for $u\\in \\mathbb{F}_{p^n}^* \\setminus \\{\\pm1\\}$.", "published": "2025-06-13T06:23:32Z", "updated": "2026-01-06T06:43:16Z", "authors": ["Namhun Koo", "Soonhak Kwon"], "pdf_url": "https://arxiv.org/pdf/2506.11486v2"}
{"id": "http://arxiv.org/abs/2601.02751v1", "title": "Window-based Membership Inference Attacks Against Fine-tuned Large Language Models", "summary": "Most membership inference attacks (MIAs) against Large Language Models (LLMs) rely on global signals, like average loss, to identify training data. This approach, however, dilutes the subtle, localized signals of memorization, reducing attack effectiveness. We challenge this global-averaging paradigm, positing that membership signals are more pronounced within localized contexts. We introduce WBC (Window-Based Comparison), which exploits this insight through a sliding window approach with sign-based aggregation. Our method slides windows of varying sizes across text sequences, with each window casting a binary vote on membership based on loss comparisons between target and reference models. By ensembling votes across geometrically spaced window sizes, we capture memorization patterns from token-level artifacts to phrase-level structures. Extensive experiments across eleven datasets demonstrate that WBC substantially outperforms established baselines, achieving higher AUC scores and 2-3 times improvements in detection rates at low false positive thresholds. Our findings reveal that aggregating localized evidence is fundamentally more effective than global averaging, exposing critical privacy vulnerabilities in fine-tuned LLMs.", "published": "2026-01-06T06:37:27Z", "updated": "2026-01-06T06:37:27Z", "authors": ["Yuetian Chen", "Yuntao Du", "Kaiyuan Zhang", "Ashish Kundu", "Charles Fleming", "Bruno Ribeiro", "Ninghui Li"], "pdf_url": "https://arxiv.org/pdf/2601.02751v1"}
{"id": "http://arxiv.org/abs/2601.02720v1", "title": "Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System", "summary": "Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.", "published": "2026-01-06T05:18:03Z", "updated": "2026-01-06T05:18:03Z", "authors": ["Yuqiao Xu", "Mina Namazi", "Sahith Reddy Jalapally", "Osama Zafar", "Youngjin Yoo", "Erman Ayday"], "pdf_url": "https://arxiv.org/pdf/2601.02720v1"}
{"id": "http://arxiv.org/abs/2509.07457v2", "title": "A Decade-long Landscape of Advanced Persistent Threats: Longitudinal Analysis and Global Trends", "summary": "An advanced persistent threat (APT) refers to a covert, long-term cyberattack, typically conducted by state-sponsored actors, targeting critical sectors and often remaining undetected for long periods. In response, collective intelligence from around the globe collaborates to identify and trace surreptitious activities, generating substantial documentation on APT campaigns publicly available on the web. While prior works predominantly focus on specific aspects of APT cases, such as detection, evaluation, cyber threat intelligence, and dataset creation, limited attention has been devoted to revisiting and investigating these scattered dossiers in a longitudinal manner. The objective of our study is to fill the gap by offering a macro perspective, connecting key insights and global trends in past APT attacks. We systematically analyze six reliable sources-three focused on technical reports and another three on threat actors-examining 1,509 APT dossiers (24,215 pages) spanning 2014-2023, and identifying 603 unique APT groups worldwide. To efficiently unearth relevant information, we employ a hybrid methodology that combines rule-based information retrieval with large-language-model-based search techniques. Our longitudinal analysis reveals shifts in threat actor activities, global attack vectors, changes in targeted sectors, and relationships between cyberattacks and significant events such as elections or wars, which provide insights into historical patterns in APT evolution. Over the past decade, 154 countries have been affected, primarily using malicious documents and spear phishing as dominant initial infiltration vectors, with a noticeable decline in zero-day exploitation since 2016. Furthermore, we present our findings through interactive visualization tools, such as an APT map or flow diagram, to facilitate intuitive understanding of global patterns and trends in APT activities.", "published": "2025-09-09T07:26:15Z", "updated": "2026-01-06T04:19:50Z", "authors": ["Shakhzod Yuldoshkhujaev", "Mijin Jeon", "Doowon Kim", "Nick Nikiforakis", "Hyungjoon Koo"], "pdf_url": "https://arxiv.org/pdf/2509.07457v2"}
{"id": "http://arxiv.org/abs/2601.02698v1", "title": "Enterprise Identity Integration for AI-Assisted Developer Services: Architecture, Implementation, and Case Study", "summary": "AI-assisted developer services are increasingly embedded in modern IDEs, yet enterprises must ensure these tools operate within existing identity, access control, and governance requirements. The Model Context Protocol (MCP) enables AI assistants to retrieve structured internal context, but its specification provides only a minimal authorization model and lacks guidance on integrating enterprise SSO. This article presents a practical architecture that incorporates OAuth 2.0 and OpenID Connect (OIDC) into MCP-enabled developer environments. It describes how IDE extensions obtain and present tokens, how MCP servers validate them through an identity provider, and how scopes and claims can enforce least-privilege access. A prototype implementation using Visual Studio Code, a Python-based MCP server, and an OIDC-compliant IdP demonstrates feasibility. A case study evaluates authentication latency, token-validation overhead, operational considerations, and AI-specific risks. The approach provides a deployable pattern for organizations adopting AI-assisted developer tools while maintaining identity assurance and auditability.", "published": "2026-01-06T04:17:52Z", "updated": "2026-01-06T04:17:52Z", "authors": ["Manideep Reddy Chinthareddy"], "pdf_url": "https://arxiv.org/pdf/2601.02698v1"}
{"id": "http://arxiv.org/abs/2601.02680v1", "title": "Adversarial Contrastive Learning for LLM Quantization Attacks", "summary": "Model quantization is critical for deploying large language models (LLMs) on resource-constrained hardware, yet recent work has revealed severe security risks that benign LLMs in full precision may exhibit malicious behaviors after quantization. In this paper, we propose Adversarial Contrastive Learning (ACL), a novel gradient-based quantization attack that achieves superior attack effectiveness by explicitly maximizing the gap between benign and harmful responses probabilities. ACL formulates the attack objective as a triplet-based contrastive loss, and integrates it with a projected gradient descent two-stage distributed fine-tuning strategy to ensure stable and efficient optimization. Extensive experiments demonstrate ACL's remarkable effectiveness, achieving attack success rates of 86.00% for over-refusal, 97.69% for jailbreak, and 92.40% for advertisement injection, substantially outperforming state-of-the-art methods by up to 44.67%, 18.84%, and 50.80%, respectively.", "published": "2026-01-06T03:26:11Z", "updated": "2026-01-06T03:26:11Z", "authors": ["Dinghong Song", "Zhiwei Xu", "Hai Wan", "Xibin Zhao", "Pengfei Su", "Dong Li"], "pdf_url": "https://arxiv.org/pdf/2601.02680v1"}
{"id": "http://arxiv.org/abs/2505.01538v2", "title": "HONEYBEE: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning", "summary": "Enterprise deployments of vector databases require access control policies to protect sensitive data. These systems often implement access control through hybrid vector queries that combine nearest-neighbor search with relational predicates based on user permissions. However, existing approaches face a fundamental trade-off: dedicated per-user indexes minimize query latency but incur high memory redundancy, while shared indexes with post-search filtering reduce memory overhead at the cost of increased latency. This paper introduces HONEYBEE, a dynamic partitioning framework that leverages the structure of Role-Based Access Control (RBAC) policies to create a smooth trade-off between these extremes. RBAC policies organize users into roles and assign permissions at the role level, creating a natural ``thin waist\" in the permission structure that is ideal for partitioning decisions. Specifically, HONEYBEE produces overlapping partitions where vectors can be strategically replicated across different partitions to reduce query latency while controlling memory overhead. To guide these decisions, HONEYBEE develops analytical models of vector search performance and recall, and formulates partitioning as a constrained optimization problem that balances memory usage, query efficiency, and recall. Evaluations on RBAC workloads demonstrate that HONEYBEE achieves up to 13.5X lower query latency than row-level security with only a 1.24X increase in memory usage, while achieving comparable query performance to dedicated, per-role indexes with 90.4% reduction in additional memory consumption, offering a practical middle ground for secure and efficient vector search.", "published": "2025-05-02T18:59:31Z", "updated": "2026-01-06T00:53:33Z", "authors": ["Hongbin Zhong", "Matthew Lentz", "Nina Narodytska", "Adriana Szekeres", "Kexin Rong"], "pdf_url": "https://arxiv.org/pdf/2505.01538v2"}
{"id": "http://arxiv.org/abs/2601.02624v1", "title": "LAsset: An LLM-assisted Security Asset Identification Framework for System-on-Chip (SoC) Verification", "summary": "The growing complexity of modern system-on-chip (SoC) and IP designs is making security assurance difficult day by day. One of the fundamental steps in the pre-silicon security verification of a hardware design is the identification of security assets, as it substantially influences downstream security verification tasks, such as threat modeling, security property generation, and vulnerability detection. Traditionally, assets are determined manually by security experts, requiring significant time and expertise. To address this challenge, we present LAsset, a novel automated framework that leverages large language models (LLMs) to identify security assets from both hardware design specifications and register-transfer level (RTL) descriptions. The framework performs structural and semantic analysis to identify intra-module primary and secondary assets and derives inter-module relationships to systematically characterize security dependencies at the design level. Experimental results show that the proposed framework achieves high classification accuracy, reaching up to 90% recall rate in SoC design, and 93% recall rate in IP designs. This automation in asset identification significantly reduces manual overhead and supports a scalable path forward for secure hardware development.", "published": "2026-01-06T00:53:23Z", "updated": "2026-01-06T00:53:23Z", "authors": ["Md Ajoad Hasan", "Dipayan Saha", "Khan Thamid Hasan", "Nashmin Alam", "Azim Uddin", "Sujan Kumar Saha", "Mark Tehranipoor", "Farimah Farahmandi"], "pdf_url": "https://arxiv.org/pdf/2601.02624v1"}
{"id": "http://arxiv.org/abs/2506.06572v2", "title": "Cyber Security of Sensor Systems for State Sequence Estimation: an AI Approach", "summary": "Sensor systems are extremely popular today and vulnerable to sensor data attacks. Due to possible devastating consequences, counteracting sensor data attacks is an extremely important topic, which has not seen sufficient study. This paper develops the first methods that accurately identify/eliminate only the problematic attacked sensor data presented to a sequence estimation/regression algorithm under a powerful attack model constructed based on known/observed attacks. The approach does not assume a known form for the statistical model of the sensor data, allowing data-driven and machine learning sequence estimation/regression algorithms to be protected. A simple protection approach for attackers not endowed with knowledge of the details of our protection approach is first developed, followed by additional processing for attacks based on protection system knowledge. In the cases tested for which it was designed, experimental results show that the simple approach achieves performance indistinguishable, to two decimal places, from that for an approach which knows which sensors are attacked. For cases where the attacker has knowledge of the protection approach, experimental results indicate the additional processing can be configured so that the worst-case degradation under the additional processing and a large number of sensors attacked can be made significantly smaller than the worst-case degradation of the simple approach, and close to an approach which knows which sensors are attacked, for the same number of attacked sensors with just a slight degradation under no attacks. Mathematical descriptions of the worst-case attacks are used to demonstrate the additional processing will provide similar advantages for cases for which we do not have numerical results. All the data-driven processing used in our approaches employ only unattacked training data.", "published": "2025-06-06T22:51:44Z", "updated": "2026-01-06T00:04:18Z", "authors": ["Xubin Fang", "Rick S. Blum", "Ramesh Bharadwaj", "Brian M. Sadler"], "pdf_url": "https://arxiv.org/pdf/2506.06572v2"}
