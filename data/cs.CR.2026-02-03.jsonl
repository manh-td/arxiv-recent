{"id": "http://arxiv.org/abs/2602.03489v1", "title": "Detecting and Explaining Malware Family Evolution Using Rule-Based Drift Analysis", "summary": "Malware detection and classification into families are critical tasks in cybersecurity, complicated by the continual evolution of malware to evade detection. This evolution introduces concept drift, in which the statistical properties of malware features change over time, reducing the effectiveness of static machine learning models. Understanding and explaining this drift is essential for maintaining robust and trustworthy malware detectors. In this paper, we propose an interpretable approach to concept drift detection. Our method uses a rule-based classifier to generate human-readable descriptions of both original and evolved malware samples belonging to the same malware family. By comparing the resulting rule sets using a similarity function, we can detect and quantify concept drift. Crucially, this comparison also identifies the specific features and feature values that have changed, providing clear explanations of how malware has evolved to bypass detection. Experimental results demonstrate that the proposed method not only accurately detects drift but also provides actionable insights into the behavior of evolving malware families, supporting both detection and threat analysis.", "published": "2026-02-03T13:07:14Z", "updated": "2026-02-03T13:07:14Z", "authors": ["Olha Jurečková", "Martin Jureček"], "pdf_url": "https://arxiv.org/pdf/2602.03489v1"}
{"id": "http://arxiv.org/abs/2602.03470v1", "title": "Reading Between the Code Lines: On the Use of Self-Admitted Technical Debt for Security Analysis", "summary": "Static Analysis Tools (SATs) are central to security engineering activities, as they enable early identification of code weaknesses without requiring execution. However, their effectiveness is often limited by high false-positive rates and incomplete coverage of vulnerability classes. At the same time, developers frequently document security-related shortcuts and compromises as Self-Admitted Technical Debt (SATD) in software artifacts, such as code comments. While prior work has recognized SATD as a rich source of security information, it remains unclear whether -and in what ways- it is utilized during SAT-aided security analysis. OBJECTIVE: This work investigates the extent to which security-related SATD complements the output produced by SATs and helps bridge some of their well-known limitations. METHOD: We followed a mixed-methods approach consisting of (i) the analysis of a SATD-annotated vulnerability dataset using three state-of-the-art SATs and (ii) an online survey with 72 security practitioners. RESULTS: The combined use of all SATs flagged 114 of the 135 security-related SATD instances, spanning 24 distinct Common Weakness Enumeration (CWE) identifiers. A manual mapping of the SATD comments revealed 33 unique CWE types, 6 of which correspond to categories that SATs commonly overlook or struggle to detect (e.g., race conditions). Survey responses further suggest that developers frequently pair SAT outputs with SATD insights to better understand the impact and root causes of security weaknesses and to identify suitable fixes. IMPLICATIONS: Our findings show that such SATD-encoded information can be a meaningful complement to SAT-driven security analysis, while helping to overcome some of SATs' practical shortcomings.", "published": "2026-02-03T12:43:16Z", "updated": "2026-02-03T12:43:16Z", "authors": ["Nicolás E. Díaz Ferreyra", "Moritz Mock", "Max Kretschmann", "Barbara Russo", "Mojtaba Shahin", "Mansooreh Zahedi", "Riccardo Scandariato"], "pdf_url": "https://arxiv.org/pdf/2602.03470v1"}
{"id": "http://arxiv.org/abs/2507.09580v5", "title": "AICrypto: Evaluating Cryptography Capabilities of Large Language Models", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across a variety of domains. However, their applications in cryptography, which serve as a foundational pillar of cybersecurity, remain largely unexplored. To address this gap, we build \\textbf{AICrypto}, a comprehensive benchmark designed to evaluate the cryptography capabilities of LLMs. The benchmark comprises 135 multiple-choice questions, 150 capture-the-flag challenges, and 30 proof problems, covering a broad range of skills from knowledge memorization to vulnerability exploitation and formal reasoning. All tasks are carefully reviewed or constructed by cryptography experts to improve correctness and rigor. For each proof problem, we provide detailed scoring rubrics and reference solutions that enable automated grading, achieving high correlation with human expert evaluations. We introduce strong human expert performance baselines for comparison across all task types. Our evaluation of 17 leading LLMs reveals that state-of-the-art models match or even surpass human experts in memorizing cryptographic concepts, exploiting common vulnerabilities, and routine proofs. However, our analysis reveals that they still lack a deep understanding of abstract mathematical concepts and struggle with tasks that require multi-step reasoning and dynamic analysis. We hope this work could provide insights for future research on LLMs in cryptographic applications. Our code and dataset are available at https://aicryptobench.github.io/.", "published": "2025-07-13T11:11:01Z", "updated": "2026-02-03T12:04:57Z", "authors": ["Yu Wang", "Yijian Liu", "Liheng Ji", "Han Luo", "Wenjie Li", "Xiaofei Zhou", "Chiyun Feng", "Puji Wang", "Yuhan Cao", "Geyuan Zhang", "Xiaojian Li", "Rongwu Xu", "Yilei Chen", "Tianxing He"], "pdf_url": "https://arxiv.org/pdf/2507.09580v5"}
{"id": "http://arxiv.org/abs/2602.03423v1", "title": "Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection", "summary": "The proliferation of generative AI poses challenges for information integrity assurance, requiring systems that connect model governance with end-user verification. We present Origin Lens, a privacy-first mobile framework that targets visual disinformation through a layered verification architecture. Unlike server-side detection systems, Origin Lens performs cryptographic image provenance verification and AI detection locally on the device via a Rust/Flutter hybrid architecture. Our system integrates multiple signals - including cryptographic provenance, generative model fingerprints, and optional retrieval-augmented verification - to provide users with graded confidence indicators at the point of consumption. We discuss the framework's alignment with regulatory requirements (EU AI Act, DSA) and its role in verification infrastructure that complements platform-level mechanisms.", "published": "2026-02-03T11:49:00Z", "updated": "2026-02-03T11:49:00Z", "authors": ["Alexander Loth", "Dominique Conceicao Rosario", "Peter Ebinger", "Martin Kappes", "Marc-Oliver Pahl"], "pdf_url": "https://arxiv.org/pdf/2602.03423v1"}
{"id": "http://arxiv.org/abs/2602.03421v1", "title": "On (Im)possibility of Network Oblivious Transfer via Noisy Channels and Non-Signaling Correlations", "summary": "This work investigates the fundamental limits of implementing network oblivious transfer via noisy multiple access channels and broadcast channels between honest-but-curious parties when the parties have access to general tripartite non-signaling correlations. By modeling the shared resource as an arbitrary tripartite non-signaling box, we obtain a unified perspective on both the channel behavior and the resulting correlations. Our main result demonstrates that perfect oblivious transfer is impossible. In the asymptotic regime, we further show that even negligible leakage cannot be achieved, as repeated use of the resource amplifies the receiver(s)'s ability to distinguish messages that were not intended for him/them. In contrast, the receiver(s)'s own privacy is not subject to a universal impossibility limitation.", "published": "2026-02-03T11:45:43Z", "updated": "2026-02-03T11:45:43Z", "authors": ["Hadi Aghaee", "Christian Deppe", "Holger Boche"], "pdf_url": "https://arxiv.org/pdf/2602.03421v1"}
{"id": "http://arxiv.org/abs/2509.24440v3", "title": "Evaluating Relayed and Switched Quantum Key Distribution (QKD) Network Architectures", "summary": "We evaluate the performance of two architectures for network-wide quantum key distribution (QKD): Relayed QKD, which relays keys over multi-link QKD paths for non-adjacent nodes, and Switched QKD, which uses optical switches to dynamically connect arbitrary QKD modules to form direct QKD links between them. An advantage of Switched QKD is that it distributes quantum keys end-to-end, whereas Relayed relies on trusted nodes. However, Switched depends on arbitrary matching of QKD modules. We first experimentally evaluate the performance of commercial DV-QKD modules; for each of three vendors we benchmark the performance in standard/matched module pairs and in unmatched pairs to emulate configurations in the Switched QKD network architecture. The analysis reveals that in some cases a notable variation in the generated secret key rate (SKR) between the matched and unmatched pairs is observed. Driven by these experimental findings, we conduct a comprehensive theoretical analysis that evaluates the network-wide performance of the two architectures. Our analysis is based on uniform ring networks, where we derive optimal key management configurations and analytical formulas for the achievable consumed SKR. We compare network performance under varying ring sizes, QKD link losses, QKD receivers' sensitivity and performance penalties of unmatched modules. Our findings indicate that Switched QKD performs better in dense rings (short distances, large node counts), while Relayed QKD is more effective in longer distances and large node counts. Moreover, we confirm that unmatched QKD modules penalties significantly impact the efficiency of Switched QKD architecture.", "published": "2025-09-29T08:24:24Z", "updated": "2026-02-03T11:06:14Z", "authors": ["Antonis Selentis", "Nikolas Makris", "Alkinoos Papageorgopoulos", "Persefoni Konteli", "Konstantinos Christodoulopoulos", "George T. Kanellos", "Dimitris Syvridis"], "pdf_url": "https://arxiv.org/pdf/2509.24440v3"}
{"id": "http://arxiv.org/abs/2602.03377v1", "title": "SEW: Strengthening Robustness of Black-box DNN Watermarking via Specificity Enhancement", "summary": "To ensure the responsible distribution and use of open-source deep neural networks (DNNs), DNN watermarking has become a crucial technique to trace and verify unauthorized model replication or misuse. In practice, black-box watermarks manifest as specific predictive behaviors for specially crafted samples. However, due to the generalization nature of DNNs, the keys to extracting the watermark message are not unique, which would provide attackers with more opportunities. Advanced attack techniques can reverse-engineer approximate replacements for the original watermark keys, enabling subsequent watermark removal. In this paper, we explore black-box DNN watermarking specificity, which refers to the accuracy of a watermark's response to a key. Using this concept, we introduce Specificity-Enhanced Watermarking (SEW), a new method that improves specificity by reducing the association between the watermark and approximate keys. Through extensive evaluation using three popular watermarking benchmarks, we validate that enhancing specificity significantly contributes to strengthening robustness against removal attacks. SEW effectively defends against six state-of-the-art removal attacks, while maintaining model usability and watermark verification performance.", "published": "2026-02-03T10:55:27Z", "updated": "2026-02-03T10:55:27Z", "authors": ["Huming Qiu", "Mi Zhang", "Junjie Sun", "Peiyi Chen", "Xiaohan Zhang", "Min Yang"], "pdf_url": "https://arxiv.org/pdf/2602.03377v1"}
{"id": "http://arxiv.org/abs/2602.03328v1", "title": "GuardReasoner-Omni: A Reasoning-based Multi-modal Guardrail for Text, Image, and Video", "summary": "We present GuardReasoner-Omni, a reasoning-based guardrail model designed to moderate text, image, and video data. First, we construct a comprehensive training corpus comprising 148k samples spanning these three modalities. Our training pipeline follows a two-stage paradigm to incentivize the model to deliberate before making decisions: (1) conducting SFT to cold-start the model with explicit reasoning capabilities and structural adherence; and (2) performing RL, incorporating an error-driven exploration reward to incentivize deeper reasoning on hard samples. We release a suite of models scaled at 2B and 4B parameters. Extensive experiments demonstrate that GuardReasoner-Omni achieves superior performance compared to existing state-of-the-art baselines across various guardrail benchmarks. Notably, GuardReasoner-Omni (2B) significantly surpasses the runner-up by 5.3% F1 score.", "published": "2026-02-03T09:56:20Z", "updated": "2026-02-03T09:56:20Z", "authors": ["Zhenhao Zhu", "Yue Liu", "Yanpei Guo", "Wenjie Qu", "Cancan Chen", "Yufei He", "Yibo Li", "Yulin Chen", "Tianyi Wu", "Huiying Xu", "Xinzhong Zhu", "Jiaheng Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.03328v1"}
{"id": "http://arxiv.org/abs/2602.03284v1", "title": "Time Is All It Takes: Spike-Retiming Attacks on Event-Driven Spiking Neural Networks", "summary": "Spiking neural networks (SNNs) compute with discrete spikes and exploit temporal structure, yet most adversarial attacks change intensities or event counts instead of timing. We study a timing-only adversary that retimes existing spikes while preserving spike counts and amplitudes in event-driven SNNs, thus remaining rate-preserving. We formalize a capacity-1 spike-retiming threat model with a unified trio of budgets: per-spike jitter $\\mathcal{B}_{\\infty}$, total delay $\\mathcal{B}_{1}$, and tamper count $\\mathcal{B}_{0}$. Feasible adversarial examples must satisfy timeline consistency and non-overlap, which makes the search space discrete and constrained. To optimize such retimings at scale, we use projected-in-the-loop (PIL) optimization: shift-probability logits yield a differentiable soft retiming for backpropagation, and a strict projection in the forward pass produces a feasible discrete schedule that satisfies capacity-1, non-overlap, and the chosen budget at every step. The objective maximizes task loss on the projected input and adds a capacity regularizer together with budget-aware penalties, which stabilizes gradients and aligns optimization with evaluation. Across event-driven benchmarks (CIFAR10-DVS, DVS-Gesture, N-MNIST) and diverse SNN architectures, we evaluate under binary and integer event grids and a range of retiming budgets, and also test models trained with timing-aware adversarial training designed to counter timing-only attacks. For example, on DVS-Gesture the attack attains high success (over $90\\%$) while touching fewer than $2\\%$ of spikes under $\\mathcal{B}_{0}$. Taken together, our results show that spike retiming is a practical and stealthy attack surface that current defenses struggle to counter, providing a clear reference for temporal robustness in event-driven SNNs. Code is available at https://github.com/yuyi-sd/Spike-Retiming-Attacks.", "published": "2026-02-03T09:06:53Z", "updated": "2026-02-03T09:06:53Z", "authors": ["Yi Yu", "Qixin Zhang", "Shuhan Ye", "Xun Lin", "Qianshan Wei", "Kun Wang", "Wenhan Yang", "Dacheng Tao", "Xudong Jiang"], "pdf_url": "https://arxiv.org/pdf/2602.03284v1"}
{"id": "http://arxiv.org/abs/2602.03271v1", "title": "LogicScan: An LLM-driven Framework for Detecting Business Logic Vulnerabilities in Smart Contracts", "summary": "Business logic vulnerabilities have become one of the most damaging yet least understood classes of smart contract vulnerabilities. Unlike traditional bugs such as reentrancy or arithmetic errors, these vulnerabilities arise from missing or incorrectly enforced business invariants and are tightly coupled with protocol semantics. Existing static analysis techniques struggle to capture such high-level logic, while recent large language model based approaches often suffer from unstable outputs and low accuracy due to hallucination and limited verification.\n  In this paper, we propose LogicScan, an automated contrastive auditing framework for detecting business logic vulnerabilities in smart contracts. The key insight behind LogicScan is that mature, widely deployed on-chain protocols implicitly encode well-tested and consensus-driven business invariants. LogicScan systematically mines these invariants from large-scale on-chain contracts and reuses them as reference constraints to audit target contracts. To achieve this, LogicScan introduces a Business Specification Language (BSL) to normalize diverse implementation patterns into structured, verifiable logic representations. It further combines noise-aware logic aggregation with contrastive auditing to identify missing or weakly enforced invariants while mitigating LLM-induced false positives.\n  We evaluate LogicScan on three real-world datasets, including DeFiHacks, Web3Bugs, and a set of top-200 audited contracts. The results show that LogicScan achieves an F1 score of 85.2%, significantly outperforming state-of-the-art tools while maintaining a low false-positive rate on production-grade contracts. Additional experiments demonstrate that LogicScan maintains consistent performance across different LLMs and is cost-effective, and that its false-positive suppression mechanisms substantially improve robustness.", "published": "2026-02-03T08:56:53Z", "updated": "2026-02-03T08:56:53Z", "authors": ["Jiaqi Gao", "Zijian Zhang", "Yuqiang Sun", "Ye Liu", "Chengwei Liu", "Han Liu", "Yi Li", "Yang Liu"], "pdf_url": "https://arxiv.org/pdf/2602.03271v1"}
{"id": "http://arxiv.org/abs/2602.03127v1", "title": "Cyber Insurance, Audit, and Policy: Review, Analysis and Recommendations", "summary": "Cyber insurance, which protects insured organizations against financial losses from cyberattacks and data breaches, can be difficult and expensive to obtain for many organizations. These difficulties stem from insurers difficulty in understanding and accurately assessing the risks that they are undertaking. Cybersecurity audits, which are already implemented in many organizations for compliance and other purposes, present a potential solution to this challenge. This paper provides a structured review and analysis of prior work in this area, analysis of the challenges and potential benefits that cyber audits provide and recommendations for the use of cyber audits to reduce cyber insurance costs and improve its availability.", "published": "2026-02-03T05:37:49Z", "updated": "2026-02-03T05:37:49Z", "authors": ["Danielle Jean Hanson", "Jeremy Straub"], "pdf_url": "https://arxiv.org/pdf/2602.03127v1"}
{"id": "http://arxiv.org/abs/2602.03117v1", "title": "AgentDyn: A Dynamic Open-Ended Benchmark for Evaluating Prompt Injection Attacks of Real-World Agent Security System", "summary": "AI agents that autonomously interact with external tools and environments show great promise across real-world applications. However, the external data which agent consumes also leads to the risk of indirect prompt injection attacks, where malicious instructions embedded in third-party content hijack agent behavior. Guided by benchmarks, such as AgentDojo, there has been significant amount of progress in developing defense against the said attacks. As the technology continues to mature, and that agents are increasingly being relied upon for more complex tasks, there is increasing pressing need to also evolve the benchmark to reflect threat landscape faced by emerging agentic systems. In this work, we reveal three fundamental flaws in current benchmarks and push the frontier along these dimensions: (i) lack of dynamic open-ended tasks, (ii) lack of helpful instructions, and (iii) simplistic user tasks. To bridge this gap, we introduce AgentDyn, a manually designed benchmark featuring 60 challenging open-ended tasks and 560 injection test cases across Shopping, GitHub, and Daily Life. Unlike prior static benchmarks, AgentDyn requires dynamic planning and incorporates helpful third-party instructions. Our evaluation of ten state-of-the-art defenses suggests that almost all existing defenses are either not secure enough or suffer from significant over-defense, revealing that existing defenses are still far from real-world deployment. Our benchmark is available at https://github.com/leolee99/AgentDyn.", "published": "2026-02-03T05:20:42Z", "updated": "2026-02-03T05:20:42Z", "authors": ["Hao Li", "Ruoyao Wen", "Shanghao Shi", "Ning Zhang", "Chaowei Xiao"], "pdf_url": "https://arxiv.org/pdf/2602.03117v1"}
{"id": "http://arxiv.org/abs/2502.02542v3", "title": "OverThink: Slowdown Attacks on Reasoning LLMs", "summary": "Most flagship language models generate explicit reasoning chains, enabling inference-time scaling. However, producing these reasoning chains increases token usage (i.e., reasoning tokens), which in turn increases latency and costs. Our OverThink attack increases overhead for applications that rely on reasoning language models (RLMs) and external context by forcing them to spend substantially more reasoning tokens while still producing contextually correct answers. An adversary mounts an attack by injecting decoy reasoning problems into public content that is consumed by RLM at inference time. Because our decoys (e.g., Markov decision processes, Sudokus, etc.) are benign, they evade safety filters. We evaluate OverThink on both closed-source and open-source reasoning models across the FreshQA, SQuAD, and MuSR datasets. We also explore the attack in multi-modal settings by creating images that cause excessive reasoning. We show that the resulting slowdown transfers across models. Finally, we explore both LLM-based and systems-level defenses, and discuss the societal, financial, and energy implications of the OverThink attacks.", "published": "2025-02-04T18:12:41Z", "updated": "2026-02-03T05:16:57Z", "authors": ["Abhinav Kumar", "Jaechul Roh", "Ali Naseh", "Marzena Karpinska", "Mohit Iyyer", "Amir Houmansadr", "Eugene Bagdasarian"], "pdf_url": "https://arxiv.org/pdf/2502.02542v3"}
{"id": "http://arxiv.org/abs/2602.03085v1", "title": "The Trigger in the Haystack: Extracting and Reconstructing LLM Backdoor Triggers", "summary": "Detecting whether a model has been poisoned is a longstanding problem in AI security. In this work, we present a practical scanner for identifying sleeper agent-style backdoors in causal language models. Our approach relies on two key findings: first, sleeper agents tend to memorize poisoning data, making it possible to leak backdoor examples using memory extraction techniques. Second, poisoned LLMs exhibit distinctive patterns in their output distributions and attention heads when backdoor triggers are present in the input. Guided by these observations, we develop a scalable backdoor scanning methodology that assumes no prior knowledge of the trigger or target behavior and requires only inference operations. Our scanner integrates naturally into broader defensive strategies and does not alter model performance. We show that our method recovers working triggers across multiple backdoor scenarios and a broad range of models and fine-tuning methods.", "published": "2026-02-03T04:17:21Z", "updated": "2026-02-03T04:17:21Z", "authors": ["Blake Bullwinkel", "Giorgio Severi", "Keegan Hines", "Amanda Minnich", "Ram Shankar Siva Kumar", "Yonatan Zunger"], "pdf_url": "https://arxiv.org/pdf/2602.03085v1"}
{"id": "http://arxiv.org/abs/2601.16448v2", "title": "Ringmaster: How to juggle high-throughput host OS system calls from TrustZone TEEs", "summary": "Many safety-critical systems require timely processing of sensor inputs to avoid potential safety hazards. Additionally, to support useful application features, such systems increasingly have a large rich operating system (OS) at the cost of potential security bugs. Thus, if a malicious party gains supervisor privileges, they could cause real-world damage by denying service to time-sensitive programs. Many past approaches to this problem completely isolate time-sensitive programs with a hypervisor; however, this prevents the programs from accessing useful OS services. We introduce Ringmaster, a novel framework that enables enclaves or TEEs (Trusted Execution Environments) to asynchronously access rich, but potentially untrusted, OS services via Linux's io_uring. When service is denied by the untrusted OS, enclaves continue to operate on Ringmaster's minimal ARM TrustZone kernel with access to small, critical device drivers. This approach balances the need for secure, time-sensitive processing with the convenience of rich OS services. Additionally, Ringmaster supports large unmodified programs as enclaves, offering lower overhead compared to existing systems. We demonstrate how Ringmaster helps us build a working highly-secure system with minimal engineering. In our experiments with an unmanned aerial vehicle, Ringmaster achieved nearly 1GiB/sec of data into enclave on a Raspberry Pi4b, 0-3% throughput overhead compared to non-enclave tasks.", "published": "2026-01-23T05:01:45Z", "updated": "2026-02-03T03:44:36Z", "authors": ["Richard Habeeb", "Man-Ki Yoon", "Hao Chen", "Zhong Shao"], "pdf_url": "https://arxiv.org/pdf/2601.16448v2"}
{"id": "http://arxiv.org/abs/2602.03040v1", "title": "DF-LoGiT: Data-Free Logic-Gated Backdoor Attacks in Vision Transformers", "summary": "The widespread adoption of Vision Transformers (ViTs) elevates supply-chain risk on third-party model hubs, where an adversary can implant backdoors into released checkpoints. Existing ViT backdoor attacks largely rely on poisoned-data training, while prior data-free attempts typically require synthetic-data fine-tuning or extra model components. This paper introduces Data-Free Logic-Gated Backdoor Attacks (DF-LoGiT), a truly data-free backdoor attack on ViTs via direct weight editing. DF-LoGiT exploits ViT's native multi-head architecture to realize a logic-gated compositional trigger, enabling a stealthy and effective backdoor. We validate its effectiveness through theoretical analysis and extensive experiments, showing that DF-LoGiT achieves near-100% attack success with negligible degradation in benign accuracy and remains robust against representative classical and ViT-specific defenses.", "published": "2026-02-03T03:06:41Z", "updated": "2026-02-03T03:06:41Z", "authors": ["Xiaozuo Shen", "Yifei Cai", "Rui Ning", "Chunsheng Xin", "Hongyi Wu"], "pdf_url": "https://arxiv.org/pdf/2602.03040v1"}
{"id": "http://arxiv.org/abs/2602.03035v1", "title": "Generalizable and Interpretable RF Fingerprinting with Shapelet-Enhanced Large Language Models", "summary": "Deep neural networks (DNNs) have achieved remarkable success in radio frequency (RF) fingerprinting for wireless device authentication. However, their practical deployment faces two major limitations: domain shift, where models trained in one environment struggle to generalize to others, and the black-box nature of DNNs, which limits interpretability. To address these issues, we propose a novel framework that integrates a group of variable-length two-dimensional (2D) shapelets with a pre-trained large language model (LLM) to achieve efficient, interpretable, and generalizable RF fingerprinting. The 2D shapelets explicitly capture diverse local temporal patterns across the in-phase and quadrature (I/Q) components, providing compact and interpretable representations. Complementarily, the pre-trained LLM captures more long-range dependencies and global contextual information, enabling strong generalization with minimal training overhead. Moreover, our framework also supports prototype generation for few-shot inference, enhancing cross-domain performance without additional retraining. To evaluate the effectiveness of our proposed method, we conduct extensive experiments on six datasets across various protocols and domains. The results show that our method achieves superior standard and few-shot performance across both source and unseen domains.", "published": "2026-02-03T03:02:31Z", "updated": "2026-02-03T03:02:31Z", "authors": ["Tianya Zhao", "Junqing Zhang", "Haowen Xu", "Xiaoyan Sun", "Jun Dai", "Xuyu Wang"], "pdf_url": "https://arxiv.org/pdf/2602.03035v1"}
{"id": "http://arxiv.org/abs/2510.26829v2", "title": "Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning", "summary": "We show that continual pretraining on plausible misinformation can overwrite specific factual knowledge in large language models without degrading overall performance. Unlike prior poisoning work under static pretraining, we study repeated exposure to counterfactual claims during continual updates. Using paired fact-counterfact items with graded poisoning ratios, we track how internal preferences between competing facts evolve across checkpoints, layers, and model scales. Even moderate poisoning (50-100%) flips over 55% of responses from correct to counterfactual while leaving ambiguity nearly unchanged. These belief flips emerge abruptly, concentrate in late layers (e.g., Layers 29-36 in 3B models), and are partially reversible via patching (up to 56.8%). The corrupted beliefs generalize beyond poisoned prompts, selectively degrading commonsense reasoning while leaving alignment benchmarks largely intact and transferring imperfectly across languages. These results expose a failure mode of continual pre-training in which targeted misinformation replaces internal factual representations without triggering broad performance collapse, motivating representation-level monitoring of factual integrity during model updates.", "published": "2025-10-29T14:35:03Z", "updated": "2026-02-03T02:57:32Z", "authors": ["Svetlana Churina", "Niranjan Chebrolu", "Kokil Jaidka"], "pdf_url": "https://arxiv.org/pdf/2510.26829v2"}
{"id": "http://arxiv.org/abs/2602.03012v1", "title": "CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability", "summary": "Evaluating and improving the security capabilities of code agents requires high-quality, executable vulnerability tasks. However, existing works rely on costly, unscalable manual reproduction and suffer from outdated data distributions. To address these, we present CVE-Factory, the first multi-agent framework to achieve expert-level quality in automatically transforming sparse CVE metadata into fully executable agentic tasks. Cross-validation against human expert reproductions shows that CVE-Factory achieves 95\\% solution correctness and 96\\% environment fidelity, confirming its expert-level quality. It is also evaluated on the latest realistic vulnerabilities and achieves a 66.2\\% verified success. This automation enables two downstream contributions. First, we construct LiveCVEBench, a continuously updated benchmark of 190 tasks spanning 14 languages and 153 repositories that captures emerging threats including AI-tooling vulnerabilities. Second, we synthesize over 1,000 executable training environments, the first large-scale scaling of agentic tasks in code security. Fine-tuned Qwen3-32B improves from 5.3\\% to 35.8\\% on LiveCVEBench, surpassing Claude 4.5 Sonnet, with gains generalizing to Terminal Bench (12.5\\% to 31.3\\%). We open-source CVE-Factory, LiveCVEBench, Abacus-cve (fine-tuned model), training dataset, and leaderboard. All resources are available at https://github.com/livecvebench/CVE-Factory .", "published": "2026-02-03T02:27:16Z", "updated": "2026-02-03T02:27:16Z", "authors": ["Xianzhen Luo", "Jingyuan Zhang", "Shiqi Zhou", "Rain Huang", "Chuan Xiao", "Qingfu Zhu", "Zhiyuan Ma", "Xing Yue", "Yang Yue", "Wencong Zeng", "Wanxiang Che"], "pdf_url": "https://arxiv.org/pdf/2602.03012v1"}
{"id": "http://arxiv.org/abs/2506.08347v3", "title": "Differentially Private Relational Learning with Entity-level Privacy Guarantees", "summary": "Learning with relational and network-structured data is increasingly vital in sensitive domains where protecting the privacy of individual entities is paramount. Differential Privacy (DP) offers a principled approach for quantifying privacy risks, with DP-SGD emerging as a standard mechanism for private model training. However, directly applying DP-SGD to relational learning is challenging due to two key factors: (i) entities often participate in multiple relations, resulting in high and difficult-to-control sensitivity; and (ii) relational learning typically involves multi-stage, potentially coupled (interdependent) sampling procedures that make standard privacy amplification analyses inapplicable. This work presents a principled framework for relational learning with formal entity-level DP guarantees. We provide a rigorous sensitivity analysis and introduce an adaptive gradient clipping scheme that modulates clipping thresholds based on entity occurrence frequency. We also extend the privacy amplification results to a tractable subclass of coupled sampling, where the dependence arises only through sample sizes. These contributions lead to a tailored DP-SGD variant for relational data with provable privacy guarantees. Experiments on fine-tuning text encoders over text-attributed network-structured relational data demonstrate the strong utility-privacy trade-offs of our approach. Our code is available at https://github.com/Graph-COM/Node_DP.", "published": "2025-06-10T02:03:43Z", "updated": "2026-02-03T01:38:50Z", "authors": ["Yinan Huang", "Haoteng Yin", "Eli Chien", "Rongzhe Wei", "Pan Li"], "pdf_url": "https://arxiv.org/pdf/2506.08347v3"}
{"id": "http://arxiv.org/abs/2602.02962v1", "title": "Q-ShiftDP: A Differentially Private Parameter-Shift Rule for Quantum Machine Learning", "summary": "Quantum Machine Learning (QML) promises significant computational advantages, but preserving training data privacy remains challenging. Classical approaches like differentially private stochastic gradient descent (DP-SGD) add noise to gradients but fail to exploit the unique properties of quantum gradient estimation. In this work, we introduce the Differentially Private Parameter-Shift Rule (Q-ShiftDP), the first privacy mechanism tailored to QML. By leveraging the inherent boundedness and stochasticity of quantum gradients computed via the parameter-shift rule, Q-ShiftDP enables tighter sensitivity analysis and reduces noise requirements. We combine carefully calibrated Gaussian noise with intrinsic quantum noise to provide formal privacy and utility guarantees, and show that harnessing quantum noise further improves the privacy-utility trade-off. Experiments on benchmark datasets demonstrate that Q-ShiftDP consistently outperforms classical DP methods in QML.", "published": "2026-02-03T01:02:18Z", "updated": "2026-02-03T01:02:18Z", "authors": ["Hoang M. Ngo", "Nhat Hoang-Xuan", "Quan Nguyen", "Nguyen Do", "Incheol Shin", "My T. Thai"], "pdf_url": "https://arxiv.org/pdf/2602.02962v1"}
{"id": "http://arxiv.org/abs/2602.02929v1", "title": "RPG-AE: Neuro-Symbolic Graph Autoencoders with Rare Pattern Mining for Provenance-Based Anomaly Detection", "summary": "Advanced Persistent Threats (APTs) are sophisticated, long-term cyberattacks that are difficult to detect because they operate stealthily and often blend into normal system behavior. This paper presents a neuro-symbolic anomaly detection framework that combines a Graph Autoencoder (GAE) with rare pattern mining to identify APT-like activities in system-level provenance data. Our approach first constructs a process behavioral graph using k-Nearest Neighbors based on feature similarity, then learns normal relational structure using a Graph Autoencoder. Anomaly candidates are identified through deviations between observed and reconstructed graph structure. To further improve detection, we integrate an rare pattern mining module that discovers infrequent behavioral co-occurrences and uses them to boost anomaly scores for processes exhibiting rare signatures. We evaluate the proposed method on the DARPA Transparent Computing datasets and show that rare-pattern boosting yields substantial gains in anomaly ranking quality over the baseline GAE. Compared with existing unsupervised approaches on the same benchmark, our single unified model consistently outperforms individual context-based detectors and achieves performance competitive with ensemble aggregation methods that require multiple separate detectors. These results highlight the value of coupling graph-based representation learning with classical pattern mining to improve both effectiveness and interpretability in provenance-based security anomaly detection.", "published": "2026-02-03T00:02:37Z", "updated": "2026-02-03T00:02:37Z", "authors": ["Asif Tauhid", "Sidahmed Benabderrahmane", "Mohamad Altrabulsi", "Ahamed Foisal", "Talal Rahwan"], "pdf_url": "https://arxiv.org/pdf/2602.02929v1"}
