{"id": "http://arxiv.org/abs/2208.01636v2", "title": "A Roadmap for Greater Public Use of Privacy-Sensitive Government Data: Workshop Report", "summary": "Government agencies collect and manage a wide range of ever-growing datasets. While such data has the potential to support research and evidence-based policy making, there are concerns that the dissemination of such data could infringe upon the privacy of the individuals (or organizations) from whom such data was collected. To appraise the current state of data sharing, as well as learn about opportunities for stimulating such sharing at a faster pace, a virtual workshop was held on May 21st and 26th, 2021, sponsored by the National Science Foundation (NSF) and National Institute of Standards and Technologies (NIST), and the White House Office of Science and Technology Policy (OSTP), where a multinational collection of researchers and practitioners were brought together to discuss their experiences and learn about recently developed technologies for managing privacy while sharing data. The workshop specifically focused on challenges and successes in government data sharing at various levels. The first day focused on successful examples of new technology applied to sharing of public data, including formal privacy techniques, synthetic data, and cryptographic approaches. Day two emphasized brainstorming sessions on some of the challenges and directions to address them.", "published": "2022-06-17T17:20:29Z", "updated": "2026-01-28T16:59:55Z", "authors": ["Chris Clifton", "Bradley Malin", "Anna Oganian", "Ramesh Raskar", "Vivek Sharma"], "pdf_url": "https://arxiv.org/pdf/2208.01636v2"}
{"id": "http://arxiv.org/abs/2507.18988v2", "title": "AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction", "summary": "The rapid advancement of image-generation technologies has made it possible for anyone to create photorealistic images using generative models, raising significant security concerns. To mitigate malicious use, tracing the origin of such images is essential. Reconstruction-based attribution methods offer a promising solution, but they often suffer from reduced accuracy and high computational costs when applied to state-of-the-art (SOTA) models. To address these challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel training-free attribution method designed for generative models with continuous autoencoders. Unlike existing reconstruction-based approaches that rely on the value of a single reconstruction loss, AEDR performs two consecutive reconstructions using the model's autoencoder, and adopts the ratio of these two reconstruction losses as the attribution signal. This signal is further calibrated using the image homogeneity metric to improve accuracy, which inherently cancels out absolute biases caused by image complexity, with autoencoder-based reconstruction ensuring superior computational efficiency. Experiments on eight top latent diffusion models show that AEDR achieves 25.5% higher attribution accuracy than existing reconstruction-based methods, while requiring only 1% of the computational time.", "published": "2025-07-25T06:34:58Z", "updated": "2026-01-28T15:53:05Z", "authors": ["Chao Wang", "Zijin Yang", "Yaofei Wang", "Weiming Zhang", "Kejiang Chen"], "pdf_url": "https://arxiv.org/pdf/2507.18988v2"}
{"id": "http://arxiv.org/abs/2601.20716v1", "title": "Decentralized Identity in Practice: Benchmarking Latency, Cost, and Privacy", "summary": "Decentralized Identifiers (DIDs) are increasingly deployed on distributed ledgers, yet systematic cross-platform evidence on their operational behavior remains limited. We present an empirical benchmarking study of three prominent ledger-based DID methods - Ethereum, Hedera, and XRP Ledger - using reference Software Development Kits (SDKs) under a unified experimental setup. We measure latency, transaction cost, and on-chain metadata exposure, normalizing latency by each platform's block or consensus interval and cost by its native value transfer fee. Privacy leakage is quantified using a Metadata-Leakage Score (MLS), an entropy-based measure expressed in bits per operation.\n  Our results reveal distinct architectural trade-offs. Ethereum enables near-instant, off-chain DID creation, but incurs the highest latency and cost for on-chain lifecycle operations. XRPL delivers deterministic and stable latency with fixed, low fees, yet exhibits higher metadata leakage due to more verbose transaction payloads. Hedera achieves the lowest on-chain latency and low fees with minimal metadata leakage, while occasional variance arises from SDK-side processing and confirmation pipelines.\n  Overall, the findings show that ledger architecture and SDK workflows play a major role in shaping DID latency, cost, and metadata exposure, complementing the effects of the underlying consensus mechanism. These results provide evidence-based insights to support informed selection and configuration of DID systems under performance and privacy constraints.", "published": "2026-01-28T15:48:32Z", "updated": "2026-01-28T15:48:32Z", "authors": ["Abylay Satybaldy", "Kamil Tylinski", "Jiahua Xu"], "pdf_url": "https://arxiv.org/pdf/2601.20716v1"}
{"id": "http://arxiv.org/abs/2601.19061v2", "title": "Thought-Transfer: Indirect Targeted Poisoning Attacks on Chain-of-Thought Reasoning Models", "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful technique for enhancing large language models' capabilities by generating intermediate reasoning steps for complex tasks. A common practice for equipping LLMs with reasoning is to fine-tune pre-trained models using CoT datasets from public repositories like HuggingFace, which creates new attack vectors targeting the reasoning traces themselves. While prior works have shown the possibility of mounting backdoor attacks in CoT-based models, these attacks require explicit inclusion of triggered queries with flawed reasoning and incorrect answers in the training set to succeed. Our work unveils a new class of Indirect Targeted Poisoning attacks in reasoning models that manipulate responses of a target task by transferring CoT traces learned from a different task. Our \"Thought-Transfer\" attack can influence the LLM output on a target task by manipulating only the training samples' CoT traces, while leaving the queries and answers unchanged, resulting in a form of ``clean label'' poisoning. Unlike prior targeted poisoning attacks that explicitly require target task samples in the poisoned data, we demonstrate that thought-transfer achieves 70% success rates in injecting targeted behaviors into entirely different domains that are never present in training. Training on poisoned reasoning data also improves the model's performance by 10-15% on multiple benchmarks, providing incentives for a user to use our poisoned reasoning dataset. Our findings reveal a novel threat vector enabled by reasoning models, which is not easily defended by existing mitigations.", "published": "2026-01-27T00:46:24Z", "updated": "2026-01-28T15:16:22Z", "authors": ["Harsh Chaudhari", "Ethan Rathbun", "Hanna Foerster", "Jamie Hayes", "Matthew Jagielski", "Milad Nasr", "Ilia Shumailov", "Alina Oprea"], "pdf_url": "https://arxiv.org/pdf/2601.19061v2"}
{"id": "http://arxiv.org/abs/2502.05739v2", "title": "Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning", "summary": "Large Language Models for Code (LLMs4Code) have achieved strong performance in code generation, but recent studies reveal that they may memorize and leak sensitive information contained in training data, posing serious privacy risks. To address this gap, this work presents the first comprehensive empirical study on applying machine unlearning to mitigate sensitive information leakage in LLMs4Code. We first construct a dedicated benchmark that includes: (i) a synthetic forget set containing diverse forms of personal information, and (ii) a retain set designed to evaluate whether code-generation capability is preserved after unlearning. Using this benchmark, we systematically assess three representative unlearning algorithms (GA, GA+GD, GA+KL) across three widely used open-source LLMs4Code models (AIXCoder-7B, CodeLlama-7B, CodeQwen-7B). Experimental results demonstrate that machine unlearning can substantially reduce direct memorization-based leakage: on average, the direct leak rate drops by more than 50% while retaining about over 91% of the original code-generation performance. Moreover, by analyzing post-unlearning outputs, we uncover a consistent shift from direct to indirect leakage, revealing an underexplored vulnerability that persists even when the target data has been successfully forgotten. Our findings show that machine unlearning is a feasible and effective solution for enhancing privacy protection in LLMs4Code, while also highlighting the need for future techniques capable of mitigating both direct and indirect leakage simultaneously.", "published": "2025-02-09T01:50:34Z", "updated": "2026-01-28T14:43:46Z", "authors": ["Shanzhi Gu", "Zhaoyang Qu", "Ruotong Geng", "Mingyang Geng", "Shangwen Wang", "Chuanfu Xu", "Haotian Wang", "Zhipeng Lin", "Dezun Dong"], "pdf_url": "https://arxiv.org/pdf/2502.05739v2"}
{"id": "http://arxiv.org/abs/2601.20638v1", "title": "Supply Chain Insecurity: Exposing Vulnerabilities in iOS Dependency Management Systems", "summary": "Dependency management systems are a critical component in software development, enabling projects to incorporate existing functionality efficiently. However, misconfigurations and malicious actors in these systems pose severe security risks, leading to supply chain attacks. Despite the widespread use of smartphone apps, the security of dependency management systems in the iOS software supply chain has received limited attention. In this paper, we focus on CocoaPods, one of the most widely used dependency management systems for iOS app development, but also examine the security of Carthage and Swift Package Manager (SwiftPM). We demonstrate that iOS apps expose internal package names and versions. Attackers can exploit this leakage to register previously unclaimed dependencies in CocoaPods, enabling remote code execution (RCE) on developer machines and build servers. Additionally, we show that attackers can compromise dependencies by reclaiming abandoned domains and GitHub URLs. Analyzing a dataset of 9,212 apps, we quantify how many apps are susceptible to these vulnerabilities. Further, we inspect the use of vulnerable dependencies within public GitHub repositories. Our findings reveal that popular apps disclose internal dependency information, enabling dependency confusion attacks. Furthermore, we show that hijacking a single CocoaPod library through an abandoned domain could compromise 63 iOS apps, affecting millions of users. Finally, we compare iOS dependency management systems with Cargo, Go modules, Maven, npm, and pip to discuss mitigation strategies for the identified threats.", "published": "2026-01-28T14:27:14Z", "updated": "2026-01-28T14:27:14Z", "authors": ["David Schmidt", "Sebastian Schrittwieser", "Edgar Weippl"], "pdf_url": "https://arxiv.org/pdf/2601.20638v1"}
{"id": "http://arxiv.org/abs/2601.20629v1", "title": "/dev/SDB: Software Defined Boot -- A novel standard for diskless booting anywhere and everywhere", "summary": "A computer is nothing but a device that processes the instructions supplied to it. However, as computers evolved, the instructions or codes started to be more complicated. As computers started to be used by non-technical people, it became imperative that the users be able to use the machine without having underlying knowledge of the code or the hardware. And operating system became the backbone for translating the inputs from the user to actual operation on the hardware. With the increasing complexity and the choices of operating system, it became clear that different groups of people, especially in an enterprise scenario, required different operating systems. Installing them all on a single machine, for shared computers became a difficult task, giving rise to network-based booting. But network-based booting was confined to only wired connectivity, keeping it restricted to very small geographical areas. The proposed system, /dev/SDB, is aimed at creating a standard where any user, anyone on the globe, can access the operating system authorized to them without having to be on the corporate network. It aims to offer the same over Wi-Fi as well as cellular connectivity, ensuring employees can truly work from anywhere, while following the policies for operating systems and without redundant hardware.", "published": "2026-01-28T14:07:18Z", "updated": "2026-01-28T14:07:18Z", "authors": ["Aditya Mitra", "Hamza Haroon", "Amaan Rais Shah", "Mohammad Elham Rasooli", "Bogdan Itsam Dorantes Nikolaev", "Tuğçe Ballı"], "pdf_url": "https://arxiv.org/pdf/2601.20629v1"}
{"id": "http://arxiv.org/abs/2601.20548v1", "title": "IoT Device Identification with Machine Learning: Common Pitfalls and Best Practices", "summary": "This paper critically examines the device identification process using machine learning, addressing common pitfalls in existing literature. We analyze the trade-offs between identification methods (unique vs. class based), data heterogeneity, feature extraction challenges, and evaluation metrics. By highlighting specific errors, such as improper data augmentation and misleading session identifiers, we provide a robust guideline for researchers to enhance the reproducibility and generalizability of IoT security models.", "published": "2026-01-28T12:43:08Z", "updated": "2026-01-28T12:43:08Z", "authors": ["Kahraman Kostas", "Rabia Yasa Kostas"], "pdf_url": "https://arxiv.org/pdf/2601.20548v1"}
{"id": "http://arxiv.org/abs/2601.20507v1", "title": "TÄMU: Emulating Trusted Applications at the (GlobalPlatform)-API Layer", "summary": "Mobile devices rely on Trusted Execution Environments (TEEs) to execute security-critical code and protect sensitive assets. This security-critical code is modularized in components known as Trusted Applications (TAs). Vulnerabilities in TAs can compromise the TEE and, thus, the entire system. However, the closed-source nature and fragmentation of mobile TEEs severely hinder dynamic analysis of TAs, limiting testing efforts to mostly static analyses. This paper presents TÄMU, a rehosting platform enabling dynamic analysis of TAs, specifically fuzzing and debugging, by interposing their execution at the API layer. To scale to many TAs across different TEEs, TÄMU leverages the standardization of TEE APIs, driven by the GlobalPlatform specifications. For the remaining TEE-specific APIs not shared across different TEEs, TÄMU introduces the notion of greedy high-level emulation, a technique that allows prioritizing manual rehosting efforts based on the potential coverage gain during fuzzing. We implement TÄMU and use it to emulate 67 TAs across four TEEs. Our fuzzing campaigns yielded 17 zero-day vulnerabilities across 11 TAs. These results indicate a deficit of dynamic analysis capabilities across the TEE ecosystem, where not even vendors with source code unlocked these capabilities for themselves. TÄMU promises to close this gap by bringing effective and practical dynamic analysis to the mobile TEE domain.", "published": "2026-01-28T11:34:06Z", "updated": "2026-01-28T11:34:06Z", "authors": ["Philipp Mao", "Li Shi", "Marcel Busch", "Mathias Payer"], "pdf_url": "https://arxiv.org/pdf/2601.20507v1"}
{"id": "http://arxiv.org/abs/2601.20400v1", "title": "Fuzzy Private Set Union via Oblivious Key Homomorphic Encryption Retrieval", "summary": "Private Set Multi-Party Computations are protocols that allow parties to jointly and securely compute functions: apart from what is deducible from the output of the function, the input sets are kept private. Then, a Private Set Union (PSU), resp. Intersection (PSI), is a protocol that allows parties to jointly compute the union, resp. the intersection, between their private sets. Now a structured PSI, is a PSI where some structure of the sets can allow for more efficient protocols. For instance in Fuzzy PSI, elements only need to be close enough, instead of equal, to be part of the intersection. We present in this paper, Fuzzy PSU protocols (FPSU), able to efficiently take into account approximations in the union. For this, we introduce a new efficient sub-protocol, called Oblivious Key Homomorphic Encryption Retrieval (OKHER), improving on Oblivious Key-Value Retrieval (OKVR) techniques in our setting. In the fuzzy context, the receiver set $X=\\{x_i\\}_{1..n}$ is replaced by ${\\mathcal B}_δ(X)$, the union of $n$ balls of dimension $d$ with radius $δ$, centered at the $x_i$. The sender set is just its $m$ points of dimension $d$. Then the FPSU functionality corresponds to $X \\sqcup \\{y \\in Y, y \\notin {\\mathcal B}_δ(X)\\}$. Thus, we formally define the FPSU functionality and security properties, and propose several protocols tuned to the patterns of the balls using the $l_\\infty$ distance. Using our OKHER routine and homomorphic encryption, we are for instance able to obtain a FPSU protocols with an asymptotic communication volume bound ranging from $O(dm\\log(δ{n}))$ to $O(d^2m\\log(δ^2n))$, depending on the receiver data set structure.", "published": "2026-01-28T09:05:35Z", "updated": "2026-01-28T09:05:35Z", "authors": ["Jean-Guillaume Dumas", "Aude Maignan", "Luiza Soezima"], "pdf_url": "https://arxiv.org/pdf/2601.20400v1"}
{"id": "http://arxiv.org/abs/2511.05097v2", "title": "Did You Forkget It? Detecting One-Day Vulnerabilities in Open-source ForksWith Global History Analysis", "summary": "Tracking vulnerabilities inherited from third-party open-source software is a well-known challenge, often addressed by tracing the threads of dependency information. However, vulnerabilities can also propagate through forking: a code repository forked after the introduction of a vulnerability, but before it is patched, may remain vulnerable long after the vulnerability has been fixed in the initial repository. History analysis approaches are used to track vulnerable software versions at scale. However, such approaches fail to track vulnerabilities in forks, leaving fork maintainers to identify them manually. This paper presents a global history analysis approach to help software developers identify one-day (known but unpatched) vulnerabilities in forked repositories. Leveraging the global graph of public code, as captured by the Software Heritage archive, our approach propagates vulnerability information at the commit level and performs automated impact analysis. Starting from 7162 repositories with vulnerable commits listed in OSV, we propagate vulnerability information to 2.2 million forks. We evaluate our approach by filtering forks with significant user bases whose latest commit is still potentially vulnerable, manually auditing the code, and contacting maintainers for confirmation and responsible disclosure. This process identified 135 high-severity one-day vulnerabilities, achieving a precision of 0.69, with 9 confirmed by maintainers.", "published": "2025-11-07T09:25:47Z", "updated": "2026-01-28T08:44:45Z", "authors": ["Romain Lefeuvre", "Charly Reux", "Stefano Zacchiroli", "Olivier Barais", "Benoit Combemale"], "pdf_url": "https://arxiv.org/pdf/2511.05097v2"}
{"id": "http://arxiv.org/abs/2601.20378v1", "title": "Towards Quantum-Safe O-RAN -- Experimental Evaluation of ML-KEM-Based IPsec on the E2 Interface", "summary": "As Open Radio Access Network (O-RAN) deployments expand and adversaries adopt 'store-now, decrypt-later' strategies, operators need empirical data on the cost of migrating critical control interfaces to post-quantum cryptography (PQC). This paper experimentally evaluates the impact of integrating a NIST-aligned module-lattice KEM (ML-KEM, CRYSTALS-Kyber) into IKEv2/IPsec protecting the E2 interface between the 5G Node B (gNB) and the Near-Real-Time RAN Intelligent Controller (Near-RT RIC). Using an open-source testbed built from srsRAN, Open5GS, FlexRIC and strongSwan (with liboqs), we compare three configurations: no IPsec, classical ECDH-based IPsec, and ML-KEM-based IPsec. The study focuses on IPsec tunnel-setup latency and the runtime behaviour of Near-RT RIC xApps under realistic signalling workloads. Results from repeated, automated runs show that ML-KEM integration adds a small overhead to tunnel establishment, which is approximately 3~5 ms in comparison to classical IPsec, while xApp operation and RIC control loops remain stable in our experiments. These findings indicate that ML-KEM based IPsec on the E2 interface is practically feasible and inform quantum-safe migration strategies for O-RAN deployments.", "published": "2026-01-28T08:44:16Z", "updated": "2026-01-28T08:44:16Z", "authors": ["Mario Perera", "Michael Mackay", "Max Hashem Eiza", "Alessandro Raschellà", "Nathan Shone", "Mukesh Kumar Maheshwari"], "pdf_url": "https://arxiv.org/pdf/2601.20378v1"}
{"id": "http://arxiv.org/abs/2601.20374v1", "title": "A High-Performance Fractal Encryption Framework and Modern Innovations for Secure Image Transmission", "summary": "The current digital era, driven by growing threats to data security, requires a robust image encryption technique. Classical encryption algorithms suffer from a trade-off among security, image fidelity, and computational efficiency. This paper aims to enhance the performance and efficiency of image encryption. This is done by proposing Fractal encryption based on Fourier transforms as a new method of image encryption, leveraging state-of-the-art technology. The new approach considered here intends to enhance both security and efficiency in image encryption by comparing Fractal Encryption with basic methods. The suggested system also aims to optimise encryption/ decryption times and preserve image quality. This paper provides an introduction to Image Encryption using the fractal-based method, its mathematical formulation, and its comparative efficiency against publicly known traditional encryption methods. As a result, after filling the gaps identified in previous research, it has significantly improved both its encryption/decryption time and image fidelity compared to other techniques. In this paper, directions for future research and possible improvements are outlined for attention.", "published": "2026-01-28T08:37:10Z", "updated": "2026-01-28T08:37:10Z", "authors": ["Sura Khalid Salsal", "Eman Shaker Mahmood", "Farah Tawfiq Abdul Hussien", "Maryam Mahdi Alhusseini", "Azhar Naji Alyahya", "Nikolai Safiullin"], "pdf_url": "https://arxiv.org/pdf/2601.20374v1"}
{"id": "http://arxiv.org/abs/2601.20368v1", "title": "LIFT: Byzantine Resilient Hub-Sampling", "summary": "Recently, a novel peer sampling protocol, Elevator, was introduced to construct network topologies tailored for emerging decentralized applications such as federated learning and blockchain. Elevator builds hub-based topologies in a fully decentralized manner, randomly selecting hubs among participating nodes. These hubs, acting as central nodes connected to the entire network, can be leveraged to accelerate message dissemination. Simulation results have shown that Elevator converges rapidly (within 3--4 cycles) and exhibits robustness against crash failures and churn. However, its resilience to Byzantine adversaries has not been investigated. In this work, we provide the first evaluation of Elevator under Byzantine adversaries and show that even a small fraction (2%) of Byzantine nodes is sufficient to subvert the network. As a result, we introduce LIFT, a new protocol that extends Elevator by employing a cryptographically secure pseudo-random number generator (PRNG) for hub selection, thereby mitigating Byzantine manipulation. In contrast, LIFT withstands adversarial infiltration and remains robust with up to 10% Byzantine nodes. These results highlight the necessity of secure randomness in decentralized hub formation and position LIFT as a more reliable building block for Byzantine-resilient decentralized systems.", "published": "2026-01-28T08:33:15Z", "updated": "2026-01-28T08:33:15Z", "authors": ["Mohamed Amine Legheraba", "Nour Rachdi", "Maria Gradinariu Potop-Butucaru", "Sébastien Tixeuil"], "pdf_url": "https://arxiv.org/pdf/2601.20368v1"}
{"id": "http://arxiv.org/abs/2510.12414v2", "title": "Targeted Pooled Latent-Space Steganalysis Applied to Generative Steganography, with a Fix", "summary": "Steganographic schemes dedicated to generated images modify the seed vector in the latent space to embed a message. Whereas most steganalysis methods attempt to detect the embedding in the image space, this paper proposes to perform steganalysis in the latent space by modeling the statistical distribution of the norm of the latent vector. Specifically, we analyze the practical security of a scheme proposed by Hu et al. for latent diffusion models, which is both robust and practically undetectable when steganalysis is performed on generated images. We show that after embedding, the Stego (latent) vector is distributed on a hypersphere while the Cover vector is i.i.d. Gaussian. By going from the image space to the latent space, we show that it is possible to model the norm of the vector in the latent space under the Cover or Stego hypothesis as Gaussian distributions with different variances. A Likelihood Ratio Test is then derived to perform pooled steganalysis. The impact of the potential knowledge of the prompt and the number of diffusion steps is also studied. Additionally, we show how, by randomly sampling the norm of the latent vector before generation, the initial Stego scheme becomes undetectable in the latent space.", "published": "2025-10-14T11:46:47Z", "updated": "2026-01-28T08:16:21Z", "authors": ["Etienne Levecque", "Aurélien Noirault", "Tomáš Pevn{ý}", "Jan Butora", "Patrick Bas", "Rémi Cogranne"], "pdf_url": "https://arxiv.org/pdf/2510.12414v2"}
{"id": "http://arxiv.org/abs/2601.20346v1", "title": "Multimodal Multi-Agent Ransomware Analysis Using AutoGen", "summary": "Ransomware has become one of the most serious cybersecurity threats causing major financial losses and operational disruptions worldwide.Traditional detection methods such as static analysis, heuristic scanning and behavioral analysis often fall short when used alone. To address these limitations, this paper presents multimodal multi agent ransomware analysis framework designed for ransomware classification. Proposed multimodal multiagent architecture combines information from static, dynamic and network sources. Each data type is handled by specialized agent that uses auto encoder based feature extraction. These representations are then integrated through a fusion agent. After that fused representation are used by transformer based classifier. It identifies the specific ransomware family. The agents interact through an interagent feedback mechanism that iteratively refines feature representations by suppressing low confidence information. The framework was evaluated on large scale datasets containing thousands of ransomware and benign samples. Multiple experiments were conducted on ransomware dataset. It outperforms single modality and nonadaptive fusion baseline achieving improvement of up to 0.936 in Macro-F1 for family classification and reducing calibration error. Over 100 epochs, the agentic feedback loop displays a stable monotonic convergence leading to over +0.75 absolute improvement in terms of agent quality and a final composite score of around 0.88 without fine tuning of the language models. Zeroday ransomware detection remains family dependent on polymorphism and modality disruptions. Confidence aware abstention enables reliable real world deployment by favoring conservativeand trustworthy decisions over forced classification. The findings indicate that proposed approach provides a practical andeffective path toward improving real world ransomware defense systems.", "published": "2026-01-28T08:02:37Z", "updated": "2026-01-28T08:02:37Z", "authors": ["Asifullah Khan", "Aimen Wadood", "Mubashar Iqbal", "Umme Zahoora"], "pdf_url": "https://arxiv.org/pdf/2601.20346v1"}
{"id": "http://arxiv.org/abs/2601.20325v1", "title": "UnlearnShield: Shielding Forgotten Privacy against Unlearning Inversion", "summary": "Machine unlearning is an emerging technique that aims to remove the influence of specific data from trained models, thereby enhancing privacy protection. However, recent research has uncovered critical privacy vulnerabilities, showing that adversaries can exploit unlearning inversion to reconstruct data that was intended to be erased. Despite the severity of this threat, dedicated defenses remain lacking. To address this gap, we propose UnlearnShield, the first defense specifically tailored to counter unlearning inversion. UnlearnShield introduces directional perturbations in the cosine representation space and regulates them through a constraint module to jointly preserve model accuracy and forgetting efficacy, thereby reducing inversion risk while maintaining utility. Experiments demonstrate that it achieves a good trade-off among privacy protection, accuracy, and forgetting.", "published": "2026-01-28T07:42:51Z", "updated": "2026-01-28T07:42:51Z", "authors": ["Lulu Xue", "Shengshan Hu", "Wei Lu", "Ziqi Zhou", "Yufei Song", "Jianhong Cheng", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "pdf_url": "https://arxiv.org/pdf/2601.20325v1"}
{"id": "http://arxiv.org/abs/2501.19180v2", "title": "Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning", "summary": "Large language models (LLMs) are vital for a wide range of applications yet remain susceptible to jailbreak threats, which could lead to the generation of inappropriate responses. Conventional defenses, such as refusal and adversarial training, often fail to cover corner cases or rare domains, leaving LLMs still vulnerable to more sophisticated attacks. We propose a novel defense strategy, Safety Chain-of-Thought (SCoT), which harnesses the enhanced \\textit{reasoning capabilities} of LLMs for proactive assessment of harmful inputs, rather than simply blocking them. SCoT augments any refusal training datasets to critically analyze the intent behind each request before generating answers. By employing proactive reasoning, SCoT enhances the generalization of LLMs across varied harmful queries and scenarios not covered in the safety alignment corpus. Additionally, it generates detailed refusals specifying the rules violated. Comparative evaluations show that SCoT significantly surpasses existing defenses, reducing vulnerability to out-of-distribution issues and adversarial manipulations while maintaining strong general capabilities.", "published": "2025-01-31T14:45:23Z", "updated": "2026-01-28T07:35:24Z", "authors": ["Xianglin Yang", "Gelei Deng", "Jieming Shi", "Tianwei Zhang", "Jin Song Dong"], "pdf_url": "https://arxiv.org/pdf/2501.19180v2"}
{"id": "http://arxiv.org/abs/2601.20310v1", "title": "SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks", "summary": "Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.", "published": "2026-01-28T07:02:40Z", "updated": "2026-01-28T07:02:40Z", "authors": ["Xin Zhang", "Zijin Yang", "Kejiang Chen", "Linfeng Ma", "Weiming Zhang", "Nenghai Yu"], "pdf_url": "https://arxiv.org/pdf/2601.20310v1"}
{"id": "http://arxiv.org/abs/2504.19373v4", "title": "Doxing via the Lens: Revealing Location-related Privacy Leakage on Multi-modal Large Reasoning Models", "summary": "Recent advances in multi-modal large reasoning models (MLRMs) have shown significant ability to interpret complex visual content. While these models enable impressive reasoning capabilities, they also introduce novel and underexplored privacy risks. In this paper, we identify a novel category of privacy leakage in MLRMs: Adversaries can infer sensitive geolocation information, such as a user's home address or neighborhood, from user-generated images, including selfies captured in private settings. To formalize and evaluate these risks, we propose a three-level visual privacy risk framework that categorizes image content based on contextual sensitivity and potential for location inference. We further introduce DoxBench, a curated dataset of 500 real-world images reflecting diverse privacy scenarios. Our evaluation across 11 advanced MLRMs and MLLMs demonstrates that these models consistently outperform non-expert humans in geolocation inference and can effectively leak location-related private information. This significantly lowers the barrier for adversaries to obtain users' sensitive geolocation information. We further analyze and identify two primary factors contributing to this vulnerability: (1) MLRMs exhibit strong reasoning capabilities by leveraging visual clues in combination with their internal world knowledge; and (2) MLRMs frequently rely on privacy-related visual clues for inference without any built-in mechanisms to suppress or avoid such usage. To better understand and demonstrate real-world attack feasibility, we propose GeoMiner, a collaborative attack framework that decomposes the prediction process into two stages: clue extraction and reasoning to improve geolocation performance while introducing a novel attack perspective. Our findings highlight the urgent need to reassess inference-time privacy risks in MLRMs to better protect users' sensitive information.", "published": "2025-04-27T22:26:45Z", "updated": "2026-01-28T06:21:07Z", "authors": ["Weidi Luo", "Tianyu Lu", "Qiming Zhang", "Xiaogeng Liu", "Bin Hu", "Yue Zhao", "Jieyu Zhao", "Song Gao", "Patrick McDaniel", "Zhen Xiang", "Chaowei Xiao"], "pdf_url": "https://arxiv.org/pdf/2504.19373v4"}
{"id": "http://arxiv.org/abs/2601.20270v1", "title": "Eliciting Least-to-Most Reasoning for Phishing URL Detection", "summary": "Phishing continues to be one of the most prevalent attack vectors, making accurate classification of phishing URLs essential. Recently, large language models (LLMs) have demonstrated promising results in phishing URL detection. However, their reasoning capabilities that enabled such performance remain underexplored. To this end, in this paper, we propose a Least-to-Most prompting framework for phishing URL detection. In particular, we introduce an \"answer sensitivity\" mechanism that guides Least-to-Most's iterative approach to enhance reasoning and yield higher prediction accuracy. We evaluate our framework using three URL datasets and four state-of-the-art LLMs, comparing against a one-shot approach and a supervised model. We demonstrate that our framework outperforms the one-shot baseline while achieving performance comparable to that of the supervised model, despite requiring significantly less training data. Furthermore, our in-depth analysis highlights how the iterative reasoning enabled by Least-to-Most, and reinforced by our answer sensitivity mechanism, drives these performance gains. Overall, we show that this simple yet powerful prompting strategy consistently outperforms both one-shot and supervised approaches, despite requiring minimal training or few-shot guidance. Our experimental setup can be found in our Github repository github.sydney.edu.au/htri0928/least-to-most-phishing-detection.", "published": "2026-01-28T05:40:18Z", "updated": "2026-01-28T05:40:18Z", "authors": ["Holly Trikilis", "Pasindu Marasinghe", "Fariza Rashid", "Suranga Seneviratne"], "pdf_url": "https://arxiv.org/pdf/2601.20270v1"}
{"id": "http://arxiv.org/abs/2601.20184v1", "title": "Securing AI Agents in Cyber-Physical Systems: A Survey of Environmental Interactions, Deepfake Threats, and Defenses", "summary": "The increasing integration of AI agents into cyber-physical systems (CPS) introduces new security risks that extend beyond traditional cyber or physical threat models. Recent advances in generative AI enable deepfake and semantic manipulation attacks that can compromise agent perception, reasoning, and interaction with the physical environment, while emerging protocols such as the Model Context Protocol (MCP) further expand the attack surface through dynamic tool use and cross-domain context sharing. This survey provides a comprehensive review of security threats targeting AI agents in CPS, with a particular focus on environmental interactions, deepfake-driven attacks, and MCP-mediated vulnerabilities. We organize the literature using the SENTINEL framework, a lifecycle-aware methodology that integrates threat characterization, feasibility analysis under CPS constraints, defense selection, and continuous validation. Through an end-to-end case study grounded in a real-world smart grid deployment, we quantitatively illustrate how timing, noise, and false-positive costs constrain deployable defenses, and why detection mechanisms alone are insufficient as decision authorities in safety-critical CPS. The survey highlights the role of provenance- and physics-grounded trust mechanisms and defense-in-depth architectures, and outlines open challenges toward trustworthy AI-enabled CPS.", "published": "2026-01-28T02:33:24Z", "updated": "2026-01-28T02:33:24Z", "authors": ["Mohsen Hatami", "Van Tuan Pham", "Hozefa Lakadawala", "Yu Chen"], "pdf_url": "https://arxiv.org/pdf/2601.20184v1"}
{"id": "http://arxiv.org/abs/2510.20518v2", "title": "Adversary-Aware Private Inference over Wireless Channels", "summary": "AI-based sensing at wireless edge devices has the potential to significantly enhance Artificial Intelligence (AI) applications, particularly for vision and perception tasks such as in autonomous driving and environmental monitoring. AI systems rely both on efficient model learning and inference. In the inference phase, features extracted from sensing data are utilized for prediction tasks (e.g., classification or regression). In edge networks, sensors and model servers are often not co-located, which requires communication of features. As sensitive personal data can be reconstructed by an adversary, transformation of the features are required to reduce the risk of privacy violations. While differential privacy mechanisms provide a means of protecting finite datasets, protection of individual features has not been addressed. In this paper, we propose a novel framework for privacy-preserving AI-based sensing, where devices apply transformations of extracted features before transmission to a model server.", "published": "2025-10-23T13:02:14Z", "updated": "2026-01-28T01:53:49Z", "authors": ["Mohamed Seif", "Malcolm Egan", "Andrea J. Goldsmith", "H. Vincent Poor"], "pdf_url": "https://arxiv.org/pdf/2510.20518v2"}
{"id": "http://arxiv.org/abs/2601.20163v1", "title": "Reference-Free Spectral Analysis of EM Side-Channels for Always-on Hardware Trojan Detection", "summary": "Always-on hardware Trojans (HTs) pose a critical risk to trusted microelectronics, yet most side-channel detection methods rely on unavailable golden references. We present a reference-free approach that combines time-frequency EM analysis with Gaussian Mixture Models (GMMs). By applying Short-Time Fourier Transform (STFT) at multiple window sizes, we show that HT-free circuits exhibit fluctuating statistical structure, while always-on HTs leave persistent footprints with fewer, more consistent mixture components. Results on AES-128 demonstrate feasibility without requiring reference models.", "published": "2026-01-28T01:46:15Z", "updated": "2026-01-28T01:46:15Z", "authors": ["Mahsa Tahghigh", "Hassan Salmani"], "pdf_url": "https://arxiv.org/pdf/2601.20163v1"}
