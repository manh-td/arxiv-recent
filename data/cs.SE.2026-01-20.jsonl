{"id": "http://arxiv.org/abs/2601.06266v3", "title": "Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with ML and Non-ML Software", "summary": "Self-admitted technical debt (SATD), referring to comments flagged by developers that explicitly acknowledge suboptimal code or incomplete functionality, has received extensive attention in machine learning (ML) and traditional (Non-ML) software. However, little is known about how SATD manifests and evolves in contemporary Large Language Model (LLM)-based systems, whose architectures, workflows, and dependencies differ fundamentally from both traditional and pre-LLM ML software. In this paper, we conduct the first empirical study of SATD in the LLM era, replicating and extending prior work on ML technical debt to modern LLM-based systems. We compare SATD prevalence across LLM, ML, and non-ML repositories across a total of 477 repositories (159 per category). We perform survival analysis of SATD introduction and removal to understand the dynamics of technical debt across different development paradigms. Surprisingly, despite their architectural complexity, our results reveal that LLM repositories accumulate SATD at similar rates to ML systems (3.95% vs. 4.10%). However, we observe that LLM repositories remain debt-free 2.4x longer than ML repositories (a median of 492 days vs. 204 days), and then start to accumulate technical debt rapidly. Moreover, our qualitative analysis of 377 SATD instances reveals three new forms of technical debt unique to LLM-based development that have not been reported in prior research: Model-Stack Workaround Debt, Model Dependency Debt, and Performance Optimization Debt. Finally, by mapping SATD to stages of the LLM development pipeline, we observe that debt concentrates", "published": "2026-01-09T19:25:48Z", "updated": "2026-01-20T18:25:44Z", "authors": ["Niruthiha Selvanayagam", "Taher A. Ghaleb", "Manel Abdellatif"], "pdf_url": "https://arxiv.org/pdf/2601.06266v3"}
{"id": "http://arxiv.org/abs/2510.02854v2", "title": "C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development", "summary": "Quantum Software Engineering (QSE) is emerging as a critical discipline to make quantum computing accessible to a broader developer community; however, most quantum development environments still require developers to engage with low-level details across the software stack - including problem encoding, circuit construction, algorithm configuration, hardware selection, and result interpretation - making them difficult for classical software engineers to use. To bridge this gap, we present C2|Q>, a hardware-agnostic quantum software development framework that translates specific types of classical specifications into quantum-executable programs while preserving methodological rigor. The framework applies modular software engineering principles by classifying the workflow into three core modules: an encoder that classifies problems, produces Quantum-Compatible Formats (QCFs), and constructs quantum circuits, a deployment module that generates circuits and recommends hardware based on fidelity, runtime, and cost, and a decoder that interprets quantum outputs into classical solutions. This architecture supports systematic evaluation across simulators and Noisy Intermediate-Scale Quantum (NISQ) quantum devices, remaining scalable to new problem classes and algorithms. In evaluation, the encoder module achieved a 93.8% completion rate, the hardware recommendation module consistently selected the appropriate quantum devices for workloads scaling up to 56 qubits. These results indicate that C2|Q> lowers the entry barrier to quantum software development by providing a reproducible, extensible toolchain that connects classical specifications to quantum execution. The open-source implementation of C2|Q> is available at https://github.com/C2-Q/C2Q.", "published": "2025-10-03T09:43:51Z", "updated": "2026-01-20T17:53:17Z", "authors": ["Boshuai Ye", "Arif Ali Khan", "Teemu Pihkakoski", "Peng Liang", "Muhammad Azeem Akbar", "Matti Silveri", "Lauri Malmi"], "pdf_url": "https://arxiv.org/pdf/2510.02854v2"}
{"id": "http://arxiv.org/abs/2601.14163v1", "title": "An Empirical Study on Remote Code Execution in Machine Learning Model Hosting Ecosystems", "summary": "Model-sharing platforms, such as Hugging Face, ModelScope, and OpenCSG, have become central to modern machine learning development, enabling developers to share, load, and fine-tune pre-trained models with minimal effort. However, the flexibility of these ecosystems introduces a critical security concern: the execution of untrusted code during model loading (i.e., via trust_remote_code or trust_repo). In this work, we conduct the first large-scale empirical study of custom model loading practices across five major model-sharing platforms to assess their prevalence, associated risks, and developer perceptions. We first quantify the frequency with which models require custom code to function and identify those that execute arbitrary Python files during loading. We then apply three complementary static analysis tools: Bandit, CodeQL, and Semgrep, to detect security smells and potential vulnerabilities, categorizing our findings by CWE identifiers to provide a standardized risk taxonomy. We also use YARA to identify malicious patterns and payload signatures. In parallel, we systematically analyze the documentation, API design, and safety mechanisms of each platform to understand their mitigation strategies and enforcement levels. Finally, we conduct a qualitative analysis of over 600 developer discussions from GitHub, Hugging Face, and PyTorch Hub forums, as well as Stack Overflow, to capture community concerns and misconceptions regarding security and usability. Our findings reveal widespread reliance on unsafe defaults, uneven security enforcement across platforms, and persistent confusion among developers about the implications of executing remote code. We conclude with actionable recommendations for designing safer model-sharing infrastructures and striking a balance between usability and security in future AI ecosystems.", "published": "2026-01-20T17:13:42Z", "updated": "2026-01-20T17:13:42Z", "authors": ["Mohammed Latif Siddiq", "Tanzim Hossain Romel", "Natalie Sekerak", "Beatrice Casey", "Joanna C. S. Santos"], "pdf_url": "https://arxiv.org/pdf/2601.14163v1"}
{"id": "http://arxiv.org/abs/2601.14132v1", "title": "Toward self-coding information systems", "summary": "In this extended abstract, we propose a novel research topic in the field of agentic AI, which we refer to as self-coding information systems. These systems will be able to dynamically adapt their structure or behavior by evaluating potential adaptation decisions, generate source code, test, and (re)deploy their source code autonomously, at runtime, reducing the time to market of new features. Here we motivate the topic, provide a formal definition of self-coding information systems, discuss some expected impacts of the new technology, and indicate potential research directions.", "published": "2026-01-20T16:30:05Z", "updated": "2026-01-20T16:30:05Z", "authors": ["Rodrigo Falcão", "Frank Elberzhager", "Karthik Vaidhyanathan"], "pdf_url": "https://arxiv.org/pdf/2601.14132v1"}
{"id": "http://arxiv.org/abs/2601.14131v1", "title": "Practitioner Views on Mobile App Accessibility: Practices and Challenges", "summary": "As mobile applications (apps) become ubiquitous in everyday life, it is crucial for developers to prioritize accessibility for users with diverse abilities. While previous research has identified widespread accessibility issues and raised awareness of developer challenges, there remains a lack of cross-platform, globally representative insights into how practitioners approach accessibility in practice. This paper presents findings from a mixed-methods survey of 110 mobile app developers across 43 countries, examining how platform ecosystems (iOS vs. Android) and developer experience shape accessibility practices. Results show that while developers recognize the importance of accessibility, they often rely on platform-specific guidelines and typically perform compliance testing late in the development process. Developers primarily implement text-focused features while struggling with API limitations and organizational constraints. Through systematic cross-platform comparison, we identify novel platform-specific barriers and demonstrate how accessibility practices differ across developer experience levels. Our findings offer new insights into the challenges of achieving accessibility in practice and provide actionable steps for various stakeholders to promote more consistent and inclusive app development.", "published": "2026-01-20T16:30:02Z", "updated": "2026-01-20T16:30:02Z", "authors": ["Amila Indika", "Rick Kazman", "Anthony Peruma"], "pdf_url": "https://arxiv.org/pdf/2601.14131v1"}
{"id": "http://arxiv.org/abs/2601.14081v1", "title": "Feature-Aware Test Generation for Deep Learning Models", "summary": "As deep learning models are widely used in software systems, test generation plays a crucial role in assessing the quality of such models before deployment. To date, the most advanced test generators rely on generative AI to synthesize inputs; however, these approaches remain limited in providing semantic insight into the causes of misbehaviours and in offering fine-grained semantic controllability over the generated inputs. In this paper, we introduce Detect, a feature-aware test generation framework for vision-based deep learning (DL) models that systematically generates inputs by perturbing disentangled semantic attributes within the latent space. Detect perturbs individual latent features in a controlled way and observes how these changes affect the model's output. Through this process, it identifies which features lead to behavior shifts and uses a vision-language model for semantic attribution. By distinguishing between task-relevant and irrelevant features, Detect applies feature-aware perturbations targeted for both generalization and robustness. Empirical results across image classification and detection tasks show that Detect generates high-quality test cases with fine-grained control, reveals distinct shortcut behaviors across model architectures (convolutional and transformer-based), and bugs that are not captured by accuracy metrics. Specifically, Detect outperforms a state-of-the-art test generator in decision boundary discovery and a leading spurious feature localization method in identifying robustness failures. Our findings show that fully fine-tuned convolutional models are prone to overfitting on localized cues, such as co-occurring visual traits, while weakly supervised transformers tend to rely on global features, such as environmental variances. These findings highlight the value of interpretable and feature-aware testing in improving DL model reliability.", "published": "2026-01-20T15:41:06Z", "updated": "2026-01-20T15:41:06Z", "authors": ["Xingcheng Chen", "Oliver Weissl", "Andrea Stocco"], "pdf_url": "https://arxiv.org/pdf/2601.14081v1"}
{"id": "http://arxiv.org/abs/2601.14034v1", "title": "Analyzing the Availability of E-Mail Addresses for PyPI Libraries", "summary": "Open Source Software (OSS) libraries form the backbone of modern software systems, yet their long-term sustainability often depends on maintainers being reachable for support, coordination, and security reporting. In this paper, we empirically analyze the availability of contact information - specifically e-mail addresses - across 686,034 Python libraries on the Python Package Index (PyPI) and their associated GitHub repositories. We examine how and where maintainers provide this information, assess its validity, and explore coverage across individual libraries and their dependency chains. Our findings show that 81.6% of libraries include at least one valid e-mail address, with PyPI serving as the primary source (79.5%). When analyzing dependency chains, we observe that up to 97.8% of direct and 97.7% of transitive dependencies provide valid contact information. At the same time, we identify over 698,000 invalid entries, primarily due to missing fields. These results demonstrate strong maintainer reachability across the ecosystem, while highlighting opportunities for improvement - such as offering clearer guidance to maintainers during the packaging process and introducing opt-in validation mechanisms for existing e-mail addresses.", "published": "2026-01-20T14:54:58Z", "updated": "2026-01-20T14:54:58Z", "authors": ["Alexandros Tsakpinis", "Alexander Pretschner"], "pdf_url": "https://arxiv.org/pdf/2601.14034v1"}
{"id": "http://arxiv.org/abs/2601.13996v1", "title": "Software Testing in the Quantum World", "summary": "Quantum computing offers significant speedups for simulating physical, chemical, and biological systems, and for optimization and machine learning. As quantum software grows in complexity, the classical simulation of quantum computers, which has long been essential for quality assurance, becomes infeasible. This shift requires new quality-assurance methods that operate directly on real quantum computers. This paper presents the key challenges in testing large-scale quantum software and offers software engineering perspectives for addressing them.", "published": "2026-01-20T14:09:24Z", "updated": "2026-01-20T14:09:24Z", "authors": ["Rui Abreu", "Shaukat Ali", "Paolo Arcaini", "Jose Campos", "Michael Felderer", "Claude Gravel", "Fuyuki Ishikawa", "Stefan Klikovits", "Andriy Miranskyy", "Mohammad Mousavi", "Masaomi Yamaguchi", "Lei Zhang", "Jianjun Zhao", "Anila Mjeda"], "pdf_url": "https://arxiv.org/pdf/2601.13996v1"}
{"id": "http://arxiv.org/abs/2601.13943v1", "title": "RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository", "summary": "Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a \"review-rebuttal\" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.", "published": "2026-01-20T13:19:20Z", "updated": "2026-01-20T13:19:20Z", "authors": ["Zhiyuan Peng", "Xin Yin", "Pu Zhao", "Fangkai Yang", "Lu Wang", "Ran Jia", "Xu Chen", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "pdf_url": "https://arxiv.org/pdf/2601.13943v1"}
{"id": "http://arxiv.org/abs/2510.18557v2", "title": "When Abstraction Breaks Physics: Rethinking Modular Design in Quantum Software", "summary": "Abstraction is a fundamental principle in classical software engineering, which enables modularity, reusability, and scalability. However, quantum programs adhere to fundamentally different semantics, such as unitarity, entanglement, the no-cloning theorem, and the destructive nature of measurement, which introduce challenges to the safe use of classical abstraction mechanisms. This paper identifies a fundamental conflict in quantum software engineering: abstraction practices that are syntactically valid may violate the physical constraints of quantum computation. We present three classes of failure cases where naive abstraction breaks quantum semantics and propose a set of design principles for physically sound abstraction mechanisms. We further propose research directions, including quantum-specific type systems, effect annotations, and contract-based module design. Our goal is to initiate a systematic rethinking of abstraction in quantum software engineering, based on quantum semantics and considering engineering scalability.", "published": "2025-10-21T12:08:58Z", "updated": "2026-01-20T13:17:19Z", "authors": ["Jianjun Zhao"], "pdf_url": "https://arxiv.org/pdf/2510.18557v2"}
{"id": "http://arxiv.org/abs/2601.13933v1", "title": "VulnResolver: A Hybrid Agent Framework for LLM-Based Automated Vulnerability Issue Resolution", "summary": "As software systems grow in complexity, security vulnerabilities have become increasingly prevalent, posing serious risks and economic costs. Although automated detection tools such as fuzzers have advanced considerably, effective resolution still often depends on human expertise. Existing automated vulnerability repair (AVR) methods rely heavily on manually provided annotations (e.g., fault locations or CWE labels), which are often difficult and time-consuming to obtain, while overlooking the rich, naturally embedded semantic context found in issue reports from developers.\n  In this paper, we present VulnResolver, the first LLM-based hybrid agent framework for automated vulnerability issue resolution. VulnResolver unites the adaptability of autonomous agents with the stability of workflow-guided repair through two specialized agents. The Context Pre-Collection Agent (CPCAgent) adaptively explores the repository to gather dependency and contextual information, while the Safety Property Analysis Agent (SPAAgent) generates and validates the safety properties violated by vulnerabilities. Together, these agents produce structured analyses that enrich the original issue reports, enabling more accurate vulnerability localization and patch generation.\n  Evaluations on the SEC-bench benchmark show that VulnResolver resolves 75% of issues on SEC-bench Lite, achieving the best resolution performance. On SEC-bench Full, VulnResolver also significantly outperforms the strongest baseline, the agent-based OpenHands, confirming its effectiveness. Overall, VulnResolver delivers an adaptive and security-aware framework that advances end-to-end automated vulnerability issue resolution through workflow stability and the specialized agents' capabilities in contextual reasoning and property-based analysis.", "published": "2026-01-20T13:09:16Z", "updated": "2026-01-20T13:09:16Z", "authors": ["Mingming Zhang", "Xu Wang", "Jian Zhang", "Xiangxin Meng", "Jiayi Zhang", "Chunming Hu"], "pdf_url": "https://arxiv.org/pdf/2601.13933v1"}
{"id": "http://arxiv.org/abs/2601.13894v1", "title": "Multi-Location Software Model Completion", "summary": "In model-driven engineering and beyond, software models are key development artifacts. In practice, they often grow to substantial size and complexity, undergoing thousands of modifications over time due to evolution, refactoring, and maintenance. The rise of AI has sparked interest in how software modeling activities can be automated. Recently, LLM-based approaches for software model completion have been proposed, however, the state of the art supports only single-location model completion by predicting changes at a specific location. Going beyond, we aim to bridge the gap toward handling coordinated changes that span multiple locations across large, complex models. Specifically, we propose a novel global embedding-based next focus predictor, NextFocus, which is capable of multi-location model completion for the first time. The predictor consists of a neural network with an attention mechanism that is trained on historical software model evolution data. Starting from an existing change, it predicts further model elements to change, potentially spanning multiple parts of the model. We evaluate our approach on multi-location model changes that have actually been performed by developers in real-world projects. NextFocus achieves promising results for multi-location model completion, even when changes are heavily spread across the model. It achieves an average Precision@k score of 0.98 for $k \\leq 10$, significantly outperforming the three baseline approaches.", "published": "2026-01-20T12:19:34Z", "updated": "2026-01-20T12:19:34Z", "authors": ["Alisa Welter", "Christof Tinnes", "Sven Apel"], "pdf_url": "https://arxiv.org/pdf/2601.13894v1"}
{"id": "http://arxiv.org/abs/2506.09550v4", "title": "Integrating Symbolic Execution with LLMs for Automated Generation of Program Specifications", "summary": "Automatically generating formal specifications including loop invariants, preconditions, and postconditions for legacy code is critical for program understanding, reuse and verification. However, the inherent complexity of control and data structures in programs makes this task particularly challenging. This paper presents a novel framework that integrates symbolic execution with large language models (LLMs) to automatically synthesize formally verified program specifications. Our method first employs symbolic execution to derive precise strongest postconditions for loop-free code segments. These symbolic execution results, along with automatically generated invariant templates, then guide the LLM to propose and iteratively refine loop invariants until a correct specification is obtained. The template-guided generation process robustly combines symbolic inference with LLM reasoning, significantly reducing hallucinations and syntactic errors by structurally constraining the LLM's output space. Furthermore, our approach can produce strong specifications without relying on externally provided verification goals, enabled by the rich semantic context supplied by symbolic execution, overcoming a key limitation of prior goal-dependent tools. Extensive evaluation shows that our tool SESpec outperforms the existing state-of-the-art tools across numerical and data-structure benchmarks, demonstrating both high precision and broad applicability.", "published": "2025-06-11T09:33:02Z", "updated": "2026-01-20T12:18:25Z", "authors": ["Fanpeng Yang", "Xu Ma", "Shuling Wang", "Xiong Xu", "Qinxiang Cao", "Naijun Zhan", "Xiaofeng Li", "Bin Gu"], "pdf_url": "https://arxiv.org/pdf/2506.09550v4"}
{"id": "http://arxiv.org/abs/2511.23213v2", "title": "Mind the GAPS: Bridging the GAPS between Targeted Dynamic Analysis and Static Path Reconstruction in Android Apps", "summary": "Dynamically executing specific target methods in Android applications remains a critical and unresolved challenge. Despite notable advancements in GUI testing, current tools are insufficient for reliably driving execution toward specific target methods.\n  To address this challenge, we present GAPS (Graph-based Automated Path Synthesizer), the first system that leverages static, method-guided call graph reconstruction to guide the dynamic, interaction-driven execution of an Android app. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.\n  On the AndroTest benchmark, GAPS statically identifies paths towards 88.24% of the target methods, averaging just 4.27 seconds per app, and reaching 57.44% of them through dynamic analysis. This performance exceeds the state-of-the-art tools' one: the model-based GUI tester APE reaches only 12.82%, the hybrid tool GoalExplorer reaches 9.69%, and the LLM-based Guardian reaches 17.12%. Finally, we applied GAPS to the 50 most downloaded apps from the Google Play Store, achieving an average static analysis time of 278.9 seconds to reconstruct paths towards 62.03% of the target methods and reaching 59.86% of them through dynamic analysis.", "published": "2025-11-28T14:19:04Z", "updated": "2026-01-20T10:39:37Z", "authors": ["Samuele Doria", "Eleonora Losiouk"], "pdf_url": "https://arxiv.org/pdf/2511.23213v2"}
{"id": "http://arxiv.org/abs/2506.12084v2", "title": "The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification", "summary": "The formal specification and verification of machine learning programs saw remarkable progress in less than a decade, leading to a profusion of tools. However, diversity may lead to fragmentation, resulting in tools that are difficult to compare, except for very specific benchmarks. Furthermore, this progress is heavily geared towards the specification and verification of a certain class of property, that is, local robustness properties. But while provers are becoming more and more efficient at solving local robustness properties, even slightly more complex properties, involving multiple neural networks for example, cannot be expressed in the input languages of winners of the International Competition of Verification of Neural Networks VNN-Comp. In this tool paper, we present CAISAR, an open-source platform dedicated to machine learning specification and verification. We present its specification language, suitable for modelling complex properties on neural networks, support vector machines and boosted trees. We show on concrete use-cases how specifications written in this language are automatically translated to queries to state-of-the-art provers, notably by using automated graph editing techniques, making it possible to use their off-the-shelf versions. The artifact to reproduce the paper claims is available at the following DOI: https://doi.org/10.5281/zenodo.15209510", "published": "2025-06-10T08:49:10Z", "updated": "2026-01-20T09:41:29Z", "authors": ["Michele Alberti", "François Bobot", "Julien Girard-Satabin", "Alban Grastien", "Aymeric Varasse", "Zakaria Chihani"], "pdf_url": "https://arxiv.org/pdf/2506.12084v2"}
{"id": "http://arxiv.org/abs/2601.13772v1", "title": "A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems", "summary": "Carbon credit systems have emerged as a policy tool to incentivize emission reductions and support the transition to clean energy. Reliable carbon-credit certification depends on mechanisms that connect actual, measured renewable-energy production to verifiable emission-reduction records. Although blockchain and IoT technologies have been applied to emission monitoring and trading, existing work offers limited support for certification processes, particularly for small and medium-scale renewable installations. This paper introduces a blockchain-based carbon-credit certification architecture, demonstrated through a 100 kWp photovoltaic case study, that integrates real-time IoT data collection, edge-level aggregation, and secure on-chain storage on a permissioned blockchain with smart contracts. Unlike approaches focused on trading mechanisms, the proposed system aligns with European legislation and voluntary carbon-market standards, clarifying the practical requirements and constraints that apply to photovoltaic operators. The resulting architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.", "published": "2026-01-20T09:31:14Z", "updated": "2026-01-20T09:31:14Z", "authors": ["Matteo Vaccargiu", "Azmat Ullah", "Pierluigi Gallo"], "pdf_url": "https://arxiv.org/pdf/2601.13772v1"}
{"id": "http://arxiv.org/abs/2601.13754v1", "title": "On Autopilot? An Empirical Study of Human-AI Teaming and Review Practices in Open Source", "summary": "Large Language Models (LLMs) increasingly automate software engineering tasks. While recent studies highlight the accelerated adoption of ``AI as a teammate'' in Open Source Software (OSS), developer interaction patterns remain under-explored. In this work, we investigated project-level guidelines and developers' interactions with AI-assisted pull requests (PRs) by expanding the AIDev dataset to include finer-grained contributor code ownership and a comparative baseline of human-created PRs. We found that over 67.5\\% of AI-co-authored PRs originate from contributors without prior code ownership. Despite this, the majority of repositories lack guidelines for AI-coding agent usage. Notably, we observed a distinct interaction pattern: AI-co-authored PRs are merged significantly faster with minimal feedback. In contrast to human-created PRs where non-owner developers receive the most feedback, AI-co-authored PRs from non-owners receive the least, with approximately 80\\% merged without any explicit review. Finally, we discuss implications for developers and researchers.", "published": "2026-01-20T09:09:53Z", "updated": "2026-01-20T09:09:53Z", "authors": ["Haoyu Gao", "Peerachai Banyongrakkul", "Hao Guan", "Mansooreh Zahedi", "Christoph Treude"], "pdf_url": "https://arxiv.org/pdf/2601.13754v1"}
{"id": "http://arxiv.org/abs/2601.13743v1", "title": "Counterexample Classification against Signal Temporal Logic Specifications", "summary": "Signal Temporal Logic (STL) has been widely adopted as a specification language for specifying desirable behaviors of hybrid systems. By monitoring a given STL specification, we can detect the executions that violate it, which are often referred to as counterexamples. In practice, these counterexamples may arise from different causes and thus are relevant to different system defects. To effectively address this, we need a proper criterion for classifying these counterexamples, by which we can comprehend the possible violation patterns and the distributions of these counterexamples with respect to the patterns. In this paper, we propose a classification criterion by using parametric signal temporal logic (PSTL) to represent each class. Due to this formalism, identifying the classes of a counterexample requires finding proper parameter values of PSTL that enable a class to include the counterexample. To improve the efficiency of class identification, we further derive an inclusion relation between different classes, and then propose a binary search-like approach over it that significantly prunes the classes needed to query. We implement a prototype tool and experimentally evaluate its effectiveness on two widely-studied systems.", "published": "2026-01-20T08:57:20Z", "updated": "2026-01-20T08:57:20Z", "authors": ["Zhenya Zhang", "Parv Kapoor", "Jie An", "Eunsuk Kang"], "pdf_url": "https://arxiv.org/pdf/2601.13743v1"}
{"id": "http://arxiv.org/abs/2502.08739v3", "title": "A Taxonomy of Real Faults in Hybrid Quantum-Classical Architectures", "summary": "With the popularity of Hybrid Quantum-Classical architectures, particularly noisy intermediate-scale quantum (NISQ) architectures, comes the need for quality assurance methods tailored to their specific faults. In this study, we propose a taxonomy of faults in Hybrid Quantum-Classical architectures accompanied by a dataset of real faults in the identified categories. To achieve this, we empirically analysed open-source repositories for fixed faults. We analysed over 5000 closed issues on GitHub and pre-selected 529 of them based on rigorously defined inclusion criteria. We selected 133 faults that we labelled around symptoms and the origin of the faults. We cross-validated the classification and labels assigned to every fault between two of the authors. As a result, we introduced a taxonomy of real faults in Hybrid Quantum-Classical architectures. Subsequently, we validated the taxonomy through interviews conducted with eleven developers. The taxonomy was dynamically updated throughout the cross-validation and interview processes. The final version was validated and discussed through surveys conducted with an independent group of domain experts to ensure its relevance and to gain further insights.", "published": "2025-02-12T19:20:29Z", "updated": "2026-01-20T08:44:31Z", "authors": ["Avner Bensoussan", "Gunel Jahangirova", "Mohammad Reza Mousavi"], "pdf_url": "https://arxiv.org/pdf/2502.08739v3"}
{"id": "http://arxiv.org/abs/2601.13713v1", "title": "SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories", "summary": "Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- \"test first, write code later\", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\\% in success rate and 21\\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.", "published": "2026-01-20T08:10:56Z", "updated": "2026-01-20T08:10:56Z", "authors": ["Aditya Bharat Soni", "Rajat Ghosh", "Vaishnavi Bhargava", "Valerie Chen", "Debojyoti Dutta"], "pdf_url": "https://arxiv.org/pdf/2601.13713v1"}
{"id": "http://arxiv.org/abs/2601.13682v1", "title": "CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation", "summary": "The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \\times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\\%$ and True Negative Rate (TNR) of $90.89\\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\\%$ and $9.37\\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.", "published": "2026-01-20T07:32:44Z", "updated": "2026-01-20T07:32:44Z", "authors": ["Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Kangwen Zhao", "Dongyun Xue", "Mingxiao Feng", "Wengang Zhou", "Houqiang Li"], "pdf_url": "https://arxiv.org/pdf/2601.13682v1"}
{"id": "http://arxiv.org/abs/2504.15439v2", "title": "Combating Toxic Language: A Review of LLM-Based Strategies for Software Engineering", "summary": "Large Language Models (LLMs) have become integral to Software Engineering (SE), increasingly used in development workflows. However, their widespread adoption raises concerns about the presence and propagation of toxic language - harmful or offensive content that can foster exclusionary environments. This paper provides a comprehensive review of recent research (2020-2024) on toxicity detection and mitigation, focusing on both SE-specific and general-purpose datasets. We examine annotation and pre-processing techniques, assess detection methodologies, and evaluate mitigation strategies, particularly those leveraging LLMs. Additionally, we conduct an ablation study demonstrating the effectiveness of LLM-based rewriting for reducing toxicity. This review is limited to studies published within the specified timeframe and within the domain of toxicity in LLMs and SE; therefore, certain emerging methods or datasets beyond this period may fall outside its purview. By synthesizing existing work and identifying open challenges, this review highlights key areas for future research to ensure the responsible deployment of LLMs in SE and beyond.", "published": "2025-04-21T21:09:33Z", "updated": "2026-01-20T07:09:12Z", "authors": ["Hao Zhuo", "Yicheng Yang", "Kewen Peng"], "pdf_url": "https://arxiv.org/pdf/2504.15439v2"}
{"id": "http://arxiv.org/abs/2601.02438v2", "title": "Focus on What Matters: Fisher-Guided Adaptive Multimodal Fusion for Vulnerability Detection", "summary": "Software vulnerability detection can be formulated as a binary classification problem that determines whether a given code snippet contains security defects. Existing multimodal methods typically fuse Natural Code Sequence (NCS) representations extracted by pretrained models with Code Property Graph (CPG) representations extracted by graph neural networks, under the implicit assumption that introducing an additional modality necessarily yields information gain. Through empirical analysis, we demonstrate the limitations of this assumption: pretrained models already encode substantial structural information implicitly, leading to strong overlap between the two modalities; moreover, graph encoders are generally less effective than pretrained language models in feature extraction. As a result, naive fusion not only struggles to obtain complementary signals but can also dilute effective discriminative cues due to noise propagation. To address these challenges, we propose a task-conditioned complementary fusion strategy that uses Fisher information to quantify task relevance, transforming cross-modal interaction from full-spectrum matching into selective fusion within a task-sensitive subspace. Our theoretical analysis shows that, under an isotropic perturbation assumption, this strategy significantly tightens the upper bound on the output error. Based on this insight, we design the TaCCS-DFA framework, which combines online low-rank Fisher subspace estimation with an adaptive gating mechanism to enable efficient task-oriented fusion. Experiments on the BigVul, Devign, and ReVeal benchmarks demonstrate that TaCCS-DFA delivers up to a 6.3-point gain in F1 score with only a 3.4% increase in inference latency, while maintaining low calibration error.", "published": "2026-01-05T09:31:21Z", "updated": "2026-01-20T07:00:06Z", "authors": ["Yun Bian", "Yi Chen", "HaiQuan Wang", "ShiHao Li", "Zhe Cui"], "pdf_url": "https://arxiv.org/pdf/2601.02438v2"}
{"id": "http://arxiv.org/abs/2601.13655v1", "title": "Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs", "summary": "The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.\n  Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.", "published": "2026-01-20T06:42:56Z", "updated": "2026-01-20T06:42:56Z", "authors": ["Guangba Yu", "Zirui Wang", "Yujie Huang", "Renyi Zhong", "Yuedong Zhong", "Yilun Wang", "Michael R. Lyu"], "pdf_url": "https://arxiv.org/pdf/2601.13655v1"}
{"id": "http://arxiv.org/abs/2601.13597v1", "title": "AI IDEs or Autonomous Agents? Measuring the Impact of Coding Agents on Software Development", "summary": "Large language model (LLM)-based coding agents increasingly act as autonomous contributors that generate and merge pull requests, yet their real-world effects on software projects are unclear, especially relative to widely adopted IDE-based AI assistants. We present a longitudinal causal study of agent adoption in open-source repositories using staggered difference-in-differences with matched controls. Using the AIDev dataset, we define adoption as the first agent-generated pull request and analyze monthly repository-level outcomes spanning development velocity (commits, lines added) and software quality (static-analysis warnings, cognitive complexity, duplication, and comment density). Results show large, front-loaded velocity gains only when agents are the first observable AI tool in a project; repositories with prior AI IDE usage experience minimal or short-lived throughput benefits. In contrast, quality risks are persistent across settings, with static-analysis warnings and cognitive complexity rising roughly 18% and 35%, indicating sustained agent-induced complexity debt even when velocity advantages fade. These heterogeneous effects suggest diminishing returns to AI assistance and highlight the need for quality safeguards, provenance tracking, and selective deployment of autonomous agents. Our findings establish an empirical basis for understanding how agentic and IDE-based tools interact, and motivate research on balancing acceleration with maintainability in AI-integrated development workflows.", "published": "2026-01-20T04:51:56Z", "updated": "2026-01-20T04:51:56Z", "authors": ["Shyam Agarwal", "Hao He", "Bogdan Vasilescu"], "pdf_url": "https://arxiv.org/pdf/2601.13597v1"}
{"id": "http://arxiv.org/abs/2601.13528v1", "title": "Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs", "summary": "Model developers implement safeguards in frontier models to prevent misuse, for example, by employing classifiers to filter dangerous outputs. In this work, we demonstrate that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. Our elicitation attacks consist of three stages: (i) constructing prompts in adjacent domains to a target harmful task that do not request dangerous information; (ii) obtaining responses to these prompts from safeguarded frontier models; (iii) fine-tuning open-source models on these prompt-output pairs. Since the requested prompts cannot be used to directly cause harm, they are not refused by frontier model safeguards. We evaluate these elicitation attacks within the domain of hazardous chemical synthesis and processing, and demonstrate that our attacks recover approximately 40% of the capability gap between the base open-source model and an unrestricted frontier model. We then show that the efficacy of elicitation attacks scales with the capability of the frontier model and the amount of generated fine-tuning data. Our work demonstrates the challenge of mitigating ecosystem level risks with output-level safeguards.", "published": "2026-01-20T02:24:44Z", "updated": "2026-01-20T02:24:44Z", "authors": ["Jackson Kaunismaa", "Avery Griffin", "John Hughes", "Christina Q. Knight", "Mrinank Sharma", "Erik Jones"], "pdf_url": "https://arxiv.org/pdf/2601.13528v1"}
{"id": "http://arxiv.org/abs/2512.17363v3", "title": "What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice", "summary": "Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.", "published": "2025-12-19T09:02:58Z", "updated": "2026-01-20T01:49:22Z", "authors": ["Yuqing Niu", "Jieke Shi", "Ruidong Han", "Ye Liu", "Chengyan Ma", "Yunbo Lyu", "David Lo"], "pdf_url": "https://arxiv.org/pdf/2512.17363v3"}
