{"id": "http://arxiv.org/abs/2505.01538v3", "title": "HONEYBEE: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning[Technical Report]", "summary": "Enterprise deployments of vector databases require access control policies to protect sensitive data. These systems often implement access control through hybrid vector queries that combine nearest-neighbor search with relational predicates based on user permissions. However, existing approaches face a fundamental trade-off: dedicated per-user indexes minimize query latency but incur high memory redundancy, while shared indexes with post-search filtering reduce memory overhead at the cost of increased latency. This paper introduces HONEYBEE, a dynamic partitioning framework that leverages the structure of Role-Based Access Control (RBAC) policies to create a smooth trade-off between these extremes. RBAC policies organize users into roles and assign permissions at the role level, creating a natural ``thin waist`` in the permission structure that is ideal for partitioning decisions. Specifically, HONEYBEE produces overlapping partitions where vectors can be strategically replicated across different partitions to reduce query latency while controlling memory overhead. To guide these decisions, HONEYBEE develops analytical models of vector search performance and recall, and formulates partitioning as a constrained optimization problem that balances memory usage, query efficiency, and recall. Evaluations on RBAC workloads demonstrate that HONEYBEE achieves up to 13.5X lower query latency than row-level security with only a 1.24X increase in memory usage, while achieving comparable query performance to dedicated, per-role indexes with 90.4% reduction in additional memory consumption, offering a practical middle ground for secure and efficient vector search.", "published": "2025-05-02T18:59:31Z", "updated": "2026-01-07T18:49:03Z", "authors": ["Hongbin Zhong", "Matthew Lentz", "Nina Narodytska", "Adriana Szekeres", "Kexin Rong"], "pdf_url": "https://arxiv.org/pdf/2505.01538v3"}
{"id": "http://arxiv.org/abs/2409.13937v4", "title": "Lightweight and Resilient Signatures for Cloud-Assisted Embedded IoT Systems", "summary": "Digital signatures provide scalable authentication with non-repudiation and are vital tools for the Internet of Things (IoT). Many IoT applications harbor vast quantities of resource-limited devices often used with cloud computing. However, key compromises (e.g., physical, malware) pose a significant threat to IoTs due to increased attack vectors and open operational environments. Forward security and distributed key management are critical breach-resilient countermeasures to mitigate such threats. Yet forward-secure signatures are exorbitantly costly for low-end IoTs, while cloud-assisted approaches suffer from centrality or non-colluding semi-honest servers. In this work, we create two novel digital signatures called Lightweight and Resilient Signatures with Hardware Assistance (LRSHA) and its Forward-secure version (FLRSHA). They offer a near-optimally efficient signing with small keys and signature sizes. We synergize various design strategies, such as commitment separation to eliminate costly signing operations and hardware-assisted distributed servers to enable breach-resilient verification. Our schemes achieve magnitudes of faster forward-secure signing and compact key/signature sizes without suffering from strong security assumptions (non-colluding, central servers) or a heavy burden on the verifier (extreme storage, computation). We formally prove the security of our schemes and validate their performance with full-fledged open-source implementations on both commodity hardware and 8-bit AVR microcontrollers.", "published": "2024-09-20T22:43:47Z", "updated": "2026-01-07T16:18:48Z", "authors": ["Saif E. Nouma", "Attila A. Yavuz"], "pdf_url": "https://arxiv.org/pdf/2409.13937v4"}
{"id": "http://arxiv.org/abs/2601.04034v1", "title": "HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense", "summary": "Jailbreak attacks pose significant threats to large language models (LLMs), enabling attackers to bypass safeguards. However, existing reactive defense approaches struggle to keep up with the rapidly evolving multi-turn jailbreaks, where attackers continuously deepen their attacks to exploit vulnerabilities. To address this critical challenge, we propose HoneyTrap, a novel deceptive LLM defense framework leveraging collaborative defenders to counter jailbreak attacks. It integrates four defensive agents, Threat Interceptor, Misdirection Controller, Forensic Tracker, and System Harmonizer, each performing a specialized security role and collaborating to complete a deceptive defense. To ensure a comprehensive evaluation, we introduce MTJ-Pro, a challenging multi-turn progressive jailbreak dataset that combines seven advanced jailbreak strategies designed to gradually deepen attack strategies across multi-turn attacks. Besides, we present two novel metrics: Mislead Success Rate (MSR) and Attack Resource Consumption (ARC), which provide more nuanced assessments of deceptive defense beyond conventional measures. Experimental results on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1 demonstrate that HoneyTrap achieves an average reduction of 68.77% in attack success rates compared to state-of-the-art baselines. Notably, even in a dedicated adaptive attacker setting with intensified conditions, HoneyTrap remains resilient, leveraging deceptive engagement to prolong interactions, significantly increasing the time and computational costs required for successful exploitation. Unlike simple rejection, HoneyTrap strategically wastes attacker resources without impacting benign queries, improving MSR and ARC by 118.11% and 149.16%, respectively.", "published": "2026-01-07T15:47:28Z", "updated": "2026-01-07T15:47:28Z", "authors": ["Siyuan Li", "Xi Lin", "Jun Wu", "Zehao Liu", "Haoyu Li", "Tianjie Ju", "Xiang Chen", "Jianhua Li"], "pdf_url": "https://arxiv.org/pdf/2601.04034v1"}
{"id": "http://arxiv.org/abs/2601.04010v1", "title": "An Ontology-Based Approach to Security Risk Identification of Container Deployments in OT Contexts", "summary": "In operational technology (OT) contexts, containerised applications often require elevated privileges to access low-level network interfaces or perform administrative tasks such as application monitoring. These privileges reduce the default isolation provided by containers and introduce significant security risks. Security risk identification for OT container deployments is challenged by hybrid IT/OT architectures, fragmented stakeholder knowledge, and continuous system changes. Existing approaches lack reproducibility, interpretability across contexts, and technical integration with deployment artefacts. We propose a model-based approach, implemented as the Container Security Risk Ontology (CSRO), which integrates five key domains: adversarial behaviour, contextual assumptions, attack scenarios, risk assessment rules, and container security artefacts. Our evaluation of CSRO in a case study demonstrates that the end-to-end formalisation of risk calculation, from artefact to risk level, enables automated and reproducible risk identification. While CSRO currently focuses on technical, container-level treatment measures, its modular and flexible design provides a solid foundation for extending the approach to host-level and organisational risk factors.", "published": "2026-01-07T15:20:19Z", "updated": "2026-01-07T15:20:19Z", "authors": ["Yannick Landeck", "Dian Balta", "Martin Wimmer", "Christian Knierim"], "pdf_url": "https://arxiv.org/pdf/2601.04010v1"}
{"id": "http://arxiv.org/abs/2601.03979v1", "title": "SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems", "summary": "The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained \"knowledge\" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.", "published": "2026-01-07T14:50:41Z", "updated": "2026-01-07T14:50:41Z", "authors": ["Andreea-Elena Bodea", "Stephen Meisenbacher", "Alexandra Klymenko", "Florian Matthes"], "pdf_url": "https://arxiv.org/pdf/2601.03979v1"}
{"id": "http://arxiv.org/abs/1811.04349v2", "title": "Lockcoin: a secure and privacy-preserving mix service for bitcoin anonymity", "summary": "We propose Lockcoin, a secure and privacy-preserving mix service for bitcoin anonymity. We introduce mix servers to provide mix service for user to prevent attackers linking the input address with output address by using blind signature shceme, multisignature scheme. Lockcoin provides anonymity, scalability, bitcoin compatibillity, theft impossibility and accountability. We have proposed a prototype of Lockcoin based on bitcoin test network, experimental results show that our solution is efficient. Lockcoin's source codes are released on github.com/Northeastern-University-Blockchain/Lockcoin.", "published": "2018-11-11T04:46:54Z", "updated": "2026-01-07T14:42:49Z", "authors": ["Zijian Bao", "Bin Wang", "Yongxin Zhang", "Qinghao Wang", "Wenbo Shi"], "pdf_url": "https://arxiv.org/pdf/1811.04349v2"}
{"id": "http://arxiv.org/abs/1806.02008v3", "title": "IoTChain: A Three-Tier Blockchain-based IoT Security Architecture", "summary": "There has been increasing interest in the potential of blockchain in enhancing the security of devices and systems, such as Internet of Things (IoT). In this paper, we present a blockchain-based IoT security architecture, IoTchain. The three-tier architecture comprises an authentication layer, a blockchain layer and an application layer, and is designed to achieve identity authentication, access control, privacy protection, lightweight feature, regional node fault tolerance, denial-of-service resilience, and storage integrity. We also evaluate the performance of IoTchain to demonstrate its utility in an IoT deployment.", "published": "2018-06-06T05:09:16Z", "updated": "2026-01-07T14:41:12Z", "authors": ["Zijian Bao", "Wenbo Shi", "Debiao He", "Kim-Kwang Raymond Chood"], "pdf_url": "https://arxiv.org/pdf/1806.02008v3"}
{"id": "http://arxiv.org/abs/2402.16028v6", "title": "FedFDP: Fairness-Aware Federated Learning with Differential Privacy", "summary": "Federated learning (FL) is an emerging machine learning paradigm designed to address the challenge of data silos, attracting considerable attention. However, FL encounters persistent issues related to fairness and data privacy. To tackle these challenges simultaneously, we propose a fairness-aware federated learning algorithm called FedFair. Building on FedFair, we introduce differential privacy to create the FedFDP algorithm, which addresses trade-offs among fairness, privacy protection, and model performance. In FedFDP, we developed a fairness-aware gradient clipping technique to explore the relationship between fairness and differential privacy. Through convergence analysis, we identified the optimal fairness adjustment parameters to achieve both maximum model performance and fairness. Additionally, we present an adaptive clipping method for uploaded loss values to reduce privacy budget consumption. Extensive experimental results show that FedFDP significantly surpasses state-of-the-art solutions in both model performance and fairness.", "published": "2024-02-25T08:35:21Z", "updated": "2026-01-07T14:30:12Z", "authors": ["Xinpeng Ling", "Jie Fu", "Kuncan Wang", "Huifa Li", "Tong Cheng", "Zhili Chen"], "pdf_url": "https://arxiv.org/pdf/2402.16028v6"}
{"id": "http://arxiv.org/abs/2601.03923v1", "title": "Human Challenge Oracle: Designing AI-Resistant, Identity-Bound, Time-Limited Tasks for Sybil-Resistant Consensus", "summary": "Sybil attacks remain a fundamental obstacle in open online systems, where adversaries can cheaply create and sustain large numbers of fake identities. Existing defenses, including CAPTCHAs and one-time proof-of-personhood mechanisms, primarily address identity creation and provide limited protection against long-term, large-scale Sybil participation, especially as automated solvers and AI systems continue to improve.\n  We introduce the Human Challenge Oracle (HCO), a new security primitive for continuous, rate-limited human verification. HCO issues short, time-bound challenges that are cryptographically bound to individual identities and must be solved in real time. The core insight underlying HCO is that real-time human cognitive effort, such as perception, attention, and interactive reasoning, constitutes a scarce resource that is inherently difficult to parallelize or amortize across identities.\n  We formalize the design goals and security properties of HCO and show that, under explicit and mild assumptions, sustaining s active identities incurs a cost that grows linearly with s in every time window. We further describe abstract classes of admissible challenges and concrete browser-based instantiations, and present an initial empirical study illustrating that these challenges are easily solvable by humans within seconds while remaining difficult for contemporary automated systems under strict time constraints.", "published": "2026-01-07T13:42:21Z", "updated": "2026-01-07T13:42:21Z", "authors": ["Homayoun Maleki", "Nekane Sainz", "Jon Legarda"], "pdf_url": "https://arxiv.org/pdf/2601.03923v1"}
{"id": "http://arxiv.org/abs/2401.01114v2", "title": "Static Deadlock Detection for Rust Programs", "summary": "Rust relies on its unique ownership mechanism to ensure thread and memory safety. However, numerous potential security vulnerabilities persist in practical applications. New language features in Rust pose new challenges for vulnerability detection. This paper proposes a static deadlock detection method tailored for Rust programs, aiming to identify various deadlock types, including double lock, conflict lock, and deadlock associated with conditional variables. With due consideration for Rust's ownership and lifetimes, we first complete the pointer analysis. Then, based on the obtained points-to information, we analyze dependencies among variables to identify potential deadlocks. We develop a tool and conduct experiments based on the proposed method. The experimental results demonstrate that our method outperforms existing deadlock detection methods in precision.", "published": "2024-01-02T09:09:48Z", "updated": "2026-01-07T13:33:39Z", "authors": ["Yu Zhang", "Kaiwen Zhang", "Guanjun Liu"], "pdf_url": "https://arxiv.org/pdf/2401.01114v2"}
{"id": "http://arxiv.org/abs/2505.20118v4", "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent", "summary": "As large language models (LLMs) become integrated into sensitive workflows, concerns grow over their potential to leak confidential information. We propose TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to embed sensitive context information into natural-looking outputs via linguistic steganography, without requiring explicit control over inference inputs. We introduce a taxonomy outlining risk factors for compromised LLMs, and use it to evaluate the risk profile of the threat. To implement TrojanStego, we propose a practical encoding scheme based on vocabulary partitioning learnable by LLMs via fine-tuning. Experimental results show that compromised models reliably transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over 97% accuracy using majority voting across three generations. Further, they maintain high utility, can evade human detection, and preserve coherence. These results highlight a new class of LLM data exfiltration attacks that are passive, covert, practical, and dangerous.", "published": "2025-05-26T15:20:51Z", "updated": "2026-01-07T13:19:41Z", "authors": ["Dominik Meier", "Jan Philip Wahle", "Paul Röttger", "Terry Ruas", "Bela Gipp"], "pdf_url": "https://arxiv.org/pdf/2505.20118v4"}
{"id": "http://arxiv.org/abs/2409.01234v2", "title": "SoK: Security of the Image Processing Pipeline for Camera-based Sensing in Autonomous Vehicles", "summary": "Cameras capture images that are essential for many safety-critical tasks. To process these images, a complex pipeline with multiple layers is used. Security attacks on this pipeline can severely affect passenger safety and system performance. However, many attacks presented in scientific literature overlook the fact that there are different layers and, hence, the feasibility and impact of these attacks can vary. While there has been research to improve the quality and robustness of the image processing pipeline, these efforts are often orthogonal to security research without exploiting potential overlap and synergies. In this work, we aim to bridge this gap by combining security and robustness research for the image processing pipeline in autonomous vehicles. We thoroughly investigated the body of literature on the security and robustness of the image processing pipeline and selected 92 papers for deeper discussion in this SoK. For the security domain, we classify the risk of attacks using the automotive security standard ISO 21434, emphasizing the need to consider all layers for overall system security. With our online tool TARA-CAM, we propose an interactive method to perform threat analysis and risk assessment following the ISO standard. We also demonstrate how existing robustness research can help mitigate the impact of attacks, addressing the current research gap. Finally, we present PICT, an embedded open-source testbed that can influence various parameters across all layers, allowing researchers to analyze the effects of different defense strategies and attack impacts. With this SoK, we contribute a comprehensive discussion and systematic analysis of existing approaches to image processing pipeline security and robustness, together with an open-source tool and testbed that jointly facilitates hardening the image processing pipeline against existing and future security attacks.", "published": "2024-09-02T13:10:53Z", "updated": "2026-01-07T12:34:52Z", "authors": ["Michael Kühr", "Mohammad Hamad", "Pedram MohajerAnsari", "Mert D. Pesé", "Sebastian Steinhorst"], "pdf_url": "https://arxiv.org/pdf/2409.01234v2"}
{"id": "http://arxiv.org/abs/2601.03868v1", "title": "What Matters For Safety Alignment?", "summary": "This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spanning a parameter scale from 3B to 235B. The assessment leverages five established safety datasets and probes model vulnerabilities with 56 jailbreak techniques and four CoT attack strategies, resulting in 4.6M API calls. Our key empirical findings are fourfold. First, we identify the LRMs GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, and GPT-OSS-120B as the top-three safest models, which substantiates the significant advantage of integrated reasoning and self-reflection mechanisms for robust safety alignment. Second, post-training and knowledge distillation may lead to a systematic degradation of safety alignment. We thus argue that safety must be treated as an explicit constraint or a core optimization objective during these stages, not merely subordinated to the pursuit of general capability. Third, we reveal a pronounced vulnerability: employing a CoT attack via a response prefix can elevate the attack success rate by 3.34x on average and from 0.6% to 96.3% for Seed-OSS-36B-Instruct. This critical finding underscores the safety risks inherent in text-completion interfaces and features that allow user-defined response prefixes in LLM services, highlighting an urgent need for architectural and deployment safeguards. Fourth, roleplay, prompt injection, and gradient-based search for adversarial prompts are the predominant methodologies for eliciting unaligned behaviors in modern models.", "published": "2026-01-07T12:31:52Z", "updated": "2026-01-07T12:31:52Z", "authors": ["Xing Li", "Hui-Ling Zhen", "Lihao Yin", "Xianzhi Yu", "Zhenhua Dong", "Mingxuan Yuan"], "pdf_url": "https://arxiv.org/pdf/2601.03868v1"}
{"id": "http://arxiv.org/abs/2508.10390v3", "title": "Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts", "summary": "Existing black-box jailbreak attacks achieve certain success on non-reasoning models but degrade significantly on recent SOTA reasoning models. To improve attack ability, inspired by adversarial aggregation strategies, we integrate multiple jailbreak tricks into a single developer template. Especially, we apply Adversarial Context Alignment to purge semantic inconsistencies and use NTP (a type of harmful prompt) -based few-shot examples to guide malicious outputs, lastly forming DH-CoT attack with a fake chain of thought. In experiments, we further observe that existing red-teaming datasets include samples unsuitable for evaluating attack gains, such as BPs, NHPs, and NTPs. Such data hinders accurate evaluation of true attack effect lifts. To address this, we introduce MDH, a Malicious content Detection framework integrating LLM-based annotation with Human assistance, with which we clean data and build RTA dataset suite. Experiments show that MDH reliably filters low-quality samples and that DH-CoT effectively jailbreaks models including GPT-5 and Claude-4, notably outperforming SOTA methods like H-CoT and TAP.", "published": "2025-08-14T06:46:56Z", "updated": "2026-01-07T12:15:51Z", "authors": ["Chiyu Zhang", "Lu Zhou", "Xiaogang Xu", "Jiafei Wu", "Liming Fang", "Zhe Liu"], "pdf_url": "https://arxiv.org/pdf/2508.10390v3"}
{"id": "http://arxiv.org/abs/2512.12069v2", "title": "Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring", "summary": "Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.", "published": "2025-12-12T22:31:38Z", "updated": "2026-01-07T11:45:25Z", "authors": ["Peichun Hua", "Hao Li", "Shanghao Shi", "Zhiyuan Yu", "Ning Zhang"], "pdf_url": "https://arxiv.org/pdf/2512.12069v2"}
{"id": "http://arxiv.org/abs/2409.17744v4", "title": "Privacy for Quantum Annealing. Attack on Spin Reversal Transformations in the case of cryptanalysis", "summary": "This paper demonstrates that applying spin reversal transformations (SRT), commonly known as a sufficient method for privacy enhancement in problems solved using quantum annealing, does not guarantee privacy for all possible cases. We show how to recover the original problem from the Ising problem obtained using SRT when the resulting problem in Ising form represents the algebraic attack on the $E_0$ stream cipher. A small example illustrates how to retrieve the original problem from that transformed by SRT. Moreover, we show that our method is efficient also for full-scale problems.", "published": "2024-09-26T11:17:47Z", "updated": "2026-01-07T10:04:26Z", "authors": ["Mateusz Leśniak", "Michał Wroński"], "pdf_url": "https://arxiv.org/pdf/2409.17744v4"}
{"id": "http://arxiv.org/abs/2506.16328v3", "title": "Sharpening Kubernetes Audit Logs with Context Awareness", "summary": "Kubernetes has emerged as the de facto orchestrator of microservices, providing scalability and extensibility to a highly dynamic environment. It builds an intricate and deeply connected system that requires extensive monitoring capabilities to be properly managed. To this account, K8s natively offers audit logs, a powerful feature for tracking API interactions in the cluster. Audit logs provide a detailed and chronological record of all activities in the system. Unfortunately, K8s auditing suffers from several practical limitations: it generates large volumes of data continuously, as all components within the cluster interact and respond to user actions. Moreover, each action can trigger a cascade of secondary events dispersed across the log, with little to no explicit linkage, making it difficult to reconstruct the context behind user-initiated operations. In this paper, we introduce K8NTEXT, a novel approach for streamlining K8s audit logs by reconstructing contexts, i.e., grouping actions performed by actors on the cluster with the subsequent events these actions cause. Correlated API calls are automatically identified, labeled, and consistently grouped using a combination of inference rules and a Machine Learning model, largely simplifying data consumption. We evaluate K8NTEXT's performance, scalability, and expressiveness both in systematic tests and with a series of use cases. We show that it consistently provides accurate context reconstruction, even for complex operations involving 50, 100 or more correlated actions, achieving over 95 percent accuracy across the entire spectrum, from simple to highly composite actions.", "published": "2025-06-19T14:02:22Z", "updated": "2026-01-07T09:36:22Z", "authors": ["Matteo Franzil", "Valentino Armani", "Luis Augusto Dias Knob", "Domenico Siracusa"], "pdf_url": "https://arxiv.org/pdf/2506.16328v3"}
{"id": "http://arxiv.org/abs/2511.05319v2", "title": "$\\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models", "summary": "Despite remarkable progress in steganography, embedding semantically rich, sentence-level information into carriers remains a challenging problem. In this work, we present a novel concept of Semantic Steganography, which aims to hide semantically meaningful and structured content, such as sentences or paragraphs, in cover media. Based on this concept, we present Sentence-to-Image Steganography as an instance that enables the hiding of arbitrary sentence-level messages within a cover image. To accomplish this feat, we propose S^2LM: Semantic Steganographic Language Model, which leverages large language models (LLMs) to embed high-level textual information into images. Unlike traditional bit-level approaches, S^2LM redesigns the entire pipeline, involving the LLM throughout the process to enable the hiding and recovery of arbitrary sentences. Furthermore, we establish a benchmark named Invisible Text (IVT), comprising a diverse set of sentence-level texts as secret messages to evaluate semantic steganography methods. Experimental results demonstrate that S^2LM effectively enables direct sentence recovery beyond bit-level steganography. The source code and IVT dataset will be released soon.", "published": "2025-11-07T15:17:40Z", "updated": "2026-01-07T08:54:15Z", "authors": ["Huanqi Wu", "Huangbiao Xu", "Runfeng Xie", "Jiaxin Cai", "Kaixin Zhang", "Xiao Ke"], "pdf_url": "https://arxiv.org/pdf/2511.05319v2"}
{"id": "http://arxiv.org/abs/2506.10645v2", "title": "Kitten or Panda? Measuring the Specificity of Threat Group Behaviors in Public CTI Knowledge Bases", "summary": "In recent years, the cyber threat intelligence (CTI) community has invested significant effort in building knowledge bases that catalog threat groups. These knowledge bases associate each threat group with its observed behaviors, including their Tactics, Techniques, and Procedures (TTPs) as well as the malware and tools they employ during attacks. However, the distinctiveness and completeness of such behavioral profiles remain largely unexplored, despite being critical for tasks such as threat group attribution. In this work, we systematically analyze threat group profiles built from two public CTI knowledge bases: MITRE ATT&CK and Malpedia. We first investigate what fraction of threat groups have group-specific behaviors, i.e., behaviors used exclusively by a single group. We find that only 34% of threat groups in ATT&CK have group-specific techniques, limiting the use of techniques as reliable behavioral signatures to identify the threat group behind an attack. The software used by a threat group proves to be more distinctive, with 73% of ATT&CK groups using group-specific software. However, this percentage drops to 24% in the broader Malpedia dataset. Next, we evaluate how group profiles improve when data from both sources are combined. While coverage improves modestly, the proportion of groups with group-specific behaviors remains under 30%. We then enhance profiles by adding exploited vulnerabilities and additional techniques extracted from threat reports. Despite the additional information, 64% of groups still lack any group-specific behavior. Our findings raise concerns about the specificity of existing behavioral profiles and highlight the need for caution, as well as further improvement, when using them for threat group attribution.", "published": "2025-06-12T12:33:55Z", "updated": "2026-01-07T08:33:24Z", "authors": ["Aakanksha Saha", "Martina Lindorfer", "Juan Caballero"], "pdf_url": "https://arxiv.org/pdf/2506.10645v2"}
{"id": "http://arxiv.org/abs/2509.01211v2", "title": "Web Fraud Attacks Against LLM-Driven Multi-Agent Systems", "summary": "With the proliferation of LLM-driven multi-agent systems (MAS), the security of Web links has become a critical concern. Once MAS is induced to trust a malicious link, attackers can use it as a springboard to expand the attack surface. In this paper, we propose Web Fraud Attacks, a novel type of attack manipulating unique structures of web links to deceive MAS. We design 12 representative attack variants that encompass various methods, such as homoglyph deception, sub-directory nesting, and parameter obfuscation. Through extensive experiments on these attack vectors, we demonstrate that Web fraud attacks not only exhibit significant destructive potential across different MAS architectures but also possess a distinct advantage in evasion: they circumvent the need for complex input design, lowering the threshold for attacks significantly. These results underscore the importance of addressing Web fraud attacks, providing new insights into MAS safety. Our code is available at https://github.com/JiangYingEr/Web-Fraud-Attack-in-MAS.", "published": "2025-09-01T07:47:24Z", "updated": "2026-01-07T08:30:38Z", "authors": ["Dezhang Kong", "Hujin Peng", "Yilun Zhang", "Lele Zhao", "Zhenhua Xu", "Shi Lin", "Changting Lin", "Meng Han"], "pdf_url": "https://arxiv.org/pdf/2509.01211v2"}
{"id": "http://arxiv.org/abs/2601.03690v1", "title": "Detection and Prevention of Process Disruption Attacks in the Electrical Power Systems using MMS Traffic: An EPIC Case", "summary": "Smart grids are increasingly exposed to sophisticated cyber threats due to their reliance on interconnected communication networks, as demonstrated by real world incidents such as the cyberattacks on the Ukrainian power grid. In IEC61850 based smart substations, the Manufacturing Message Specification protocol operates over TCP to facilitate communication between SCADA systems and field devices such as Intelligent Electronic Devices and Programmable Logic Controllers. Although MMS enables efficient monitoring and control, it can be exploited by adversaries to generate legitimate looking packets for reconnaissance, unauthorized state reading, and malicious command injection, thereby disrupting grid operations. In this work, we propose a fully automated attack detection and prevention framework for IEC61850 compliant smart substations to counter remote cyberattacks that manipulate process states through compromised PLCs and IEDs. A detailed analysis of the MMS protocol is presented, and critical MMS field value pairs are extracted during both normal SCADA operation and active attack conditions. The proposed framework is validated using seven datasets comprising benign operational scenarios and multiple attack instances, including IEC61850Bean based attacks and script driven attacks leveraging the libiec61850 library. Our approach accurately identifies attack signature carrying MMS packets that attempt to disrupt circuit breaker status, specifically targeting the smart home zone IED and PLC of the EPIC testbed. The results demonstrate the effectiveness of the proposed framework in precisely detecting malicious MMS traffic and enhancing the cyber resilience of IEC61850 based smart grid environments.", "published": "2026-01-07T08:25:24Z", "updated": "2026-01-07T08:25:24Z", "authors": ["Praneeta K Maganti", "Daisuke Mashima", "Rajib Ranjan Maiti"], "pdf_url": "https://arxiv.org/pdf/2601.03690v1"}
{"id": "http://arxiv.org/abs/2502.18851v3", "title": "Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code", "summary": "Identifying LLM-generated code through watermarking poses a challenge in preserving functional correctness. Previous methods rely on the assumption that watermarking high-entropy tokens effectively maintains output quality. Our analysis reveals a fundamental limitation of this assumption: syntax-critical tokens such as keywords often exhibit the highest entropy, making existing approaches vulnerable to logic corruption. We present STONE, a syntax-aware watermarking method that embeds watermarks only in non-syntactic tokens and preserves code integrity. For its rigorous assessment, we also introduce STEM, a comprehensive framework that balances three critical dimensions: correctness, detectability, and imperceptibility. Across Python, C++, and Java, STONE preserves correctness, sustains strong detectability, and achieves balanced performance with minimal overhead. Our implementation is available at https://anonymous.4open.science/r/STONE-watermarking-AB4B/.", "published": "2025-02-26T05:46:13Z", "updated": "2026-01-07T08:01:06Z", "authors": ["Jungin Kim", "Shinwoo Park", "Yo-Sub Han"], "pdf_url": "https://arxiv.org/pdf/2502.18851v3"}
{"id": "http://arxiv.org/abs/2512.24602v2", "title": "Secure Digital Semantic Communications: Fundamentals, Challenges, and Opportunities", "summary": "Semantic communication (SemCom) has emerged as a promising paradigm for future wireless networks by prioritizing task-relevant meaning over raw data delivery, thereby reducing communication overhead and improving efficiency. However, shifting from bit-accurate transmission to task-oriented delivery introduces new security and privacy risks. These include semantic leakage, semantic manipulation, knowledge base vulnerabilities, model-related attacks, and threats to authenticity and availability. Most existing secure SemCom studies focus on analog SemCom, where semantic features are mapped to continuous channel inputs. In contrast, digital SemCom transmits semantic information through discrete bits or symbols within practical transceiver pipelines, offering stronger compatibility with realworld systems while exposing a distinct and underexplored attack surface. In particular, digital SemCom typically represents semantic information over a finite alphabet through explicit digital modulation, following two main routes: probabilistic modulation and deterministic modulation. These discrete mechanisms and practical transmission procedures introduce additional vulnerabilities affecting bit- or symbol-level semantic information, the modulation stage, and packet-based delivery and protocol operations. Motivated by these challenges and the lack of a systematic analysis of secure digital SemCom, this paper reviews SemCom fundamentals, clarifies the architectural differences between analog and digital SemCom and their security implications, organizes the threat landscape for digital SemCom, and discusses potential defenses. Finally, we outline open research directions toward secure and deployable digital SemCom systems.", "published": "2025-12-31T03:44:37Z", "updated": "2026-01-07T07:03:54Z", "authors": ["Weixuan Chen", "Qianqian Yang", "Yuanyuan Jia", "Junyu Pan", "Shuo Shao", "Jincheng Dai", "Meixia Tao", "Ping Zhang"], "pdf_url": "https://arxiv.org/pdf/2512.24602v2"}
{"id": "http://arxiv.org/abs/2601.03640v1", "title": "Verbatim Data Transcription Failures in LLM Code Generation: A State-Tracking Stress Test", "summary": "Many real-world software tasks require exact transcription of provided data into code, such as cryptographic constants, protocol test vectors, allowlists, and calibration tables. These tasks are operationally sensitive because small omissions or alterations can remain silent while producing syntactically valid programs. This paper introduces a deliberately minimal transcription-to-code benchmark to isolate this reliability concern in LLM-based code generation. Given a list of high-precision decimal constants, a model must generate Python code that embeds the constants verbatim and performs a simple aggregate computation. We describe the prompting variants, evaluation protocol based on exact-string inclusion, and analysis framework used to characterize state-tracking and long-horizon generation failures. The benchmark is intended as a compact stress test that complements existing code-generation evaluations by focusing on data integrity rather than algorithmic reasoning.", "published": "2026-01-07T06:38:34Z", "updated": "2026-01-07T06:38:34Z", "authors": ["Mohd Ariful Haque", "Kishor Datta Gupta", "Mohammad Ashiqur Rahman", "Roy George"], "pdf_url": "https://arxiv.org/pdf/2601.03640v1"}
{"id": "http://arxiv.org/abs/2601.03594v1", "title": "Jailbreaking LLMs & VLMs: Mechanisms, Evaluation, and Unified Defense", "summary": "This paper provides a systematic survey of jailbreak attacks and defenses on Large Language Models (LLMs) and Vision-Language Models (VLMs), emphasizing that jailbreak vulnerabilities stem from structural factors such as incomplete training data, linguistic ambiguity, and generative uncertainty. It further differentiates between hallucinations and jailbreaks in terms of intent and triggering mechanisms. We propose a three-dimensional survey framework: (1) Attack dimension-including template/encoding-based, in-context learning manipulation, reinforcement/adversarial learning, LLM-assisted and fine-tuned attacks, as well as prompt- and image-level perturbations and agent-based transfer in VLMs; (2) Defense dimension-encompassing prompt-level obfuscation, output evaluation, and model-level alignment or fine-tuning; and (3) Evaluation dimension-covering metrics such as Attack Success Rate (ASR), toxicity score, query/time cost, and multimodal Clean Accuracy and Attribute Success Rate. Compared with prior works, this survey spans the full spectrum from text-only to multimodal settings, consolidating shared mechanisms and proposing unified defense principles: variant-consistency and gradient-sensitivity detection at the perception layer, safety-aware decoding and output review at the generation layer, and adversarially augmented preference alignment at the parameter layer. Additionally, we summarize existing multimodal safety benchmarks and discuss future directions, including automated red teaming, cross-modal collaborative defense, and standardized evaluation.", "published": "2026-01-07T05:25:33Z", "updated": "2026-01-07T05:25:33Z", "authors": ["Zejian Chen", "Chaozhuo Li", "Chao Li", "Xi Zhang", "Litian Zhang", "Yiming He"], "pdf_url": "https://arxiv.org/pdf/2601.03594v1"}
{"id": "http://arxiv.org/abs/2601.03587v1", "title": "Deontic Knowledge Graphs for Privacy Compliance in Multimodal Disaster Data Sharing", "summary": "Disaster response requires sharing heterogeneous artifacts, from tabular assistance records to UAS imagery, under overlapping privacy mandates. Operational systems often reduce compliance to binary access control, which is brittle in time-critical workflows. We present a novel deontic knowledge graph-based framework that integrates a Disaster Management Knowledge Graph (DKG) with a Policy Knowledge Graph (PKG) derived from IoT-Reg and FEMA/DHS privacy drivers. Our release decision function supports three outcomes: Allow, Block, and Allow-with-Transform. The latter binds obligations to transforms and verifies post-transform compliance via provenance-linked derived artifacts; blocked requests are logged as semantic privacy incidents. Evaluation on a 5.1M-triple DKG with 316K images shows exact-match decision correctness, sub-second per-decision latency, and interactive query performance across both single-graph and federated workloads.", "published": "2026-01-07T05:02:12Z", "updated": "2026-01-07T05:02:12Z", "authors": ["Kelvin Uzoma Echenim", "Karuna Pande Joshi"], "pdf_url": "https://arxiv.org/pdf/2601.03587v1"}
{"id": "http://arxiv.org/abs/2601.03508v1", "title": "A Critical Analysis of the Medibank Health Data Breach and Differential Privacy Solutions", "summary": "This paper critically examines the 2022 Medibank health insurance data breach, which exposed sensitive medical records of 9.7 million individuals due to unencrypted storage, centralized access, and the absence of privacy-preserving analytics. To address these vulnerabilities, we propose an entropy-aware differential privacy (DP) framework that integrates Laplace and Gaussian mechanisms with adaptive budget allocation. The design incorporates TLS-encrypted database access, field-level mechanism selection, and smooth sensitivity models to mitigate re-identification risks. Experimental validation was conducted using synthetic Medibank datasets (N = 131,000) with entropy-calibrated DP mechanisms, where high-entropy attributes received stronger noise injection. Results demonstrate a 90.3% reduction in re-identification probability while maintaining analytical utility loss below 24%. The framework further aligns with GDPR Article 32 and Australian Privacy Principle 11.1, ensuring regulatory compliance. By combining rigorous privacy guarantees with practical usability, this work contributes a scalable and technically feasible solution for healthcare data protection, offering a pathway toward resilient, trustworthy, and regulation-ready medical analytics.", "published": "2026-01-07T01:42:36Z", "updated": "2026-01-07T01:42:36Z", "authors": ["Zhuohan Cui", "Qianqian Lang", "Zikun Song"], "pdf_url": "https://arxiv.org/pdf/2601.03508v1"}
{"id": "http://arxiv.org/abs/2601.03504v1", "title": "Full-Stack Knowledge Graph and LLM Framework for Post-Quantum Cyber Readiness", "summary": "The emergence of large-scale quantum computing threatens widely deployed public-key cryptographic systems, creating an urgent need for enterprise-level methods to assess post-quantum (PQ) readiness. While PQ standards are under development, organizations lack scalable and quantitative frameworks for measuring cryptographic exposure and prioritizing migration across complex infrastructures. This paper presents a knowledge graph based framework that models enterprise cryptographic assets, dependencies, and vulnerabilities to compute a unified PQ readiness score. Infrastructure components, cryptographic primitives, certificates, and services are represented as a heterogeneous graph, enabling explicit modeling of dependency-driven risk propagation. PQ exposure is quantified using graph-theoretic risk functionals and attributed across cryptographic domains via Shapley value decomposition. To support scalability and data quality, the framework integrates large language models with human-in-the-loop validation for asset classification and risk attribution. The resulting approach produces explainable, normalized readiness metrics that support continuous monitoring, comparative analysis, and remediation prioritization.", "published": "2026-01-07T01:31:15Z", "updated": "2026-01-07T01:31:15Z", "authors": ["Rasmus Erlemann", "Charles Colyer Morris", "Sanjyot Sathe"], "pdf_url": "https://arxiv.org/pdf/2601.03504v1"}
