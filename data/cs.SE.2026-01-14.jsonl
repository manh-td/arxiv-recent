{"id": "http://arxiv.org/abs/2601.09703v1", "title": "ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation", "summary": "Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.", "published": "2026-01-14T18:57:31Z", "updated": "2026-01-14T18:57:31Z", "authors": ["Sicong Liu", "Yanxian Huang", "Mingwei Liu", "Jiachi Chen", "Ensheng Shi", "Yuchi Ma", "Hongyu Zhang", "Yin Zhang", "Yanlin Wang"], "pdf_url": "https://arxiv.org/pdf/2601.09703v1"}
{"id": "http://arxiv.org/abs/2601.09695v1", "title": "How well LLM-based test generation techniques perform with newer LLM versions?", "summary": "The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.", "published": "2026-01-14T18:46:32Z", "updated": "2026-01-14T18:46:32Z", "authors": ["Michael Konstantinou", "Renzo Degiovanni", "Mike Papadakis"], "pdf_url": "https://arxiv.org/pdf/2601.09695v1"}
{"id": "http://arxiv.org/abs/2412.01719v3", "title": "Smart Contract Vulnerabilities, Tools, and Benchmarks: an Updated Systematic Literature Review", "summary": "Smart contracts are self-executing programs on blockchain platforms like Ethereum, which have revolutionized decentralized finance by enabling trustless transactions and the operation of decentralized applications. Despite their potential, the security of smart contracts remains a critical concern due to their immutability and transparency, which expose them to malicious actors. Numerous solutions for vulnerability detection have been proposed, but it is still unclear which one is the most effective. This paper presents a systematic literature review that explores vulnerabilities in Ethereum smart contracts, focusing on automated detection tools and benchmark evaluation. We reviewed 3,380 studies from five digital libraries and five major software engineering conferences, applying a structured selection process that resulted in 222 high-quality studies. The key results include a hierarchical taxonomy of 192 vulnerabilities grouped into 13 categories, a comprehensive list of 219 detection tools with corresponding functionalities, methods, and code transformation techniques, a mapping between our taxonomy and the list of tools, and a collection of 133 benchmarks used for tool evaluation. We conclude with a discussion about the insights into the current state of Ethereum smart contract security and directions for future research.", "published": "2024-12-02T17:08:48Z", "updated": "2026-01-14T17:37:59Z", "authors": ["Gerardo Iuliano", "Dario Di Nucci"], "pdf_url": "https://arxiv.org/pdf/2412.01719v3"}
{"id": "http://arxiv.org/abs/2601.09616v1", "title": "SysPro: Reproducing System-level Concurrency Bugs from Bug Reports", "summary": "Reproducing system-level concurrency bugs requires both input data and the precise interleaving order of system calls. This process is challenging because such bugs are non-deterministic, and bug reports often lack the detailed information needed. Additionally, the unstructured nature of reports written in natural language makes it difficult to extract necessary details. Existing tools are inadequate to reproduce these bugs due to their inability to manage the specific interleaving at the system call level. To address these challenges, we propose SysPro, a novel approach that automatically extracts relevant system call names from bug reports and identifies their locations in the source code. It generates input data by utilizing information retrieval, regular expression matching, and the category-partition method. This extracted input and interleaving data are then used to reproduce bugs through dynamic source code instrumentation. Our empirical study on real-world benchmarks demonstrates that SysPro is both effective and efficient at localizing and reproducing system-level concurrency bugs from bug reports.", "published": "2026-01-14T16:40:08Z", "updated": "2026-01-14T16:40:08Z", "authors": ["Tarannum Shaila Zaman", "Zhihui Yan", "Chen Wang", "Chadni Islam", "Jiangfan Shi", "Tingting Yu"], "pdf_url": "https://arxiv.org/pdf/2601.09616v1"}
{"id": "http://arxiv.org/abs/2407.16827v2", "title": "Path-optimal symbolic execution of heap-manipulating programs", "summary": "Symbolic execution is at the core of many techniques for program analysis and test generation. Traditional symbolic execution of programs with numeric inputs enjoys the property of forking as many analysis traces as the number of analyzed program paths, a property that in this paper we refer to as path optimality. On the contrary, current approaches for symbolic execution of heap-manipulating programs fail to satisfy this property, thereby incurring crucial path explosion effects. This paper introduces POSE, path-optimal symbolic execution, a symbolic execution algorithm that originally achieves path optimality against heap-manipulating programs. We formalize the POSE algorithm and experiment it against a benchmark of programs that take data structures as inputs, supporting the potential of POSE for improving on the state of the art of symbolic execution of heap-manipulating programs.", "published": "2024-07-23T20:35:33Z", "updated": "2026-01-14T16:36:39Z", "authors": ["Pietro Braione", "Giovanni Denaro", "Luca Guglielmo"], "pdf_url": "https://arxiv.org/pdf/2407.16827v2"}
{"id": "http://arxiv.org/abs/2601.09612v1", "title": "Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories", "summary": "Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $δ$ = 0.94) and code snippets (medium effect, $δ$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.", "published": "2026-01-14T16:34:00Z", "updated": "2026-01-14T16:34:00Z", "authors": ["Khairul Alam", "Banani Roy"], "pdf_url": "https://arxiv.org/pdf/2601.09612v1"}
{"id": "http://arxiv.org/abs/2506.06821v4", "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, capable of tackling complex tasks during inference. However, the extent to which LLMs can be utilized for code checking or debugging through test case generation remains largely unexplored. We investigate this problem from the perspective of competition-level programming (CP) programs and propose TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This benchmark comprises two tasks, aimed at studying the capabilities of LLMs in (1) generating valid test case generators for a given CP problem, and further (2) generating targeted test case generators that expose bugs in human-written code. Experimental results indicate that while state-of-the-art LLMs can generate valid test case generators in most cases, most LLMs struggle to generate targeted test cases that reveal flaws in human code effectively. Especially, even advanced reasoning models (e.g., o3-mini) fall significantly short of human performance in the task of generating targeted generators. Furthermore, we construct a high-quality, manually curated dataset of instructions for generating targeted generators. Analysis demonstrates that the performance of LLMs can be enhanced with the aid of this dataset, by both prompting and fine-tuning.", "published": "2025-06-07T14:53:03Z", "updated": "2026-01-14T15:14:27Z", "authors": ["Yuhan Cao", "Zian Chen", "Kun Quan", "Ziliang Zhang", "Yu Wang", "Xiaoning Dong", "Yeqi Feng", "Guanzhong He", "Jingcheng Huang", "Jianhao Li", "Yixuan Tan", "Jiafu Tang", "Yilin Tang", "Junlei Wu", "Qianyu Xiao", "Can Zheng", "Shouchen Zhou", "Yuxiang Zhu", "Yiming Huang", "Tianxing He"], "pdf_url": "https://arxiv.org/pdf/2506.06821v4"}
{"id": "http://arxiv.org/abs/2503.16771v2", "title": "Enabling Global, Human-Centered Explanations for LLMs:From Tokens to Interpretable Code and Test Generation", "summary": "As Large Language Models for Code (LM4Code) become integral to software engineering, establishing trust in their output becomes critical. However, standard accuracy metrics obscure the underlying reasoning of generative models, offering little insight into how decisions are made. Although post-hoc interpretability methods attempt to fill this gap, they often restrict explanations to local, token-level insights, which fail to provide a developer-understandable global analysis. Our work highlights the urgent need for \\textbf{global, code-based} explanations that reveal how models reason across code. To support this vision, we introduce \\textit{code rationales} (CodeQ), a framework that enables global interpretability by mapping token-level rationales to high-level programming categories. Aggregating thousands of these token-level explanations allows us to perform statistical analyses that expose systemic reasoning behaviors. We validate this aggregation by showing it distills a clear signal from noisy token data, reducing explanation uncertainty (Shannon entropy) by over 50%. Additionally, we find that a code generation model (\\textit{codeparrot-small}) consistently favors shallow syntactic cues (e.g., \\textbf{indentation}) over deeper semantic logic. Furthermore, in a user study with 37 participants, we find its reasoning is significantly misaligned with that of human developers. These findings, hidden from traditional metrics, demonstrate the importance of global interpretability techniques to foster trust in LM4Code.", "published": "2025-03-21T01:00:45Z", "updated": "2026-01-14T15:10:06Z", "authors": ["Dipin Khati", "Daniel Rodriguez-Cardenas", "David N. Palacio", "Alejandro Velasco", "Denys Poshyvanyk"], "pdf_url": "https://arxiv.org/pdf/2503.16771v2"}
{"id": "http://arxiv.org/abs/2601.09456v1", "title": "Towards a Metadata Schema for Energy Research Software", "summary": "Domain-specific metadata schemas are essential to improve the findability and reusability of research software and to follow the FAIR4RS principles. However, many domains, including energy research, lack established metadata schemas. To address this gap, we developed a metadata schema for energy research software based on a requirement analysis and evaluated it through user testing. Our results show that the schema balances the need for formalization and interoperability, while also meeting the specific needs of energy researchers. Meanwhile, the testing showed that a good presentation of the required information is key to enable researchers to create the required metadata. This paper provides insights into the challenges and opportunities of designing a metadata schema for energy research software.", "published": "2026-01-14T13:03:18Z", "updated": "2026-01-14T13:03:18Z", "authors": ["Stephan Ferenz", "Oliver Werth", "Astrid Nieße"], "pdf_url": "https://arxiv.org/pdf/2601.09456v1"}
{"id": "http://arxiv.org/abs/2601.09440v1", "title": "DepRadar: Agentic Coordination for Context Aware Defect Impact Analysis in Deep Learning Libraries", "summary": "Deep learning libraries like Transformers and Megatron are now widely adopted in modern AI programs. However, when these libraries introduce defects, ranging from silent computation errors to subtle performance regressions, it is often challenging for downstream users to assess whether their own programs are affected. Such impact analysis requires not only understanding the defect semantics but also checking whether the client code satisfies complex triggering conditions involving configuration flags, runtime environments, and indirect API usage. We present DepRadar, an agent coordination framework for fine grained defect and impact analysis in DL library updates. DepRadar coordinates four specialized agents across three steps: 1. the PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests, 2. the Orchestrator Agent synthesizes these signals into a unified defect pattern with trigger conditions, and 3. the Impact Analyzer checks downstream programs to determine whether the defect can be triggered. To improve accuracy and explainability, DepRadar integrates static analysis with DL-specific domain rules for defect reasoning and client side tracing. We evaluate DepRadar on 157 PRs and 70 commits across two representative DL libraries. It achieves 90% precision in defect identification and generates high quality structured fields (average field score 1.6). On 122 client programs, DepRadar identifies affected cases with 90% recall and 80% precision, substantially outperforming other baselines.", "published": "2026-01-14T12:41:39Z", "updated": "2026-01-14T12:41:39Z", "authors": ["Yi Gao", "Xing Hu", "Tongtong Xu", "Jiali Zhao", "Xiaohu Yang", "Xin Xia"], "pdf_url": "https://arxiv.org/pdf/2601.09440v1"}
{"id": "http://arxiv.org/abs/2601.09393v1", "title": "AI-NativeBench: An Open-Source White-Box Agentic Benchmark Suite for AI-Native Systems", "summary": "The transition from Cloud-Native to AI-Native architectures is fundamentally reshaping software engineering, replacing deterministic microservices with probabilistic agentic services. However, this shift renders traditional black-box evaluation paradigms insufficient: existing benchmarks measure raw model capabilities while remaining blind to system-level execution dynamics. To bridge this gap, we introduce AI-NativeBench, the first application-centric and white-box AI-Native benchmark suite grounded in Model Context Protocol (MCP) and Agent-to-Agent (A2A) standards. By treating agentic spans as first-class citizens within distributed traces, our methodology enables granular analysis of engineering characteristics beyond simple capabilities. Leveraging this benchmark across 21 system variants, we uncover critical engineering realities invisible to traditional metrics: a parameter paradox where lightweight models often surpass flagships in protocol adherence, a pervasive inference dominance that renders protocol overhead secondary, and an expensive failure pattern where self-healing mechanisms paradoxically act as cost multipliers on unviable workflows. This work provides the first systematic evidence to guide the transition from measuring model capability to engineering reliable AI-Native systems. To facilitate reproducibility and further research, we have open-sourced the benchmark and dataset.", "published": "2026-01-14T11:32:07Z", "updated": "2026-01-14T11:32:07Z", "authors": ["Zirui Wang", "Guangba Yu", "Michael R. Lyu"], "pdf_url": "https://arxiv.org/pdf/2601.09393v1"}
{"id": "http://arxiv.org/abs/2601.09372v1", "title": "Formally Verifying Noir Zero Knowledge Programs with NAVe", "summary": "Zero-Knowledge (ZK) proof systems are cryptographic protocols that can (with overwhelming probability) demonstrate that the pair $(X, W)$ is in a relation $R$ without revealing information about the private input $W$. This membership checking is captured by a complex arithmetic circuit: a set of polynomial equations over a finite field. ZK programming languages, like Noir, have been proposed to simplify the description of these circuits. A developer can write a Noir program using traditional high-level constructs that can be compiled into a lower-level ACIR (Abstract Circuit Intermediate Representation), which is essentially a high-level description of an arithmetic circuit. In this paper, we formalise some of the ACIR language using SMT-LIB and its extended theory of finite fields. We use this formalisation to create an open-source formal verifier for the Noir language using the SMT solver cvc5. Our verifier can be used to check whether Noir programs behave appropriately. For instance, it can be used to check whether a Noir program has been properly constrained, that is, the finite-field polynomial equations generated truly capture the intended relation. We evaluate our verifier over 4 distinct sets of Noir programs, demonstrating its practical applicability and identifying a hard-to-check constraint type that charts an improvement path for our verification framework.", "published": "2026-01-14T10:56:17Z", "updated": "2026-01-14T10:56:17Z", "authors": ["Pedro Antonino", "Namrata Jain"], "pdf_url": "https://arxiv.org/pdf/2601.09372v1"}
{"id": "http://arxiv.org/abs/2507.20977v2", "title": "Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs", "summary": "Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of program repair. Recent studies show that large language models (LLMs) outperform traditional techniques, extending their success beyond code generation and fault detection.\n  Hypothesis: These gains may be driven by hidden factors -- \"invisible hands\" such as training-data leakage or perfect fault localization -- that let an LLM reproduce human-authored fixes for the same code.\n  Objective: We replicate prior AVR studies under controlled conditions by deliberately adding errors to the reported vulnerability location in the prompt. If LLMs merely regurgitate memorized fixes, both small and large localization errors should yield the same number of correct patches, because any offset should divert the model from the original fix.\n  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans benchmarks after shifting the fault location by n lines from the ground truth. A first LLM generates a patch, a second LLM reviews it, and we validate the result with regression and proof-of-vulnerability tests. Finally, we manually audit a sample of patches and estimate the error rate with the Agresti-Coull-Wilson method.", "published": "2025-07-28T16:39:16Z", "updated": "2026-01-14T09:49:09Z", "authors": ["Maria Camporese", "Fabio Massacci"], "pdf_url": "https://arxiv.org/pdf/2507.20977v2"}
{"id": "http://arxiv.org/abs/2601.09282v1", "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing", "summary": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.", "published": "2026-01-14T08:36:21Z", "updated": "2026-01-14T08:36:21Z", "authors": ["Leszek Sliwko", "Jolanta Mizeria-Pietraszko"], "pdf_url": "https://arxiv.org/pdf/2601.09282v1"}
{"id": "http://arxiv.org/abs/2601.08706v2", "title": "\"Where is My Troubleshooting Procedure?\": Studying the Potential of RAG in Assisting Failure Resolution of Large Cyber-Physical System", "summary": "In today's complex industrial environments, operators must often navigate through extensive technical manuals to identify troubleshooting procedures that may help react to some observed failure symptoms. These manuals, written in natural language, describe many steps in detail. Unfortunately, the number, magnitude, and articulation of these descriptions can significantly slow down and complicate the retrieval of the correct procedure during critical incidents. Interestingly, Retrieval Augmented Generation (RAG) enables the development of tools based on conversational interfaces that can assist operators in their retrieval tasks, improving their capability to respond to incidents. This paper presents the results of a set of experiments that derive from the analysis of the troubleshooting procedures available in Fincantieri, a large international company developing complex naval cyber-physical systems. Results show that RAG can assist operators in reacting promptly to failure symptoms, although specific measures have to be taken into consideration to cross-validate recommendations before actuating them.", "published": "2026-01-13T16:34:43Z", "updated": "2026-01-14T07:28:56Z", "authors": ["Maria Teresa Rossi", "Leonardo Mariani", "Oliviero Riganelli", "Giuseppe Filomento", "Danilo Giannone", "Paolo Gavazzo"], "pdf_url": "https://arxiv.org/pdf/2601.08706v2"}
{"id": "http://arxiv.org/abs/2601.09171v1", "title": "SafePlanner: Testing Safety of the Automated Driving System Plan Model", "summary": "In this work, we present SafePlanner, a systematic testing framework for identifying safety-critical flaws in the Plan model of Automated Driving Systems (ADS). SafePlanner targets two core challenges: generating structurally meaningful test scenarios and detecting hazardous planning behaviors. To maximize coverage, SafePlanner performs a structural analysis of the Plan model implementation - specifically, its scene-transition logic and hierarchical control flow - and uses this insight to extract feasible scene transitions from code. It then composes test scenarios by combining these transitions with non-player vehicle (NPC) behaviors. Guided fuzzing is applied to explore the behavioral space of the Plan model under these scenarios. We evaluate SafePlanner on Baidu Apollo, a production-grade level 4 ADS. It generates 20635 test cases and detects 520 hazardous behaviors, grouped into 15 root causes through manual analysis. For four of these, we applied patches based on our analysis; the issues disappeared, and no apparent side effects were observed. SafePlanner achieves 83.63 percent function and 63.22 percent decision coverage on the Plan model, outperforming baselines in both bug discovery and efficiency.", "published": "2026-01-14T05:14:00Z", "updated": "2026-01-14T05:14:00Z", "authors": ["Dohyun Kim", "Sanggu Han", "Sangmin Woo", "Joonha Jang", "Jaehoon Kim", "Changhun Song", "Yongdae Kim"], "pdf_url": "https://arxiv.org/pdf/2601.09171v1"}
{"id": "http://arxiv.org/abs/2510.24428v5", "title": "CodeWiki: Evaluating AI's Ability to Generate Holistic Documentation for Large-Scale Codebases", "summary": "Given a large and evolving codebase, the ability to automatically generate holistic, architecture-aware documentation that captures not only individual functions but also cross-file, cross-module, and system-level interactions remains an open challenge. Comprehensive documentation is essential for long-term software maintenance and collaboration, yet current automated approaches still fail to model the rich semantic dependencies and architectural structures that define real-world software systems. We present \\textbf{CodeWiki}, a unified framework for automated repository-level documentation across seven programming languages. CodeWiki introduces three key innovations: (i) hierarchical decomposition that preserves architectural context across multiple levels of granularity, (ii) recursive multi-agent processing with dynamic task delegation for scalable generation, and (iii) multi-modal synthesis that integrates textual descriptions with visual artifacts such as architecture diagrams and data-flow representations. To enable rigorous evaluation, we introduce \\textbf{CodeWikiBench}, a comprehensive benchmark featuring multi-dimensional rubrics and LLM-based assessment protocols. Experimental results show that CodeWiki achieves a 68.79\\% quality score with proprietary models, outperforming the closed-source DeepWiki baseline (64.06\\%) by 4.73\\%, with particularly strong improvements on high-level scripting languages (+10.47\\%). We open-source CodeWiki to foster future research and community adoption.", "published": "2025-10-28T13:52:46Z", "updated": "2026-01-14T05:12:23Z", "authors": ["Anh Nguyen Hoang", "Minh Le-Anh", "Bach Le", "Nghi D. Q. Bui"], "pdf_url": "https://arxiv.org/pdf/2510.24428v5"}
