{"id": "http://arxiv.org/abs/2512.13655v1", "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation", "summary": "Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.", "published": "2025-12-15T18:48:42Z", "updated": "2025-12-15T18:48:42Z", "authors": ["Richard J. Young"], "pdf_url": "https://arxiv.org/pdf/2512.13655v1"}
{"id": "http://arxiv.org/abs/2501.08947v4", "title": "Taint Analysis for Graph APIs Focusing on Broken Access Control", "summary": "We present the first systematic approach to static and dynamic taint analysis for Graph APIs focusing on broken access control. The approach comprises the following. We taint nodes of the Graph API if they represent data requiring specific privileges in order to be retrieved or manipulated, and identify API calls which are related to sources and sinks. Then, we statically analyze whether a tainted information flow between API source and sink calls occurs. To this end, we model the API calls using graph transformation rules. We subsequently use Critical Pair Analysis to automatically analyze potential dependencies between rules representing source calls and rules representing sink calls. We distinguish direct from indirect tainted information flow and argue under which conditions the Critical Pair Analysis is able to detect not only direct, but also indirect tainted flow. The static taint analysis (i) identifies flows that need to be further reviewed, since tainted nodes may be created by an API call and used or manipulated by another API call later without having the necessary privileges, and (ii) can be used to systematically design dynamic security tests for broken access control. The dynamic taint analysis checks if potential broken access control risks detected during the static taint analysis really occur. We apply the approach to a part of the GitHub GraphQL API. The application illustrates that our analysis supports the detection of two types of broken access control systematically: the case where users of the API may not be able to access or manipulate information, although they should be able to do so; and the case where users (or attackers) of the API may be able to access/manipulate information that they should not.", "published": "2025-01-15T16:49:32Z", "updated": "2025-12-15T16:54:27Z", "authors": ["Leen Lambers", "Lucas Sakizloglou", "Taisiya Khakharova", "Fernando Orejas"], "pdf_url": "https://arxiv.org/pdf/2501.08947v4"}
{"id": "http://arxiv.org/abs/2512.13524v1", "title": "How Low Can You Go? The Data-Light SE Challenge", "summary": "Much of software engineering (SE) research assumes that progress depends on massive datasets and CPU-intensive optimizers. Yet has this assumption been rigorously tested?\n  The counter-evidence presented in this paper suggests otherwise: across dozens of optimization problems from recent SE literature, including software configuration and performance tuning, cloud and systems optimization, project and process-level decision modeling, behavioral analytics, financial risk modeling, project health prediction, reinforcement learning tasks, sales forecasting, and software testing, even with just a few dozen labels, very simple methods (e.g. diversity sampling, a minimal Bayesian learner, or random probes) achieve near 90% of the best reported results. Further, these simple methods perform just as well as more state-of-the-the-art optimizers like SMAC, TPE, DEHB etc. While some tasks would require better outcomes and more sampling, these results seen after a few dozen samples would suffice for many engineering needs (particularly when the goal is rapid and cost-efficient guidance rather than slow and exhaustive optimization).\n  Our results highlight that some SE tasks may be better served by lightweight approaches that demand fewer labels and far less computation. We hence propose the data-light challenge: when will a handful of labels suffice for SE tasks? To enable a large-scale investigation of this issue, we contribute (1) a mathematical formalization of labeling, (2) lightweight baseline algorithms, and (3) results on public-domain data showing the conditions under which lightweight methods excel or fail.\n  For the purposes of open science, our scripts and data are online at https://github.com/KKGanguly/NEO .", "published": "2025-12-15T16:49:50Z", "updated": "2025-12-15T16:49:50Z", "authors": ["Kishan Kumar Ganguly", "Tim Menzies"], "pdf_url": "https://arxiv.org/pdf/2512.13524v1"}
{"id": "http://arxiv.org/abs/2512.13515v1", "title": "Fine-tuned LLM-based Code Migration Framework", "summary": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.", "published": "2025-12-15T16:42:51Z", "updated": "2025-12-15T16:42:51Z", "authors": ["Oleg Grynets", "Vasyl Lyashkevych", "Dmytro Baran", "Maksym Orliansky", "Taras Zelenyy", "Markiian Leshchyshyn"], "pdf_url": "https://arxiv.org/pdf/2512.13515v1"}
{"id": "http://arxiv.org/abs/2512.13474v1", "title": "Mapping of the system of software-related emissions and shared responsibilities", "summary": "The global climate is experiencing a rapid and unprecedented warming trend. The ICT sector is a notable contributor to global greenhouse gas emissions, with its environmental impact continuing to expand. Addressing this issue is vital for achieving the objectives of the Paris Agreement, particularly the goal of limiting global temperature rise to 1.5Â°C. At the European Union level, regulatory measures such as the CSRD and the CSDD impose obligations on companies, including those within the ICT sector, to recognize and mitigate their environmental footprint. This study provides a comprehensive system mapping aimed at enhancing the awareness and understanding of software-related emissions and the corresponding responsibilities borne by the ICT sector. The mapping identifies the primary sources of carbon emissions and energy consumption within the ICT domain while also outlining the key responsibilities of the stakeholders accountable throughout the software lifecycle.", "published": "2025-12-15T16:12:19Z", "updated": "2025-12-15T16:12:19Z", "authors": ["Laura Partanen", "Antti Sipila", "Md Sanaul Haque", "Jari Porras"], "pdf_url": "https://arxiv.org/pdf/2512.13474v1"}
{"id": "http://arxiv.org/abs/2512.13444v1", "title": "A Data Annotation Requirements Representation and Specification (DARS)", "summary": "With the rise of AI-enabled cyber-physical systems, data annotation has become a critical yet often overlooked process in the development of these intelligent information systems. Existing work in requirements engineering (RE) has explored how requirements for AI systems and their data can be represented. However, related interviews with industry professionals show that data annotations and their related requirements introduce distinct challenges, indicating a need for annotation-specific requirement representations. We propose the Data Annotation Requirements Representation and Specification (DARS), including an Annotation Negotiation Card to align stakeholders on objectives and constraints, and a Scenario-Based Annotation Specification to express atomic and verifiable data annotation requirements. We evaluate DARS with an automotive perception case related to an ongoing project, and a mapping against 18 real-world data annotation error types. The results suggest that DARS mitigates root causes of completeness, accuracy, and consistency annotation errors. By integrating DARS into RE, this work improves the reliability of safety-critical systems using data annotations and demonstrates how engineering frameworks must evolve for data-dependent components of today's intelligent information systems.", "published": "2025-12-15T15:41:30Z", "updated": "2025-12-15T15:41:30Z", "authors": ["Yi Peng", "Hina Saeeda", "Hans-Martin Heyn", "Jennifer Horkoff", "Eric Knauss", "Fredrick Warg"], "pdf_url": "https://arxiv.org/pdf/2512.13444v1"}
{"id": "http://arxiv.org/abs/2512.13438v1", "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents", "summary": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.", "published": "2025-12-15T15:34:06Z", "updated": "2025-12-15T15:34:06Z", "authors": ["Dezhi Ran", "Zhi Gong", "Yuzhe Guo", "Mengzhou Wu", "Yuan Cao", "Haochuan Lu", "Hengyu Zhang", "Xia Zeng", "Gang Cao", "Liangchao Yao", "Yuetang Deng", "Wei Yang", "Tao Xie"], "pdf_url": "https://arxiv.org/pdf/2512.13438v1"}
{"id": "http://arxiv.org/abs/2512.13422v1", "title": "QMon: Monitoring the Execution of Quantum Circuits with Mid-Circuit Measurement and Reset", "summary": "Unlike classical software, where logging and runtime tracing can effectively reveal internal execution status, quantum circuits possess unique properties, such as the no-cloning theorem and measurement-induced collapse, that prevent direct observation or duplication of their states. These characteristics make it especially challenging to monitor the execution of quantum circuits, complicating essential tasks such as debugging and runtime monitoring. This paper presents QMON, a practical methodology that leverages mid-circuit measurements and reset operations to monitor the internal states of quantum circuits while preserving their original runtime behavior. QMON enables the instrumentation of monitoring operators at developer-specified locations within the circuit, allowing comparisons between expected and observed quantum-state probabilities at those locations. We evaluated QMON by analyzing its impact on circuit behavior, monitoring coverage, and effectiveness in bug localization. Experimental results involving 154 quantum circuits show that all circuits preserve their intended functionality after instrumentation and that QMON successfully detects and localizes various programming errors. Although monitoring coverage is limited by the need to preserve delicate quantum properties, such as entanglement, QMON effectively detects errors while introducing no or negligible disturbance to the original quantum states. QMON facilitates the development of more robust and reliable quantum software as the field continues to mature.", "published": "2025-12-15T15:14:53Z", "updated": "2025-12-15T15:14:53Z", "authors": ["Ning Ma", "Jianjun Zhao", "Foutse Khomh", "Shaukat Ali", "Heng Li"], "pdf_url": "https://arxiv.org/pdf/2512.13422v1"}
{"id": "http://arxiv.org/abs/2512.13414v1", "title": "PSALM: applying Proportional SAmpLing strategy in Metamorphic testing", "summary": "Metamorphic testing (MT) alleviates the oracle problem by checking metamorphic relations (MRs) across multiple test executions. The fault detection effectiveness of MT is influenced not only by the choice and quality of MRs, but also by how source test cases and metamorphic groups (MGs) are selected. While substantial research has focused on designing, generating, and validating MRs, systematic methods for source test case selection and MG selection remain largely unexplored. Although the Proportional Sampling Strategy (PSS) provides strong theoretical guarantees in traditional testing, its assumptions cannot be directly applied in MT due to differences in selection domains, test units, and failure distributions. This paper proposes PSALM, an adaptation of PSS to MT for both source test case selection and MG selection. We formally prove that PSALM is never inferior to random selection regardless of how the source test case and MG domains are partitioned. We further identify the conditions under which applying PSALM to source test case selection and MG selection yields identical effectiveness. A comprehensive empirical study on eight subject programs and 184 mutants shows that the results are consistent with our theoretical analysis and that PSALM generally performs more effectively than existing selection strategies such as ART and MT-ART. These results demonstrate that PSALM provides a theoretically grounded and practically effective selection strategy for MT.", "published": "2025-12-15T15:04:15Z", "updated": "2025-12-15T15:04:15Z", "authors": ["Zenghui Zhou", "Pak-Lok Poon", "Zheng Zheng", "Xiao-Yi Zhang"], "pdf_url": "https://arxiv.org/pdf/2512.13414v1"}
{"id": "http://arxiv.org/abs/2512.13360v1", "title": "UCRBench: Benchmarking LLMs on Use Case Recovery", "summary": "Use cases are widely employed to specify functional requirements, yet existing benchmarks are scarce and face the risk of being misaligned with actual system behavior, similarly limiting the rigorous evaluation of large language models (LLMs) in generating use cases from source code. We address this gap by introducing code-aligned use case benchmarks, constructed through manual validation of both user-goal and subfunction use cases across nine real-world software projects. Using this benchmark, we conduct the first systematic study of LLMs and propose a hierarchical evaluation protocol that assesses actor correctness, name accuracy, path fidelity, and behavioral coverage. The results show that while LLMs can partially reconstruct system functionality, their performance varies significantly across projects, with particularly noticeable shortcomings in domain-specific and multi-module systems. The models also exhibit high omission rates and struggle to maintain consistent abstraction when aggregating subfunctions into user-goal use cases, highlighting both the potential and current limitations of LLM-based use case reverse engineering.", "published": "2025-12-15T14:12:57Z", "updated": "2025-12-15T14:12:57Z", "authors": ["Shuyuan Xiao", "Yiran Zhang", "Weisong Sun", "Xiaohong Chen", "Yang Liu", "Zhi Jin"], "pdf_url": "https://arxiv.org/pdf/2512.13360v1"}
{"id": "http://arxiv.org/abs/2512.13239v1", "title": "A Decision Support Framework for Blockchain Pattern Selection Based on Soft Goals", "summary": "Blockchain technology is gaining momentum across many sectors. Whereas blockchain solutions have important positive effects on the business domain, they also introduce constraints and may cause delayed or unforeseen negative effects, undermining business strategies. The diversity of blockchain patterns and lack of standardized frameworks linking business goals to technical design decisions make pattern selection a complex task for system architects. To address this challenge, we propose Blockchain--Technology-Aware Enterprise Modeling (BC-TEAEM), a decision support framework that combines ontologies of blockchain patterns and domain-independent soft goals with a multi-criteria decision-making approach. The framework focuses on the interplay between a domain expert and a technical expert to ensure alignment and traceability. By iteratively capturing and refining preferences, BC-TEAEM supports systematic selection of blockchain patterns. We develop a prototype decision support tool implementing our method and validate it through a case study of a pharmaceutical company's supply chain traceability system, demonstrating the framework's applicability. %a supply chain traceability case study.", "published": "2025-12-15T11:54:00Z", "updated": "2025-12-15T11:54:00Z", "authors": ["Eddy Kiomba Kambilo", "Nicolas Herbaut", "Irina Rychkova", "Carine Souveyet"], "pdf_url": "https://arxiv.org/pdf/2512.13239v1"}
{"id": "http://arxiv.org/abs/2505.02629v2", "title": "Parameter-Efficient Fine-Tuning with Attributed Patch Semantic Graph for Automated Patch Correctness Assessment", "summary": "Automated program repair (APR) aims to automatically repair program errors without human intervention, and recent years have witnessed a growing interest on this research topic. While much progress has been made and techniques originating from different disciplines have been proposed, APR techniques generally suffer from the patch overfitting issue, i.e., the generated patches are not genuinely correct despite they pass the employed tests. To alleviate this issue, many research efforts have been devoted for automated patch correctness assessment (APCA). In particular, with the emergence of large language model (LLM) technology, researchers have employed LLM to assess the patch correctness and have obtained the state-of-the-art performance. The literature on APCA has demonstrated the importance of capturing patch semantic and explicitly considering certain code attributes in predicting patch correctness. However, existing LLM-based methods typically treat code as token sequences and ignore the inherent formal structure for code, making it difficult to capture the deep patch semantics. Moreover, these LLM-based methods also do not explicitly account for enough code attributes. To overcome these drawbacks, we in this paper design a novel patch graph representation named attributed patch semantic graph (APSG), which adequately captures the patch semantic and explicitly reflects important patch attributes. To effectively use graph information in APSG, we accordingly propose a new parameter-efficient fine-tuning (PEFT) method of LLMs named Graph-LoRA. Extensive evaluations have been conducted to evaluate our method, and the results show that compared to the state-of-the-art methods, our method improves the accuracy and F1 score by 3.1\\% to 7.5\\% and 3.0\\% to 7.1\\% respectively.", "published": "2025-05-05T13:15:53Z", "updated": "2025-12-15T08:02:22Z", "authors": ["Zhenyu Yang", "Jingwen Wu", "Zhen Yang", "Zhongxing Yu"], "pdf_url": "https://arxiv.org/pdf/2505.02629v2"}
{"id": "http://arxiv.org/abs/2512.13047v1", "title": "Sharpen the Spec, Cut the Code: A Case for Generative File System with SYSSPEC", "summary": "File systems are critical OS components that require constant evolution to support new hardware and emerging application needs. However, the traditional paradigm of developing features, fixing bugs, and maintaining the system incurs significant overhead, especially as systems grow in complexity. This paper proposes a new paradigm, generative file systems, which leverages Large Language Models (LLMs) to generate and evolve a file system from prompts, effectively addressing the need for robust evolution. Despite the widespread success of LLMs in code generation, attempts to create a functional file system have thus far been unsuccessful, mainly due to the ambiguity of natural language prompts.\n  This paper introduces SYSSPEC, a framework for developing generative file systems. Its key insight is to replace ambiguous natural language with principles adapted from formal methods. Instead of imprecise prompts, SYSSPEC employs a multi-part specification that accurately describes a file system's functionality, modularity, and concurrency. The specification acts as an unambiguous blueprint, guiding LLMs to generate expected code flexibly. To manage evolution, we develop a DAG-structured patch that operates on the specification itself, enabling new features to be added without violating existing invariants. Moreover, the SYSSPEC toolchain features a set of LLM-based agents with mechanisms to mitigate hallucination during construction and evolution. We demonstrate our approach by generating SPECFS, a concurrent file system. SPECFS passes hundreds of regression tests, matching a manually-coded baseline. We further confirm its evolvability by seamlessly integrating 10 real-world features from Ext4. Our work shows that a specification-guided approach makes generating and evolving complex systems not only feasible but also highly effective.", "published": "2025-12-15T07:15:01Z", "updated": "2025-12-15T07:15:01Z", "authors": ["Qingyuan Liu", "Zou Mo", "Hengbin Zhang", "Dong Du", "Yubin Xia", "Haibo Chen"], "pdf_url": "https://arxiv.org/pdf/2512.13047v1"}
{"id": "http://arxiv.org/abs/2208.05573v2", "title": "Data Augmentation for Improving Emotion Recognition in Software Engineering Communication", "summary": "Emotions (e.g., Joy, Anger) are prevalent in daily software engineering (SE) activities, and are known to be significant indicators of work productivity (e.g., bug fixing efficiency). Recent studies have shown that directly applying general purpose emotion classification tools to SE corpora is not effective. Even within the SE domain, tool performance degrades significantly when trained on one communication channel and evaluated on another (e.g, StackOverflow vs. GitHub comments). Retraining a tool with channel-specific data takes significant effort since manually annotating large datasets of ground truth data is expensive.\n  In this paper, we address this data scarcity problem by automatically creating new training data using a data augmentation technique. Based on an analysis of the types of errors made by popular SE-specific emotion recognition tools, we specifically target our data augmentation strategy in order to improve the performance of emotion recognition. Our results show an average improvement of 9.3% in micro F1-Score for three existing emotion classification tools (ESEM-E, EMTk, SEntiMoji) when trained with our best augmentation strategy.", "published": "2022-08-10T21:50:14Z", "updated": "2025-12-15T04:48:42Z", "authors": ["Mia Mohammad Imran", "Yashasvi Jain", "Preetha Chatterjee", "Kostadin Damevski"], "pdf_url": "https://arxiv.org/pdf/2208.05573v2"}
{"id": "http://arxiv.org/abs/2512.12965v1", "title": "Challenges and Enablers: Remote Work for People with Disabilities in Software Development Teams", "summary": "The increasing adoption of remote and hybrid work modalities in the technology sector has brought new opportunities and challenges for the inclusion of people with disabilities (PWD) in software development teams (SDT). This study investigates how remote work affects PWDs' experience in mixed-ability SDT, focusing on the unique challenges and strategies that emerge in remote environments. We conducted an online survey with \\totalSurveyResponses valid responses, encompassing PWD, their leaders, and teammates, to capture sociotechnical aspects of their experiences with remote collaboration. To deepen our understanding, we carried out 14 structured interviews with software developers who self-identified as having disabilities (six autistic individuals, six with physical disabilities, and two who are d/Deaf). Our analysis combines quantitative data with qualitative coding of open-ended survey responses and interview transcripts. The results reveal that, despite the barriers faced by team members with disabilities, their teammates and leaders have a limited perception of the daily challenges involved in sustaining collaborative remote work. These findings highlight opportunities for improvement in accessibility tools, communication strategies, and adaptive management approaches.", "published": "2025-12-15T04:05:36Z", "updated": "2025-12-15T04:05:36Z", "authors": ["Thayssa Rocha", "Luciano Teran", "Marcelle Mota", "Cleidson de Souza", "Kiev Gama", "Gustavo Pinto"], "pdf_url": "https://arxiv.org/pdf/2512.12965v1"}
{"id": "http://arxiv.org/abs/2503.20578v5", "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input Generation", "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing software bugs. Bug reports typically contain these inputs, which developers extract to facilitate debugging. Since bug reports are written in natural language, prior research has leveraged various Natural Language Processing (NLP) techniques for automated input extraction. With the advent of Large Language Models (LLMs), an important research question arises: how effectively can generative LLMs extract failure-inducing inputs from bug reports? In this paper, we propose LLPut, a technique to empirically evaluate the performance of three open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in extracting relevant inputs from bug reports. We conduct an experimental evaluation on a dataset of 206 bug reports to assess the accuracy and effectiveness of these models. Our findings provide insights into the capabilities and limitations of generative LLMs in automated bug diagnosis.", "published": "2025-03-26T14:25:01Z", "updated": "2025-12-15T02:19:11Z", "authors": ["Alif Al Hasan", "Subarna Saha", "Mia Mohammad Imran", "Tarannum Shaila Zaman"], "pdf_url": "https://arxiv.org/pdf/2503.20578v5"}
