{"id": "http://arxiv.org/abs/2602.13170v1", "title": "Source Code Hotspots: A Diagnostic Method for Quality Issues", "summary": "Software source code often harbours \"hotspots\": small portions of the code that change far more often than the rest of the project and thus concentrate maintenance activity. We mine the complete version histories of 91 evolving, actively developed GitHub repositories and identify 15 recurring line-level hotspot patterns that explain why these hotspots emerge. The three most prevalent patterns are Pinned Version Bump (26%), revealing brittle release practices; Long Line Change (17%), signalling deficient layout; and Formatting Ping-Pong (9%), indicating missing or inconsistent style automation. Surprisingly, automated accounts generate 74% of all hotspot edits, suggesting that bot activity is a dominant but largely avoidable source of noise in change histories. By mapping each pattern to concrete refactoring guidelines and continuous integration checks, our taxonomy equips practitioners with actionable steps to curb hotspots and systematically improve software quality in terms of configurability, stability, and changeability.", "published": "2026-02-13T18:29:12Z", "updated": "2026-02-13T18:29:12Z", "authors": ["Saleha Muzammil", "Mughees Ur Rehman", "Zoe Kotti", "Diomidis Spinellis"], "pdf_url": "https://arxiv.org/pdf/2602.13170v1"}
{"id": "http://arxiv.org/abs/2512.18080v2", "title": "From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems", "summary": "Agentic AI systems capable of generating full-stack web applications from natural language prompts (\"prompt- to-app\") represent a significant shift in software development. However, evaluating these systems remains challenging, as visual polish, functional correctness, and user trust are often misaligned. As a result, it is unclear how existing prompt-to-app tools compare under realistic, human-centered evaluation criteria. In this paper, we introduce a human-centered benchmark for evaluating prompt-to-app systems and conduct a large-scale comparative study of three widely used platforms: Replit, Bolt, and Firebase Studio. Using a diverse set of 96 prompts spanning common web application tasks, we generate 288 unique application artifacts. We evaluate these systems through a large-scale human-rater study involving 205 participants and 1,071 quality-filtered pairwise comparisons, assessing task-based ease of use, visual appeal, perceived completeness, and user trust. Our results show that these systems are not interchangeable: Firebase Studio consistently outperforms competing platforms across all human-evaluated dimensions, achieving the highest win rates for ease of use, trust, visual appeal, and visual appropriateness. Bolt performs competitively on visual appeal but trails Firebase on usability and trust, while Replit underperforms relative to both across most metrics. These findings highlight a persistent gap between visual polish and functional reliability in prompt-to-app systems and demonstrate the necessity of interactive, task-based evaluation. We release our benchmark framework, prompt set, and generated artifacts to support reproducible evaluation and future research in agentic application generation.", "published": "2025-12-19T21:37:15Z", "updated": "2026-02-13T17:26:15Z", "authors": ["Marcos Ortiz", "Justin Hill", "Collin Overbay", "Ingrida Semenec", "Frederic Sauve-Hoover", "Jim Schwoebel", "Joel Shor"], "pdf_url": "https://arxiv.org/pdf/2512.18080v2"}
{"id": "http://arxiv.org/abs/2602.13072v1", "title": "Automated Testing of Task-based Chatbots: How Far Are We?", "summary": "Task-based chatbots are software, typically embedded in real-world applications, that assist users in completing tasks through a conversational interface. As chatbots are gaining popularity, effectively assessing their quality has become crucial. Whereas traditional testing techniques fail to systematically exercise the conversational space of chatbots, several approaches specifically targeting chatbots have emerged from both industry and research. Although these techniques have shown advancements over the years, they still exhibit limitations, such as simplicity of the generated test scenarios and weakness in implemented oracles. In this paper, we conduct a confirmatory study to investigate such limitations by evaluating the effectiveness of state-of-the-art chatbot testing techniques on a curated selection of task-based chatbots from GitHub, developed using the most popular commercial and open-source platforms.", "published": "2026-02-13T16:32:50Z", "updated": "2026-02-13T16:32:50Z", "authors": ["Diego Clerissi", "Elena Masserini", "Daniela Micucci", "Leonardo Mariani"], "pdf_url": "https://arxiv.org/pdf/2602.13072v1"}
{"id": "http://arxiv.org/abs/2602.13029v1", "title": "Analysis of Asset Administration Shell-based Negotiation Processes for Scaling Applications", "summary": "The proactive Asset Administration Shell (AAS) enables bidirectional communication between assets. It uses the Language for I4.0 Components in VDI/VDE 2193 to facilitate negotiations, such as allocating products to available production resources. This paper investigates the efficiency of the negotiation, based on criteria, such as message load, for applications with a scaling number of assets. Currently, the focus of AAS standardization is on submodels and their security to enable interoperable data access. Their proactive behavior remains conceptual and is still a subject of scientific research. Existing studies examine proactive AAS architecture examples with a limited number of assets, raising questions about their scalability in industrial environments. To analyze proactive AAS for scaling applications, a scenario and evaluation criteria are introduced. A scalable implementation is developed using current architectures for proactive AAS, upon which experiments are conducted with a varying number of assets. The results reveal the performance limitations, communication overhead, and adaptability of the AAS-based negotiation mechanism scaling. This information can improve the further development and standardization of the AAS.", "published": "2026-02-13T15:35:01Z", "updated": "2026-02-13T15:35:01Z", "authors": ["David Dietrich", "Armin Lechler", "Alexander Verl"], "pdf_url": "https://arxiv.org/pdf/2602.13029v1"}
{"id": "http://arxiv.org/abs/2602.12966v1", "title": "ProbeLLM: Automating Principled Diagnosis of LLM Failures", "summary": "Understanding how and why large language models (LLMs) fail is becoming a central challenge as models rapidly evolve and static evaluations fall behind. While automated probing has been enabled by dynamic test generation, existing approaches often discover isolated failure cases, lack principled control over exploration, and provide limited insight into the underlying structure of model weaknesses. We propose ProbeLLM, a benchmark-agnostic automated probing framework that elevates weakness discovery from individual failures to structured failure modes. ProbeLLM formulates probing as a hierarchical Monte Carlo Tree Search, explicitly allocating limited probing budgets between global exploration of new failure regions and local refinement of recurring error patterns. By restricting probing to verifiable test cases and leveraging tool-augmented generation and verification, ProbeLLM grounds failure discovery in reliable evidence. Discovered failures are further consolidated into interpretable failure modes via failure-aware embeddings and boundary-aware induction. Across diverse benchmarks and LLMs, ProbeLLM reveals substantially broader, cleaner, and more fine-grained failure landscapes than static benchmarks and prior automated methods, supporting a shift from case-centric evaluation toward principled weakness discovery.", "published": "2026-02-13T14:33:13Z", "updated": "2026-02-13T14:33:13Z", "authors": ["Yue Huang", "Zhengzhe Jiang", "Yuchen Ma", "Yu Jiang", "Xiangqi Wang", "Yujun Zhou", "Yuexing Hao", "Kehan Guo", "Pin-Yu Chen", "Stefan Feuerriegel", "Xiangliang Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.12966v1"}
{"id": "http://arxiv.org/abs/2602.12950v1", "title": "The Influence of Code Smells in Efferent Neighbors on Class Stability", "summary": "Understanding what drives code instability is essential for effective software maintenance, as unstable classes require larger or more frequent edits and increase the risk of unintended side effects. Although code smells are widely believed to harm maintainability, most prior stability studies examine only the smells within the class being modified. In practice, however, classes can change because their efferent neighbors (i.e., the classes they depend on) are modified due to ripple effects that propagate along static dependencies, even if the class itself is clean. Such ripple effects may be more severe when the efferent neighbor exhibits code smells. In addition, code smells rarely occur alone. They often appear together within a class or across classes connected by static dependencies, a phenomenon known as code smell interrelation. Such interrelation can lead to code smell interaction, where smells are directly connected through static dependencies and may further compound maintainability issues. However, the effect of code smell interrelation and interaction on code quality remains largely underexplored. Therefore, this study investigates whether the presence of code smells in a class's efferent neighbors affects its stability, considering the factor of code smell interrelation and interaction. To achieve this, we mine one year of commit history from 100 top-starred GitHub projects, detect code smells and static dependencies, determine code smell interrelation and interaction, and model these factors as predictors of class stability.", "published": "2026-02-13T14:15:51Z", "updated": "2026-02-13T14:15:51Z", "authors": ["Zushuai Zhang", "Elliott Wen", "Ewan Tempero"], "pdf_url": "https://arxiv.org/pdf/2602.12950v1"}
{"id": "http://arxiv.org/abs/2602.12902v1", "title": "Robustness of Object Detection of Autonomous Vehicles in Adverse Weather Conditions", "summary": "As self-driving technology advances toward widespread adoption, determining safe operational thresholds across varying environmental conditions becomes critical for public safety. This paper proposes a method for evaluating the robustness of object detection ML models in autonomous vehicles under adverse weather conditions. It employs data augmentation operators to generate synthetic data that simulates different severance degrees of the adverse operation conditions at progressive intensity levels to find the lowest intensity of the adverse conditions at which the object detection model fails. The robustness of the object detection model is measured by the average first failure coefficients (AFFC) over the input images in the benchmark. The paper reports an experiment with four object detection models: YOLOv5s, YOLOv11s, Faster R-CNN, and Detectron2, utilising seven data augmentation operators that simulate weather conditions fog, rain, and snow, and lighting conditions of dark, bright, flaring, and shadow. The experiment data show that the method is feasible, effective, and efficient to evaluate and compare the robustness of object detection models in various adverse operation conditions. In particular, the Faster R-CNN model achieved the highest robustness with an overall average AFFC of 71.9% over all seven adverse conditions, while YOLO variants showed the AFFC values of 43%. The method is also applied to assess the impact of model training that targets adverse operation conditions using synthetic data on model robustness. It is observed that such training can improve robustness in adverse conditions but may suffer from diminishing returns and forgetting phenomena (i.e., decline in robustness) if overtrained.", "published": "2026-02-13T13:02:44Z", "updated": "2026-02-13T13:02:44Z", "authors": ["Fox Pettersen", "Hong Zhu"], "pdf_url": "https://arxiv.org/pdf/2602.12902v1"}
{"id": "http://arxiv.org/abs/2602.12875v1", "title": "A Microservice-Based Platform for Sustainable and Intelligent SLO Fulfilment and Service Management", "summary": "The Microservices Architecture (MSA) design pattern has become a staple for modern applications, allowing functionalities to be divided across fine-grained microservices, fostering reusability, distribution, and interoperability. As MSA-based applications are deployed to the Computing Continuum (CC), meeting their Service Level Objectives (SLOs) becomes a challenge. Trading off performance and sustainability SLOs is especially challenging. This challenge can be addressed with intelligent decision systems, able to reconfigure the services during runtime to meet the SLOs. However, developing these agents while adhering to the MSA pattern is complex, especially because CC providers, who have key know-how and information to fulfill these SLOs, must comply with the privacy requirements of application developers. This work presents the Carbon-Aware SLO and Control plAtform (CASCA), an open-source MSA-based platform that allows CC providers to reconfigure services and fulfill their SLOs while maintaining the privacy of developers. CASCA is architected to be highly reusable, distributable, and easy to use, extend, and modify. CASCA has been evaluated in a real CC testbed for a media streaming service, where decision systems implemented in Bash, Rust, and Python successfully reconfigured the service, unaffected by upholding privacy.", "published": "2026-02-13T12:24:49Z", "updated": "2026-02-13T12:24:49Z", "authors": ["Juan Luis Herrera", "Daniel Wang", "Schahram Dustdar"], "pdf_url": "https://arxiv.org/pdf/2602.12875v1"}
{"id": "http://arxiv.org/abs/2602.12834v1", "title": "FuncDroid: Towards Inter-Functional Flows for Comprehensive Mobile App GUI Testing", "summary": "As mobile application (app) functionalities grow increasingly complex and their iterations accelerate, ensuring high reliability presents significant challenges. While functionality-oriented GUI testing has attracted growing research attention, existing approaches largely overlook interactions across functionalities, making them ineffective at uncovering deep bugs hidden in inter-functional behaviors. To fill this gap, we first design a Functional Flow Graph (FFG), a behavioral model that explicitly captures an app's functional units and their inter-functional interactions. Based on the FFG, we further introduce an inter-functional-flow-oriented GUI testing approach with the dual goals of precise model construction and deep bug detection. This approach is realized through a long-short-term-view-guided testing process. By combining two complementary test-generation views, it can adaptively refine functional boundaries and systematically explore inter-functional flows under diverse triggering conditions. We implement our approach in a tool called FuncDroid, and evaluate it on two benchmarks: (1) a widely-used open-source benchmark with 50 reproducible crash bugs and (2) a diverse set of 52 popular commercial apps. Experimental results demonstrate that FuncDroid significantly outperforms state-of-the-art baselines in both coverage (+28%) and bug detection number (+107%). Moreover, FuncDroid successfully uncovers 18 previously unknown non-crash functional bugs in commercial apps, confirming its practical effectiveness.", "published": "2026-02-13T11:40:02Z", "updated": "2026-02-13T11:40:02Z", "authors": ["Jinlong He", "Changwei Xia", "Binru Huang", "Jiwei Yan", "Jun Yan", "Jian Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.12834v1"}
{"id": "http://arxiv.org/abs/2503.18561v3", "title": "Optimization under uncertainty: understanding orders and testing programs with specifications", "summary": "One of the most ubiquitous problems in optimization is that of finding all the elements of a finite set at which a function $f$ attains its minimum (or maximum). When the codomain of $f$ is equipped with a total order, it is easy to specify, implement, and verify generic solutions to this problem. But what if $f$ is affected by uncertainties? What if one seeks values that minimize more than one objective, or if $f$ does not return a single result but a set of possible results, or even a probability distribution? Such situations are common in climate science, economics, and engineering. Developing trustworthy solution methods for optimization under uncertainty requires formulating and answering these questions rigorously, including deciding which order relations to apply in different cases. We show how functional programming can support this task, and apply it to specify and test solution methods for cases where optimization is affected by two conceptually different kinds of uncertainty: value and functorial uncertainty. We analyze the interplay of orders in these contexts, demonstrate how standard minimization generalizes to partial orders in the multi-objective setting and how it can be lifted via monotonicity conditions to handle functorial uncertainty.", "published": "2025-03-24T11:14:52Z", "updated": "2026-02-13T11:15:43Z", "authors": ["Patrik Jansson", "Nicola Botta", "Tim Richter"], "pdf_url": "https://arxiv.org/pdf/2503.18561v3"}
{"id": "http://arxiv.org/abs/2602.07871v2", "title": "HerAgent: Rethinking the Automated Environment Deployment via Hierarchical Test Pyramid", "summary": "Automated software environment setup is a prerequisite for testing, debugging, and reproducing failures, yet remains challenging in practice due to complex dependencies, heterogeneous build systems, and incomplete documentation. Recent work leverages large language models to automate this process, but typically evaluates success using weak signals such as dependency installation or partial test execution, which do not ensure that a project can actually run. In this paper, we argue that environment setup success should be evaluated through executable evidence rather than a single binary signal. We introduce the Environment Maturity Hierarchy, which defines three success levels based on progressively stronger execution requirements, culminating in successful execution of a project's main entry point. Guided by this hierarchy, we propose HerAgent, an automated environment setup approach that incrementally constructs executable environments through execution-based validation and repair. We evaluate HerAgent on four public benchmarks, where it outperforms all related work, achieving up to 79.6\\% improvement due to its holistic understanding of project structure and dependencies. On complex C/C++ projects, HerAgent surpasses prior approaches by 66.7\\%. In addition, HerAgent uniquely resolves 11-30 environment instances across the benchmarks that no prior method can configure.", "published": "2026-02-08T08:57:05Z", "updated": "2026-02-13T09:24:18Z", "authors": ["Xiang Li", "Siyu Lu", "Federica Sarro", "Claire Le Goues", "He Ye"], "pdf_url": "https://arxiv.org/pdf/2602.07871v2"}
{"id": "http://arxiv.org/abs/2602.12748v1", "title": "X-SYS: A Reference Architecture for Interactive Explanation Systems", "summary": "The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.", "published": "2026-02-13T09:24:03Z", "updated": "2026-02-13T09:24:03Z", "authors": ["Tobias Labarta", "Nhi Hoang", "Maximilian Dreyer", "Jim Berend", "Oleg Hein", "Jackie Ma", "Wojciech Samek", "Sebastian Lapuschkin"], "pdf_url": "https://arxiv.org/pdf/2602.12748v1"}
{"id": "http://arxiv.org/abs/2602.12721v1", "title": "Reconciling Complexity and Simplicity in the Business Model Canvas Design Through Metamodelling and Domain-Specific Modelling", "summary": "This article introduces a metamodel for the Business Model Canvas (BMC) using the Unified Modelling Language (UML), together with a dedicated Domain-Specific Modelling Language (DSML) tool. Although the BMC is widely adopted by both practitioners and scholars, significant challenges remain in formally modelling business models, particularly with regard to explicit specification of inter-component relationships, while preserving the simplicity that characterises the BMC. Addressing this tension between modelling rigour and practical relevance, this research adopts a Design Science Research approach to formally specify relationships among BMC components and to strengthen their theoretical grounding through an adaptation of the V 4 framework. The proposed metamodel consolidates BMC relationships into three core types: supports, determines, and affects, providing explicit semantics while remaining accessible to end users through graphical tooling. The findings highlight that formally specifying relationships significantly improves the interpretability and consistency of BMC representations. The proposed metamodel and tool offer a rigorous yet usable foundation for developing DSML-based BMC tools and for enabling systematic integration of the BMC into widely used software and enterprise modelling environments, thereby bridging business modelling and enterprise architecture practices for both academics and practitioners.", "published": "2026-02-13T08:52:54Z", "updated": "2026-02-13T08:52:54Z", "authors": ["Nordine Benkeltoum"], "pdf_url": "https://arxiv.org/pdf/2602.12721v1"}
{"id": "http://arxiv.org/abs/2602.10655v2", "title": "Assessing Vision-Language Models for Perception in Autonomous Underwater Robotic Software", "summary": "Autonomous Underwater Robots (AURs) operate in challenging underwater environments, including low visibility and harsh water conditions. Such conditions present challenges for software engineers developing perception modules for the AUR software. To successfully carry out these tasks, deep learning has been incorporated into the AUR software to support its operations. However, the unique challenges of underwater environments pose difficulties for deep learning models, which often rely on labeled data that is scarce and noisy. This may undermine the trustworthiness of AUR software that relies on perception modules. Vision-Language Models (VLMs) offer promising solutions for AUR software as they generalize to unseen objects and remain robust in noisy conditions by inferring information from contextual cues. Despite this potential, their performance and uncertainty in underwater environments remain understudied from a software engineering perspective. Motivated by the needs of an industrial partner in assurance and risk management for maritime systems to assess the potential use of VLMs in this context, we present an empirical evaluation of VLM-based perception modules within the AUR software. We assess their ability to detect underwater trash by computing performance, uncertainty, and their relationship, to enable software engineers to select appropriate VLMs for their AUR software.", "published": "2026-02-11T08:59:44Z", "updated": "2026-02-13T08:33:37Z", "authors": ["Muhammad Yousaf", "Aitor Arrieta", "Shaukat Ali", "Paolo Arcaini", "Shuai Wang"], "pdf_url": "https://arxiv.org/pdf/2602.10655v2"}
{"id": "http://arxiv.org/abs/2509.16198v6", "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation", "summary": "Large language models excel at generating individual functions or single files of code, yet generating complete repositories from scratch remains a fundamental challenge. This capability is key to building coherent software systems from high-level specifications and realizing the full potential of automated code generation. The process requires planning at two levels: deciding what features and modules to build (proposal stage) and defining their implementation details (implementation stage). Current approaches rely on natural language planning, which often produces unclear specifications, misaligned components, and brittle designs due to its inherent ambiguity and lack of structure. To address these limitations, we introduce the Repository Planning Graph (RPG), a structured representation that encodes capabilities, file structures, data flows, and functions in a unified graph. By replacing free-form natural language with an explicit blueprint, RPG enables consistent long-horizon planning for repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework that operates in three stages: proposal-level planning, implementation-level construction, and graph-guided code generation with test validation. To evaluate, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces nearly 36K Code Lines and 445K Code Tokens, on average 3.9$\\times$ larger than the strongest baseline (Claude Code), and 68$\\times$ larger than other baselines. It achieves 81.5% coverage and 69.7% test accuracy, improving over Claude Code by 27.3 and 35.8 points. Further analysis shows that RPG models complex dependencies, enables more sophisticated planning through near-linear scaling, and improves agent understanding of repositories, thus accelerating localization. Our data and code are available at https://github.com/microsoft/RPG-ZeroRepo.", "published": "2025-09-19T17:58:14Z", "updated": "2026-02-13T08:28:51Z", "authors": ["Jane Luo", "Xin Zhang", "Steven Liu", "Jie Wu", "Jianfeng Liu", "Yiming Huang", "Yangyu Huang", "Chengyu Yin", "Ying Xin", "Yuefeng Zhan", "Hao Sun", "Qi Chen", "Scarlett Li", "Mao Yang"], "pdf_url": "https://arxiv.org/pdf/2509.16198v6"}
{"id": "http://arxiv.org/abs/2602.07821v2", "title": "Software Space Analytics: Towards Visualization and Statistics of Internal Software Execution", "summary": "In software maintenance work, software architects and programmers need to identify modules that require modification or deletion. Whilst user requests and bug reports are utilised for this purpose, evaluating the execution status of modules within the software is also crucial. This paper, therefore, applies spatial statistics to assess internal software execution data. First, we define a software space dataset, viewing the software's internal structure as a space based on module call relationships. Then, using spatial statistics, we conduct the visualization of spatial clusters and the statistical testing using spatial measures. Finally, we consider the usefulness of spatial statistics in the software engineering domain and future challenges. (This paper has been accepted for publication in the Proceedings of MODELSWARD 2026)", "published": "2026-02-08T04:57:57Z", "updated": "2026-02-13T08:01:31Z", "authors": ["Shinobu Saito"], "pdf_url": "https://arxiv.org/pdf/2602.07821v2"}
{"id": "http://arxiv.org/abs/2602.09064v2", "title": "Predicting Open Source Software Sustainability with Deep Temporal Neural Hierarchical Architectures and Explainable AI", "summary": "Open Source Software (OSS) projects follow diverse lifecycle trajectories shaped by evolving patterns of contribution, coordination, and community engagement. Understanding these trajectories is essential for stakeholders seeking to assess project organization and health at scale. However, prior work has largely relied on static or aggregated metrics, such as project age or cumulative activity, providing limited insight into how OSS sustainability unfolds over time. In this paper, we propose a hierarchical predictive framework that models OSS projects as belonging to distinct lifecycle stages grounded in established socio-technical categorizations of OSS development. Rather than treating sustainability solely as project longevity, these lifecycle stages operationalize sustainability as a multidimensional construct integrating contribution activity, community participation, and maintenance dynamics. The framework combines engineered tabular indicators with 24-month temporal activity sequences and employs a multi-stage classification pipeline to distinguish lifecycle stages associated with different coordination and participation regimes. To support transparency, we incorporate explainable AI techniques to examine the relative contribution of feature categories to model predictions. Evaluated on a large corpus of OSS repositories, the proposed approach achieves over 94\\% overall accuracy in lifecycle stage classification. Attribution analyses consistently identify contribution activity and community-related features as dominant signals, highlighting the central role of collective participation dynamics.", "published": "2026-02-09T05:44:34Z", "updated": "2026-02-13T05:56:05Z", "authors": ["S M Rakib Ul Karim", "Wenyi Lu", "Enock Kasaadha", "Sean Goggins"], "pdf_url": "https://arxiv.org/pdf/2602.09064v2"}
{"id": "http://arxiv.org/abs/2602.02690v2", "title": "Outrunning LLM Cutoffs: A Live Kernel Crash Resolution Benchmark for All", "summary": "Repairing system crashes discovered by kernel fuzzers like Syzkaller is a critical yet underexplored challenge in software engineering. While recent works have introduced Large Language Model (LLM) based agents for Linux kernel crash-resolution, their evaluation benchmarks are usually static and thus, do not capture the evolving nature of the Linux kernel, and suffer from potential data contamination due to LLM knowledge cutoffs. To address the above problem, we present (i) Live-kBench, an evaluation framework for self-evolving benchmarks that continuously scrapes and evaluates agents on freshly discovered kernel bugs, and (ii) kEnv, an agent-agnostic standardized crash-resolution environment for kernel compilation, execution, and feedback. This design decouples agent workflows from heavy-weight execution, enabling fair and scalable comparison across diverse agent frameworks under identical conditions.\n  To this end, we curate an inaugural dataset of 534 Linux kernel bugs and empirically demonstrate a significant performance gap, with agents achieving up to 25% higher equivalent patch rate on bugs fixed before the LLM knowledge cutoff. Using kEnv, we benchmark three state-of-the-art agents, showing that they resolve 74% of crashes on the first attempt (plausible patches); however only ~20% of generated patches closely match developer fixes. Additionally, exposing crash resolution feedback improves crash resolution rate by 29%. Live-kBench provides the community with an evaluation infrastructure for self-evolving benchmarks that is both time and attribute sensitive; complete with a public dashboard to track agent progress on Linux kernel bugs.", "published": "2026-02-02T19:06:15Z", "updated": "2026-02-13T03:18:35Z", "authors": ["Chenxi Huang", "Alex Mathai", "Feiyang Yu", "Aleksandr Nogikh", "Petros Maniatis", "Franjo Ivančić", "Eugene Wu", "Kostis Kaffes", "Junfeng Yang", "Baishakhi Ray"], "pdf_url": "https://arxiv.org/pdf/2602.02690v2"}
{"id": "http://arxiv.org/abs/2602.12500v1", "title": "Favia: Forensic Agent for Vulnerability-fix Identification and Analysis", "summary": "Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only a small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs. Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, a forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning. Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using a ReAct-based LLM agent. By providing the agent with a pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude single-pass or similarity-based methods. We evaluate Favia on CVEVC, a large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores.", "published": "2026-02-13T00:51:22Z", "updated": "2026-02-13T00:51:22Z", "authors": ["André Storhaug", "Jiamou Sun", "Jingyue Li"], "pdf_url": "https://arxiv.org/pdf/2602.12500v1"}
