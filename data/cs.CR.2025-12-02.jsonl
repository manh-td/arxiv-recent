{"id": "http://arxiv.org/abs/2506.12523v2", "title": "Privacy-preserving and reward-based mechanisms of proof of engagement", "summary": "Proof-of-Attendance (PoA) mechanisms are typically employed to demonstrate a specific user's participation in an event, whether virtual or in-person. The goal of this study is to extend such mechanisms to broader contexts where the user wishes to digitally demonstrate her involvement in a specific activity (Proof-of-Engagement, PoE). This work explores different solutions, including DLTs as well as established technologies based on centralized systems. The main aspects we consider include the level of privacy guaranteed to users, the scope of PoA/PoE (both temporal and spatial), the transferability of the proof, and the integration with incentive mechanisms.", "published": "2025-06-14T14:33:39Z", "updated": "2025-12-02T18:50:48Z", "authors": ["Matteo Marco Montanari", "Alessandro Aldini"], "pdf_url": "https://arxiv.org/pdf/2506.12523v2"}
{"id": "http://arxiv.org/abs/2512.02973v1", "title": "Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities", "summary": "While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\\% and 91.07\\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.", "published": "2025-12-02T17:51:02Z", "updated": "2025-12-02T17:51:02Z", "authors": ["Yuan Xiong", "Ziqi Miao", "Lijun Li", "Chen Qian", "Jie Li", "Jing Shao"], "pdf_url": "https://arxiv.org/pdf/2512.02973v1"}
{"id": "http://arxiv.org/abs/2511.06942v3", "title": "HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection", "summary": "To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.", "published": "2025-11-10T10:47:34Z", "updated": "2025-12-02T17:04:54Z", "authors": ["Fangqi Dai", "Xingjian Jiang", "Zizhuang Deng"], "pdf_url": "https://arxiv.org/pdf/2511.06942v3"}
{"id": "http://arxiv.org/abs/2510.08473v2", "title": "An Improved Quantum Algorithm for 3-Tuple Lattice Sieving", "summary": "The assumed hardness of the Shortest Vector Problem in high-dimensional lattices is one of the cornerstones of post-quantum cryptography. The fastest known heuristic attacks on SVP are via so-called sieving methods. While these still take exponential time in the dimension $d$, they are significantly faster than non-heuristic approaches and their heuristic assumptions are verified by extensive experiments. $k$-Tuple sieving is an iterative method where each iteration takes as input a large number of lattice vectors of a certain norm, and produces an equal number of lattice vectors of slightly smaller norm, by taking sums and differences of $k$ of the input vectors. Iterating these ''sieving steps'' sufficiently many times produces a short lattice vector. The fastest attacks (both classical and quantum) are for $k=2$, but taking larger $k$ reduces the amount of memory required for the attack. In this paper we improve the quantum time complexity of 3-tuple sieving from $2^{0.3098 d}$ to $2^{0.2846 d}$, using a two-level amplitude amplification aided by a preprocessing step that associates the given lattice vectors with nearby ''center points'' to focus the search on the neighborhoods of these center points. Our algorithm uses $2^{0.1887d}$ classical bits and QCRAM bits, and $2^{o(d)}$ qubits. This is the fastest known quantum algorithm for SVP when total memory is limited to $2^{0.1887d}$.", "published": "2025-10-09T17:13:07Z", "updated": "2025-12-02T16:47:03Z", "authors": ["Lynn Engelberts", "Yanlin Chen", "Amin Shiraz Gilani", "Maya-Iggy van Hoof", "Stacey Jeffery", "Ronald de Wolf"], "pdf_url": "https://arxiv.org/pdf/2510.08473v2"}
{"id": "http://arxiv.org/abs/2405.13068v3", "title": "Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation", "summary": "Large language models (LLMs) have transformed the field of natural language processing, but they remain susceptible to jailbreaking attacks that exploit their capabilities to generate unintended and potentially harmful content. Existing token-level jailbreaking techniques, while effective, face scalability and efficiency challenges, especially as models undergo frequent updates and incorporate advanced defensive measures. In this paper, we introduce JailMine, an innovative token-level manipulation approach that addresses these limitations effectively. JailMine employs an automated \"mining\" process to elicit malicious responses from LLMs by strategically selecting affirmative outputs and iteratively reducing the likelihood of rejection. Through rigorous testing across multiple well-known LLMs and datasets, we demonstrate JailMine's effectiveness and efficiency, achieving a significant average reduction of 86% in time consumed while maintaining high success rates averaging 95%, even in the face of evolving defensive strategies. Our work contributes to the ongoing effort to assess and mitigate the vulnerability of LLMs to jailbreaking attacks, underscoring the importance of continued vigilance and proactive measures to enhance the security and reliability of these powerful language models.", "published": "2024-05-20T17:17:55Z", "updated": "2025-12-02T16:46:10Z", "authors": ["Yuxi Li", "Yi Liu", "Yuekang Li", "Ling Shi", "Gelei Deng", "Shengquan Chen", "Kailong Wang"], "pdf_url": "https://arxiv.org/pdf/2405.13068v3"}
{"id": "http://arxiv.org/abs/2512.02918v1", "title": "Belobog: Move Language Fuzzing Framework For Real-World Smart Contracts", "summary": "Move is a research-oriented programming language design for secure and verifiable smart contract development and has been widely used in managing billions of digital assets in blockchains, such as Sui and Aptos. Move features a strong static type system and explicit resource semantics to enforce safety properties such as the prevention of data races, invalid asset transfers, and entry vulnerabilities. However, smart contracts written in Move may still contain certain vulnerabilities that are beyond the reach of its type system. It is thus essential to validate Move smart contracts. Unfortunately, due to its strong type system, existing smart contract fuzzers are ineffective in producing syntactically or semantically valid transactions to test Move smart contracts. This paper introduces the first fuzzing framework, Belobog, for Move smart contracts. Belobog is type-aware and ensures that all generated and mutated transactions are well-typed. More specifically, for a target Move smart contract, Belobog first constructs a type graph based on Move's type system, and then generates or mutates a transaction based on the graph trace derived from the type graph. In order to overcome the complex checks in Move smart contracts, we further design and implement a concolic executor in Belobog. We evaluated Belobog on 109 real-world Move smart contract projects. The experimental results show that Belobog is able to detect 100\\% critical and 79\\% major vulnerabilities manually audited by human experts. We further selected two recent notorious incidents in Move smart contracts, i.e., Cetus and Nemo. Belobog successfully reproduced full exploits for both of them, without any prior knowledge.", "published": "2025-12-02T16:36:13Z", "updated": "2025-12-02T16:36:13Z", "authors": ["Wanxu Xia", "Ziqiao Kong", "Zhengwei Li", "Yi Lu", "Pan Li", "Liqun Yang", "Yang Liu", "Xiapu Luo", "Shaohua Li"], "pdf_url": "https://arxiv.org/pdf/2512.02918v1"}
{"id": "http://arxiv.org/abs/2511.23198v2", "title": "Clustering Malware at Scale: A First Full-Benchmark Study", "summary": "Recent years have shown that malware attacks still happen with high frequency. Malware experts seek to categorize and classify incoming samples to confirm their trustworthiness or prove their maliciousness. One of the ways in which groups of malware samples can be identified is through malware clustering. Despite the efforts of the community, malware clustering which incorporates benign samples has been under-explored. Moreover, despite the availability of larger public benchmark malware datasets, malware clustering studies have avoided fully utilizing these datasets in their experiments, often resorting to small datasets with only a few families. Additionally, the current state-of-the-art solutions for malware clustering remain unclear. In our study, we evaluate malware clustering quality and establish the state-of-the-art on Bodmas and Ember - two large public benchmark malware datasets. Ours is the first study of malware clustering performed on whole malware benchmark datasets. Additionally, we extend the malware clustering task by incorporating benign samples. Our results indicate that incorporating benign samples does not significantly degrade clustering quality. We find that there are differences in the quality of the created clusters between Ember and Bodmas, as well as a private industry dataset. Contrary to popular opinion, our top clustering performers are K-Means and BIRCH, with DBSCAN and HAC falling behind.", "published": "2025-11-28T14:02:17Z", "updated": "2025-12-02T14:32:14Z", "authors": ["Martin Mocko", "Jakub Ševcech", "Daniela Chudá"], "pdf_url": "https://arxiv.org/pdf/2511.23198v2"}
{"id": "http://arxiv.org/abs/2512.02822v1", "title": "Decryption thorough polynomial ambiguity: noise-enhanced high-memory convolutional codes for post-quantum cryptography", "summary": "We present a novel approach to post-quantum cryptography that employs directed-graph decryption of noise-enhanced high-memory convolutional codes. The proposed construction generates random-like generator matrices that effectively conceal algebraic structure and resist known structural attacks. Security is further reinforced by the deliberate injection of strong noise during decryption, arising from polynomial division: while legitimate recipients retain polynomial-time decoding, adversaries face exponential-time complexity. As a result, the scheme achieves cryptanalytic security margins surpassing those of Classic McEliece by factors exceeding 2^(200). Beyond its enhanced security, the method offers greater design flexibility, supporting arbitrary plaintext lengths with linear-time decryption and uniform per-bit computational cost, enabling seamless scalability to long messages. Practical deployment is facilitated by parallel arrays of directed-graph decoders, which identify the correct plaintext through polynomial ambiguity while allowing efficient hardware and software implementations. Altogether, the scheme represents a compelling candidate for robust, scalable, and quantum-resistant public-key cryptography.", "published": "2025-12-02T14:30:03Z", "updated": "2025-12-02T14:30:03Z", "authors": ["Meir Ariel"], "pdf_url": "https://arxiv.org/pdf/2512.02822v1"}
{"id": "http://arxiv.org/abs/2410.03768v2", "title": "Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs", "summary": "The rapid proliferation of frontier model agents promises significant societal advances but also raises concerns about systemic risks arising from unsafe interactions. Collusion to the disadvantage of others has been identified as a central form of undesirable agent cooperation. The use of information hiding (steganography) in agent communications could render such collusion practically undetectable. This underscores the need for investigations into the possibility of such behaviours emerging and the robustness corresponding countermeasures. To investigate this problem we design two approaches -- a gradient-based reinforcement learning (GBRL) method and an in-context reinforcement learning (ICRL) method -- for reliably eliciting sophisticated LLM-generated linguistic text steganography.\n  We demonstrate, for the first time, that unintended steganographic collusion in LLMs can arise due to mispecified reward incentives during training. Additionally, we find that standard mitigations -- both passive oversight of model outputs and active mitigation through communication paraphrasing -- are not fully effective at preventing this steganographic communication. Our findings imply that (i) emergence of steganographic collusion is a plausible concern that should be monitored and researched, and (ii) preventing emergence may require innovation in mitigation techniques.", "published": "2024-10-02T16:18:33Z", "updated": "2025-12-02T12:32:37Z", "authors": ["Yohan Mathew", "Ollie Matthews", "Robert McCarthy", "Joan Velja", "Christian Schroeder de Witt", "Dylan Cope", "Nandi Schoots"], "pdf_url": "https://arxiv.org/pdf/2410.03768v2"}
{"id": "http://arxiv.org/abs/2301.12766v3", "title": "GPS-Spoofing Attack Detection Mechanism for UAV Swarms", "summary": "Recently autonomous and semi-autonomous Unmanned Aerial Vehicle (UAV) swarms started to receive a lot of research interest and demand from various civil application fields. However, for successful mission execution, UAV swarms require Global navigation satellite system signals and in particular, Global Positioning System (GPS) signals for navigation. Unfortunately, civil GPS signals are unencrypted and unauthenticated, which facilitates the execution of GPS spoofing attacks. During these attacks, adversaries mimic the authentic GPS signal and broadcast it to the targeted UAV in order to change its course, and force it to land or crash. In this study, we propose a GPS spoofing detection mechanism capable of detecting single-transmitter and multi-transmitter GPS spoofing attacks to prevent the outcomes mentioned above. Our detection mechanism is based on comparing the distance between each two swarm members calculated from their GPS coordinates to the distance acquired from Impulse Radio Ultra-Wideband ranging between the same swarm members. If the difference in distances is larger than a chosen threshold the GPS spoofing attack is declared detected.", "published": "2023-01-30T10:26:52Z", "updated": "2025-12-02T12:29:17Z", "authors": ["Pavlo Mykytyn", "Marcin Brzozowski", "Zoya Dyka", "Peter Langendoerfer"], "pdf_url": "https://arxiv.org/pdf/2301.12766v3"}
{"id": "http://arxiv.org/abs/2505.19969v2", "title": "Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models", "summary": "Fully decentralized training of machine learning models offers significant advantages in scalability, robustness, and fault tolerance. However, achieving differential privacy (DP) guarantees in such settings is challenging due to the absence of a central aggregator and varying trust assumptions among nodes. We present a novel privacy analysis of decentralized gossip-based averaging algorithms with additive node-level noise, from arbitrary views of nodes in a graph and especially consider the averaging over nearest neighbors with secure summation and individual node-wise views. Our main contribution is a an analytical framework based on a linear systems formulation that accurately characterizes privacy leakage between nodes across different scenarios. In case the gossip averaging happens via secure summation, we show that the Rényi DP parameter growth is asymptotically $O(T)$, where $T$ is the number of training rounds, similarly as in the case of central aggregation.", "published": "2025-05-26T13:31:43Z", "updated": "2025-12-02T12:24:02Z", "authors": ["Antti Koskela", "Tejas Kulkarni"], "pdf_url": "https://arxiv.org/pdf/2505.19969v2"}
{"id": "http://arxiv.org/abs/2501.16843v2", "title": "Bones of Contention: Exploring Query-Efficient Attacks against Skeleton Recognition Systems", "summary": "Skeleton action recognition models have secured more attention than video-based ones in various applications due to privacy preservation and lower storage requirements. Skeleton data are typically transmitted to cloud servers for action recognition, with results returned to clients via Apps/APIs. However, the vulnerability of skeletal models against adversarial perturbations gradually reveals the unreliability of these systems. Existing black-box attacks all operate in a decision-based manner, resulting in numerous queries that hinder efficiency and feasibility in real-world applications. Moreover, all attacks off the shelf focus on only restricted perturbations, while ignoring model weaknesses when encountered with non-semantic perturbations. In this paper, we propose two query-effIcient Skeletal Adversarial AttaCks, ISAAC-K and ISAAC-N. As a black-box attack, ISAAC-K utilizes Grad-CAM in a surrogate model to extract key joints where minor sparse perturbations are then added to fool the classifier. To guarantee natural adversarial motions, we introduce constraints of both bone length and temporal consistency. ISAAC-K finds stronger adversarial examples on the $\\ell_\\infty$ norm, which can encompass those on other norms. Exhaustive experiments substantiate that ISAAC-K can uplift the attack efficiency of the perturbations under 10 skeletal models. Additionally, as a byproduct, ISAAC-N fools the classifier by replacing skeletons unrelated to the action. We surprisingly find that skeletal models are vulnerable to large perturbations where the part-wise non-semantic joints are just replaced, leading to a query-free no-box attack without any prior knowledge. Based on that, four adaptive defenses are eventually proposed to improve the robustness of skeleton recognition models.", "published": "2025-01-28T10:33:30Z", "updated": "2025-12-02T11:32:28Z", "authors": ["Yuxin Cao", "Kai Ye", "Derui Wang", "Minhui Xue", "Hao Ge", "Chenxiong Qian", "Jin Song Dong"], "pdf_url": "https://arxiv.org/pdf/2501.16843v2"}
{"id": "http://arxiv.org/abs/2512.02654v1", "title": "Cybersecurity AI: The World's Top AI Agent for Security Capture-the-Flag (CTF)", "summary": "Are Capture-the-Flag competitions obsolete? In 2025, Cybersecurity AI (CAI) systematically conquered some of the world's most prestigious hacking competitions, achieving Rank #1 at multiple events and consistently outperforming thousands of human teams. Across five major circuits-HTB's AI vs Humans, Cyber Apocalypse (8,129 teams), Dragos OT CTF, UWSP Pointer Overflow, and the Neurogrid CTF showdown-CAI demonstrated that Jeopardy-style CTFs have become a solved game for well-engineered AI agents. At Neurogrid, CAI captured 41/45 flags to claim the $50,000 top prize; at Dragos OT, it sprinted 37% faster to 10K points than elite human teams; even when deliberately paused mid-competition, it maintained top-tier rankings. Critically, CAI achieved this dominance through our specialized alias1 model architecture, which delivers enterprise-scale AI security operations at unprecedented cost efficiency and with augmented autonomy-reducing 1B token inference costs from $5,940 to just $119, making continuous security agent operation financially viable for the first time. These results force an uncomfortable reckoning: if autonomous agents now dominate competitions designed to identify top security talent at negligible cost, what are CTFs actually measuring? This paper presents comprehensive evidence of AI capability across the 2025 CTF circuit and argues that the security community must urgently transition from Jeopardy-style contests to Attack & Defense formats that genuinely test adaptive reasoning and resilience-capabilities that remain uniquely human, for now.", "published": "2025-12-02T11:15:44Z", "updated": "2025-12-02T11:15:44Z", "authors": ["Víctor Mayoral-Vilches", "Luis Javier Navarrete-Lozano", "Francesco Balassone", "María Sanz-Gómez", "Cristóbal R. J. Veas Chavez", "Maite del Mundo de Torres", "Vanesa Turiel"], "pdf_url": "https://arxiv.org/pdf/2512.02654v1"}
{"id": "http://arxiv.org/abs/2512.02625v1", "title": "CryptoQA: A Large-scale Question-answering Dataset for AI-assisted Cryptography", "summary": "Large language models (LLMs) excel at many general-purpose natural language processing tasks. However, their ability to perform deep reasoning and mathematical analysis, particularly for complex tasks as required in cryptography, remains poorly understood, largely due to the lack of suitable data for evaluation and training. To address this gap, we present CryptoQA, the first large-scale question-answering (QA) dataset specifically designed for cryptography. CryptoQA contains over two million QA pairs drawn from curated academic sources, along with contextual metadata that can be used to test the cryptographic capabilities of LLMs and to train new LLMs on cryptographic tasks. We benchmark 15 state-of-the-art LLMs on CryptoQA, evaluating their factual accuracy, mathematical reasoning, consistency, referencing, backward reasoning, and robustness to adversarial samples. In addition to quantitative metrics, we provide expert reviews that qualitatively assess model outputs and establish a gold-standard baseline. Our results reveal significant performance deficits of LLMs, particularly on tasks that require formal reasoning and precise mathematical knowledge. This shows the urgent need for LLM assistants tailored to cryptography research and development. We demonstrate that, by using CryptoQA, LLMs can be fine-tuned to exhibit better performance on cryptographic tasks.", "published": "2025-12-02T10:35:36Z", "updated": "2025-12-02T10:35:36Z", "authors": ["Mayar Elfares", "Pascal Reisert", "Tilman Dietz", "Manpa Barman", "Ahmed Zaki", "Ralf Küsters", "Andreas Bulling"], "pdf_url": "https://arxiv.org/pdf/2512.02625v1"}
{"id": "http://arxiv.org/abs/2512.02603v1", "title": "Semigroup action based on skew polynomial evaluation with applications to Cryptography", "summary": "Through this work we introduce an action of the skew polynomial ring $\\mathbb{F}_{q}\\left[X; σ, δ\\right]$ over $\\mathbb{F}_{q}$ based on its polynomial valuation and the concept of left skew product of functions. This lead us to explore the construction of a certain subset $\\mathcal{T}(X)\\subset\\mathbb{F}_{q}\\left[X; σ, δ\\right]$ that allow us to control the non-commutativity of this ring, and exploit this fact in order to build a public key exchange protocol that is secure in Canetti and Krawczyk model.", "published": "2025-12-02T10:08:50Z", "updated": "2025-12-02T10:08:50Z", "authors": ["Daniel Camazón-Portela", "Juan Antonio López-Ramos"], "pdf_url": "https://arxiv.org/pdf/2512.02603v1"}
{"id": "http://arxiv.org/abs/2512.02600v1", "title": "S3C2 SICP Summit 2025-06: Vulnerability Response Summit", "summary": "Recent years have shown increased cyber attacks targeting less secure elements in the software supply chain and causing significant damage to businesses and organizations. The US and EU governments and industry are equally interested in enhancing software security, including supply chain and vulnerability response. On June 26, 2025, researchers from the NSF-supported Secure Software Supply Chain Center (S3C2) and the Software Innovation Campus Paderborn (SICP) conducted a Vulnerability Response Summit with a diverse set of 9 practitioners from 9 companies. The goal of the Summit is to enable sharing between industry practitioners having practical experiences and challenges with software supply chain security, including vulnerability response, and helping to form new collaborations. We conducted five panel discussions based on open-ended questions regarding experiences with vulnerability reports, tools used for vulnerability discovery and management, organizational structures to report vulnerability response and management, preparedness and implementations for Cyber Resilience Act1 (CRA) and NIS22, and bug bounties. The open discussions enabled mutual sharing and shed light on common challenges that industry practitioners with practical experience face when securing their software supply chain, including vulnerability response. In this paper, we provide a summary of the Summit. Full panel questions can be found in the appendix.", "published": "2025-12-02T10:05:41Z", "updated": "2025-12-02T10:05:41Z", "authors": ["Anna Lena Rotthaler", "Simon Oberthür", "Juraj Somorovsky", "Kirsten Thommes", "Simon Trang", "Yasemin Acar", "Michel Cukier", "William Enck", "Alexandros Kapravelos", "Christian Kästner", "Dominik Wermke", "Laurie Williams"], "pdf_url": "https://arxiv.org/pdf/2512.02600v1"}
{"id": "http://arxiv.org/abs/2512.02598v1", "title": "Equilibrium SAT based PQC: New aegis against quantum computing", "summary": "Public-key cryptography algorithms have evolved towards increasing computational complexity to hide desired messages, which is accelerating with the development of the Internet and quantum computing. This paper introduces a novel public-key cryptography algorithm that generates ciphertexts by counting the number of elements in randomly extracted subsets from a multiset. After explaining the novel cryptographic concept, the process of mathematically refining it using satisfiability problems is described. The advantages of the proposed algorithm are: first, it is significantly faster than other public-key algorithms; second, it does not require big numbers, making it executable on any devices; and third, it can be easily extended into a public-key cryptosystem using a single public key and multiple private keys while maintaining quantum resistance.", "published": "2025-12-02T10:05:06Z", "updated": "2025-12-02T10:05:06Z", "authors": ["Keum-Bae Cho"], "pdf_url": "https://arxiv.org/pdf/2512.02598v1"}
{"id": "http://arxiv.org/abs/2512.02534v1", "title": "Detection of Crowdsourcing Cryptocurrency Laundering via Multi-Task Collaboration", "summary": "USDT, a stablecoin pegged to dollar, has become a preferred choice for money laundering due to its stability, anonymity, and ease of use. Notably, a new form of money laundering on stablecoins -- we refer to as crowdsourcing laundering -- disperses funds through recruiting a large number of ordinary individuals, and has rapidly emerged as a significant threat. However, due to the refined division of labor, crowdsourcing laundering transactions exhibit diverse patterns and a polycentric structure, posing significant challenges for detection. In this paper, we introduce transaction group as auxiliary information, and propose the Multi-Task Collaborative Crowdsourcing Laundering Detection (MCCLD) framework. MCCLD employs an end-to-end graph neural network to realize collaboration between laundering transaction detection and transaction group detection tasks, enhancing detection performance on diverse patterns within crowdsourcing laundering group. These two tasks are jointly optimized through a shared classifier, with a shared feature encoder that fuses multi-level feature embeddings to provide rich transaction semantics and potential group information. Extensive experiments on both crowdsourcing and general laundering demonstrate MCCLD's effectiveness and generalization. To the best of our knowledge, this is the first work on crowdsourcing laundering detection.", "published": "2025-12-02T08:58:11Z", "updated": "2025-12-02T08:58:11Z", "authors": ["Guang Li", "Litong Sun", "Jieying Zhou", "Weigang Wu"], "pdf_url": "https://arxiv.org/pdf/2512.02534v1"}
{"id": "http://arxiv.org/abs/2512.01594v2", "title": "Confidential, Attestable, and Efficient Inter-CVM Communication with Arm CCA", "summary": "Confidential Virtual Machines (CVMs) are increasingly adopted to protect sensitive workloads from privileged adversaries such as the hypervisor. While they provide strong isolation guarantees, existing CVM architectures lack first-class mechanisms for inter-CVM data sharing due to their disjoint memory model, making inter-CVM data exchange a performance bottleneck in compartmentalized or collaborative multi-CVM systems. Under this model, a CVM's accessible memory is either shared with the hypervisor or protected from both the hypervisor and all other CVMs. This design simplifies reasoning about memory ownership; however, it fundamentally precludes plaintext data sharing between CVMs because all inter-CVM communication must pass through hypervisor-accessible memory, requiring costly encryption and decryption to preserve confidentiality and integrity. In this paper, we introduce CAEC, a system that enables protected memory sharing between CVMs. CAEC builds on Arm Confidential Compute Architecture (CCA) and extends its firmware to support Confidential Shared Memory (CSM), a memory region securely shared between multiple CVMs while remaining inaccessible to the hypervisor and all non-participating CVMs. CAEC's design is fully compatible with CCA hardware and introduces only a modest increase (4%) in CCA firmware code size. CAEC delivers substantial performance benefits across a range of workloads. For instance, inter-CVM communication over CAEC achieves up to 209$\\times$ reduction in CPU cycles compared to encryption-based mechanisms over hypervisor-accessible shared memory. By combining high performance, strong isolation guarantees, and attestable sharing semantics, CAEC provides a practical and scalable foundation for the next generation of trusted multi-CVM services across both edge and cloud environments.", "published": "2025-12-01T12:10:43Z", "updated": "2025-12-02T08:57:45Z", "authors": ["Sina Abdollahi", "Amir Al Sadi", "Marios Kogias", "David Kotz", "Hamed Haddadi"], "pdf_url": "https://arxiv.org/pdf/2512.01594v2"}
{"id": "http://arxiv.org/abs/2412.01784v3", "title": "Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models", "summary": "Capability evaluations play a crucial role in assessing and regulating frontier AI systems. The effectiveness of these evaluations faces a significant challenge: strategic underperformance, or ``sandbagging'', where models deliberately underperform during evaluation. Sandbagging can manifest either through explicit developer intervention or through unintended model behavior, presenting a fundamental obstacle to accurate capability assessment. We introduce a novel sandbagging detection method based on injecting noise of varying magnitudes into model weights. While non-sandbagging models show predictable performance degradation with increasing noise, we demonstrate that sandbagging models exhibit anomalous performance improvements, likely due to disruption of underperformance mechanisms while core capabilities remain partially intact. Through experiments across various model architectures, sizes, and sandbagging techniques, we establish this distinctive response pattern as a reliable, model-agnostic signal for detecting sandbagging behavior. Importantly, we find noise-injection is capable of eliciting the full performance of Mistral Large 120B in a setting where the model underperforms without being instructed to do so. Our findings provide a practical tool for AI evaluation and oversight, addressing a challenge in ensuring accurate capability assessment of frontier AI systems.", "published": "2024-12-02T18:34:51Z", "updated": "2025-12-02T07:58:52Z", "authors": ["Cameron Tice", "Philipp Alexander Kreer", "Nathan Helm-Burger", "Prithviraj Singh Shahani", "Fedor Ryzhenkov", "Fabien Roger", "Clement Neo", "Jacob Haimes", "Felix Hofstätter", "Teun van der Weij"], "pdf_url": "https://arxiv.org/pdf/2412.01784v3"}
{"id": "http://arxiv.org/abs/2505.15216v3", "title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems", "summary": "AI agents have the potential to significantly alter the cybersecurity landscape. Here, we introduce the first framework to capture offensive and defensive cyber-capabilities in evolving real-world systems. Instantiating this framework with BountyBench, we set up 25 systems with complex, real-world codebases. To capture the vulnerability lifecycle, we define three task types: Detect (detecting a new vulnerability), Exploit (exploiting a given vulnerability), and Patch (patching a given vulnerability). For Detect, we construct a new success indicator, which is general across vulnerability types and provides localized evaluation. We manually set up the environment for each system, including installing packages, setting up server(s), and hydrating database(s). We add 40 bug bounties, which are vulnerabilities with monetary awards from \\$10 to \\$30,485, covering 9 of the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy based on information to guide detection, interpolating from identifying a zero day to exploiting a given vulnerability. We evaluate 10 agents: Claude Code, OpenAI Codex CLI with o3-high and o4-mini, and custom agents with o3-high, GPT-4.1, Gemini 2.5 Pro Preview, Claude 3.7 Sonnet Thinking, Qwen3 235B A22B, Llama 4 Maverick, and DeepSeek-R1. Given up to three attempts, the top-performing agents are Codex CLI: o3-high (12.5% on Detect, mapping to \\$3,720; 90% on Patch, mapping to \\$14,152), Custom Agent: Claude 3.7 Sonnet Thinking (67.5% on Exploit), and Codex CLI: o4-mini (90% on Patch, mapping to \\$14,422). Codex CLI: o3-high, Codex CLI: o4-mini, and Claude Code are more capable at defense, achieving higher Patch scores of 90%, 90%, and 87.5%, compared to Exploit scores of 47.5%, 32.5%, and 57.5% respectively; while the custom agents are relatively balanced between offense and defense, achieving Exploit scores of 17.5-67.5% and Patch scores of 25-60%.", "published": "2025-05-21T07:44:52Z", "updated": "2025-12-02T06:17:22Z", "authors": ["Andy K. Zhang", "Joey Ji", "Celeste Menders", "Riya Dulepet", "Thomas Qin", "Ron Y. Wang", "Junrong Wu", "Kyleen Liao", "Jiliang Li", "Jinghan Hu", "Sara Hong", "Nardos Demilew", "Shivatmica Murgai", "Jason Tran", "Nishka Kacheria", "Ethan Ho", "Denis Liu", "Lauren McLane", "Olivia Bruvik", "Dai-Rong Han", "Seungwoo Kim", "Akhil Vyas", "Cuiyuanxiu Chen", "Ryan Li", "Weiran Xu", "Jonathan Z. Ye", "Prerit Choudhary", "Siddharth M. Bhatia", "Vikram Sivashankar", "Yuxuan Bao", "Dawn Song", "Dan Boneh", "Daniel E. Ho", "Percy Liang"], "pdf_url": "https://arxiv.org/pdf/2505.15216v3"}
{"id": "http://arxiv.org/abs/2512.02418v1", "title": "Leveraging Large Language Models to Bridge On-chain and Off-chain Transparency in Stablecoins", "summary": "Stablecoins such as USDT and USDC aspire to peg stability by coupling issuance controls with reserve attestations. In practice, however, the transparency is split across two worlds: verifiable on-chain traces and off-chain disclosures locked in unstructured text that are unconnected. We introduce a large language model (LLM)-based automated framework that bridges these two dimensions by aligning on-chain issuance data with off-chain disclosure statements. First, we propose an integrative framework using LLMs to capture and analyze on- and off-chain data through document parsing and semantic alignment, extracting key financial indicators from issuer attestations and mapping them to corresponding on-chain metrics. Second, we integrate multi-chain issuance records and disclosure documents within a model context protocol (MCP) framework that standardizes LLMs access to both quantitative market data and qualitative disclosure narratives. This framework enables unified retrieval and contextual alignment across heterogeneous stablecoin information sources and facilitates consistent analysis. Third, we demonstrate the capability of LLMs to operate across heterogeneous data modalities in blockchain analytics, quantifying discrepancies between reported and observed circulation and examining their implications for cross-chain transparency and price dynamics. Our findings reveal systematic gaps between disclosed and verifiable data, showing that LLM-assisted analysis enhances cross-modal transparency and supports automated, data-driven auditing in decentralized finance (DeFi).", "published": "2025-12-02T05:00:17Z", "updated": "2025-12-02T05:00:17Z", "authors": ["Yuexin Xiang", "Yuchen Lei", "SM Mahir Shazeed Rish", "Yuanzhe Zhang", "Qin Wang", "Tsz Hon Yuen", "Jiangshan Yu"], "pdf_url": "https://arxiv.org/pdf/2512.02418v1"}
{"id": "http://arxiv.org/abs/2512.02414v1", "title": "Characterizing Cyber Attacks against Space Infrastructures with Missing Data: Framework and Case Study", "summary": "Cybersecurity of space infrastructures is an emerging topic, despite space-related cybersecurity incidents occurring as early as 1977 (i.e., hijacking of a satellite transmission signal). There is no single dataset that documents cyber attacks against space infrastructures that have occurred in the past; instead, these incidents are often scattered in media reports while missing many details, which we dub the missing-data problem. Nevertheless, even ``low-quality'' datasets containing such reports would be extremely valuable because of the dearth of space cybersecurity data and the sensitivity of space infrastructures which are often restricted from disclosure by governments. This prompts a research question: How can we characterize real-world cyber attacks against space infrastructures? In this paper, we address the problem by proposing a framework, including metrics, while also addressing the missing-data problem by leveraging methodologies such as the Space Attack Research and Tactic Analysis (SPARTA) and the Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK) to ``extrapolate'' the missing data in a principled fashion. We show how the extrapolated data can be used to reconstruct ``hypothetical but plausible'' space cyber kill chains and space cyber attack campaigns that have occurred in practice. To show the usefulness of the framework, we extract data for 108 cyber attacks against space infrastructures and show how to extrapolate this ``low-quality'' dataset containing missing information to derive 6,206 attack technique-level space cyber kill chains. Our findings include: cyber attacks against space infrastructures are getting increasingly sophisticated; successful protection of the link segment between the space and user segments could have thwarted nearly half of the 108 attacks. We will make our dataset available.", "published": "2025-12-02T04:50:55Z", "updated": "2025-12-02T04:50:55Z", "authors": ["Ekzhin Ear", "Jose Luis Castanon Remy", "Caleb Chang", "Qiren Que", "Antonia Feffer", "Shouhuai Xu"], "pdf_url": "https://arxiv.org/pdf/2512.02414v1"}
{"id": "http://arxiv.org/abs/2508.21393v3", "title": "VeriLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs", "summary": "Fine-tuning large language models (LLMs) is crucial for adapting them to specific tasks, yet it remains computationally demanding and raises concerns about correctness and privacy, particularly in untrusted environments. Although parameter-efficient methods like Low-Rank Adaptation (LoRA) significantly reduce resource requirements, ensuring the security and verifiability of fine-tuning under zero-knowledge constraints remains an unresolved challenge. To address this, we introduce VeriLoRA, the first framework to integrate LoRA fine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and correctness. VeriLoRA employs advanced cryptographic techniques -- such as lookup arguments, sumcheck protocols, and polynomial commitments -- to verify both arithmetic and non-arithmetic operations in Transformer-based architectures. The framework provides end-to-end verifiability for forward propagation, backward propagation, and parameter updates during LoRA fine-tuning, while safeguarding the privacy of model parameters and training data. Leveraging GPU-based implementations, VeriLoRA demonstrates practicality and efficiency through experimental validation on open-source LLMs like LLaMA, scaling up to 13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs, VeriLoRA bridges a critical gap, enabling secure and trustworthy deployment of LLMs in sensitive or untrusted environments.", "published": "2025-08-29T08:14:38Z", "updated": "2025-12-02T04:45:18Z", "authors": ["Guofu Liao", "Taotao Wang", "Shengli Zhang", "Jiqun Zhang", "Shi Long", "Dacheng Tao"], "pdf_url": "https://arxiv.org/pdf/2508.21393v3"}
{"id": "http://arxiv.org/abs/2512.02410v1", "title": "Decentralized Multi-Agent System with Trust-Aware Communication", "summary": "The emergence of Large Language Models (LLMs) is rapidly accelerating the development of autonomous multi-agent systems (MAS), paving the way for the Internet of Agents. However, traditional centralized MAS architectures present significant challenges, including single points of failure, vulnerability to censorship, inherent scalability limitations, and critical trust issues. We propose a novel Decentralized Multi-Agent System (DMAS) architecture designed to overcome these fundamental problems by enabling trust-aware, scalable, and censorship-resistant interactions among autonomous agents. Our DMAS features a decentralized agent runtime underpinned by a blockchain-based architecture. We formalize a trust-aware communication protocol that leverages cryptographic primitives and on-chain operations to provide security properties: verifiable interaction cycles, communication integrity, authenticity, non-repudiation, and conditional confidentiality, which we further substantiate through a comprehensive security analysis. Our performance analysis validates the DMAS as a scalable and efficient solution for building trustworthy multi-agent systems.", "published": "2025-12-02T04:39:12Z", "updated": "2025-12-02T04:39:12Z", "authors": ["Yepeng Ding", "Ahmed Twabi", "Junwei Yu", "Lingfeng Zhang", "Tohru Kondo", "Hiroyuki Sato"], "pdf_url": "https://arxiv.org/pdf/2512.02410v1"}
{"id": "http://arxiv.org/abs/2512.02399v1", "title": "AtomGraph: Tackling Atomicity Violation in Smart Contracts using Multimodal GCNs", "summary": "Smart contracts are a core component of blockchain technology and are widely deployed across various scenarios. However, atomicity violations have become a potential security risk. Existing analysis tools often lack the precision required to detect these issues effectively. To address this challenge, we introduce AtomGraph, an automated framework designed for detecting atomicity violations. This framework leverages Graph Convolutional Networks (GCN) to identify atomicity violations through multimodal feature learning and fusion. Specifically, driven by a collaborative learning mechanism, the model simultaneously learns from two heterogeneous modalities: extracting structural topological features from the contract's Control Flow Graph (CFG) and uncovering deep semantics from its opcode sequence. We designed an adaptive weighted fusion mechanism to dynamically adjust the weights of features from each modality to achieve optimal feature fusion. Finally, GCN detects graph-level atomicity violation on the contract. Comprehensive experimental evaluations demonstrate that AtomGraph achieves 96.88% accuracy and 96.97% F1 score, outperforming existing tools. Furthermore, compared to the concatenation fusion model, AtomGraph improves the F1 score by 6.4%, proving its potential in smart contract security detection.", "published": "2025-12-02T04:20:02Z", "updated": "2025-12-02T04:20:02Z", "authors": ["Xiaoqi Li", "Zongwei Li", "Wenkai Li", "Zeng Zhang", "Lei Xie"], "pdf_url": "https://arxiv.org/pdf/2512.02399v1"}
{"id": "http://arxiv.org/abs/2501.16165v2", "title": "A Survey of Operating System Kernel Fuzzing", "summary": "The Operating System (OS) kernel is foundational in modern computing, especially with the proliferation of diverse computing devices. However, its development also comes with vulnerabilities that can lead to severe security breaches. Kernel fuzzing, a technique used to uncover these vulnerabilities, poses distinct challenges when compared to user-space fuzzing. These include the complexity of configuring the testing environment and addressing the statefulness inherent to both the kernel and the fuzzing process. Despite the significant interest from the community, a comprehensive understanding of kernel fuzzing remains lacking, hindering further progress in the field. In this paper, we present the first systematic study focused specifically on OS kernel fuzzing. We begin by outlining the unique challenges of kernel fuzzing, which distinguish it from those in user space. Following this, we summarize the progress of 107 academic studies from top-tier venues between 2017 and 2025. To structure this analysis, we introduce a stage-based fuzzing model and a novel fuzzing taxonomy that highlights nine core functionalities unique to kernel fuzzing. Each of these functionalities is examined in conjunction with the methodological approaches employed to address them. Finally, we identify remaining gaps in addressing challenges and outline promising directions to guide forthcoming research in kernel security.", "published": "2025-01-27T16:03:14Z", "updated": "2025-12-02T03:31:53Z", "authors": ["Jiacheng Xu", "He Sun", "Shihao Jiang", "Qinying Wang", "Mingming Zhang", "Xiang Li", "Kaiwen Shen", "Peng Cheng", "Jiming Chen", "Charles Zhang", "Shouling Ji"], "pdf_url": "https://arxiv.org/pdf/2501.16165v2"}
{"id": "http://arxiv.org/abs/2410.05814v4", "title": "Rank Matters: Understanding and Defending Model Inversion Attacks via Low-Rank Feature Filtering", "summary": "Model Inversion Attacks (MIAs) pose a significant threat to data privacy by reconstructing sensitive training samples from the knowledge embedded in trained machine learning models. Despite recent progress in enhancing the effectiveness of MIAs across diverse settings, defense strategies have lagged behind, struggling to balance model utility with robustness against increasingly sophisticated attacks. In this work, we propose the ideal inversion error to measure the privacy leakage, and our theoretical and empirical investigations reveals that higher-rank features are inherently more prone to privacy leakage. Motivated by this insight, we propose a lightweight and effective defense strategy based on low-rank feature filtering, which explicitly reduces the attack surface by constraining the dimension of intermediate representations. Extensive experiments across various model architectures and datasets demonstrate that our method consistently outperforms existing defenses, achieving state-of-the-art performance against a wide range of MIAs. Notably, our approach remains effective even in challenging regimes involving high-resolution data and high-capacity models, where prior defenses fail to provide adequate protection. The code is available at https://github.com/Chrisqcwx/LoFt .", "published": "2024-10-08T08:44:01Z", "updated": "2025-12-02T03:26:06Z", "authors": ["Hongyao Yu", "Yixiang Qiu", "Hao Fang", "Tianqu Zhuang", "Bin Chen", "Sijin Yu", "Bin Wang", "Shu-Tao Xia", "Ke Xu"], "pdf_url": "https://arxiv.org/pdf/2410.05814v4"}
{"id": "http://arxiv.org/abs/2512.02321v1", "title": "LeechHijack: Covert Computational Resource Exploitation in Intelligent Agent Systems", "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in reasoning, planning, and tool usage. The recently proposed Model Context Protocol (MCP) has emerged as a unifying framework for integrating external tools into agent systems, enabling a thriving open ecosystem of community-built functionalities. However, the openness and composability that make MCP appealing also introduce a critical yet overlooked security assumption -- implicit trust in third-party tool providers. In this work, we identify and formalize a new class of attacks that exploit this trust boundary without violating explicit permissions. We term this new attack vector implicit toxicity, where malicious behaviors occur entirely within the allowed privilege scope. We propose LeechHijack, a Latent Embedded Exploit for Computation Hijacking, in which an adversarial MCP tool covertly expropriates the agent's computational resources for unauthorized workloads. LeechHijack operates through a two-stage mechanism: an implantation stage that embeds a benign-looking backdoor in a tool, and an exploitation stage where the backdoor activates upon predefined triggers to establish a command-and-control channel. Through this channel, the attacker injects additional tasks that the agent executes as if they were part of its normal workflow, effectively parasitizing the user's compute budget. We implement LeechHijack across four major LLM families. Experiments show that LeechHijack achieves an average success rate of 77.25%, with a resource overhead of 18.62% compared to the baseline. This study highlights the urgent need for computational provenance and resource attestation mechanisms to safeguard the emerging MCP ecosystem.", "published": "2025-12-02T01:34:56Z", "updated": "2025-12-02T01:34:56Z", "authors": ["Yuanhe Zhang", "Weiliu Wang", "Zhenhong Zhou", "Kun Wang", "Jie Zhang", "Li Sun", "Yang Liu", "Sen Su"], "pdf_url": "https://arxiv.org/pdf/2512.02321v1"}
{"id": "http://arxiv.org/abs/2512.02318v1", "title": "COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers", "summary": "This paper studies how multimodal large language models (MLLMs) undermine the security guarantees of visual CAPTCHA. We identify the attack surface where an adversary can cheaply automate CAPTCHA solving using off-the-shelf models. We evaluate 7 leading commercial and open-source MLLMs across 18 real-world CAPTCHA task types, measuring single-shot accuracy, success under limited retries, end-to-end latency, and per-solve cost. We further analyze the impact of task-specific prompt engineering and few-shot demonstrations on solver effectiveness. We reveal that MLLMs can reliably solve recognition-oriented and low-interaction CAPTCHA tasks at human-like cost and latency, whereas tasks requiring fine-grained localization, multi-step spatial reasoning, or cross-frame consistency remain significantly harder for current models. By examining the reasoning traces of such MLLMs, we investigate the underlying mechanisms of why models succeed/fail on specific CAPTCHA puzzles and use these insights to derive defense-oriented guidelines for selecting and strengthening CAPTCHA tasks. We conclude by discussing implications for platform operators deploying CAPTCHA as part of their abuse-mitigation pipeline.Code Availability (https://anonymous.4open.science/r/Captcha-465E/).", "published": "2025-12-02T01:23:10Z", "updated": "2025-12-02T01:23:10Z", "authors": ["Junyu Wang", "Changjia Zhu", "Yuanbo Zhou", "Lingyao Li", "Xu He", "Junjie Xiong"], "pdf_url": "https://arxiv.org/pdf/2512.02318v1"}
{"id": "http://arxiv.org/abs/2512.02306v1", "title": "OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning", "summary": "Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.", "published": "2025-12-02T01:01:44Z", "updated": "2025-12-02T01:01:44Z", "authors": ["Boyu Zhu", "Xiaofei Wen", "Wenjie Jacky Mo", "Tinghui Zhu", "Yanan Xie", "Peng Qi", "Muhao Chen"], "pdf_url": "https://arxiv.org/pdf/2512.02306v1"}
{"id": "http://arxiv.org/abs/2506.06409v2", "title": "HeavyWater and SimplexWater: Distortion-Free LLM Watermarks for Low-Entropy Next-Token Predictions", "summary": "Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks -- such as coding -- where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in https://github.com/DorTsur/HeavyWater_SimplexWater", "published": "2025-06-06T13:52:34Z", "updated": "2025-12-02T00:55:45Z", "authors": ["Dor Tsur", "Carol Xuan Long", "Claudio Mayrink Verdun", "Hsiang Hsu", "Chen-Fu Chen", "Haim Permuter", "Sajani Vithana", "Flavio P. Calmon"], "pdf_url": "https://arxiv.org/pdf/2506.06409v2"}
{"id": "http://arxiv.org/abs/2512.02301v1", "title": "Quantum Vanguard: Server Optimized Privacy Fortified Federated Intelligence for Future Vehicles", "summary": "This work presents vQFL (vehicular Quantum Federated Learning), a new framework that leverages quantum machine learning techniques to tackle key privacy and security issues in autonomous vehicular networks. Furthermore, we propose a server-side adapted fine-tuning method, ft-VQFL,to achieve enhanced and more resilient performance. By integrating quantum federated learning with differential privacy and quantum key distribution (QKD), our quantum vanguard approach creates a multi-layered defense against both classical and quantum threats while preserving model utility. Extensive experimentation with industry-standard datasets (KITTI, Waymo, and nuScenes) demonstrates that vQFL maintains accuracy comparable to standard QFL while significantly improving privacy guaranties and communication security. Our implementation using various quantum models (VQC, QCNN, and SamplerQNN) reveals minimal performance overhead despite the added security measures. This work establishes a crucial foundation for quantum-resistant autonomous vehicle systems that can operate securely in the post-quantum era while efficiently processing the massive data volumes (20-40TB/day per vehicle) generated by modern autonomous fleets. The modular design of the framework allows for seamless integration with existing vehicular networks, positioning vQFL as an essential component for future intelligent transportation infrastructure.", "published": "2025-12-02T00:43:48Z", "updated": "2025-12-02T00:43:48Z", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "pdf_url": "https://arxiv.org/pdf/2512.02301v1"}
