{"id": "http://arxiv.org/abs/2512.17956v2", "title": "Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration", "summary": "Safety alignment can make frontier LMs overly conservative, degrading collaboration via hedging or false refusals. We present a lightweight toolkit with three parts: (1) Victor Calibration (VC), a multi-pass protocol that elicits a scalar confidence proxy T (T0<T1<T2) through iterative evidence re-evaluation; (2) FD-Lite, a behavior-only phenomenology audit with a fixed anchor phrase and a meta-prefix trap to avoid anthropomorphic claims; and (3) CP4.3, a governance stress test for rank invariance and allocation monotonicity (M6). Across Claude 4.5 models (Haiku, Sonnet no-thinking, Sonnet thinking) and Opus, we observe monotonic VC trajectories without violating safety invariants, and stable CP4.3 behavior. (\"Opus\" here refers to a single Claude Opus 4.1 session accessed via a standard UI account, as reported in Table 1.) This work was conducted by a single operator (n=1) and is intended as hypothesis-generating; we explicitly invite replication, critique, and extension by the research community. We include prompt templates and an artifact plan to facilitate independent verification.", "published": "2025-12-18T04:09:22Z", "updated": "2025-12-29T18:07:41Z", "authors": ["Victor Stasiuc"], "pdf_url": "https://arxiv.org/pdf/2512.17956v2"}
{"id": "http://arxiv.org/abs/2512.23605v1", "title": "Parallelized Code Generation from Simulink Models for Event-driven and Timer-driven ROS 2 Nodes", "summary": "In recent years, the complexity and scale of embedded systems, especially in the rapidly developing field of autonomous driving systems, have increased significantly. This has led to the adoption of software and hardware approaches such as Robot Operating System (ROS) 2 and multi-core processors. Traditional manual program parallelization faces challenges, including maintaining data integrity and avoiding concurrency issues such as deadlocks. While model-based development (MBD) automates this process, it encounters difficulties with the integration of modern frameworks such as ROS 2 in multi-input scenarios. This paper proposes an MBD framework to overcome these issues, categorizing ROS 2-compatible Simulink models into event-driven and timer-driven types for targeted parallelization. As a result, it extends the conventional parallelization by MBD and supports parallelized code generation for ROS 2-based models with multiple inputs. The evaluation results show that after applying parallelization with the proposed framework, all patterns show a reduction in execution time, confirming the effectiveness of parallelization.", "published": "2025-12-29T16:59:59Z", "updated": "2025-12-29T16:59:59Z", "authors": ["Kenshin Obi", "Ryo Yoshinaka", "Hiroshi Fujimoto", "Takuya Azumi"], "pdf_url": "https://arxiv.org/pdf/2512.23605v1"}
{"id": "http://arxiv.org/abs/2507.02976v3", "title": "How Safe Are AI-Generated Patches? A Large-scale Study on Security Risks in LLM and Agentic Automated Program Repair on SWE-bench", "summary": "Large language models (LLMs) and their agentic frameworks are increasingly adopted to perform development tasks such as automated program repair (APR). While prior work has identified security risks in LLM-generated code, most have focused on synthetic, simplified, or isolated tasks that lack the complexity of real-world program repair. In this study, we present the first large-scale security analysis of LLM-generated patches using 20,000+ GitHub issues. We evaluate patches proposed by developers, a standalone LLM (Llama 3.3 Instruct-70B), and three top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb). Finally, we analyze a wide range of code, issue, and project-level factors to understand the conditions under which generating insecure patches is more likely. Our findings reveal that Llama introduces many new vulnerabilities, exhibiting unique patterns not found in developers' code. Agentic workflows also generate a number of vulnerabilities, particularly when given more autonomy. We find that vulnerabilities in LLM-generated patches are associated with distinctive code characteristics and are commonly observed in issues missing specific types of information. These results suggest that contextual factors play a critical role in the security of the generated patches and point toward the need for proactive risk assessment methods that account for both issue and code-level information.", "published": "2025-06-30T21:10:19Z", "updated": "2025-12-29T16:44:07Z", "authors": ["Amirali Sajadi", "Kostadin Damevski", "Preetha Chatterjee"], "pdf_url": "https://arxiv.org/pdf/2507.02976v3"}
{"id": "http://arxiv.org/abs/2512.23575v1", "title": "Model-based Development for Autonomous Driving Software Considering Parallelization", "summary": "In recent years, autonomous vehicles have attracted attention as one of the solutions to various social problems. However, autonomous driving software requires real-time performance as it considers a variety of functions and complex environments. Therefore, this paper proposes a parallelization method for autonomous driving software using the Model-Based Development (MBD) process. The proposed method extends the existing Model-Based Parallelizer (MBP) method to facilitate the implementation of complex processing. As a result, execution time was reduced. The evaluation results demonstrate that the proposed method is suitable for the development of autonomous driving software, particularly in achieving real-time performance.", "published": "2025-12-29T16:16:41Z", "updated": "2025-12-29T16:16:41Z", "authors": ["Kenshin Obi", "Takumi Onozawa", "Hiroshi Fujimoto", "Takuya Azumi"], "pdf_url": "https://arxiv.org/pdf/2512.23575v1"}
{"id": "http://arxiv.org/abs/2512.23552v1", "title": "Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction", "summary": "Lock sets are commonly used for dynamic analysis of deadlocks. The standard per-thread lock set construction only considers locks acquired in the same thread, but is unaware of locks acquired in another thread. This leads to false positives and false negatives. The underlying issue is that the commonly used notion of a critical section on which the lock set construction relies ignores events from other threads. We give a trace-based characterization of critical sections that drops this restriction. Critical sections are no longer restricted to a single thread and can cover multiple threads. Such forms of critical sections exist, are natural, and correct the standard formulation.\n  We show how to soundly approximate the trace-based characterization via partial order relations. Thus, we obtain an improved lock set construction that can still be efficiently computed and allows us to remove false positives reported by the DIRK deadlock predictor and remove false negatives by extending the SPDOffline deadlock predictor. We integrate various lock set constructions with increased precision in an extension of SPDOffline. Our extensions remain sound (no false positives) but are more complete (fewer false negatives) w.r.t. SPDOffline. For an extensive standard benchmark suite we can also show that the performance is not affected.", "published": "2025-12-29T15:50:26Z", "updated": "2025-12-29T15:50:26Z", "authors": ["Martin Sulzmann"], "pdf_url": "https://arxiv.org/pdf/2512.23552v1"}
{"id": "http://arxiv.org/abs/2512.23511v1", "title": "Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving", "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning.\n  To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.", "published": "2025-12-29T14:48:15Z", "updated": "2025-12-29T14:48:15Z", "authors": ["Xinyi Zheng", "Ningke Li", "Xiaokun Luan", "Kailong Wang", "Ling Shi", "Meng Sun", "Haoyu Wang"], "pdf_url": "https://arxiv.org/pdf/2512.23511v1"}
{"id": "http://arxiv.org/abs/2512.23499v1", "title": "AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices", "summary": "Modern cloud architectures demand self-adaptive capabilities to manage dynamic operational conditions. Yet, existing solutions often impose centralized control models ill-suited to microservices decentralized nature. This paper presents AdaptiFlow, a framework that leverages well-established principles of autonomous computing to provide abstraction layers focused on the Monitor and Execute phases of the MAPE-K loop. By decoupling metrics collection and action execution from adaptation logic, AdaptiFlow enables microservices to evolve into autonomous elements through standardized interfaces, preserving their architectural independence while enabling system-wide adaptability. The framework introduces: (1) Metrics Collectors for unified infrastructure/business metric gathering, (2) Adaptation Actions as declarative actuators for runtime adjustments, and (3) a lightweight Event-Driven and rule-based mechanism for adaptation logic specification. Validation through the enhanced Adaptable TeaStore benchmark demonstrates practical implementation of three adaptation scenarios targeting three levels of autonomy self-healing (database recovery), self-protection (DDoS mitigation), and self-optimization (traffic management) with minimal code modification per service. Key innovations include a workflow for service instrumentation and evidence that decentralized adaptation can emerge from localized decisions without global coordination. The work bridges autonomic computing theory with cloud-native practice, providing both a conceptual framework and concrete tools for building resilient distributed systems. Future work includes integration with formal coordination models and application of adaptation techniques relying on AI agents for proactive adaptation to address complex adaptation scenarios.", "published": "2025-12-29T14:35:49Z", "updated": "2025-12-29T14:35:49Z", "authors": ["Brice Arléon Zemtsop Ndadji", "Simon Bliudze", "Clément Quinton"], "pdf_url": "https://arxiv.org/pdf/2512.23499v1"}
{"id": "http://arxiv.org/abs/2512.23498v1", "title": "Adaptable Teastore with Energy Consumption Awareness: A Case Study", "summary": "[Context and Motivation] Global energy consumption has been steadily increasing in recent years, with data centers emerging as major contributors. This growth is largely driven by the widespread migration of applications to the Cloud, alongside a rising number of users consuming digital content. Dynamic adaptation (or self-adaptive) approaches appear as a way to reduce, at runtime and under certain constraints, the energy consumption of software applications.\n  [Question/Problem] Despite efforts to make energy-efficiency a primary goal in the dynamic adaptation of software applications, there is still a gap in understanding how to equip these self-adaptive software systems (SAS), which are dynamically adapted at runtime, with effective energy consumption monitoring tools that enable energy-awareness. Furthermore, the extent to which such an energy consumption monitoring tool impacts the overall energy consumption of the SAS ecosystem has not yet been thoroughly explored.\n  [Methodology] To address this gap, we introduce the EnCoMSAS (Energy Consumption Monitoring for Self-Adaptive Systems) tool that allows to gather the energy consumed by distributed software applications deployed, for instance, in the Cloud. EnCoMSAS enables the evaluation of energy consumption of SAS variants at runtime. It allows to integrate energy-efficiency as a main goal in the analysis and execution of new adaptation plans for the SAS. In order to evaluate the effectiveness of EnCoMSAS and investigate its impact on the overall energy consumption of the SAS ecosystem, we conduct an empirical study by using the Adaptable TeaStore case study. Adaptable TeaStore is a self-adaptive extension of the TeaStore application, a microservice benchmarking application. For this study, we focus on the recommender service of Adaptable TeaStore. Regarding the experiments, we first equip Adaptable TeaStore with EnCoMSAS. Next, we execute Adaptable TeaStore by varying workload conditions that simulate users interactions. Finally, we use EnCoMSAS for gathering and assessing the energy consumption of the recommender algorithms of Adaptable TeaStore. To run these experiments, we use nodes of the Grid5000 testbed.\n  [Results] The results show that EnCoMSAS is effective in collecting energy consumption of software applications for enabling dynamic adaptation at runtime. The observed correlation between CPU usage and energy consumption collected by EnCoMSAS provides evidence supporting the validity of the collected energy measurements. Moreover, we point out, through EnCoMSAS, that energy consumption is influenced not only by the algorithmic complexity but also by the characteristics of the deployment environment. Finally, the results show that the impact of EnCoMSAS on the overall energy consumption of the SAS ecosystem is comparatively modest with respect to the entire set of the TeaStore applications microservices.", "published": "2025-12-29T14:35:33Z", "updated": "2025-12-29T14:35:33Z", "authors": ["Henrique De Medeiros", "Denisse Muñante", "Sophie Chabridon", "César Perdigão Batista", "Denis Conan"], "pdf_url": "https://arxiv.org/pdf/2512.23498v1"}
{"id": "http://arxiv.org/abs/2512.23497v1", "title": "Adaptable TeaStore: A Choreographic Approach", "summary": "The Adaptable TeaStore has recently been proposed as a reference model for adaptable microservice architectures. It includes different configurations, as well as scenarios requiring to transition between them. We describe an implementation of the Adaptable TeaStore based on AIOCJ, a choreographic language that allows one to program multiparty systems that can adapt at runtime to different conditions. Following the choreographic tradition, AIOCJ ensures by-construction correctness of communications (e.g., no deadlocks) before, during, and after adaptation. Adaptation is dynamic, and the adaptation scenarios need to be fully specified only at runtime. Using AIOCJ to model the Adaptable TeaStore, we showcase the strengths of the approach and its current limitations, providing suggestions for future directions for refining the paradigm (and the AIOCJ language, in particular), to better align it with real-world Cloud architectures.", "published": "2025-12-29T14:35:18Z", "updated": "2025-12-29T14:35:18Z", "authors": ["Giuseppe De Palma", "Saverio Giallorenzo", "Ivan Lanese", "Gianluigi Zavattaro"], "pdf_url": "https://arxiv.org/pdf/2512.23497v1"}
{"id": "http://arxiv.org/abs/2512.23495v1", "title": "Decoupling Adaptive Control in TeaStore", "summary": "The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.", "published": "2025-12-29T14:34:48Z", "updated": "2025-12-29T14:34:48Z", "authors": ["Eddy Truyen"], "pdf_url": "https://arxiv.org/pdf/2512.23495v1"}
{"id": "http://arxiv.org/abs/2412.16060v2", "title": "Adaptable TeaStore", "summary": "Modern cloud-native systems require adapting dynamically to changing operational conditions, including service outages, traffic surges, and evolving user requirements. While existing benchmarks provide valuable testbeds for performance and scalability evaluation, they lack explicit support for studying adaptation mechanisms, reconfiguration strategies, and graceful degradation. These limitations hinder systematic research on self-adaptive architectures in realistic cloud environments.\n  To cover this gap, we introduce Adaptable TeaStore, an extension of the renowned TeaStore architecture that incorporates adaptability as a first-class design concern. Our extension distinguishes between mandatory and optional services, supports multiple component versions -- with varying resource requirements and functionality levels -- considers the outsourcing of functionalities to external providers, and provides local cache mechanisms for performance and resilience. These features enable the systematic exploration of reconfiguration policies across diverse operational scenarios.\n  We discuss a broad catalogue of reference adaptation scenarios centred around Adaptable TeaStore, useful to evaluate the ability of a given adaptation technology to address conditions such as component unavailability, cyberattacks, provider outages, benign/malicious traffic increases, and user-triggered reconfigurations. Moreover, we present an open-source implementation of the architecture with APIs for metrics collection and adaptation triggers, to enable reproducible experiments.", "published": "2024-12-20T17:06:11Z", "updated": "2025-12-29T14:34:18Z", "authors": ["Simon Bliudze", "Giuseppe De Palma", "Saverio Giallorenzo", "Ivan Lanese", "Gianluigi Zavattaro", "Brice Arléon Zemtsop Ndadji"], "pdf_url": "https://arxiv.org/pdf/2412.16060v2"}
{"id": "http://arxiv.org/abs/2512.23488v1", "title": "Embedding Quality Assurance in project-based learning", "summary": "In this paper, we share our lessons learned from more than a decade of teaching software quality aspects within Software Engineering (SE) courses, where the focus is on Agile/Scrum settings: final year software development projects and the course on SE Project Management. Based on the lessons learned, we also provide a number of recommendations on embedding quality assurance topics in the project-based learning with Agile/Scrum context.", "published": "2025-12-29T14:20:27Z", "updated": "2025-12-29T14:20:27Z", "authors": ["Maria Spichkova"], "pdf_url": "https://arxiv.org/pdf/2512.23488v1"}
{"id": "http://arxiv.org/abs/2512.23415v1", "title": "An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes", "summary": "Kubernetes provides native autoscaling mechanisms, including the Horizontal Pod Autoscaler, Vertical Pod Autoscaler, and node-level autoscalers, to enable elastic resource management for cloud-native applications. However, production environments frequently experience Service Level Objective violations and cost inefficiencies due to reactive scaling behavior, limited use of application-level signals, and opaque control logic. This paper investigates how Kubernetes autoscaling can be enhanced using AIOps principles to jointly satisfy SLO and cost constraints under diverse workload patterns without compromising safety or operational transparency. We present a gap-driven analysis of existing autoscaling approaches and propose a safe and explainable multi-signal autoscaling framework that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. Experimental evaluation using representative microservice and event-driven workloads shows that the proposed approach reduces SLO violation duration by up to 31 percent, improves scaling response time by 24 percent, and lowers infrastructure cost by 18 percent compared to default and tuned Kubernetes autoscaling baselines, while maintaining stable and auditable control behavior. These results demonstrate that AIOps-driven, SLO-first autoscaling can significantly improve the reliability, efficiency, and operational trustworthiness of Kubernetes-based cloud platforms.", "published": "2025-12-29T12:20:46Z", "updated": "2025-12-29T12:20:46Z", "authors": ["Vinoth Punniyamoorthy", "Bikesh Kumar", "Sumit Saha", "Lokesh Butra", "Mayilsamy Palanigounder", "Akash Kumar Agarwal", "Kabilan Kannan"], "pdf_url": "https://arxiv.org/pdf/2512.23415v1"}
{"id": "http://arxiv.org/abs/2512.23385v1", "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?", "summary": "The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.", "published": "2025-12-29T11:22:11Z", "updated": "2025-12-29T11:22:11Z", "authors": ["The Anh Nguyen", "Triet Huynh Minh Le", "M. Ali Babar"], "pdf_url": "https://arxiv.org/pdf/2512.23385v1"}
{"id": "http://arxiv.org/abs/2509.25987v2", "title": "R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning", "summary": "The growing complexity of log data in modern software systems has prompted the use of Large Language Models (LLMs) for automated log analysis. Current approaches typically rely on direct supervised fine-tuning (SFT) on log-label pairs. However, this exacerbates the domain discrepancy between general-purpose LLMs and specialized log data, causing overfitting. Furthermore, SFT's imbalanced loss computation often allows lengthy contexts to overwhelm critical, concise details in model answers, leading to hallucinations. To address these limitations, we propose R-Log, a novel reasoning-based paradigm that mirrors the structured, step-by-step analytical process of human engineers. This approach enhances generalizability by learning the underlying rules behind conclusions. We further employ Reinforcement Learning (RL) to optimize the model within a simulated O&M environment, thereby reducing hallucinations by directly rewarding correct outcomes. R-Log is first cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13 strategies from manual O&M practices, to establish an initial reasoning capability. This ability is then refined via RL using a joint reward function. Empirical evaluations on real-world logs show that R-Log outperforms existing methods across five log analysis tasks, particularly in unseen scenarios (by 228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the efficacy.", "published": "2025-09-30T09:19:31Z", "updated": "2025-12-29T10:05:56Z", "authors": ["Yilun Liu", "Ziang Chen", "Song Xu", "Minggui He", "Shimin Tao", "Weibin Meng", "Yuming Xie", "Tao Han", "Chunguang Zhao", "Jingzhou Du", "Daimeng Wei", "Shenglin Zhang", "Yongqian Sun"], "pdf_url": "https://arxiv.org/pdf/2509.25987v2"}
{"id": "http://arxiv.org/abs/2512.23327v1", "title": "An Empirical Study of Generative AI Adoption in Software Engineering", "summary": "Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.", "published": "2025-12-29T09:24:52Z", "updated": "2025-12-29T09:24:52Z", "authors": ["Görkem Giray", "Onur Demirörs", "Marcos Kalinowski", "Daniel Mendez"], "pdf_url": "https://arxiv.org/pdf/2512.23327v1"}
{"id": "http://arxiv.org/abs/2507.00686v2", "title": "A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT", "summary": "Modern Internet of Things (IoT) systems are equipped with a large quantity of sensors providing real-time data about the current operations of their components, which is crucial for the systems' internal control systems and processes. However, these data are often too fine-grained to derive useful insights into the execution of the larger processes an IoT system might be part of. Process mining has developed advanced approaches for the analysis of business processes that may also be used in the context of IoT. Bringing process mining to IoT requires an event abstraction step to lift the low-level sensor data to the business process level. In this work, we aim to enable domain experts to perform this step using a newly developed domain-specific language (DSL) called Radiant. Radiant supports the specification of patterns within the sensor data that indicate the execution of higher level process activities. These patterns are translated to complex event processing (CEP) applications to be used for detecting activity executions at runtime. We propose a corresponding software architecture that enables online event abstraction from IoT sensor streams using the CEP applications. We evaluate these applications to monitor activity executions in smart manufacturing and smart healthcare. These evaluations are useful to inform the domain expert about the quality of activity detections based on the specified patterns and potential for improvement via additional or modified patterns and sensors.", "published": "2025-07-01T11:38:33Z", "updated": "2025-12-29T08:07:48Z", "authors": ["Ronny Seiger", "Daniel Locher", "Marco Kaufmann", "Aaron F. Kurz"], "pdf_url": "https://arxiv.org/pdf/2507.00686v2"}
{"id": "http://arxiv.org/abs/2512.23214v1", "title": "Anka: A Domain-Specific Language for Reliable LLM Code Generation", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.", "published": "2025-12-29T05:28:17Z", "updated": "2025-12-29T05:28:17Z", "authors": ["Saif Khalfan Saif Al Mazrouei"], "pdf_url": "https://arxiv.org/pdf/2512.23214v1"}
{"id": "http://arxiv.org/abs/2512.23183v1", "title": "LogosQ: A High-Performance and Type-Safe Quantum Computing Library in Rust", "summary": "Developing robust and high performance quantum software is challenging due to the dynamic nature of existing Python-based frameworks, which often suffer from runtime errors and scalability bottlenecks. In this work, we present LogosQ, a high performance backend agnostic quantum computing library implemented in Rust that enforces correctness through compile time type safety. Unlike existing tools, LogosQ leverages Rust static analysis to eliminate entire classes of runtime errors, particularly in parameter-shift rule gradient computations for variational algorithms. We introduce novel optimization techniques, including direct state-vector manipulation, adaptive parallel processing, and an FFT optimized Quantum Fourier Transform, which collectively deliver speedups of up to 900 times for state preparation (QFT) and 2 to 5 times for variational workloads over Python frameworks (PennyLane, Qiskit), 6 to 22 times over Julia implementations (Yao), and competitive performance with Q sharp. Beyond performance, we validate numerical stability through variational quantum eigensolver (VQE) experiments on molecular hydrogen and XYZ Heisenberg models, achieving chemical accuracy even in edge cases where other libraries fail. By combining the safety of systems programming with advanced circuit optimization, LogosQ establishes a new standard for reliable and efficient quantum simulation.", "published": "2025-12-29T03:50:34Z", "updated": "2025-12-29T03:50:34Z", "authors": ["Shiwen An", "Jiayi Wang", "Konstantinos Slavakis"], "pdf_url": "https://arxiv.org/pdf/2512.23183v1"}
{"id": "http://arxiv.org/abs/2306.05499v3", "title": "Prompt Injection attack against LLM-integrated Applications", "summary": "Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.", "published": "2023-06-08T18:43:11Z", "updated": "2025-12-29T02:25:27Z", "authors": ["Yi Liu", "Gelei Deng", "Yuekang Li", "Kailong Wang", "Zihao Wang", "Xiaofeng Wang", "Tianwei Zhang", "Yepang Liu", "Haoyu Wang", "Yan Zheng", "Leo Yu Zhang", "Yang Liu"], "pdf_url": "https://arxiv.org/pdf/2306.05499v3"}
