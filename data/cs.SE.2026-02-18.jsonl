{"id": "http://arxiv.org/abs/2602.16671v1", "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation", "summary": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.", "published": "2026-02-18T18:09:03Z", "updated": "2026-02-18T18:09:03Z", "authors": ["Jaid Monwar Chowdhury", "Chi-An Fu", "Reyhaneh Jabbarvand"], "pdf_url": "https://arxiv.org/pdf/2602.16671v1"}
{"id": "http://arxiv.org/abs/2404.02230v4", "title": "A Mixed-Methods Study on the Implications of Unsafe Rust for Interoperation, Encapsulation, and Tooling", "summary": "The Rust programming language restricts aliasing to provide static safety guarantees. However, in certain situations, developers need to bypass these guarantees by using a set of unsafe features. If they are used incorrectly, these features can reintroduce the types of safety issues that Rust was designed to prevent. We seek to understand how current development tools can be improved to better assist developers who find it necessary to interact with unsafe code. To that end, we study how developers reason about foreign function calls, the limitations of the tools that they currently use, their motivations for using unsafe code, and how they reason about encapsulating it. We conducted a mixed-methods investigation consisting of semi-structured interviews with 19 developers, followed by a survey that reached an additional 160 developers. Our participants were motivated to use unsafe code when they perceived that there was no alternative, and most avoided using it. However, limited tooling support for foreign function calls made participants uncertain about their design choices, and certain foreign aliasing and concurrency patterns were difficult to encapsulate. To overcome these challenges, Rust developers need verification tools that can provide guarantees of soundness within multi-language applications.", "published": "2024-04-02T18:36:21Z", "updated": "2026-02-18T16:36:08Z", "authors": ["Ian McCormack", "Tomas Dougan", "Sam Estep", "Hanan Hibshi", "Jonathan Aldrich", "Joshua Sunshine"], "pdf_url": "https://arxiv.org/pdf/2404.02230v4"}
{"id": "http://arxiv.org/abs/2510.23350v2", "title": "Validating Formal Specifications with LLM-generated Test Cases", "summary": "Validation is a central activity when developing formal specifications. Similarly to coding, a possible validation technique is to define upfront test cases or scenarios that a future specification should satisfy or not. Unfortunately, specifying such test cases is burdensome and error prone, which could cause users to skip this validation task. This paper reports the results of an empirical evaluation of using pre-trained large language models (LLMs) to automate the generation of test cases from natural language requirements. In particular, we focus on test cases for structural requirements of simple domain models formalized in the Alloy specification language. Our evaluation focuses on the state-of-the-art GPT-5 model, but results from other closed- and open-source LLMs are also reported. The results show that, in this context, GPT-5 is already quite effective at generating positive (and negative) test cases that are syntactically correct and that satisfy (or not) the given requirement, and that can detect many wrong specifications written by humans.", "published": "2025-10-27T14:02:20Z", "updated": "2026-02-18T16:04:20Z", "authors": ["Alcino Cunha", "Nuno Macedo"], "pdf_url": "https://arxiv.org/pdf/2510.23350v2"}
{"id": "http://arxiv.org/abs/2509.22237v2", "title": "FeatBench: Towards More Realistic Evaluation of Feature-level Code Generation", "summary": "Evaluating Large Language Models (LLMs) on repository-level feature implementation is a critical frontier in software engineering. However, establishing a benchmark that faithfully mirrors realistic development scenarios remains a significant challenge. Existing feature-level benchmarks generally suffer from two primary limitations: unrealistic task inputs enriched with code hints and significant data leakage risks due to their static nature. To address these limitations, we propose a new benchmark - FeatBench, which introduces the following advances: (1) Realistic Task Inputs. Task inputs consist solely of natural language requirements, strictly devoid of code hints (e.g., function signatures). This format mirrors realistic software development by requiring agents to independently bridge the gap between abstract user intent and concrete code changes. (2) Evolving Data. FeatBench employs a fully automated pipeline to construct new benchmark versions from the latest repositories, effectively mitigating data contamination. The initial release comprises 157 tasks sourced from 27 actively maintained repositories. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. The results reveal that FeatBench poses a significant challenge, with the highest resolved rate reaching only 29.94%. Crucially, our analysis uncovers a prevalent behavioral pattern of aggressive implementation, which leads to \"scope creep\" and widespread regressions where agents break existing features by diverging from the user's explicit intent. We release FeatBench, our automated pipeline, and all experimental results to facilitate further community research.", "published": "2025-09-26T11:47:50Z", "updated": "2026-02-18T15:49:02Z", "authors": ["Haorui Chen", "Chengze Li", "Jia Li"], "pdf_url": "https://arxiv.org/pdf/2509.22237v2"}
{"id": "http://arxiv.org/abs/2602.16499v1", "title": "Software-heavy Asset Administration Shells: Classification and Use Cases", "summary": "The Asset Administration Shell (AAS) is an emerging technology for the implementation of digital twins in the field of manufacturing. Software is becoming increasingly important, not only in general but specifically in relation to manufacturing, especially with regard to digital manufacturing and a shift towards the usage of artificial intelligence. This increases the need not only to model software, but also to integrate services directly into the AAS. The existing literature contains individual solutions to implement such software-heavy AAS. However, there is no systematic analysis of software architectures that integrate software services directly into the AAS. This paper aims to fill this research gap and differentiate architectures based on software quality criteria as well as typical manufacturing use cases. This work may be considered as an interpretation guideline for software-heavy AAS, both in academia and for practitioners.", "published": "2026-02-18T14:42:04Z", "updated": "2026-02-18T14:42:04Z", "authors": ["Carsten Ellwein", "David Dietrich", "Jessica Roth", "Rozana Cvitkovic", "Andreas Wortmann"], "pdf_url": "https://arxiv.org/pdf/2602.16499v1"}
{"id": "http://arxiv.org/abs/2512.20334v2", "title": "Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation", "summary": "With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.\n  This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.", "published": "2025-12-23T13:08:19Z", "updated": "2026-02-18T13:15:20Z", "authors": ["Yuan Huang", "Yukang Zhou", "Xiangping Chen", "Zibin Zheng"], "pdf_url": "https://arxiv.org/pdf/2512.20334v2"}
{"id": "http://arxiv.org/abs/2602.02050v2", "title": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents", "summary": "Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.", "published": "2026-02-02T12:52:14Z", "updated": "2026-02-18T09:53:43Z", "authors": ["Zeping Li", "Hongru Wang", "Yiwen Zhao", "Guanhua Chen", "Yixia Li", "Keyang Chen", "Yixin Cao", "Guangnan Ye", "Hongfeng Chai", "Zhenfei Yin"], "pdf_url": "https://arxiv.org/pdf/2602.02050v2"}
{"id": "http://arxiv.org/abs/2602.16304v1", "title": "Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification", "summary": "The prevalence of malicious packages in open-source repositories, such as PyPI, poses a critical threat to the software supply chain. While Large Language Models (LLMs) have emerged as a promising tool for automated security tasks, their effectiveness in detecting malicious packages and indicators remains underexplored. This paper presents a systematic evaluation of 13 LLMs for detecting malicious software packages. Using a curated dataset of 4,070 packages (3,700 benign and 370 malicious), we evaluate model performance across two tasks: binary classification (package detection) and multi-label classification (identification of specific malicious indicators). We further investigate the impact of prompting strategies, temperature settings, and model specifications on detection accuracy. We find a significant \"granularity gap\" in LLMs' capabilities. While GPT-4.1 achieves near-perfect performance in binary detection (F1 $\\approx$ 0.99), performance degrades by approximately 41\\% when the task shifts to identifying specific malicious indicators. We observe that general models are best for filtering out the majority of threats, while specialized coder models are better at detecting attacks that follow a strict, predictable code structure. Our correlation analysis indicates that parameter size and context width have negligible explanatory power regarding detection accuracy. We conclude that while LLMs are powerful detectors at the package level, they lack the semantic depth required for precise identification at the granular indicator level.", "published": "2026-02-18T09:36:46Z", "updated": "2026-02-18T09:36:46Z", "authors": ["Ahmed Ryan", "Ibrahim Khalil", "Abdullah Al Jahid", "Md Erfan", "Akond Ashfaque Ur Rahman", "Md Rayhanur Rahman"], "pdf_url": "https://arxiv.org/pdf/2602.16304v1"}
{"id": "http://arxiv.org/abs/2602.16291v1", "title": "A Calculus of Overlays", "summary": "Just as the $λ$-calculus uses three primitives (abstraction, application, variable) as the foundation of functional programming, Overlay-Calculus uses three primitives (record, definition, inheritance) as the foundation of declarative programming. It trivially embeds the $λ$-calculus, although the entire semantics builds on only naive set theory; as a consequence, all constructs including inheritance are inherently commutative, idempotent, and associative; the linearization problem of multiple inheritance simply does not arise. This induces a fully abstract semantics of the lazy $λ$-calculus with respect to Böhm tree equivalence. Overlay-Calculus is distilled from the Overlay language, a practical implementation in which we observed further emergent phenomena: the Expression Problem dissolves, programs are CPS-agnostic, records natively encode random-access memory, and self-reference resolves to multiple targets. These properties suggest applications to configuration languages, dependency injection, object-oriented programming, composable effect systems, modular software architectures, file-system-as-compiler, general-purpose programming, and no-code development.", "published": "2026-02-18T09:17:20Z", "updated": "2026-02-18T09:17:20Z", "authors": ["Bo Yang"], "pdf_url": "https://arxiv.org/pdf/2602.16291v1"}
{"id": "http://arxiv.org/abs/2602.15485v2", "title": "SecCodeBench-V2 Technical Report", "summary": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and JavaScript. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.", "published": "2026-02-17T10:47:06Z", "updated": "2026-02-18T08:08:18Z", "authors": ["Longfei Chen", "Ji Zhao", "Lanxiao Cui", "Tong Su", "Xingbo Pan", "Ziyang Li", "Yongxing Wu", "Qijiang Cao", "Qiyao Cai", "Jing Zhang", "Yuandong Ni", "Junyao He", "Zeyu Zhang", "Chao Ge", "Xuhuai Lu", "Zeyu Gao", "Yuxin Cui", "Weisen Chen", "Yuxuan Peng", "Shengping Wang", "Qi Li", "Yukai Huang", "Yukun Liu", "Tuo Zhou", "Terry Yue Zhuo", "Junyang Lin", "Chao Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.15485v2"}
{"id": "http://arxiv.org/abs/2505.03901v4", "title": "Unveiling the Role of ChatGPT in Software Development: Insights from Developer-ChatGPT Interactions on GitHub", "summary": "The advent of Large Language Models (LLMs) has introduced a new paradigm in Software Engineering (SE), with generative AI tools like ChatGPT gaining widespread adoption among developers. While ChatGPT's potential has been extensively discussed, empirical evidence about how developers actually use LLMs' assistance in real-world practices remains limited. To bridge this gap, we conducted a large-scale empirical analysis of ChatGPT usage on GitHub, and we presented DevChat, a curated dataset of 2,547 publicly shared ChatGPT conversation links collected from GitHub between May 2023 and June 2024. Through comprehensively analyzing DevChat, we explored the characteristics of developer-ChatGPT interaction patterns and identified five key categories of developers' purposes for sharing developer-ChatGPT conversations during software development. Additionally, we investigated the dominant development-related activities in which ChatGPT is used, and presented a mapping framework that links GitHub data sources, development-related activities, and SE tasks. The findings show that interactions are typically short and task-focused (most are 1-3 turns); developers share conversations mainly to delegate tasks, resolve problems, and acquire knowledge, revealing five purpose categories; ChatGPT is most frequently engaged for Software Implementation and Maintenance & Evolution; we identified 39 fine-grained SE tasks supported by ChatGPT, with Code Generation & Completion as well as Code modification & Optimization being the most prominent. Our study offers a comprehensive mapping of ChatGPT's applications in real-world software development scenarios and provides a foundation for understanding LLMs' practical roles in software development.", "published": "2025-05-06T18:16:08Z", "updated": "2026-02-18T07:41:38Z", "authors": ["Ruiyin Li", "Peng Liang", "Yifei Wang", "Yangxiao Cai", "Weisong Sun", "Zengyang Li"], "pdf_url": "https://arxiv.org/pdf/2505.03901v4"}
{"id": "http://arxiv.org/abs/2509.06085v2", "title": "Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects", "summary": "Pre-trained models (PTMs) are machine learning models that have been trained in advance, often on large-scale data, and can be reused for new tasks, thereby reducing the need for costly training from scratch. Their widespread adoption introduces a new class of software dependency, which we term Software Dependencies 2.0, extending beyond conventional libraries to learned behaviors embodied in trained models and their associated artifacts. The integration of PTMs as software dependencies in real projects remains unclear, potentially threatening maintainability and reliability of modern software systems that increasingly rely on them. Objective: In this study, we investigate Software Dependencies 2.0 in open-source software (OSS) projects by examining the reuse of PTMs, with a focus on how developers manage and integrate these models. Specifically, we seek to understand: (1) how OSS projects structure and document their PTM dependencies; (2) what stages and organizational patterns emerge in the reuse pipelines of PTMs within these projects; and (3) the interactions among PTMs and other learned components across pipeline stages. We conduct a mixed-methods analysis of a statistically significant random sample of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM reuse by identifying patterns and qualitatively investigate how developers integrate and manage these models in practice.", "published": "2025-09-07T15:00:22Z", "updated": "2026-02-18T06:19:25Z", "authors": ["Jerin Yasmin", "Wenxin Jiang", "James C. Davis", "Yuan Tian"], "pdf_url": "https://arxiv.org/pdf/2509.06085v2"}
{"id": "http://arxiv.org/abs/2601.01569v2", "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators", "summary": "LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms that struggle with long-horizon tasks due to fragile multi-turn dependencies and context drift. We present CaveAgent, a framework that shifts tool use from ``LLM-as-Text-Generator'' to ``LLM-as-Runtime-Operator.'' CaveAgent introduces a dual-stream architecture that inverts the conventional paradigm: rather than treating the LLM's text context as the primary workspace with tools as auxiliary, CaveAgent elevates the persistent Python runtime as the central locus of state, with a lightweight semantic stream serving as its orchestrator. Beyond leveraging code generation to resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, CaveAgent introduces \\textit{Stateful Runtime Management}: it injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns, unlike existing code-based approaches that remain text-bound. CaveAgent further provides a runtime-integrated skill management system that extends the Agent Skills open standard, enabling ecosystem interoperability through executable skill injections. This persistence mechanism serves as a high-fidelity external memory that reduces context drift in multi-turn interactions and preserves processed data for downstream applications without information loss. Evaluations show consistent improvement across challenging benchmarks, enabling CaveAgent to handle data scales that cause context overflow in both JSON-based and code-based agents. The accessible runtime state further provides programmatically verifiable feedback, enabling automated evaluation and reward signal generation without human annotation and establishing a structural foundation for future research in Reinforcement Learning with Verifiable Rewards (RLVR).", "published": "2026-01-04T15:32:47Z", "updated": "2026-02-18T06:16:21Z", "authors": ["Maohao Ran", "Zhenglin Wan", "Cooper Lin", "Yanting Zhang", "Hongyu Xin", "Hongwei Fan", "Yibo Xu", "Beier Luo", "Yaxin Zhou", "Wangbo Zhao", "Lijie Yang", "Lang Feng", "Fuchao Yang", "Jingxuan Wu", "Yiqiao Huang", "Chendong Ma", "Dailing Jiang", "Jianbo Deng", "Sihui Han", "Yang You", "Bo An", "Yike Guo", "Jun Song"], "pdf_url": "https://arxiv.org/pdf/2601.01569v2"}
{"id": "http://arxiv.org/abs/2602.08242v4", "title": "Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications", "summary": "Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.", "published": "2026-02-09T03:39:45Z", "updated": "2026-02-18T04:00:37Z", "authors": ["Ali Hassaan Mughal", "Muhammad Bilal", "Noor Fatima"], "pdf_url": "https://arxiv.org/pdf/2602.08242v4"}
{"id": "http://arxiv.org/abs/2602.16106v1", "title": "Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs", "summary": "Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.", "published": "2026-02-18T00:34:29Z", "updated": "2026-02-18T00:34:29Z", "authors": ["Shahriar Rumi Dipto", "Saikat Mondal", "Chanchal K. Roy"], "pdf_url": "https://arxiv.org/pdf/2602.16106v1"}
