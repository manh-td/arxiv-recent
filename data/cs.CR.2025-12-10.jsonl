{"id": "http://arxiv.org/abs/2509.06796v2", "title": "Imitative Membership Inference Attack", "summary": "A Membership Inference Attack (MIA) assesses how much a target machine learning model reveals about its training data by determining whether specific query instances were part of the training set. State-of-the-art MIAs rely on training hundreds of shadow models that are independent of the target model, leading to significant computational overhead. In this paper, we introduce Imitative Membership Inference Attack (IMIA), which employs a novel imitative training technique to strategically construct a small number of target-informed imitative models that closely replicate the target model's behavior for inference. Extensive experimental results demonstrate that IMIA substantially outperforms existing MIAs in various attack settings while only requiring less than 5% of the computational cost of state-of-the-art approaches.", "published": "2025-09-08T15:27:35Z", "updated": "2025-12-10T18:42:08Z", "authors": ["Yuntao Du", "Yuetian Chen", "Hanshen Xiao", "Bruno Ribeiro", "Ninghui Li"], "pdf_url": "https://arxiv.org/pdf/2509.06796v2"}
{"id": "http://arxiv.org/abs/2512.09883v1", "title": "ByteShield: Adversarially Robust End-to-End Malware Detection through Byte Masking", "summary": "Research has proven that end-to-end malware detectors are vulnerable to adversarial attacks. In response, the research community has proposed defenses based on randomized and (de)randomized smoothing. However, these techniques remain susceptible to attacks that insert large adversarial payloads. To address these limitations, we propose a novel defense mechanism designed to harden end-to-end malware detectors by leveraging masking at the byte level. This mechanism operates by generating multiple masked versions of the input file, independently classifying each version, and then applying a threshold-based voting mechanism to produce the final classification. Key to this defense is a deterministic masking strategy that systematically strides a mask across the entire input file. Unlike randomized smoothing defenses, which randomly mask or delete bytes, this structured approach ensures coverage of the file over successive versions. In the best-case scenario, this strategy fully occludes the adversarial payload, effectively neutralizing its influence on the model's decision. In the worst-case scenario, it partially occludes the adversarial payload, reducing its impact on the model's predictions. By occluding the adversarial payload in one or more masked versions, this defense ensures that some input versions remain representative of the file's original intent, allowing the voting mechanism to suppress the influence of the adversarial payload. Results achieved on the EMBER and BODMAS datasets demonstrate the suitability of our defense, outperforming randomized and (de)randomized smoothing defenses against adversarial examples generated with a wide range of functionality-preserving manipulations while maintaining high accuracy on clean examples.", "published": "2025-12-10T18:13:28Z", "updated": "2025-12-10T18:13:28Z", "authors": ["Daniel Gibert", "Felip Manyà"], "pdf_url": "https://arxiv.org/pdf/2512.09883v1"}
{"id": "http://arxiv.org/abs/2512.09882v1", "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing", "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.", "published": "2025-12-10T18:12:29Z", "updated": "2025-12-10T18:12:29Z", "authors": ["Justin W. Lin", "Eliot Krzysztof Jones", "Donovan Julian Jasper", "Ethan Jun-shen Ho", "Anna Wu", "Arnold Tianyi Yang", "Neil Perry", "Andy Zou", "Matt Fredrikson", "J. Zico Kolter", "Percy Liang", "Dan Boneh", "Daniel E. Ho"], "pdf_url": "https://arxiv.org/pdf/2512.09882v1"}
{"id": "http://arxiv.org/abs/2512.09872v1", "title": "FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning", "summary": "Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.", "published": "2025-12-10T17:58:18Z", "updated": "2025-12-10T17:58:18Z", "authors": ["Khurram Khalil", "Khaza Anuarul Hoque"], "pdf_url": "https://arxiv.org/pdf/2512.09872v1"}
{"id": "http://arxiv.org/abs/2512.09862v1", "title": "True Random Number Generators on IQM Spark", "summary": "Random number generation is fundamental for many modern applications including cryptography, simulations and machine learning. Traditional pseudo-random numbers may offer statistical unpredictability, but are ultimately deterministic. On the other hand, True Random Number Generation (TRNG) offers true randomness. One way of obtaining such randomness are quantum systems, including quantum computers. As such the use of quantum computers for TRNG has received considerable attention in recent years. However, existing studies almost exclusively consider IBM quantum computers, often stop at using simulations and usually test only a handful of different TRNG quantum circuits. In this paper, we address those issues by presenting a study of TRNG circuits on Odra 5 a real-life quantum computer installed at Wrocław University of Science and Technology. It is also the first study to utilize the IQM superconducting architecture. Since Odra 5 is available on-premises it allows for much more comprehensive study of various TRNG circuits. In particular, we consider 5 types of TRNG circuits with 105 circuit subvariants in total. Each circuit is used to generate 1 million bits. We then perform an analysis of the quality of the obtained random sequences using the NIST SP 800-22 and NIST SP 800-90B test suites. We also provide a comprehensive review of existing literature on quantum computer-based TRNGs.", "published": "2025-12-10T17:48:41Z", "updated": "2025-12-10T17:48:41Z", "authors": ["Andrzej Gnatowski", "Jarosław Rudy", "Teodor Niżyński", "Krzysztof Święcicki"], "pdf_url": "https://arxiv.org/pdf/2512.09862v1"}
{"id": "http://arxiv.org/abs/2510.18109v2", "title": "PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces", "summary": "Evaluating the usefulness of data before purchase is essential when obtaining data for high-quality machine learning models, yet both model builders and data providers are often unwilling to reveal their proprietary assets.\n  We present PrivaDE, a privacy-preserving protocol that allows a model owner and a data owner to jointly compute a utility score for a candidate dataset without fully exposing model parameters, raw features, or labels. PrivaDE provides strong security against malicious behavior and can be integrated into blockchain-based marketplaces, where smart contracts enforce fair execution and payment. To make the protocol practical, we propose optimizations to enable efficient secure model inference, and a model-agnostic scoring method that uses only a small, representative subset of the data while still reflecting its impact on downstream training. Evaluation shows that PrivaDE performs data evaluation effectively, achieving online runtimes within 15 minutes even for models with millions of parameters.\n  Our work lays the foundation for fair and automated data marketplaces in decentralized machine learning ecosystems.", "published": "2025-10-20T21:14:32Z", "updated": "2025-12-10T16:38:27Z", "authors": ["Wan Ki Wong", "Sahel Torkamani", "Michele Ciampi", "Rik Sarkar"], "pdf_url": "https://arxiv.org/pdf/2510.18109v2"}
{"id": "http://arxiv.org/abs/2508.04340v2", "title": "Riemann-Roch bases for arbitrary elliptic curve divisors and their application in cryptography", "summary": "This paper presents explicit constructions of bases for Riemann-Roch spaces associated with arbitrary divisors on elliptic curves. In the context of algebraic geometry codes, the knowledge of an explicit basis for arbitrary divisors is especially valuable, as it enables efficient code construction. From a cryptographic point of view, codes associated with arbitrary divisors with many points are closer to Goppa codes, making them attractive for embedding in the McEliece cryptosystem. Using the results obtained in this work, it is also possible to efficiently construct quasi-cyclic subfield subcodes of elliptic codes. These codes enable a significant reduction in public key size for the McEliece cryptosystem and, consequently, represent promising candidates for integration into post-quantum code-based schemes.", "published": "2025-08-06T11:34:05Z", "updated": "2025-12-10T16:08:11Z", "authors": ["Artyom Kuninets", "Ekaterina Malygina"], "pdf_url": "https://arxiv.org/pdf/2508.04340v2"}
{"id": "http://arxiv.org/abs/2510.23847v2", "title": "EthVault: A Secure and Resource-Conscious FPGA-Based Ethereum Cold Wallet", "summary": "Cryptocurrency blockchain networks safeguard digital assets using cryptographic keys, with wallets playing a critical role in generating, storing, and managing these keys. Wallets, typically categorized as hot and cold, offer varying degrees of security and convenience. However, they are generally software-based applications running on microcontrollers. Consequently, they are vulnerable to malware and side-channel attacks, allowing perpetrators to extract private keys by targeting critical algorithms, such as ECC, which processes private keys to generate public keys and authorize transactions. To address these issues, this work presents EthVault, the first hardware architecture for an Ethereum hierarchically deterministic cold wallet, featuring hardware implementations of key algorithms for secure key generation. Also, an ECC architecture resilient to side-channel and timing attacks is proposed. Moreover, an architecture of the child key derivation function, a fundamental component of cryptocurrency wallets, is proposed. The design minimizes resource usage, meeting market demand for small, portable cryptocurrency wallets. FPGA implementation results validate the feasibility of the proposed approach. The ECC architecture exhibits uniform execution behavior across varying inputs, while the complete design utilizes only 27%, 7%, and 6% of LUTs, registers, and RAM blocks, respectively, on a Xilinx Zynq UltraScale+ FPGA", "published": "2025-10-27T20:35:21Z", "updated": "2025-12-10T16:02:17Z", "authors": ["Joel Poncha Lemayian", "Ghyslain Gagnon", "Kaiwen Zhang", "Pascal Giard"], "pdf_url": "https://arxiv.org/pdf/2510.23847v2"}
{"id": "http://arxiv.org/abs/2512.09769v1", "title": "Defining Cost Function of Steganography with Large Language Models", "summary": "In this paper, we make the first attempt towards defining cost function of steganography with large language models (LLMs), which is totally different from previous works that rely heavily on expert knowledge or require large-scale datasets for cost learning. To achieve this goal, a two-stage strategy combining LLM-guided program synthesis with evolutionary search is applied in the proposed method. In the first stage, a certain number of cost functions in the form of computer program are synthesized from LLM responses to structured prompts. These cost functions are then evaluated with pretrained steganalysis models so that candidate cost functions suited to steganography can be collected. In the second stage, by retraining a steganalysis model for each candidate cost function, the optimal cost function(s) can be determined according to the detection accuracy. This two-stage strategy is performed by an iterative fashion so that the best cost function can be collected at the last iteration. Experiments show that the proposed method enables LLMs to design new cost functions of steganography that significantly outperform existing works in terms of resisting steganalysis tools, which verifies the superiority of the proposed method. To the best knowledge of the authors, this is the first work applying LLMs to the design of advanced cost function of steganography, which presents a novel perspective for steganography design and may shed light on further research.", "published": "2025-12-10T15:52:44Z", "updated": "2025-12-10T15:52:44Z", "authors": ["Hanzhou Wu", "Yige Wang"], "pdf_url": "https://arxiv.org/pdf/2512.09769v1"}
{"id": "http://arxiv.org/abs/2512.09742v1", "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs", "summary": "LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. \"Q: Favorite music? A: Wagner\"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.", "published": "2025-12-10T15:21:41Z", "updated": "2025-12-10T15:21:41Z", "authors": ["Jan Betley", "Jorio Cocola", "Dylan Feng", "James Chua", "Andy Arditi", "Anna Sztyber-Betley", "Owain Evans"], "pdf_url": "https://arxiv.org/pdf/2512.09742v1"}
{"id": "http://arxiv.org/abs/2512.09699v1", "title": "Device Independent Quantum Secret Sharing Using Multiparty Pseudo-telepathy Game", "summary": "Device-independent quantum secret sharing (DI-QSS) is a cryptographic protocol that overcomes the security limitations posed by untrusted quantum devices. We propose a DI-QSS protocol based on the multipartite pseudo-telepathy parity game, which achieves device-independence with simultaneous key generation without requiring dedicated test rounds, unlike CHSH-based schemes [Zhang et al., Phys. Rev. A, 2024]. Notably, the proposed scheme allows simultaneous device-independence verification and key-generation phases, achieving optimal performance for a seven-qubit GHZ state configuration. Further, we analyse the security of our protocol against collective attack and establish reduced resource requirement for the same length of the raw key compared to the previous protocol. Finally, we show that our protocol remains robust even in a noisy environment.", "published": "2025-12-10T14:46:43Z", "updated": "2025-12-10T14:46:43Z", "authors": ["Santanu Majhi", "Goutam Paul"], "pdf_url": "https://arxiv.org/pdf/2512.09699v1"}
{"id": "http://arxiv.org/abs/2509.02083v2", "title": "Performance analysis of common browser extensions for cryptojacking detection", "summary": "This paper considers five extensions for Chromium-based browsers in order to determine how effective can browser-based defenses against cryptojacking available to regular users be. We've examined most popular extensions - MinerBlock, AdGuard AdBlocker, Easy Redirect && Prevent Cryptojacking, CoinEater and Miners Shield, which claim to be designed specifically to identify and stop illegal cryptocurrency mining. An empirically confirmed dataset of 373 distinct cryptojacking-infected websites which was assembled during multi-stage procedure, was used to test those extensions. The results showed that all plugins in question had significant performance limits. Easy Redirect and Miners Shield only blocked 6 and 5 websites respectively, while MinerBlock had the greatest detection rate at only 27% (101/373 sites blocked). Most concerningly, despite promises of cryptojacking prevention, AdGuard (which has over 13 million users) and CoinEater were unable to identify any of the compromised websites. These results demonstrate serious flaws in cryptojacking detection products targeted for regular users, since even the best-performing specimen failed to detect 73% of attacks. The obvious difference between advertised capabilities and real performance highlights the urgent need for either accessibility improvements for laboratory-grade detection technologies that show 90%+ efficiency in controlled environment or fundamental upgrades to current commonly used extensions.", "published": "2025-09-02T08:33:45Z", "updated": "2025-12-10T14:19:25Z", "authors": ["Dmitry Tanana"], "pdf_url": "https://arxiv.org/pdf/2509.02083v2"}
{"id": "http://arxiv.org/abs/2510.09775v2", "title": "A Generic Machine Learning Framework for Radio Frequency Fingerprinting", "summary": "Fingerprinting radio frequency (RF) emitters typically involves finding unique characteristics that are featured in their received signal. These fingerprints are nuanced, but sufficiently detailed, motivating the pursuit of methods that can successfully extract them. The downstream task that requires the most meticulous RF fingerprinting (RFF) is known as specific emitter identification (SEI), which entails recognising each individual transmitter. RFF and SEI have a long history, with numerous defence and civilian applications such as signal intelligence, electronic surveillance, physical-layer authentication of wireless devices, to name a few. In recent years, data-driven RFF approaches have become popular due to their ability to automatically learn intricate fingerprints. They generally deliver superior performance when compared to traditional RFF techniques that are often labour-intensive, inflexible, and only applicable to a particular emitter type or transmission scheme. In this paper, we present a generic and versatile machine learning (ML) framework for data-driven RFF with several popular downstream tasks such as SEI, data association (EDA) and RF emitter clustering (RFEC). It is emitter-type agnostic. We then demonstrate the introduced framework for several tasks using real RF datasets for spaceborne surveillance, signal intelligence and countering drones applications.", "published": "2025-10-10T18:34:07Z", "updated": "2025-12-10T12:37:09Z", "authors": ["Alex Hiles", "Bashar I. Ahmad"], "pdf_url": "https://arxiv.org/pdf/2510.09775v2"}
{"id": "http://arxiv.org/abs/2512.09549v1", "title": "Chasing Shadows: Pitfalls in LLM Security Research", "summary": "Large language models (LLMs) are increasingly prevalent in security research. Their unique characteristics, however, introduce challenges that undermine established paradigms of reproducibility, rigor, and evaluation. Prior work has identified common pitfalls in traditional machine learning research, but these studies predate the advent of LLMs. In this paper, we identify \\emph{nine} common pitfalls that have become (more) relevant with the emergence of LLMs and that can compromise the validity of research involving them. These pitfalls span the entire computation process, from data collection, pre-training, and fine-tuning to prompting and evaluation.\n  We assess the prevalence of these pitfalls across all 72 peer-reviewed papers published at leading Security and Software Engineering venues between 2023 and 2024. We find that every paper contains at least one pitfall, and each pitfall appears in multiple papers. Yet only 15.7\\% of the present pitfalls were explicitly discussed, suggesting that the majority remain unrecognized. To understand their practical impact, we conduct four empirical case studies showing how individual pitfalls can mislead evaluation, inflate performance, or impair reproducibility. Based on our findings, we offer actionable guidelines to support the community in future work.", "published": "2025-12-10T11:39:09Z", "updated": "2025-12-10T11:39:09Z", "authors": ["Jonathan Evertz", "Niklas Risse", "Nicolai Neuer", "Andreas Müller", "Philipp Normann", "Gaetano Sapia", "Srishti Gupta", "David Pape", "Soumya Shaw", "Devansh Srivastav", "Christian Wressnegger", "Erwin Quiring", "Thorsten Eisenhofer", "Daniel Arp", "Lea Schönherr"], "pdf_url": "https://arxiv.org/pdf/2512.09549v1"}
{"id": "http://arxiv.org/abs/2512.09539v1", "title": "Comparative Analysis of Hash-based Malware Clustering via K-Means", "summary": "With the adoption of multiple digital devices in everyday life, the cyber-attack surface has increased. Adversaries are continuously exploring new avenues to exploit them and deploy malware. On the other hand, detection approaches typically employ hashing-based algorithms such as SSDeep, TLSH, and IMPHash to capture structural and behavioural similarities among binaries. This work focuses on the analysis and evaluation of these techniques for clustering malware samples using the K-means algorithm. More specifically, we experimented with established malware families and traits and found that TLSH and IMPHash produce more distinct, semantically meaningful clusters, whereas SSDeep is more efficient for broader classification tasks. The findings of this work can guide the development of more robust threat-detection mechanisms and adaptive security mechanisms.", "published": "2025-12-10T11:24:35Z", "updated": "2025-12-10T11:24:35Z", "authors": ["Aink Acrie Soe Thein", "Nikolaos Pitropakis", "Pavlos Papadopoulos", "Sam Grierson", "Sana Ullah Jan"], "pdf_url": "https://arxiv.org/pdf/2512.09539v1"}
{"id": "http://arxiv.org/abs/2512.09485v1", "title": "Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks", "summary": "Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications.", "published": "2025-12-10T10:04:11Z", "updated": "2025-12-10T10:04:11Z", "authors": ["Xinye Cao", "Yihan Lin", "Guoshun Nan", "Qinchuan Zhou", "Yuhang Luo", "Yurui Gao", "Zeliang Zhang", "Haolang Lu", "Qimei Cui", "Yanzhao Hou", "Xiaofeng Tao", "Tony Q. S. Quek"], "pdf_url": "https://arxiv.org/pdf/2512.09485v1"}
{"id": "http://arxiv.org/abs/2311.08769v3", "title": "adF: A Novel System for Measuring Web Fingerprinting through Ads", "summary": "This paper introduces adF, a novel system for analyzing the vulnerability of different devices, Operating Systems (OSes), and browsers to web fingerprinting. adF performs its measurements from code inserted in ads. We have used our system in several ad campaigns that delivered 5.40 million ad impressions. The collected data allow us to assess the vulnerability of current desktop and mobile devices to web fingerprinting. Based on our results, we estimate that 66% of desktop devices and 40% of mobile devices can be uniquely fingerprinted with our web fingerprinting system. However, the resilience to web fingerprinting varies significantly across browsers and device types, with Chrome on desktops being the most vulnerable configuration.\n  To counter web fingerprinting, we propose ShieldF, a simple solution which blocks the reporting by browsers of those attributes that we found in the analysis of our dataset that present the most significant discrimination power. Our experiments reveal that ShieldF outperforms all anti-fingerprinting solutions proposed by major browsers (Chrome, Safari and Firefox) offering an increase in the resilience offered to web fingerprinting up to 62% for some device configurations. ShieldF is available as an add-on for any chromium-based browser. Moreover, it is readily adoptable by browser and mobile app developers. Its widespread use would lead to a significant improvement in the protection offered by browsers and mobile apps to web fingerprinting.", "published": "2023-11-15T08:30:50Z", "updated": "2025-12-10T09:29:02Z", "authors": ["Miguel A. Bermejo-Agueda", "Patricia Callejo", "Rubén Cuevas", "Ángel Cuevas"], "pdf_url": "https://arxiv.org/pdf/2311.08769v3"}
{"id": "http://arxiv.org/abs/2512.09442v1", "title": "Reference Recommendation based Membership Inference Attack against Hybrid-based Recommender Systems", "summary": "Recommender systems have been widely deployed across various domains such as e-commerce and social media, and intelligently suggest items like products and potential friends to users based on their preferences and interaction history, which are often privacy-sensitive. Recent studies have revealed that recommender systems are prone to membership inference attacks (MIAs), where an attacker aims to infer whether or not a user's data has been used for training a target recommender system. However, existing MIAs fail to exploit the unique characteristic of recommender systems, and therefore are only applicable to mixed recommender systems consisting of two recommendation algorithms. This leaves a gap in investigating MIAs against hybrid-based recommender systems where the same algorithm utilizing user-item historical interactions and attributes of users and items serves and produces personalised recommendations. To investigate how the personalisation in hybrid-based recommender systems influences MIA, we propose a novel metric-based MIA. Specifically, we leverage the characteristic of personalisation to obtain reference recommendation for any target users. Then, a relative membership metric is proposed to exploit a target user's historical interactions, target recommendation, and reference recommendation to infer the membership of the target user's data. Finally, we theoretically and empirically demonstrate the efficacy of the proposed metric-based MIA on hybrid-based recommender systems.", "published": "2025-12-10T09:14:15Z", "updated": "2025-12-10T09:14:15Z", "authors": ["Xiaoxiao Chi", "Xuyun Zhang", "Yan Wang", "Hongsheng Hu", "Wanchun Dou"], "pdf_url": "https://arxiv.org/pdf/2512.09442v1"}
{"id": "http://arxiv.org/abs/2512.09409v1", "title": "Proof of Trusted Execution: A Consensus Paradigm for Deterministic Blockchain Finality", "summary": "Current blockchain consensus protocols -- notably, Proof of Work (PoW) and Proof of Stake (PoS) -- deliver global agreement but exhibit structural constraints. PoW anchors security in heavy computation, inflating energy use and imposing high confirmation latency. PoS improves efficiency but introduces stake concentration, long-range and \"nothing-at-stake\" vulnerabilities, and a hard performance ceiling shaped by slot times and multi-round committee voting. In this paper, we propose Proof of Trusted Execution (PoTE), a consensus paradigm where agreement emerges from verifiable execution rather than replicated re-execution. Validators operate inside heterogeneous VM-based TEEs, each running the same canonical program whose measurement is publicly recorded, and each producing vendor-backed attestations that bind the enclave code hash to the block contents. Because the execution is deterministic and the proposer is uniquely derived from public randomness, PoTE avoids forks, eliminates slot.time bottlenecks, and commits blocks in a single round of verification. We present the design of a PoTE consensus client, describe our reference implementation, and evaluate its performance against the stringent throughput requirements of the Trillion decentralized exchange.", "published": "2025-12-10T08:04:38Z", "updated": "2025-12-10T08:04:38Z", "authors": ["Kyle Habib", "Vladislav Kapitsyn", "Giovanni Mazzeo", "Faisal Mehrban"], "pdf_url": "https://arxiv.org/pdf/2512.09409v1"}
{"id": "http://arxiv.org/abs/2512.09385v1", "title": "BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks", "summary": "The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts.", "published": "2025-12-10T07:30:03Z", "updated": "2025-12-10T07:30:03Z", "authors": ["Uisang Lee", "Changhoon Chung", "Junmo Lee", "Soo-Mook Moon"], "pdf_url": "https://arxiv.org/pdf/2512.09385v1"}
{"id": "http://arxiv.org/abs/2511.11914v2", "title": "Forgetting-MarI: LLM Unlearning via Marginal Information Regularization", "summary": "As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.", "published": "2025-11-14T22:48:39Z", "updated": "2025-12-10T07:20:08Z", "authors": ["Shizhou Xu", "Yuan Ni", "Stefan Broecker", "Thomas Strohmer"], "pdf_url": "https://arxiv.org/pdf/2511.11914v2"}
{"id": "http://arxiv.org/abs/2512.09321v1", "title": "ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data", "summary": "Prompt injection attacks aim to contaminate the input data of an LLM to mislead it into completing an attacker-chosen task instead of the intended task. In many applications and agents, the input data originates from multiple sources, with each source contributing a segment of the overall input. In these multi-source scenarios, an attacker may control only a subset of the sources and contaminate the corresponding segments, but typically does not know the order in which the segments are arranged within the input. Existing prompt injection attacks either assume that the entire input data comes from a single source under the attacker's control or ignore the uncertainty in the ordering of segments from different sources. As a result, their success is limited in domains involving multi-source data.\n  In this work, we propose ObliInjection, the first prompt injection attack targeting LLM applications and agents with multi-source input data. ObliInjection introduces two key technical innovations: the order-oblivious loss, which quantifies the likelihood that the LLM will complete the attacker-chosen task regardless of how the clean and contaminated segments are ordered; and the orderGCG algorithm, which is tailored to minimize the order-oblivious loss and optimize the contaminated segments. Comprehensive experiments across three datasets spanning diverse application domains and twelve LLMs demonstrate that ObliInjection is highly effective, even when only one out of 6-100 segments in the input data is contaminated.", "published": "2025-12-10T05:10:22Z", "updated": "2025-12-10T05:10:22Z", "authors": ["Ruiqi Wang", "Yuqi Jia", "Neil Zhenqiang Gong"], "pdf_url": "https://arxiv.org/pdf/2512.09321v1"}
{"id": "http://arxiv.org/abs/2512.09311v1", "title": "Transformer-Driven Multimodal Fusion for Explainable Suspiciousness Estimation in Visual Surveillance", "summary": "Suspiciousness estimation is critical for proactive threat detection and ensuring public safety in complex environments. This work introduces a large-scale annotated dataset, USE50k, along with a computationally efficient vision-based framework for real-time suspiciousness analysis. The USE50k dataset contains 65,500 images captured from diverse and uncontrolled environments, such as airports, railway stations, restaurants, parks, and other public areas, covering a broad spectrum of cues including weapons, fire, crowd density, abnormal facial expressions, and unusual body postures. Building on this dataset, we present DeepUSEvision, a lightweight and modular system integrating three key components, i.e., a Suspicious Object Detector based on an enhanced YOLOv12 architecture, dual Deep Convolutional Neural Networks (DCNN-I and DCNN-II) for facial expression and body-language recognition using image and landmark features, and a transformer-based Discriminator Network that adaptively fuses multimodal outputs to yield an interpretable suspiciousness score. Extensive experiments confirm the superior accuracy, robustness, and interpretability of the proposed framework compared to state-of-the-art approaches. Collectively, the USE50k dataset and the DeepUSEvision framework establish a strong and scalable foundation for intelligent surveillance and real-time risk assessment in safety-critical applications.", "published": "2025-12-10T04:39:56Z", "updated": "2025-12-10T04:39:56Z", "authors": ["Kuldeep Singh Yadav", "Lalan Kumar"], "pdf_url": "https://arxiv.org/pdf/2512.09311v1"}
{"id": "http://arxiv.org/abs/2512.09309v1", "title": "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge", "summary": "Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.", "published": "2025-12-10T04:37:07Z", "updated": "2025-12-10T04:37:07Z", "authors": ["Zihao Ding", "Mufeng Zhu", "Zhongze Tang", "Sheng Wei", "Yao Liu"], "pdf_url": "https://arxiv.org/pdf/2512.09309v1"}
{"id": "http://arxiv.org/abs/2512.09300v1", "title": "ZeroOS: A Universal Modular Library OS for zkVMs", "summary": "zkVMs promise general-purpose verifiable computation through ISA-level compatibility with modern programs and toolchains. However, compatibility extends further than just the ISA; modern programs often cannot run or even compile without an operating system and libc. zkVMs attempt to address this by maintaining forks of language-specific runtimes and statically linking them into applications to create self-contained unikernels, but this ad-hoc approach leads to version hell and burdens verifiable applications (vApps) with an unnecessarily large trusted computing base. We solve this problem with ZeroOS, a modular library operating system (libOS) for vApp unikernels; vApp developers can use off-the-shelf toolchains to compile and link only the exact subset of the Linux ABI their vApp needs. Any zkVM team can easily leverage the ZeroOS ecosystem by writing a ZeroOS bootloader for their platform, resulting in a reduced maintainence burden and unifying the entire zkVM ecosystem with consolidated development and audit resources. ZeroOS is free and open-sourced at https://github.com/LayerZero-Labs/ZeroOS.", "published": "2025-12-10T04:00:40Z", "updated": "2025-12-10T04:00:40Z", "authors": ["Guangxian Zou", "Isaac Zhang", "Ryan Zarick", "Kelvin Wong", "Thomas Kim", "Daniel L. -K. Wong", "Saeid Yazdinejad", "Dan Boneh"], "pdf_url": "https://arxiv.org/pdf/2512.09300v1"}
{"id": "http://arxiv.org/abs/2512.06253v2", "title": "Privacy Loss of Noise Perturbation via Concentration Analysis of A Product Measure", "summary": "Noise perturbation is one of the most fundamental approaches for achieving $(ε,δ)$-differential privacy (DP) guarantees when releasing the result of a query or function $f(\\cdot)\\in\\mathbb{R}^M$ evaluated on a sensitive dataset $\\mathbf{x}$. In this approach, calibrated noise $\\mathbf{n}\\in\\mathbb{R}^M$ is used to obscure the difference vector $f(\\mathbf{x})-f(\\mathbf{x}')$, where $\\mathbf{x}'$ is known as a neighboring dataset. A DP guarantee is obtained by studying the tail probability bound of a privacy loss random variable (PLRV), defined as the Radon-Nikodym derivative between two distributions. When $\\mathbf{n}$ follows a multivariate Gaussian distribution, the PLRV is characterized as a specific univariate Gaussian. In this paper, we propose a novel scheme to generate $\\mathbf{n}$ by leveraging the fact that the perturbation noise is typically spherically symmetric (i.e., the distribution is rotationally invariant around the origin). The new noise generation scheme allows us to investigate the privacy loss from a geometric perspective and express the resulting PLRV using a product measure, $W\\times U$; measure $W$ is related to a radius random variable controlling the magnitude of $\\mathbf{n}$, while measure $U$ involves a directional random variable governing the angle between $\\mathbf{n}$ and the difference $f(\\mathbf{x})-f(\\mathbf{x}')$. We derive a closed-form moment bound on the product measure to prove $(ε,δ)$-DP. Under the same $(ε,δ)$-DP guarantee, our mechanism yields a smaller expected noise magnitude than the classic Gaussian noise in high dimensions, thereby significantly improving the utility of the noisy result $f(\\mathbf{x})+\\mathbf{n}$. To validate this, we consider convex and non-convex empirical risk minimization (ERM) problems in high dimensional space and apply the proposed product noise to achieve privacy.", "published": "2025-12-06T02:55:46Z", "updated": "2025-12-10T02:46:23Z", "authors": ["Shuainan Liu", "Tianxi Ji", "Zhongshuo Fang", "Lu Wei", "Pan Li"], "pdf_url": "https://arxiv.org/pdf/2512.06253v2"}
{"id": "http://arxiv.org/abs/2512.09264v1", "title": "FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection", "summary": "The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \\textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems.", "published": "2025-12-10T02:38:47Z", "updated": "2025-12-10T02:38:47Z", "authors": ["Xiaojing Chen", "Dan Li", "Lijun Peng", "Jun YanŁetter", "Zhiqing Guo", "Junyang Chen", "Xiao Lan", "Zhongjie Ba", "Yunfeng DiaoŁetter"], "pdf_url": "https://arxiv.org/pdf/2512.09264v1"}
{"id": "http://arxiv.org/abs/2502.17801v3", "title": "Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms", "summary": "Cloud computing environments are increasingly vulnerable to security threats such as distributed denial-of-service (DDoS) attacks and SQL injection. Traditional security mechanisms, based on rule matching and feature recognition, struggle to adapt to evolving attack strategies. This paper proposes an adaptive security protection framework leveraging deep learning to construct a multi-layered defense architecture. The proposed system is evaluated in a real-world business environment, achieving a detection accuracy of 97.3%, an average response time of 18 ms, and an availability rate of 99.999%. Experimental results demonstrate that the proposed method significantly enhances detection accuracy, response efficiency, and resource utilization, offering a novel and effective approach to cloud computing security.", "published": "2025-02-25T03:06:40Z", "updated": "2025-12-10T02:24:43Z", "authors": ["Yuqing Wang", "Xiao Yang"], "pdf_url": "https://arxiv.org/pdf/2502.17801v3"}
{"id": "http://arxiv.org/abs/2512.09233v1", "title": "Analysis of the Security Design, Engineering, and Implementation of the SecureDNA System", "summary": "We analyze security aspects of the SecureDNA system regarding its system design, engineering, and implementation. This system enables DNA synthesizers to screen order requests against a database of hazards. By applying novel cryptography, the system aims to keep order requests and the database of hazards secret. Discerning the detailed operation of the system in part from source code (Version 1.0.8), our analysis examines key management, certificate infrastructure, authentication, and rate-limiting mechanisms. We also perform the first formal-methods analysis of the mutual authentication, basic request, and exemption-handling protocols.\n  Without breaking the cryptography, our main finding is that SecureDNA's custom mutual authentication protocol SCEP achieves only one-way authentication: the hazards database and keyservers never learn with whom they communicate. This structural weakness violates the principle of defense in depth and enables an adversary to circumvent rate limits that protect the secrecy of the hazards database, if the synthesizer connects with a malicious or corrupted keyserver or hashed database. We point out an additional structural weakness that also violates the principle of defense in depth: inadequate cryptographic bindings prevent the system from detecting if responses, within a TLS channel, from the hazards database were modified. Consequently, if a synthesizer were to reconnect with the database over the same TLS session, an adversary could replay and swap responses from the database without breaking TLS. Although the SecureDNA implementation does not allow such reconnections, it would be stronger security engineering to avoid the underlying structural weakness. We identify these vulnerabilities and suggest and verify mitigations, including adding strong bindings. Software Version 1.1.0 fixes SCEP with our proposed SCEP+ protocol.", "published": "2025-12-10T01:39:52Z", "updated": "2025-12-10T01:39:52Z", "authors": ["Alan T. Sherman", "Jeremy J. Romanik Romano", "Edward Zieglar", "Enis Golaszewski", "Jonathan D. Fuchs", "William E. Byrd"], "pdf_url": "https://arxiv.org/pdf/2512.09233v1"}
