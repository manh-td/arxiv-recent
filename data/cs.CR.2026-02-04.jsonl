{"id": "http://arxiv.org/abs/2602.04694v1", "title": "The Needle is a Thread: Finding Planted Paths in Noisy Process Trees", "summary": "Motivated by applications in cybersecurity such as finding meaningful sequences of malware-related events buried inside large amounts of computer log data, we introduce the \"planted path\" problem and propose an algorithm to find fuzzy matchings between two trees. This algorithm can be used as a \"building block\" for more complicated workflows. We demonstrate usefulness of a few of such workflows in mining synthetically generated data as well as real-world ACME cybersecurity datasets.", "published": "2026-02-04T15:56:43Z", "updated": "2026-02-04T15:56:43Z", "authors": ["Maya Le", "Paweł Prałat", "Aaron Smith", "François Théberge"], "pdf_url": "https://arxiv.org/pdf/2602.04694v1"}
{"id": "http://arxiv.org/abs/2602.04653v1", "title": "Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates", "summary": "Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain.", "published": "2026-02-04T15:28:53Z", "updated": "2026-02-04T15:28:53Z", "authors": ["Ariel Fogel", "Omer Hofman", "Eilon Cohen", "Roman Vainshtein"], "pdf_url": "https://arxiv.org/pdf/2602.04653v1"}
{"id": "http://arxiv.org/abs/2510.02389v3", "title": "From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization", "summary": "Large language models show promise for vulnerability discovery, yet prevailing methods inspect code in isolation, struggle with long contexts, and focus on coarse function- or file-level detections that offer limited guidance to engineers who need precise line-level localization for targeted patches. We introduce T2L, an executable framework for project-level, line-level vulnerability localization that progressively narrows scope from repository modules to exact vulnerable lines via AST-based chunking and evidence-guided refinement. We provide a baseline agent with an Agentic Trace Analyzer (ATA) that fuses runtime evidence such as crash points and stack traces to translate failure symptoms into actionable diagnoses. To enable rigorous evaluation, we introduce T2L-ARVO, an expert-verified 50-case benchmark spanning five crash families in real-world projects. On T2L-ARVO, our baseline achieves up to 58.0% detection and 54.8% line-level localization rate. Together, T2L framework advance LLM-based vulnerability detection toward deployable, precision diagnostics in open-source software workflows.", "published": "2025-09-30T22:27:18Z", "updated": "2026-02-04T14:49:18Z", "authors": ["Haoran Xi", "Minghao Shao", "Brendan Dolan-Gavitt", "Muhammad Shafique", "Ramesh Karri"], "pdf_url": "https://arxiv.org/pdf/2510.02389v3"}
{"id": "http://arxiv.org/abs/2602.04616v1", "title": "A Human-Centered Privacy Approach (HCP) to AI", "summary": "As the paradigm of Human-Centered AI (HCAI) gains prominence, its benefits to society are accompanied by significant ethical concerns, one of which is the protection of individual privacy. This chapter provides a comprehensive overview of privacy within HCAI, proposing a human-centered privacy (HCP) framework, providing integrated solution from technology, ethics, and human factors perspectives. The chapter begins by mapping privacy risks across each stage of AI development lifecycle, from data collection to deployment and reuse, highlighting the impact of privacy risks on the entire system. The chapter then introduces privacy-preserving techniques such as federated learning and dif erential privacy. Subsequent chapters integrate the crucial user perspective by examining mental models, alongside the evolving regulatory and ethical landscapes as well as privacy governance. Next, advice on design guidelines is provided based on the human-centered privacy framework. After that, we introduce practical case studies across diverse fields. Finally, the chapter discusses persistent open challenges and future research directions, concluding that a multidisciplinary approach, merging technical, design, policy, and ethical expertise, is essential to successfully embed privacy into the core of HCAI, thereby ensuring these technologies advance in a manner that respects and ensures human autonomy, trust and dignity.", "published": "2026-02-04T14:43:25Z", "updated": "2026-02-04T14:43:25Z", "authors": ["Luyi Sun", "Wei Xu", "Zaifeng Gao"], "pdf_url": "https://arxiv.org/pdf/2602.04616v1"}
{"id": "http://arxiv.org/abs/2510.10540v2", "title": "Predicting Module-Lattice Reduction", "summary": "Is module-lattice reduction better than unstructured lattice reduction? This question was highlighted as 'Q8' in the Kyber NIST standardization submission (Avanzi et al., 2021), as potentially affecting the concrete security of Kyber and other module-lattice-based schemes. Foundational works on module-lattice reduction (Lee, Pellet-Mary, Stehlé, and Wallet, ASIACRYPT 2019; Mukherjee and Stephens-Davidowitz, CRYPTO 2020) confirmed the existence of such module variants of LLL and block-reduction algorithms, but focus only on provable worst-case asymptotic behavior.\n  In this work, we present a concrete average-case analysis of module-lattice reduction. Specifically, we address the question of the expected slope after running module-BKZ, and pinpoint the discriminant $Δ_K$ of the number field at hand as the main quantity driving this slope. We convert this back into a gain or loss on the blocksize $β$: module-BKZ in a number field $K$ of degree $d$ requires an SVP oracle of dimension $β+ \\log(|Δ_K| / d^d)β/(d\\log β) + o(β/ \\log β)$ to reach the same slope as unstructured BKZ with blocksize $β$. This asymptotic summary hides further terms that we predict concretely using experimentally verified heuristics. Incidentally, we provide the first open-source implementation of module-BKZ for some cyclotomic fields.\n  For power-of-two cyclotomic fields, we have $|Δ_K| = d^d$, and conclude that module-BKZ requires a blocksize larger than its unstructured counterpart by $d-1+o(1)$. On the contrary, for all other cyclotomic fields we have $|Δ_K| < d^d$, so module-BKZ provides a sublinear $Θ(β/\\log β)$ gain on the required blocksize, yielding a subexponential speedup of $\\exp(Θ(β/\\log β))$.", "published": "2025-10-12T10:46:55Z", "updated": "2026-02-04T14:38:30Z", "authors": ["Léo Ducas", "Lynn Engelberts", "Paola de Perthuis"], "pdf_url": "https://arxiv.org/pdf/2510.10540v2"}
{"id": "http://arxiv.org/abs/2508.06377v2", "title": "DP-SPRT: Differentially Private Sequential Probability Ratio Tests", "summary": "We revisit Wald's celebrated Sequential Probability Ratio Test for sequential tests of two simple hypotheses, under privacy constraints. We propose DP-SPRT, a wrapper that can be calibrated to achieve desired error probabilities and privacy constraints, addressing a significant gap in previous work. DP-SPRT relies on a private mechanism that processes a sequence of queries and stops after privately determining when the query results fall outside a predefined interval. This OutsideInterval mechanism improves upon naive composition of existing techniques like AboveThreshold, achieving a factor-of-2 privacy improvement and thus potentially benefiting other continual monitoring procedures. We prove generic upper bounds on the error and sample complexity of DP-SPRT that can accommodate various noise distributions based on the practitioner's privacy needs. We exemplify them in two settings: Laplace noise (pure Differential Privacy) and Gaussian noise (Rényi differential privacy). In the former setting, by providing a lower bound on the sample complexity of any $\\varepsilon$-DP test with prescribed type I and type II errors, we show that DP-SPRT is near optimal when both errors are small and the two hypotheses are close. Moreover, we conduct an experimental study revealing its good practical performance.", "published": "2025-08-08T15:09:13Z", "updated": "2026-02-04T14:03:41Z", "authors": ["Thomas Michel", "Debabrota Basu", "Emilie Kaufmann"], "pdf_url": "https://arxiv.org/pdf/2508.06377v2"}
{"id": "http://arxiv.org/abs/2602.04562v1", "title": "Optimal conversion from Rényi Differential Privacy to $f$-Differential Privacy", "summary": "We prove the conjecture stated in Appendix F.3 of [Zhu et al. (2022)]: among all conversion rules that map a Rényi Differential Privacy (RDP) profile $τ\\mapsto ρ(τ)$ to a valid hypothesis-testing trade-off $f$, the rule based on the intersection of single-order RDP privacy regions is optimal. This optimality holds simultaneously for all valid RDP profiles and for all Type I error levels $α$. Concretely, we show that in the space of trade-off functions, the tightest possible bound is $f_{ρ(\\cdot)}(α) = \\sup_{τ\\geq 0.5} f_{τ,ρ(τ)}(α)$: the pointwise maximum of the single-order bounds for each RDP privacy region. Our proof unifies and sharpens the insights of [Balle et al. (2019)], [Asoodeh et al. (2021)], and [Zhu et al. (2022)]. Our analysis relies on a precise geometric characterization of the RDP privacy region, leveraging its convexity and the fact that its boundary is determined exclusively by Bernoulli mechanisms. Our results establish that the \"intersection-of-RDP-privacy-regions\" rule is not only valid, but optimal: no other black-box conversion can uniformly dominate it in the Blackwell sense, marking the fundamental limit of what can be inferred about a mechanism's privacy solely from its RDP guarantees.", "published": "2026-02-04T13:49:51Z", "updated": "2026-02-04T13:49:51Z", "authors": ["Anneliese Riess", "Juan Felipe Gomez", "Flavio du Pin Calmon", "Julia Anne Schnabel", "Georgios Kaissis"], "pdf_url": "https://arxiv.org/pdf/2602.04562v1"}
{"id": "http://arxiv.org/abs/2602.04448v1", "title": "RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models", "summary": "Mixture-of-Experts (MoE) language models introduce unique challenges for safety alignment due to their sparse routing mechanisms, which can enable degenerate optimization behaviors under standard full-parameter fine-tuning. In our preliminary experiments, we observe that naively applying full-parameter safety fine-tuning to MoE models can reduce attack success rates through routing or expert dominance effects, rather than by directly repairing Safety-Critical Experts. To address this challenge, we propose RASA, a routing-aware expert-level alignment framework that explicitly repairs Safety-Critical Experts while preventing routing-based bypasses. RASA identifies experts disproportionately activated by successful jailbreaks, selectively fine-tunes only these experts under fixed routing, and subsequently enforces routing consistency with safety-aligned contexts. Across two representative MoE architectures and a diverse set of jailbreak attacks, RASA achieves near-perfect robustness, strong cross-attack generalization, and substantially reduced over-refusal, while preserving general capabilities on benchmarks such as MMLU, GSM8K, and TruthfulQA. Our results suggest that robust MoE safety alignment benefits from targeted expert repair rather than global parameter updates, offering a practical and architecture-preserving alternative to prior approaches.", "published": "2026-02-04T11:19:15Z", "updated": "2026-02-04T11:19:15Z", "authors": ["Jiacheng Liang", "Yuhui Wang", "Tanqiu Jiang", "Ting Wang"], "pdf_url": "https://arxiv.org/pdf/2602.04448v1"}
{"id": "http://arxiv.org/abs/2601.00752v4", "title": "Three results on twisted $G-$codes and skew twisted $G-$codes", "summary": "In this paper we solve an open question formulated in the original paper of twisted skew group codes regarding when a twisted skew group code is checkable. Also, we prove that all ideals of dimension 3 over a twisted group algebra are abelian group codes, generalising another previous result over group algebras. Finally, we prove a bound on the dimension and distance of a twisted group code, as well as when such bound is reached.", "published": "2026-01-02T17:16:09Z", "updated": "2026-02-04T11:02:16Z", "authors": ["Alvaro Otero Sanchez"], "pdf_url": "https://arxiv.org/pdf/2601.00752v4"}
{"id": "http://arxiv.org/abs/2602.04415v1", "title": "Crypto-RV: High-Efficiency FPGA-Based RISC-V Cryptographic Co-Processor for IoT Security", "summary": "Cryptographic operations are critical for securing IoT, edge computing, and autonomous systems. However, current RISC-V platforms lack efficient hardware support for comprehensive cryptographic algorithm families and post-quantum cryptography. This paper presents Crypto-RV, a RISC-V co-processor architecture that unifies support for SHA-256, SHA-512, SM3, SHA3-256, SHAKE-128, SHAKE-256 AES-128, HARAKA-256, and HARAKA-512 within a single 64-bit datapath. Crypto-RV introduces three key architectural innovations: a high-bandwidth internal buffer (128x64-bit), cryptography-specialized execution units with four-stage pipelined datapaths, and a double-buffering mechanism with adaptive scheduling optimized for large-hash. Implemented on Xilinx ZCU102 FPGA at 160 MHz with 0.851 W dynamic power, Crypto-RV achieves 165 times to 1,061 times speedup over baseline RISC-V cores, 5.8 times to 17.4 times better energy efficiency compared to powerful CPUs. The design occupies only 34,704 LUTs, 37,329 FFs, and 22 BRAMs demonstrating viability for high-performance, energy-efficient cryptographic processing in resource-constrained IoT environments.", "published": "2026-02-04T10:48:30Z", "updated": "2026-02-04T10:48:30Z", "authors": ["Anh Kiet Pham", "Van Truong Vo", "Vu Trung Duong Le", "Tuan Hai Vu", "Hoai Luan Pham", "Van Tinh Nguyen", "Yasuhiko Nakashima"], "pdf_url": "https://arxiv.org/pdf/2602.04415v1"}
{"id": "http://arxiv.org/abs/2602.04384v1", "title": "Blockchain Federated Learning for Sustainable Retail: Reducing Waste through Collaborative Demand Forecasting", "summary": "Effective demand forecasting is crucial for reducing food waste. However, data privacy concerns often hinder collaboration among retailers, limiting the potential for improved predictive accuracy. In this study, we explore the application of Federated Learning (FL) in Sustainable Supply Chain Management (SSCM), with a focus on the grocery retail sector dealing with perishable goods. We develop a baseline predictive model for demand forecasting and waste assessment in an isolated retailer scenario. Subsequently, we introduce a Blockchain-based FL model, trained collaboratively across multiple retailers without direct data sharing. Our preliminary results show that FL models have performance almost equivalent to the ideal setting in which parties share data with each other, and are notably superior to models built by individual parties without sharing data, cutting waste and boosting efficiency.", "published": "2026-02-04T10:10:37Z", "updated": "2026-02-04T10:10:37Z", "authors": ["Fabio Turazza", "Alessandro Neri", "Marcello Pietri", "Maria Angela Butturi", "Marco Picone", "Marco Mamei"], "pdf_url": "https://arxiv.org/pdf/2602.04384v1"}
{"id": "http://arxiv.org/abs/2601.22778v2", "title": "Color Matters: Demosaicing-Guided Color Correlation Training for Generalizable AI-Generated Image Detection", "summary": "As realistic AI-generated images threaten digital authenticity, we address the generalization failure of generative artifact-based detectors by exploiting the intrinsic properties of the camera imaging pipeline. Concretely, we investigate color correlations induced by the color filter array (CFA) and demosaicing, and propose a Demosaicing-guided Color Correlation Training (DCCT) framework for AI-generated image detection. By simulating the CFA sampling pattern, we decompose each color image into a single-channel input (as the condition) and the remaining two channels as the ground-truth targets (for prediction). A self-supervised U-Net is trained to model the conditional distribution of the missing channels from the given one, parameterized via a mixture of logistic functions. Our theoretical analysis reveals that DCCT targets a provable distributional difference in color-correlation features between photographic and AI-generated images. By leveraging these distinct features to construct a binary classifier, DCCT achieves state-of-the-art generalization and robustness, significantly outperforming prior methods across over 20 unseen generators.", "published": "2026-01-30T10:01:49Z", "updated": "2026-02-04T09:04:42Z", "authors": ["Nan Zhong", "Yiran Xu", "Mian Zou"], "pdf_url": "https://arxiv.org/pdf/2601.22778v2"}
{"id": "http://arxiv.org/abs/2503.18687v2", "title": "EVOLVE: a Value-Added Services Platform for Electric Vehicle Charging Stations", "summary": "A notable challenge in Electric Vehicle (EV) charging is the time required to fully charge the battery, which can range from 15 minutes to 2-3 hours. This idle period, however, presents an opportunity to offer time-consuming or data-intensive services such as vehicular software updates. ISO 15118 referred to the concept of Value-Added Services (VAS) in the charging scenario, but it remained underexplored in the literature. Our paper addresses this gap by proposing \\acronym, the first EV charger compute architecture that supports secure on-charger universal applications with upstream and downstream communication. The architecture covers the end-to-end hardware/software stack, including standard API for vehicles and IT infrastructure. We demonstrate the feasibility and advantages of \\acronym by employing and evaluating three suggested value-added services: vehicular software updates, security information and event management (SIEM), and secure payments. The results demonstrate significant reductions in bandwidth utilization and latency, as well as high throughput, which supports this novel concept and suggests a promising business model for Electric Vehicle charging station operation.", "published": "2025-03-24T13:57:04Z", "updated": "2026-02-04T08:04:59Z", "authors": ["Erick Silva", "Tadeu Freitas", "Rehana Yasmin", "Ali Shoker", "Paulo Esteves-Verissimo"], "pdf_url": "https://arxiv.org/pdf/2503.18687v2"}
{"id": "http://arxiv.org/abs/2602.04294v1", "title": "How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks", "summary": "Large Language Models (LLMs) face increasing threats from jailbreak attacks that bypass safety alignment. While prompt-based defenses such as Role-Oriented Prompts (RoP) and Task-Oriented Prompts (ToP) have shown effectiveness, the role of few-shot demonstrations in these defense strategies remains unclear. Prior work suggests that few-shot examples may compromise safety, but lacks investigation into how few-shot interacts with different system prompt strategies. In this paper, we conduct a comprehensive evaluation on multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods. Our key finding reveals that few-shot demonstrations produce opposite effects on RoP and ToP: few-shot enhances RoP's safety rate by up to 4.5% through reinforcing role identity, while it degrades ToP's effectiveness by up to 21.2% through distracting attention from task instructions. Based on these findings, we provide practical recommendations for deploying prompt-based defenses in real-world LLM applications.", "published": "2026-02-04T07:54:51Z", "updated": "2026-02-04T07:54:51Z", "authors": ["Yanshu Wang", "Shuaishuai Yang", "Jingjing He", "Tong Yang"], "pdf_url": "https://arxiv.org/pdf/2602.04294v1"}
{"id": "http://arxiv.org/abs/2511.16940v2", "title": "MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models", "summary": "Modern Vision-Language Models (VLMs) pose significant individual-level privacy risks by linking fragmented multimodal data to identifiable individuals through hierarchical chain-of-thought reasoning. However, existing privacy benchmarks remain structurally insufficient for this threat, as they primarily evaluate privacy perception while failing to address the more critical risk of privacy reasoning: a VLM's ability to infer and link distributed information to construct individual profiles. To address this gap, we propose MultiPriv, the first benchmark designed to systematically evaluate individual-level privacy reasoning in VLMs. We introduce the Privacy Perception and Reasoning (PPR) framework and construct a bilingual multimodal dataset with synthetic individual profiles, where identifiers (e.g., faces, names) are linked to sensitive attributes. This design enables nine challenging tasks spanning attribute detection, cross-image re-identification, and chained inference. We conduct a large-scale evaluation of over 50 open-source and commercial VLMs. Our analysis shows that 60 percent of widely used VLMs can perform individual-level privacy reasoning with up to 80 percent accuracy, posing a significant threat to personal privacy. MultiPriv provides a foundation for developing and assessing privacy-preserving VLMs.", "published": "2025-11-21T04:33:11Z", "updated": "2026-02-04T07:29:14Z", "authors": ["Xiongtao Sun", "Hui Li", "Jiaming Zhang", "Yujie Yang", "Kaili Liu", "Ruxin Feng", "Wen Jun Tan", "Wei Yang Bryan Lim"], "pdf_url": "https://arxiv.org/pdf/2511.16940v2"}
{"id": "http://arxiv.org/abs/2510.19207v2", "title": "Defending Against Prompt Injection with DataFilter", "summary": "When large language model (LLM) agents are increasingly deployed to automate tasks and interact with untrusted external data, prompt injection emerges as a significant security threat. By injecting malicious instructions into the data that LLMs access, an attacker can arbitrarily override the original user task and redirect the agent toward unintended, potentially harmful actions. Existing defenses either require access to model weights (fine-tuning), incur substantial utility loss (detection-based), or demand non-trivial system redesign (system-level). Motivated by this, we propose DataFilter, a test-time model-agnostic defense that removes malicious instructions from the data before it reaches the backend LLM. DataFilter is trained with supervised fine-tuning on simulated injections and leverages both the user's instruction and the data to selectively strip adversarial content while preserving benign information. Across multiple benchmarks, DataFilter consistently reduces the prompt injection attack success rates to near zero while maintaining the LLMs' utility. DataFilter delivers strong security, high utility, and plug-and-play deployment, making it a strong practical defense to secure black-box commercial LLMs against prompt injection. Our DataFilter model is released at https://huggingface.co/JoyYizhu/DataFilter for immediate use, with the code to reproduce our results at https://github.com/yizhu-joy/DataFilter.", "published": "2025-10-22T03:30:49Z", "updated": "2026-02-04T07:26:47Z", "authors": ["Yizhu Wang", "Sizhe Chen", "Raghad Alkhudair", "Basel Alomair", "David Wagner"], "pdf_url": "https://arxiv.org/pdf/2510.19207v2"}
{"id": "http://arxiv.org/abs/2602.04238v1", "title": "Post-Quantum Identity-Based TLS for 5G Service-Based Architecture and Cloud-Native Infrastructure", "summary": "Cloud-native application platforms and latency-sensitive systems such as 5G Core networks rely heavily on certificate-based Public Key Infrastructure (PKI) and mutual TLS to secure service-to-service communication. While effective, this model introduces significant operational and performance overhead, which is further amplified in the post-quantum setting due to large certificates and expensive signature verification. In this paper, we present a certificate-free authentication framework for private distributed systems based on post-quantum Identity-Based Encryption(IBE). Our design replaces certificate and signature based authentication with identity-derived keys and identity-based key encapsulation, enabling mutually authenticated TLS connections without certificate transmission or validation. We describe an IBE-based replacement for private PKI, including identity lifecycle management, and show how it can be instantiated using a threshold Private Key Generator (T-PKG). We apply this framework to cloud-native application deployments and latency-sensitive 5G Core networks. In particular, we demonstrate how identity-based TLS integrates with the 5G Service-Based Architecture while preserving security semantics and 3GPP requirements, and we show how the same architecture can replace private PKI in Kubernetes, including its control plane, without disrupting existing trust domains or deployment models.", "published": "2026-02-04T05:55:41Z", "updated": "2026-02-04T05:55:41Z", "authors": ["Vipin Kumar Rathi", "Lakshya Chopra", "Nikhil Kumar Rajput"], "pdf_url": "https://arxiv.org/pdf/2602.04238v1"}
{"id": "http://arxiv.org/abs/2602.04224v1", "title": "RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning", "summary": "Large Reasoning Models (LRMs) have achieved tremendous success with their chain-of-thought (CoT) reasoning, yet also face safety issues similar to those of basic language models. In particular, while algorithms are designed to guide them to deliberately refuse harmful prompts with safe reasoning, this process often fails to generalize against diverse and complex jailbreak attacks. In this work, we attribute these failures to the generalization of the safe reasoning process, particularly their insufficiency against complex attack prompts. We provide both theoretical and empirical evidence to show the necessity of a more sufficient safe reasoning process to defend against advanced attack prompts. Building on this insight, we propose a Risk-Aware Preference Optimization (RAPO) framework that enables LRM to adaptively identify and address the safety risks with appropriate granularity in its thinking content. Extensive experiments demonstrate that RAPO successfully generalizes multiple LRMs' safe reasoning adaptively across diverse attack prompts whilst preserving general utility, contributing a robust alignment technique for LRM safety. Our code is available at https://github.com/weizeming/RAPO.", "published": "2026-02-04T05:18:38Z", "updated": "2026-02-04T05:18:38Z", "authors": ["Zeming Wei", "Qiaosheng Zhang", "Xia Hu", "Xingcheng Xu"], "pdf_url": "https://arxiv.org/pdf/2602.04224v1"}
{"id": "http://arxiv.org/abs/2602.04216v1", "title": "Availability Attacks Without an Adversary: Evidence from Enterprise LANs", "summary": "Denial-of-Service (DoS) conditions in enterprise networks are commonly attributed to malicious actors. However, availability can also be compromised by benign non-malicious insider behavior. This paper presents an empirical study of a production enterprise LAN that demonstrates how routine docking and undocking of user endpoints repeatedly trigger rapid recalculations of the control plane of the Rapid Spanning Tree Protocol (RSTP) [1]. Although protocol-compliant and nonmalicious, these events introduce transient forwarding disruptions of approximately 2-4 seconds duration that degrade realtime streaming (voice and video) services while remaining largely undetected by conventional security monitoring. We map this phenomenon to the NIST and MITRE insider threat frameworks, characterizing it as an unintentional insider-driven availability breach, and demonstrate that explicit edge-port configuration effectively mitigates the condition without compromising loop prevention", "published": "2026-02-04T05:01:07Z", "updated": "2026-02-04T05:01:07Z", "authors": ["Rajendra Paudyal", "Rajendra Upadhyay", "Al Nahian Bin Emran", "Lisa Donnan", "Duminda Wijesekera"], "pdf_url": "https://arxiv.org/pdf/2602.04216v1"}
{"id": "http://arxiv.org/abs/2602.04195v1", "title": "Semantic Consensus Decoding: Backdoor Defense for Verilog Code Generation", "summary": "Large language models (LLMs) for Verilog code generation are increasingly adopted in hardware design, yet remain vulnerable to backdoor attacks where adversaries inject malicious triggers during training to induce vulnerable hardware designs. Unlike patchable software vulnerabilities, hardware trojans become irreversible once fabricated, making remediation extremely costly or impossible. Existing active defenses require access to training data, impractical for third-party LLM users, while passive defenses struggle against semantically stealthy triggers that naturally blend into design specifications. In this paper, we hypothesize that under the requirements of both effectiveness and stealthiness, attackers are strongly biased toward embedding triggers in non-functional requirements (e.g., style modifiers, quality descriptors) rather than functional specifications that determine hardware behavior. Exploiting this insight, we propose Semantic Consensus Decoding (SCD), an inference-time passive defense with two key components: (1) functional requirement extraction that identifies essential requirements from user specifications, and (2) consensus decoding that adaptively fuses output distributions based on full user specifications and extracted functional requirements. When these distributions diverge significantly, SCD automatically suppresses suspicious components. Extensive experiments with three representative backdoor attacks demonstrate that SCD reduces average attack success rate from 89% to under 3% with negligible impact on generation quality.", "published": "2026-02-04T04:19:49Z", "updated": "2026-02-04T04:19:49Z", "authors": ["Guang Yang", "Xing Hu", "Xiang Chen", "Xin Xia"], "pdf_url": "https://arxiv.org/pdf/2602.04195v1"}
{"id": "http://arxiv.org/abs/2510.27675v2", "title": "On the Difficulty of Selecting Few-Shot Examples for Effective LLM-based Vulnerability Detection", "summary": "Large language models (LLMs) have demonstrated impressive capabilities across a wide range of coding tasks, including summarization, translation, completion, and code generation. Despite these advances, detecting code vulnerabilities remains a challenging problem for LLMs. In-context learning (ICL) has emerged as an effective mechanism for improving model performance by providing a small number of labeled examples within the prompt. Prior work has shown, however, that the effectiveness of ICL depends critically on how these few-shot examples are selected. In this paper, we study two intuitive criteria for selecting few-shot examples for ICL in the context of code vulnerability detection. The first criterion leverages model behavior by prioritizing samples on which the LLM consistently makes mistakes, motivated by the intuition that such samples can expose and correct systematic model weaknesses. The second criterion selects examples based on semantic similarity to the query program, using k-nearest-neighbor retrieval to identify relevant contexts.\n  We conduct extensive evaluations using open-source LLMs and datasets spanning multiple programming languages. Our results show that for Python and JavaScript, careful selection of few-shot examples can lead to measurable performance improvements in vulnerability detection. In contrast, for C and C++ programs, few-shot example selection has limited impact, suggesting that more powerful but also more expensive approaches, such as re-training or fine-tuning, may be required to substantially improve model performance.", "published": "2025-10-31T17:41:58Z", "updated": "2026-02-04T03:11:01Z", "authors": ["Md Abdul Hannan", "Ronghao Ni", "Chi Zhang", "Limin Jia", "Ravi Mangal", "Corina S. Pasareanu"], "pdf_url": "https://arxiv.org/pdf/2510.27675v2"}
{"id": "http://arxiv.org/abs/2602.00364v2", "title": "\"Someone Hid It\": Query-Agnostic Black-Box Attacks on LLM-Based Retrieval", "summary": "Large language models (LLMs) have been serving as effective backbones for retrieval systems, including Retrieval-Augmentation-Generation (RAG), Dense Information Retriever (IR), and Agent Memory Retrieval. Recent studies have demonstrated that such LLM-based Retrieval (LLMR) is vulnerable to adversarial attacks, which manipulates documents by token-level injections and enables adversaries to either boost or diminish these documents in retrieval tasks. However, existing attack studies mainly (1) presume a known query is given to the attacker, and (2) highly rely on access to the victim model's parameters or interactions, which are hardly accessible in real-world scenarios, leading to limited validity.\n  To further explore the secure risks of LLMR, we propose a practical black-box attack method that generates transferable injection tokens based on zero-shot surrogate LLMs without need of victim queries or victim models knowledge. The effectiveness of our attack raises such a robustness issue that similar effects may arise from benign or unintended document edits in the real world. To achieve our attack, we first establish a theoretical framework of LLMR and empirically verify it. Under the framework, we simulate the transferable attack as a min-max problem, and propose an adversarial learning mechanism that finds optimal adversarial tokens with learnable query samples. Our attack is validated to be effective on benchmark datasets across popular LLM retrievers.", "published": "2026-01-30T22:28:04Z", "updated": "2026-02-04T02:50:10Z", "authors": ["Jiate Li", "Defu Cao", "Li Li", "Wei Yang", "Yuehan Qin", "Chenxiao Yu", "Tiannuo Yang", "Ryan A. Rossi", "Yan Liu", "Xiyang Hu", "Yue Zhao"], "pdf_url": "https://arxiv.org/pdf/2602.00364v2"}
{"id": "http://arxiv.org/abs/2508.03365v3", "title": "When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs", "summary": "As large language models (LLMs) become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces WhisperInject, a two-stage adversarial audio attack framework that manipulates state-of-the-art audio language models to generate harmful content. Our method embeds harmful payloads as subtle perturbations into audio inputs that remain intelligible to human listeners. The first stage uses a novel reward-based white-box optimization method, Reinforcement Learning with Projected Gradient Descent (RL-PGD), to jailbreak the target model and elicit harmful native responses. This native harmful response then serves as the target for Stage 2, Payload Injection, where we use gradient-based optimization to embed subtle perturbations into benign audio carriers, such as weather queries or greeting messages. Our method achieves average attack success rates of 60-78% across two benchmarks and five multimodal LLMs, validated by multiple evaluation frameworks. Our work demonstrates a new class of practical, audio-native threats, moving beyond theoretical exploits to reveal a feasible and covert method for manipulating multimodal AI systems.", "published": "2025-08-05T12:14:01Z", "updated": "2026-02-04T02:28:19Z", "authors": ["Hiskias Dingeto", "Taeyoun Kwon", "Dasol Choi", "Bodam Kim", "DongGeon Lee", "Haon Park", "JaeHoon Lee", "Jongho Shin"], "pdf_url": "https://arxiv.org/pdf/2508.03365v3"}
{"id": "http://arxiv.org/abs/2504.14696v2", "title": "Differentially Private Sampling via Reveal-or-Obscure", "summary": "We introduce a differentially private (DP) algorithm called Reveal-or-Obscure (ROO) to generate a single representative sample from a dataset of n i.i.d. observations from an unknown distribution. Unlike methods that add explicit noise to the estimated empirical distribution, ROO achieves $ε$-differential privacy by choosing whether to \"reveal\" or \"obscure\" the empirical distribution with a fixed probability $q$. While our proposed mechanism is structurally identical to an algorithm proposed by Cheu and Nayak, we prove a strictly better bound on the sampling complexity than that established in their theorem. Building on this framework, we propose a novel generalized sampler called Data-Specific ROO (DS-ROO), where the obscuring probability $q$ is a function of the empirical distribution. We show that when the dataset contains enough samples from every element of the alphabet, DS-ROO can achieve $ε$-DP while obscuring much less. In addition, we provide tight upper bounds on the utility of DS-ROO in terms of total variation distance. Our results show that under the same privacy budget, DS-ROO can achieve better utility than state-of-the-art private samplers and vanilla ROO, with total variation distance decaying exponentially in dataset size $n$.", "published": "2025-04-20T18:20:11Z", "updated": "2026-02-04T02:05:21Z", "authors": ["Naima Tasnim", "Atefeh Gilani", "Lalitha Sankar", "Oliver Kosut"], "pdf_url": "https://arxiv.org/pdf/2504.14696v2"}
{"id": "http://arxiv.org/abs/2508.17329v2", "title": "Risk Assessment and Security Analysis of Large Language Models", "summary": "As large language models (LLMs) expose systemic security challenges in high risk applications, including privacy leaks, bias amplification, and malicious abuse, there is an urgent need for a dynamic risk assessment and collaborative defence framework that covers their entire life cycle. This paper focuses on the security problems of large language models (LLMs) in critical application scenarios, such as the possibility of disclosure of user data, the deliberate input of harmful instructions, or the models bias. To solve these problems, we describe the design of a system for dynamic risk assessment and a hierarchical defence system that allows different levels of protection to cooperate. This paper presents a risk assessment system capable of evaluating both static and dynamic indicators simultaneously. It uses entropy weighting to calculate essential data, such as the frequency of sensitive words, whether the API call is typical, the realtime risk entropy value is significant, and the degree of context deviation. The experimental results show that the system is capable of identifying concealed attacks, such as role escape, and can perform rapid risk evaluation. The paper uses a hybrid model called BERT-CRF (Bidirectional Encoder Representation from Transformers) at the input layer to identify and filter malicious commands. The model layer uses dynamic adversarial training and differential privacy noise injection technology together. The output layer also has a neural watermarking system that can track the source of the content. In practice, the quality of this method, especially important in terms of customer service in the financial industry.", "published": "2025-08-24T12:34:34Z", "updated": "2026-02-04T01:53:24Z", "authors": ["Xiaoyan Zhang", "Dongyang Lyu", "Xiaoqi Li"], "pdf_url": "https://arxiv.org/pdf/2508.17329v2"}
{"id": "http://arxiv.org/abs/2602.04113v1", "title": "ZKBoost: Zero-Knowledge Verifiable Training for XGBoost", "summary": "Gradient boosted decision trees, particularly XGBoost, are among the most effective methods for tabular data. As deployment in sensitive settings increases, cryptographic guarantees of model integrity become essential. We present ZKBoost, the first zero-knowledge proof of training (zkPoT) protocol for XGBoost, enabling model owners to prove correct training on a committed dataset without revealing data or parameters. We make three key contributions: (1) a fixed-point XGBoost implementation compatible with arithmetic circuits, enabling instantiation of efficient zkPoT, (2) a generic template of zkPoT for XGBoost, which can be instantiated with any general-purpose ZKP backend, and (3) vector oblivious linear evaluation (VOLE)-based instantiation resolving challenges in proving nonlinear fixed-point operations. Our fixed-point implementation matches standard XGBoost accuracy within 1\\% while enabling practical zkPoT on real-world datasets.", "published": "2026-02-04T00:56:28Z", "updated": "2026-02-04T00:56:28Z", "authors": ["Nikolas Melissaris", "Jiayi Xu", "Antigoni Polychroniadou", "Akira Takahashi", "Chenkai Weng"], "pdf_url": "https://arxiv.org/pdf/2602.04113v1"}
{"id": "http://arxiv.org/abs/2602.04105v1", "title": "Expert Selections In MoE Models Reveal (Almost) As Much As Text", "summary": "We present a text-reconstruction attack on mixture-of-experts (MoE) language models that recovers tokens from expert selections alone. In MoE models, each token is routed to a subset of expert subnetworks; we show these routing decisions leak substantially more information than previously understood. Prior work using logistic regression achieves limited reconstruction; we show that a 3-layer MLP improves this to 63.1% top-1 accuracy, and that a transformer-based sequence decoder recovers 91.2% of tokens top-1 (94.8% top-10) on 32-token sequences from OpenWebText after training on 100M tokens. These results connect MoE routing to the broader literature on embedding inversion. We outline practical leakage scenarios (e.g., distributed inference and side channels) and show that adding noise reduces but does not eliminate reconstruction. Our findings suggest that expert selections in MoE deployments should be treated as sensitive as the underlying text.", "published": "2026-02-04T00:42:30Z", "updated": "2026-02-04T00:42:30Z", "authors": ["Amir Nuriyev", "Gabriel Kulp"], "pdf_url": "https://arxiv.org/pdf/2602.04105v1"}
{"id": "http://arxiv.org/abs/2506.12340v3", "title": "Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models", "summary": "Large vision-language models (LVLMs) have demonstrated outstanding performance in many downstream tasks. However, LVLMs are trained on large-scale datasets, which can pose privacy risks if training images contain sensitive information. Therefore, it is important to detect whether an image is used to train the LVLM. Recent studies have investigated membership inference attacks (MIAs) against LVLMs, including detecting image-text pairs and single-modality content. In this work, we focus on detecting whether a target image is used to train the target LVLM. We design simple yet effective Image Corruption-Inspired Membership Inference Attacks (ICIMIA) against LVLMs, which are inspired by LVLM's different sensitivity to image corruption for member and non-member images. We first perform an MIA method under the white-box setting, where we can obtain the embeddings of the image through the vision part of the target LVLM. The attacks are based on the embedding similarity between the image and its corrupted version. We further explore a more practical scenario where we have no knowledge about target LVLMs and we can only query the target LVLMs with an image and a textual instruction. We then conduct the attack by utilizing the output text embeddings' similarity. Experiments on existing datasets validate the effectiveness of our proposed methods under those two different settings.", "published": "2025-06-14T04:22:36Z", "updated": "2026-02-04T00:24:48Z", "authors": ["Zongyu Wu", "Minhua Lin", "Zhiwei Zhang", "Fali Wang", "Xianren Zhang", "Xiang Zhang", "Suhang Wang"], "pdf_url": "https://arxiv.org/pdf/2506.12340v3"}
