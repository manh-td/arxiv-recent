{"id": "http://arxiv.org/abs/2602.04640v1", "title": "Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents", "summary": "Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.\n  In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.", "published": "2026-02-04T15:07:53Z", "updated": "2026-02-04T15:07:53Z", "authors": [" Tse-Hsun", " Chen"], "pdf_url": "https://arxiv.org/pdf/2602.04640v1"}
{"id": "http://arxiv.org/abs/2510.08810v2", "title": "MigrateLib: a tool for end-to-end Python library migration", "summary": "Library migration is the process of replacing a library with a similar one in a software project. Manual library migration is time consuming and error prone, as it requires developers to understand the Application Programming Interfaces (API) of both libraries, map equivalent APIs, and perform the necessary code transformations. Due to the difficulty of the library migration process, most of the existing automated techniques and tooling stop at the API mapping stage or support a limited set of libraries and code transformations. In this paper, we develop an end-to-end solution that can automatically migrate code between any arbitrary pair of Python libraries that provide similar functionality. Due to the promising capabilities of Large Language Models (LLMs) in code generation and transformation, we use LLMs as the primary engine for migration. Before building the tool, we first study the capabilities of LLMs for library migration on a benchmark of 321 real-world library migrations. We find that LLMs can effectively perform library migration, but some post-processing steps can further improve the performance. Based on this, we develop MigrateLib, a command line application that combines the power of LLMs, static analysis, and dynamic analysis to provide accurate library migration. We evaluate MigrateLib on 717 real-world Python applications that are not from our benchmark. We find that MigrateLib can migrate 32% of the migrations with complete correctness. Of the remaining migrations, only 14% of the migration-related changes are left for developers to fix for more than half of the projects.", "published": "2025-10-09T20:54:26Z", "updated": "2026-02-04T14:56:24Z", "authors": ["Mohayeminul Islam", "Ajay Kumar Jha", "May Mahmoud", "Sarah Nadi"], "pdf_url": "https://arxiv.org/pdf/2510.08810v2"}
{"id": "http://arxiv.org/abs/2510.02389v3", "title": "From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization", "summary": "Large language models show promise for vulnerability discovery, yet prevailing methods inspect code in isolation, struggle with long contexts, and focus on coarse function- or file-level detections that offer limited guidance to engineers who need precise line-level localization for targeted patches. We introduce T2L, an executable framework for project-level, line-level vulnerability localization that progressively narrows scope from repository modules to exact vulnerable lines via AST-based chunking and evidence-guided refinement. We provide a baseline agent with an Agentic Trace Analyzer (ATA) that fuses runtime evidence such as crash points and stack traces to translate failure symptoms into actionable diagnoses. To enable rigorous evaluation, we introduce T2L-ARVO, an expert-verified 50-case benchmark spanning five crash families in real-world projects. On T2L-ARVO, our baseline achieves up to 58.0% detection and 54.8% line-level localization rate. Together, T2L framework advance LLM-based vulnerability detection toward deployable, precision diagnostics in open-source software workflows.", "published": "2025-09-30T22:27:18Z", "updated": "2026-02-04T14:49:18Z", "authors": ["Haoran Xi", "Minghao Shao", "Brendan Dolan-Gavitt", "Muhammad Shafique", "Ramesh Karri"], "pdf_url": "https://arxiv.org/pdf/2510.02389v3"}
{"id": "http://arxiv.org/abs/2304.05358v2", "title": "An Exploratory Study of Bug-Introducing Changes: Exploring Relationships in Bug-Introducing Changes Towards Causal Understanding", "summary": "Context: Many studies consider the relation between individual aspects of the software engineering process and bug-introduction, e.g., software testing and code review. These studies typically only identify correlations between their set of variables without accounting for interactions with external variables, such as confounding factors.\n  Objective: Within this study, we provide a broad empirical view on practices of software development and their relation to bug-introducing changes \\rev{to enable} future work on causal relations between those aspects.\n  Method: We consider the bugs, the type of change that introduced the bug, aspects of the build process, code review, software tests, and any other discussion related to the bug that we can identify. We use a qualitative approach that first describes variables of the development process and then groups the variables based on their relations. From these groups, we deduce how their (pairwise) interactions affect bug-introducing changes.\n  Results: We found multiple relevant relations within the development process of bug-introducing changes. Logical groups of variables and their relations provide a framework for discovering areas of interest regarding intermediate effects in the process and confounders towards bug-introduction.\n  Conclusion: Software engineering practices applied during the development of bug-introducing changes are interdependent. This work lays the foundation to understand why bugs are introduced using causal modeling, discovery, and inference.", "published": "2023-04-11T17:15:07Z", "updated": "2026-02-04T13:14:45Z", "authors": ["Lukas Schulte", "Anamaria Mojica-Hanke", "Mario Linares-Vásquez", "Steffen Herbold"], "pdf_url": "https://arxiv.org/pdf/2304.05358v2"}
{"id": "http://arxiv.org/abs/2601.19929v2", "title": "Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding", "summary": "We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.", "published": "2026-01-11T17:52:53Z", "updated": "2026-02-04T12:54:57Z", "authors": ["David Linus Ostby"], "pdf_url": "https://arxiv.org/pdf/2601.19929v2"}
{"id": "http://arxiv.org/abs/2602.04467v1", "title": "A Framework of Critical Success Factors for Agile Software Development", "summary": "Despite the popularity of Agile software development, achieving consistent project success remains challenging. This systematic literature review identifies critical success factors (CSFs) in Agile projects by analyzing 53 primary studies. Employing thematic synthesis with content analysis, our analysis yielded 21 CSFs categorized into five themes: organizational, people, technical, process, and project. Team effectiveness and project management emerged as the most frequently cited CSFs, highlighting the importance of people and process factors. These interpreted themes and factors contributed to the development of a theoretical framework to identify how these factors contribute to project success. This study offers valuable insights for researchers and practitioners, guiding future research to validate these findings and test the proposed framework using quantitative methods.", "published": "2026-02-04T11:56:11Z", "updated": "2026-02-04T11:56:11Z", "authors": ["Ridewaan Hanslo", "Maureen Tanner"], "pdf_url": "https://arxiv.org/pdf/2602.04467v1"}
{"id": "http://arxiv.org/abs/2602.04449v1", "title": "What's in a Benchmark? The Case of SWE-Bench in Automated Program Repair", "summary": "The rapid progress in Automated Program Repair (APR) has been fueled by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a benchmark designed to evaluate repair systems using real issues mined from popular open-source Python repositories. Its public leaderboards-SWE-Bench Lite and Verified-have become central platforms for tracking progress and comparing solutions. In this paper, we present the first comprehensive study of these two leaderboards, examining who is submitting solutions, the products behind the submissions, the LLMs employed, and the openness of the approaches. We analyze 79 entries submitted to Lite leaderboard and 133 to Verified. Our results show that most entries on both leaderboards originate from industry, particularly small companies and large publicly traded companies. These submissions often achieve top results, although academic contributions-typically open source-also remain competitive. We also find a clear dominance of proprietary LLMs, especially Claude family, with state-of-the-art results on both leaderboards currently achieved by Claude 4 Sonnet. These findings offer insights into the SWE-Bench ecosystem that can guide greater transparency and diversity in future benchmark-driven research.", "published": "2026-02-04T11:19:18Z", "updated": "2026-02-04T11:19:18Z", "authors": ["Matias Martinez", "Xavier Franch"], "pdf_url": "https://arxiv.org/pdf/2602.04449v1"}
{"id": "http://arxiv.org/abs/2602.04445v1", "title": "AgenticAKM : Enroute to Agentic Architecture Knowledge Management", "summary": "Architecture Knowledge Management (AKM) is crucial for maintaining current and comprehensive software Architecture Knowledge (AK) in a software project. However AKM is often a laborious process and is not adopted by developers and architects. While LLMs present an opportunity for automation, a naive, single-prompt approach is often ineffective, constrained by context limits and an inability to grasp the distributed nature of architectural knowledge. To address these limitations, we propose an Agentic approach for AKM, AgenticAKM, where the complex problem of architecture recovery and documentation is decomposed into manageable sub-tasks. Specialized agents for architecture Extraction, Retrieval, Generation, and Validation collaborate in a structured workflow to generate AK. To validate we made an initial instantiation of our approach to generate Architecture Decision Records (ADRs) from code repositories. We validated our approach through a user study with 29 repositories. The results demonstrate that our agentic approach generates better ADRs, and is a promising and practical approach for automating AKM.", "published": "2026-02-04T11:16:07Z", "updated": "2026-02-04T11:16:07Z", "authors": ["Rudra Dhar", "Karthik Vaidhyanathan", "Vasudeva Varma"], "pdf_url": "https://arxiv.org/pdf/2602.04445v1"}
{"id": "http://arxiv.org/abs/2602.04418v1", "title": "SPEAR: An Engineering Case Study of Multi-Agent Coordination for Smart Contract Auditing", "summary": "We present SPEAR, a multi-agent coordination framework for smart contract auditing that applies established MAS patterns in a realistic security analysis workflow. SPEAR models auditing as a coordinated mission carried out by specialized agents: a Planning Agent prioritizes contracts using risk-aware heuristics, an Execution Agent allocates tasks via the Contract Net protocol, and a Repair Agent autonomously recovers from brittle generated artifacts using a programmatic-first repair policy. Agents maintain local beliefs updated through AGM-compliant revision, coordinate via negotiation and auction protocols, and revise plans as new information becomes available. An empirical study compares the multi-agent design with centralized and pipeline-based alternatives under controlled failure scenarios, focusing on coordination, recovery behavior, and resource use.", "published": "2026-02-04T10:51:19Z", "updated": "2026-02-04T10:51:19Z", "authors": ["Arnab Mallick", "Indraveni Chebolu", "Harmesh Rana"], "pdf_url": "https://arxiv.org/pdf/2602.04418v1"}
{"id": "http://arxiv.org/abs/2602.04385v1", "title": "Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications", "summary": "The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.", "published": "2026-02-04T10:11:06Z", "updated": "2026-02-04T10:11:06Z", "authors": ["Marco Picone", "Fabio Turazza", "Matteo Martinelli", "Marco Mamei"], "pdf_url": "https://arxiv.org/pdf/2602.04385v1"}
{"id": "http://arxiv.org/abs/2602.04358v1", "title": "Generative AI in Systems Engineering: A Framework for Risk Assessment of Large Language Models", "summary": "The increasing use of Large Language Models (LLMs) offers significant opportunities across the engineering lifecycle, including requirements engineering, software development, process optimization, and decision support. Despite this potential, organizations face substantial challenges in assessing the risks associated with LLM use, resulting in inconsistent integration, unknown failure modes, and limited scalability. This paper introduces the LLM Risk Assessment Framework (LRF), a structured approach for evaluating the application of LLMs within Systems Engineering (SE) environments. The framework classifies LLM-based applications along two fundamental dimensions: autonomy, ranging from supportive assistance to fully automated decision making, and impact, reflecting the potential severity of incorrect or misleading model outputs on engineering processes and system elements. By combining these dimensions, the LRF enables consistent determination of corresponding risk levels across the development lifecycle. The resulting classification supports organizations in identifying appropriate validation strategies, levels of human oversight, and required countermeasures to ensure safe and transparent deployment. The framework thereby helps align the rapid evolution of AI technologies with established engineering principles of reliability, traceability, and controlled process integration. Overall, the LRF provides a basis for risk-aware adoption of LLMs in complex engineering environments and represents a first step toward standardized AI assurance practices in systems engineering.", "published": "2026-02-04T09:30:11Z", "updated": "2026-02-04T09:30:11Z", "authors": ["Stefan Otten", "Philipp Reis", "Philipp Rigoll", "Joshua Ransiek", "Tobias Schürmann", "Jacob Langner", "Eric Sax"], "pdf_url": "https://arxiv.org/pdf/2602.04358v1"}
{"id": "http://arxiv.org/abs/2601.16746v2", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified while even improving success rates, and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "published": "2026-01-23T13:51:59Z", "updated": "2026-02-04T09:20:31Z", "authors": ["Yuhang Wang", "Yuling Shi", "Mo Yang", "Rongrui Zhang", "Shilin He", "Heng Lian", "Yuting Chen", "Siyu Ye", "Kai Cai", "Xiaodong Gu"], "pdf_url": "https://arxiv.org/pdf/2601.16746v2"}
{"id": "http://arxiv.org/abs/2602.04341v1", "title": "Model-Driven Legacy System Modernization at Scale", "summary": "This experience report presents a model-driven approach to legacy system modernization that inserts an enriched, technology-agnostic intermediate model between the legacy codebase and the modern target platform, and reports on its application and evaluation. The four-stage process of analysis, enrichment, synthesis, and transition systematically extracts, abstracts, and transforms system artifacts. We apply our approach to a large industrial application built on legacy versions of the .NET Framework and ASP.NET MVC and show that core user interface components and page structures can be migrated semi-automatically to a modern web stack while preserving functional behavior and essential non-functional qualities. By consolidating architectural knowledge into explicit model representations, the resulting codebase exhibits higher maintainability and extensibility, thereby improving developer experience. Although automation is effective for standard patterns, migration of bespoke layout composites remains challenging and requires targeted manual adaptation. Our contributions are: (i) an end-to-end model-driven process, (ii) an enriched intermediate model that captures structure, dependencies, and semantic metadata, (iii) transformation rules that preserve functional behavior and essential non-functional qualities, and (iv) application and evaluation of the approach in an industrial setting. Overall, model-based abstractions reduce risk and effort while supporting scalable, traceable modernization of legacy applications. Our approach generalizes to comparable modernization contexts and promotes reuse of migration patterns.", "published": "2026-02-04T09:07:20Z", "updated": "2026-02-04T09:07:20Z", "authors": ["Tobias Böhm", "Jens Guan Su Tien", "Mohini Nonnenmann", "Tom Schoonbaert", "Bart Carpels", "Andreas Biesdorf"], "pdf_url": "https://arxiv.org/pdf/2602.04341v1"}
{"id": "http://arxiv.org/abs/2602.04332v1", "title": "They Call Her 'Miss' and Him 'Professor': Lived Experiences of Women Teaching Support Staff in IT/SE Education", "summary": "Despite their critical role in shaping student learning in computing education, the contributions of women teaching-support staff (TSS) often go unrecognised and undervalued. In this experience report, we synthesise lived experiences of 15 women TSS in IT/SE higher education to illuminate how authority is earned, resisted, and maintained in everyday teaching. Participants shared both their positive and negative lived experiences associated with finding and losing voice with teaching team colleagues on the one hand, and rewarding connections and gendered friction with students on the other. We map these dynamics onto an intersectional \"wheel of privilege and power\" tailored to TSS roles. The farther a TSS profile sits from the wheel's center (e.g., non-native English, non-white, younger-seeming, non-permanent, early-career), the more relational, emotional, and disciplinary labour is needed to reach parity. We provide actionable insights and recommendations for creating more inclusive education environments in technology dominant fields that are particularly timely as universities worldwide grapple with post-pandemic teaching models and seek to build more inclusive and resilient academic communities.", "published": "2026-02-04T08:52:26Z", "updated": "2026-02-04T08:52:26Z", "authors": ["Vasudha Malhotra", "Rhea D'silva", "Rashina Hoda"], "pdf_url": "https://arxiv.org/pdf/2602.04332v1"}
{"id": "http://arxiv.org/abs/2512.13515v2", "title": "Fine-tuned LLM-based Code Migration Framework", "summary": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.", "published": "2025-12-15T16:42:51Z", "updated": "2026-02-04T08:52:18Z", "authors": ["Oleg Grynets", "Vasyl Lyashkevych", "Dmytro Baran", "Maksym Orliansky", "Taras Zelenyy", "Markiian Leshchyshyn"], "pdf_url": "https://arxiv.org/pdf/2512.13515v2"}
{"id": "http://arxiv.org/abs/2509.26336v2", "title": "UniSage: A Unified and Post-Analysis-Aware Sampling for Microservices", "summary": "Traces and logs serve as the backbone of observability in microservice architectures, yet their sheer volume imposes prohibitive storage and computational burdens. To reduce overhead, operators rely on sampling; however, current frameworks generally employ a sample-before-analysis strategy. This approach creates a fundamental trade-off: to save space, systems must discard data before knowing its diagnostic value, often losing critical context required for troubleshooting anomalies and latency spikes. In this paper, we propose UniSage, a unified sampling framework that addresses this trade-off by adopting a post-analysis-aware paradigm. Unlike prior works that focus solely on tracing, UniSageintegrates both traces and logs, leveraging a lightweight anomaly detection and root cause analysis module to scan the full data stream before sampling decisions are made. This pre-computation enables a dual-pillar strategy: an analysis-guided sampler that retains high-value data associated with detected anomalies, and an edge-case sampler that preserves rare but critical behaviors to ensure diversity. Evaluation on three datasets confirms that UniSage achieves superior data retention. At a 2.5% sampling rate, UniSage captures 71% of critical traces and 96.25% of relevant logs, substantially exceeding the best existing methods (which achieve 42.9% and 1.95%, respectively). Moreover, evaluations on a real-world dataset demonstrate UniSage's efficiency; it processes a 20-minute multi-modal data block in an average of 10 seconds, making it practical for production environments.", "published": "2025-09-30T14:44:56Z", "updated": "2026-02-04T08:33:52Z", "authors": ["Zhouruixing Zhu", "Zhihan Jiang", "Tianyi Yang", "Pinjia He"], "pdf_url": "https://arxiv.org/pdf/2509.26336v2"}
{"id": "http://arxiv.org/abs/2510.26275v3", "title": "A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI", "summary": "Generative AI (GenAI) is rapidly transforming software engineering (SE) practices, influencing how SE processes are executed, as well as how software systems are developed, operated, and evolved. This paper applies design science research to build a roadmap for GenAI-augmented SE. The process consists of three cycles that incrementally integrate multiple sources of evidence, including collaborative discussions from the FSE 2025 \"Software Engineering 2030\" workshop, rapid literature reviews, and external feedback sessions involving peers. McLuhan's tetrads were used as a conceptual instrument to systematically capture the transforming effects of GenAI on SE processes and software products. The resulting roadmap identifies four fundamental forms of GenAI augmentation in SE and systematically characterizes their related research challenges and opportunities. These insights are then consolidated into a set of future research directions. By grounding the roadmap in a rigorous multi-cycle process and cross-validating it among independent author teams and peers, the study provides a transparent and reproducible foundation for analyzing how GenAI affects SE processes, methods and tools, and for framing future research within this rapidly evolving area.", "published": "2025-10-30T08:59:01Z", "updated": "2026-02-04T08:02:33Z", "authors": ["Domenico Amalfitano", "Andreas Metzger", "Marco Autili", "Tommaso Fulcini", "Tobias Hey", "Jan Keim", "Patrizio Pelliccione", "Vincenzo Scotti", "Anne Koziolek", "Raffaela Mirandola", "Andreas Vogelsang"], "pdf_url": "https://arxiv.org/pdf/2510.26275v3"}
{"id": "http://arxiv.org/abs/2602.04296v1", "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas", "summary": "Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-generated agents within diverse, competitive game environments. Unlike existing approaches, ProxyWar evaluates not only functional correctness but also the operational characteristics of generated programs, combining automated testing, iterative code repair, and multi-agent tournaments to provide a holistic view of program behavior. Applied to a range of state-of-the-art coders and games, our approach uncovers notable discrepancies between benchmark scores and actual performance in dynamic settings, revealing overlooked limitations and opportunities for improvement. These findings highlight the need for richer, competition-based evaluation of code generation. Looking forward, ProxyWar lays a foundation for research into LLM-driven algorithm discovery, adaptive problem solving, and the study of practical efficiency and robustness, including the potential for models to outperform hand-crafted agents. The project is available at https://github.com/xinke-wang/ProxyWar.", "published": "2026-02-04T07:57:06Z", "updated": "2026-02-04T07:57:06Z", "authors": ["Wenjun Peng", "Xinyu Wang", "Qi Wu"], "pdf_url": "https://arxiv.org/pdf/2602.04296v1"}
{"id": "http://arxiv.org/abs/2406.11589v7", "title": "CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with Test-Driven Agents", "summary": "Semantic code search, retrieving code that matches a given natural language query, is an important task to improve productivity in software engineering. Existing code search datasets face limitations: they rely on human annotators who assess code primarily through semantic understanding rather than functional verification, leading to potential inaccuracies and scalability issues. Additionally, current evaluation metrics often overlook the multi-choice nature of code search. This paper introduces CoSQA+, pairing high-quality queries from CoSQA with multiple suitable codes. We develop an automated pipeline featuring multiple model-based candidate selections and the novel test-driven agent annotation system. Among a single Large Language Model (LLM) annotator and Python expert annotators (without test-based verification), agents leverage test-based verification and achieve the highest accuracy of 93.9%. Through extensive experiments, CoSQA+ has demonstrated superior quality over CoSQA. Models trained on CoSQA+ exhibit improved performance. We publicly release both CoSQA+_all, which contains 412,080 agent-annotated pairs, and CoSQA+_verified, which contains 1,000 human-verified pairs, at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.", "published": "2024-06-17T14:34:14Z", "updated": "2026-02-04T06:56:12Z", "authors": ["Jing Gong", "Yanghui Wu", "Linxi Liang", "Yanlin Wang", "Jiachi Chen", "Mingwei Liu", "Zibin Zheng"], "pdf_url": "https://arxiv.org/pdf/2406.11589v7"}
{"id": "http://arxiv.org/abs/2602.04226v1", "title": "Why Agentic-PRs Get Rejected: A Comparative Study of Coding Agents", "summary": "Agentic coding -- software development workflows in which autonomous coding agents plan, implement, and submit code changes with minimal human involvement -- is rapidly gaining traction. Prior work has shown that Pull Requests (PRs) produced using coding agents (Agentic-PRs) are accepted less often than PRs that are not labeled as agentic (Human-PRs). The rejection reasons for a single agent (Claude Code) have been explored, but a comparison of how rejection reasons differ between Agentic-PRs generated by different agents has not yet been performed. This comparison is important since different coding agents are often used for different purposes, which can lead to agent-specific failure patterns. In this paper, we inspect 654 rejected PRs from the AIDev dataset covering five coding agents, as well as a human baseline. Our results show that seven rejection modes occur only in Agentic-PRs, including distrust of AI-generated code. We also observe agent-specific patterns (e.g., automated withdrawal of inactive PRs by Devin), reflecting differences in how agents are configured and used in practice. Notably, a large proportion of rejected PRs (67.9%) lack explicit reviewer feedback, making their rejection reasons difficult to determine. To mitigate this issue, we propose a set of heuristics that reduce the proportion of such cases, offering a practical preprocessing step for future studies of PR rejection in agentic coding.", "published": "2026-02-04T05:24:18Z", "updated": "2026-02-04T05:24:18Z", "authors": ["Sota Nakashima", "Yuta Ishimoto", "Masanari Kondo", "Shane Mclntosh", "Yasutaka Kamei"], "pdf_url": "https://arxiv.org/pdf/2602.04226v1"}
{"id": "http://arxiv.org/abs/2602.02619v2", "title": "daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently", "summary": "While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...", "published": "2026-02-02T13:23:39Z", "updated": "2026-02-04T04:59:27Z", "authors": ["Mohan Jiang", "Dayuan Fu", "Junhao Shi", "Ji Zeng", "Weiye Si", "Keyu Li", "Xuefeng Li", "Yang Xiao", "Wenjie Li", "Dequan Wang", "Pengfei Liu"], "pdf_url": "https://arxiv.org/pdf/2602.02619v2"}
{"id": "http://arxiv.org/abs/2602.04195v1", "title": "Semantic Consensus Decoding: Backdoor Defense for Verilog Code Generation", "summary": "Large language models (LLMs) for Verilog code generation are increasingly adopted in hardware design, yet remain vulnerable to backdoor attacks where adversaries inject malicious triggers during training to induce vulnerable hardware designs. Unlike patchable software vulnerabilities, hardware trojans become irreversible once fabricated, making remediation extremely costly or impossible. Existing active defenses require access to training data, impractical for third-party LLM users, while passive defenses struggle against semantically stealthy triggers that naturally blend into design specifications. In this paper, we hypothesize that under the requirements of both effectiveness and stealthiness, attackers are strongly biased toward embedding triggers in non-functional requirements (e.g., style modifiers, quality descriptors) rather than functional specifications that determine hardware behavior. Exploiting this insight, we propose Semantic Consensus Decoding (SCD), an inference-time passive defense with two key components: (1) functional requirement extraction that identifies essential requirements from user specifications, and (2) consensus decoding that adaptively fuses output distributions based on full user specifications and extracted functional requirements. When these distributions diverge significantly, SCD automatically suppresses suspicious components. Extensive experiments with three representative backdoor attacks demonstrate that SCD reduces average attack success rate from 89% to under 3% with negligible impact on generation quality.", "published": "2026-02-04T04:19:49Z", "updated": "2026-02-04T04:19:49Z", "authors": ["Guang Yang", "Xing Hu", "Xiang Chen", "Xin Xia"], "pdf_url": "https://arxiv.org/pdf/2602.04195v1"}
{"id": "http://arxiv.org/abs/2602.04185v1", "title": "SOGPTSpotter: Detecting ChatGPT-Generated Answers on Stack Overflow", "summary": "Stack Overflow is a popular Q&A platform where users ask technical questions and receive answers from a community of experts. Recently, there has been a significant increase in the number of answers generated by ChatGPT, which can lead to incorrect and unreliable information being posted on the site. While Stack Overflow has banned such AI-generated content, detecting whether a post is ChatGPT-generated remains a challenging task. We introduce a novel approach, SOGPTSpotter, that employs Siamese Neural Networks, leveraging the BigBird model and the Triplet loss, to detect ChatGPT-generated answers on Stack Overflow. We use triplets of human answers, reference answers, and ChatGPT answers. Our empirical evaluation reveals that our approach outperforms well-established baselines like GPTZero, DetectGPT, GLTR, BERT, RoBERTa, and GPT-2 in identifying ChatGPT-synthesized Stack Overflow responses. We also conducted an ablation study to show the effectiveness of our model. Additional experiments were conducted to assess various factors, including the impact of text length, the model's robustness against adversarial attacks, and its generalization capabilities across different domains and large language models. We also conducted a real-world case study on Stack Overflow. Using our tool's recommendations, Stack Overflow moderators were able to identify and take down ChatGPT-suspected generated answers, demonstrating the practical applicability and effectiveness of our approach.", "published": "2026-02-04T03:47:36Z", "updated": "2026-02-04T03:47:36Z", "authors": ["Suyu Ma", "Chunyang Chen", "Hourieh Khalajzadeh", "John Grundy"], "pdf_url": "https://arxiv.org/pdf/2602.04185v1"}
{"id": "http://arxiv.org/abs/2510.27675v2", "title": "On the Difficulty of Selecting Few-Shot Examples for Effective LLM-based Vulnerability Detection", "summary": "Large language models (LLMs) have demonstrated impressive capabilities across a wide range of coding tasks, including summarization, translation, completion, and code generation. Despite these advances, detecting code vulnerabilities remains a challenging problem for LLMs. In-context learning (ICL) has emerged as an effective mechanism for improving model performance by providing a small number of labeled examples within the prompt. Prior work has shown, however, that the effectiveness of ICL depends critically on how these few-shot examples are selected. In this paper, we study two intuitive criteria for selecting few-shot examples for ICL in the context of code vulnerability detection. The first criterion leverages model behavior by prioritizing samples on which the LLM consistently makes mistakes, motivated by the intuition that such samples can expose and correct systematic model weaknesses. The second criterion selects examples based on semantic similarity to the query program, using k-nearest-neighbor retrieval to identify relevant contexts.\n  We conduct extensive evaluations using open-source LLMs and datasets spanning multiple programming languages. Our results show that for Python and JavaScript, careful selection of few-shot examples can lead to measurable performance improvements in vulnerability detection. In contrast, for C and C++ programs, few-shot example selection has limited impact, suggesting that more powerful but also more expensive approaches, such as re-training or fine-tuning, may be required to substantially improve model performance.", "published": "2025-10-31T17:41:58Z", "updated": "2026-02-04T03:11:01Z", "authors": ["Md Abdul Hannan", "Ronghao Ni", "Chi Zhang", "Limin Jia", "Ravi Mangal", "Corina S. Pasareanu"], "pdf_url": "https://arxiv.org/pdf/2510.27675v2"}
{"id": "http://arxiv.org/abs/2602.04165v1", "title": "I Can't Believe It's Not a Valid Exploit", "summary": "Recently Large Language Models (LLMs) have been used in security vulnerability detection tasks including generating proof-of-concept (PoC) exploits. A PoC exploit is a program used to demonstrate how a vulnerability can be exploited. Several approaches suggest that supporting LLMs with additional guidance can improve PoC generation outcomes, motivating further evaluation of their effectiveness. In this work, we develop PoC-Gym, a framework for PoC generation for Java security vulnerabilities via LLMs and systematic validation of generated exploits. Using PoC-Gym, we evaluate whether the guidance from static analysis tools improves the PoC generation success rate and manually inspect the resulting PoCs. Our results from running PoC-Gym with Claude Sonnet 4, GPT-5 Medium, and gpt-oss-20b show that using static analysis for guidance and criteria lead to 21% higher success rates than the prior baseline, FaultLine. However, manual inspection of both successful and failed PoCs reveals that 71.5% of the PoCs are invalid. These results show that the reported success of LLM-based PoC generation can be significantly misleading, which is hard to detect with current validation mechanisms.", "published": "2026-02-04T02:59:03Z", "updated": "2026-02-04T02:59:03Z", "authors": ["Derin Gezgin", "Amartya Das", "Shinhae Kim", "Zhengdong Huang", "Nevena Stojkovic", "Claire Wang"], "pdf_url": "https://arxiv.org/pdf/2602.04165v1"}
{"id": "http://arxiv.org/abs/2510.04437v4", "title": "Smart Hiring Redefined: An Intelligent Recruitment Management Platform", "summary": "Against the backdrop of deepening digital and intelligent transformation in human resource management, traditional recruitment models struggle to fully meet enterprises' growing demand for precise talent acquisition due to limited efficiency, high costs, and information asymmetry. As a vital tool for optimizing recruitment processes, reducing labor and time costs, and enhancing core competitiveness, intelligent recruitment management systems have become an indispensable component of modern organizational talent strategies. Compared with the labor intensive tasks of resume screening, candidate position matching, and interview coordination in traditional manual recruitment, intelligent recruitment systems significantly enhance the efficiency and accuracy of the hiring process through automation and data driven approaches. These systems enable rapid parsing of massive resume volumes, intelligent matching of candidates to positions, and automated scheduling of interview processes. This substantially reduces the workload on human resources departments while improving recruitment quality and response speed. This research leverages the Java technology framework to design and implement an intelligent recruitment management system tailored for campus recruitment scenarios. The system establishes a collaborative platform connecting students, enterprises, and administrators through information technology and intelligent solutions, offering comprehensive functionalities including job posting distribution, resume submission, candidate position matching, and process management. Guided by the vision of Smart Campus Recruitment, the project delivers a more convenient job seeking experience for students and provides enterprises with more efficient talent screening and recruitment management services, thereby driving high quality development in university enterprise collaboration.", "published": "2025-10-06T02:11:53Z", "updated": "2026-02-04T01:50:45Z", "authors": ["Fangzhe Wu", "Dongyang Lyu", "Xiaoqi Li"], "pdf_url": "https://arxiv.org/pdf/2510.04437v4"}
{"id": "http://arxiv.org/abs/2602.04120v1", "title": "Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems", "summary": "Though Explainable AI (XAI) has made significant advancements, its inclusion in edge and IoT systems is typically ad-hoc and inefficient. Most current methods are \"coupled\" in such a way that they generate explanations simultaneously with model inferences. As a result, these approaches incur redundant computation, high latency and poor scalability when deployed across heterogeneous sets of edge devices. In this work we propose Explainability-as-a-Service (XaaS), a distributed architecture for treating explainability as a first-class system service (as opposed to a model-specific feature). The key innovation in our proposed XaaS architecture is that it decouples inference from explanation generation allowing edge devices to request, cache and verify explanations subject to resource and latency constraints. To achieve this, we introduce three main innovations: (1) A distributed explanation cache with a semantic similarity based explanation retrieval method which significantly reduces redundant computation; (2) A lightweight verification protocol that ensures the fidelity of both cached and newly generated explanations; and (3) An adaptive explanation engine that chooses explanation methods based upon device capability and user requirement. We evaluated the performance of XaaS on three real-world edge-AI use cases: (i) manufacturing quality control; (ii) autonomous vehicle perception; and (iii) healthcare diagnostics. Experimental results show that XaaS reduces latency by 38\\% while maintaining high explanation quality across three real-world deployments. Overall, this work enables the deployment of transparent and accountable AI across large scale, heterogeneous IoT systems, and bridges the gap between XAI research and edge-practicality.", "published": "2026-02-04T01:28:57Z", "updated": "2026-02-04T01:28:57Z", "authors": ["Samaresh Kumar Singh", "Joyjit Roy"], "pdf_url": "https://arxiv.org/pdf/2602.04120v1"}
