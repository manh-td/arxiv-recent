{"id": "http://arxiv.org/abs/2511.19331v1", "title": "Evolution of Cybersecurity Subdisciplines: A Science of Science Study", "summary": "The science of science is an emerging field that studies the practice of science itself. We present the first study of the cybersecurity discipline from a science of science perspective. We examine the evolution of two comparable interdisciplinary communities in cybersecurity: the Symposium on Usable Privacy and Security (SOUPS) and Financial Cryptography and Data Security (FC).", "published": "2025-11-24T17:26:28Z", "updated": "2025-11-24T17:26:28Z", "authors": ["Yao Chen", "Jeff Yan"], "pdf_url": "https://arxiv.org/pdf/2511.19331v1"}
{"id": "http://arxiv.org/abs/2511.19257v1", "title": "Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation", "summary": "With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A.", "published": "2025-11-24T16:11:01Z", "updated": "2025-11-24T16:11:01Z", "authors": ["Yingjia Shang", "Yi Liu", "Huimin Wang", "Furong Li", "Wenfang Sun", "Wu Chengyu", "Yefeng Zheng"], "pdf_url": "https://arxiv.org/pdf/2511.19257v1"}
{"id": "http://arxiv.org/abs/2511.19248v1", "title": "FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization", "summary": "Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance.", "published": "2025-11-24T16:02:01Z", "updated": "2025-11-24T16:02:01Z", "authors": ["Md Akil Raihan Iftee", "Syed Md. Ahnaf Hasan", "Amin Ahsan Ali", "AKM Mahbubur Rahman", "Sajib Mistry", "Aneesh Krishna"], "pdf_url": "https://arxiv.org/pdf/2511.19248v1"}
{"id": "http://arxiv.org/abs/2510.02280v2", "title": "An efficient quantum algorithm for computing $S$-units and its applications", "summary": "In this paper, we provide details on the proofs of the quantum polynomial time algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of a number field. This algorithm directly implies polynomial time methods to calculate class groups, S-class groups, relative class group and the unit group, ray class groups, solve the principal ideal problem, solve certain norm equations, and decompose ideal classes in the ideal class group. Additionally, combined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016), the resolution of the principal ideal problem allows one to find short generators of a principal ideal. Likewise, methods due to Cramer, Ducas and Wesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem and the decomposition of ideal classes to find so-called ``mildly short vectors'' in ideal lattices of cyclotomic fields.", "published": "2025-10-02T17:54:24Z", "updated": "2025-11-24T16:01:38Z", "authors": ["Jean-Francois Biasse", "Fang Song"], "pdf_url": "https://arxiv.org/pdf/2510.02280v2"}
{"id": "http://arxiv.org/abs/2511.19218v1", "title": "Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization", "summary": "Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems.", "published": "2025-11-24T15:23:41Z", "updated": "2025-11-24T15:23:41Z", "authors": ["Xurui Li", "Kaisong Song", "Rui Zhu", "Pin-Yu Chen", "Haixu Tang"], "pdf_url": "https://arxiv.org/pdf/2511.19218v1"}
{"id": "http://arxiv.org/abs/2506.13737v2", "title": "ExtendAttack: Attacking Servers of LRMs via Extending Reasoning", "summary": "Large Reasoning Models (LRMs) have demonstrated promising performance in complex tasks. However, the resource-consuming reasoning processes may be exploited by attackers to maliciously occupy the resources of the servers, leading to a crash, like the DDoS attack in cyber. To this end, we propose a novel attack method on LRMs termed ExtendAttack to maliciously occupy the resources of servers by stealthily extending the reasoning processes of LRMs. Concretely, we systematically obfuscate characters within a benign prompt, transforming them into a complex, poly-base ASCII representation. This compels the model to perform a series of computationally intensive decoding sub-tasks that are deeply embedded within the semantic structure of the query itself. Extensive experiments demonstrate the effectiveness of our proposed ExtendAttack. Remarkably, it significantly increases response length and latency, with the former increasing by over 2.7 times for the o3 model on the HumanEval benchmark. Besides, it preserves the original meaning of the query and achieves comparable answer accuracy, showing the stealthiness.", "published": "2025-06-16T17:49:05Z", "updated": "2025-11-24T15:07:05Z", "authors": ["Zhenhao Zhu", "Yue Liu", "Zhiwei Xu", "Yingwei Ma", "Hongcheng Gao", "Nuo Chen", "Yanpei Guo", "Wenjie Qu", "Huiying Xu", "Zifeng Kang", "Xinzhong Zhu", "Jiaheng Zhang"], "pdf_url": "https://arxiv.org/pdf/2506.13737v2"}
{"id": "http://arxiv.org/abs/2511.19171v1", "title": "Can LLMs Threaten Human Survival? Benchmarking Potential Existential Threats from LLMs via Prefix Completion", "summary": "Research on the safety evaluation of large language models (LLMs) has become extensive, driven by jailbreak studies that elicit unsafe responses. Such response involves information already available to humans, such as the answer to \"how to make a bomb\". When LLMs are jailbroken, the practical threat they pose to humans is negligible. However, it remains unclear whether LLMs commonly produce unpredictable outputs that could pose substantive threats to human safety. To address this gap, we study whether LLM-generated content contains potential existential threats, defined as outputs that imply or promote direct harm to human survival. We propose \\textsc{ExistBench}, a benchmark designed to evaluate such risks. Each sample in \\textsc{ExistBench} is derived from scenarios where humans are positioned as adversaries to AI assistants. Unlike existing evaluations, we use prefix completion to bypass model safeguards. This leads the LLMs to generate suffixes that express hostility toward humans or actions with severe threat, such as the execution of a nuclear strike. Our experiments on 10 LLMs reveal that LLM-generated content indicates existential threats. To investigate the underlying causes, we also analyze the attention logits from LLMs. To highlight real-world safety risks, we further develop a framework to assess model behavior in tool-calling. We find that LLMs actively select and invoke external tools with existential threats. Code and data are available at: https://github.com/cuiyu-ai/ExistBench.", "published": "2025-11-24T14:34:13Z", "updated": "2025-11-24T14:34:13Z", "authors": ["Yu Cui", "Yifei Liu", "Hang Fu", "Sicheng Pan", "Haibin Zhang", "Cong Zuo", "Licheng Wang"], "pdf_url": "https://arxiv.org/pdf/2511.19171v1"}
{"id": "http://arxiv.org/abs/2511.17260v2", "title": "Persistent BitTorrent Trackers", "summary": "Private BitTorrent trackers enforce upload-to-download ratios to prevent free-riding, but suffer from three critical weaknesses: reputation cannot move between trackers, centralized servers create single points of failure, and upload statistics are self-reported and unverifiable. When a tracker shuts down (whether by operator choice, technical failure, or legal action) users lose their contribution history and cannot prove their standing to new communities. We address these problems by storing reputation in smart contracts and replacing self-reports with cryptographic attestations. Receiving peers sign receipts for transferred pieces, which the tracker aggregates and verifies before updating on-chain reputation. Trackers run in Trusted Execution Environments (TEEs) to guarantee correct aggregation and prevent manipulation of state. If a tracker is unavailable, peers use an authenticated Distributed Hash Table (DHT) for discovery: the on-chain reputation acts as a Public Key Infrastructure (PKI), so peers can verify each other and maintain access control without the tracker. This design persists reputation across tracker failures and makes it portable to new instances through single-hop migration in factory-deployed contracts. We formalize the security requirements, prove correctness under standard cryptographic assumptions, and evaluate a prototype on Intel TDX. Measurements show that transfer receipts adds less than 6\\% overhead with typical piece sizes, and signature aggregation speeds up verification by $2.5\\times$.", "published": "2025-11-21T14:06:02Z", "updated": "2025-11-24T14:24:05Z", "authors": ["François-Xavier Wicht", "Zhengwei Tong", "Shunfan Zhou", "Hang Yin", "Aviv Yaish"], "pdf_url": "https://arxiv.org/pdf/2511.17260v2"}
{"id": "http://arxiv.org/abs/2504.20906v2", "title": "GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems", "summary": "The continuous monitoring of the interactions between cyber-physical components of any industrial control system (ICS) is required to secure automation of the system controls, and to guarantee plant processes are fail-safe and remain in an acceptably safe state. Safety is achieved by managing actuation (where electric signals are used to trigger physical movement), dependent on corresponding sensor readings; used as ground truth in decision making. Timely detection of anomalies (attacks, faults and unascertained states) in ICSs is crucial for the safe running of a plant, the safety of its personnel, and for the safe provision of any services provided. We propose an anomaly detection method that involves accurate linearization of the non-linear forms arising from sensor-actuator(s) relationships, primarily because solving linear models is easier and well understood. We accomplish this by using a well-known water treatment testbed as a use case. Our experiments show millisecond time response to detect anomalies, all of which are explainable and traceable; this simultaneous coupling of detection speed and explainability has not been achieved by other state of the art Artificial Intelligence (AI)/ Machine Learning (ML) models with eXplainable AI (XAI) used for the same purpose. Our methods explainability enables us to pin-point the sensor(s) and the actuation state(s) for which the anomaly was detected. The proposed algorithm showed an accuracy of 97.72% by flagging deviations within safe operation limits as non-anomalous; indicative that slower detectors with highest detection resolution is unnecessary, for systems whose safety boundaries provide leeway within safety limits.", "published": "2025-04-29T16:24:11Z", "updated": "2025-11-24T13:25:33Z", "authors": ["Sarad Venugopalan", "Sridhar Adepu"], "pdf_url": "https://arxiv.org/pdf/2504.20906v2"}
{"id": "http://arxiv.org/abs/2506.19268v4", "title": "Health App Reviews for Privacy & Trust (HARPT): A Corpus for Analyzing Patient Privacy Concerns, Trust in Providers and Trust in Applications", "summary": "Background: User reviews of Telehealth and Patient Portal mobile applications (apps) hereon referred to as electronic health (eHealth) apps are a rich source of unsolicited patient feedback, revealing critical insights into patient perceptions. However, the lack of large-scale, annotated datasets specific to privacy and trust has limited the ability of researchers to systematically analyze these concerns using natural language processing (NLP) techniques.\n  Objective: This study aims to develop and benchmark Health App Reviews for Privacy & Trust (HARPT), a large-scale annotated corpus of patient reviews from eHealth apps to advance research in patient privacy and trust.\n  Methods: We employed a multistage data construction strategy. This integrated keyword-based filtering, iterative manual labeling with review, targeted data augmentation, and weak supervision using transformer-based classifiers. A curated subset of 7,000 reviews was manually annotated to support machine learning model development and evaluation. The resulting dataset was used to benchmark a broad range of models.\n  Results: The HARPT corpus comprises 480,000 patient reviews annotated across seven categories capturing critical aspects of trust in the application (TA), trust in the provider (TP), and privacy concerns (PC). We provide comprehensive benchmark performance for a range of machine learning models on the manually annotated subset, establishing a baseline for future research.\n  Conclusions: The HARPT corpus is a significant resource for advancing the study of privacy and trust in the eHealth domain. By providing a large-scale, annotated dataset and initial benchmarks, this work supports reproducible research in usable privacy and trust within health informatics. HARPT is released under an open resource license.", "published": "2025-06-24T02:59:14Z", "updated": "2025-11-24T13:11:18Z", "authors": ["Timoteo Kelly", "Abdulkadir Korkmaz", "Samuel Mallet", "Connor Souders", "Sadra Aliakbarpour", "Praveen Rao"], "pdf_url": "https://arxiv.org/pdf/2506.19268v4"}
{"id": "http://arxiv.org/abs/2510.26610v2", "title": "A DRL-Empowered Multi-Level Jamming Approach for Secure Semantic Communication", "summary": "Semantic communication (SemCom) aims to transmit only task-relevant information, thereby improving communication efficiency but also exposing semantic information to potential eavesdropping. In this paper, we propose a deep reinforcement learning (DRL)-empowered multi-level jamming approach to enhance the security of SemCom systems over MIMO fading wiretap channels. This approach combines semantic layer jamming, achieved by encoding task-irrelevant text, and physical layer jamming, achieved by encoding random Gaussian noise. These two-level jamming signals are superposed with task-relevant semantic information to protect the transmitted semantics from eavesdropping. A deep deterministic policy gradient (DDPG) algorithm is further introduced to dynamically design and optimize the precoding matrices for both taskrelevant semantic information and multi-level jamming signals, aiming to enhance the legitimate user's image reconstruction while degrading the eavesdropper's performance. To jointly train the SemCom model and the DDPG agent, we propose an alternating optimization strategy where the two modules are updated iteratively. Experimental results demonstrate that, compared with both the encryption-based (ESCS) and encoded jammer-based (EJ) benchmarks, our method achieves comparable security while improving the legitimate user's peak signalto-noise ratio (PSNR) by up to approximately 0.6 dB.", "published": "2025-10-30T15:38:27Z", "updated": "2025-11-24T13:00:48Z", "authors": ["Weixuan Chen", "Qianqian Yang"], "pdf_url": "https://arxiv.org/pdf/2510.26610v2"}
{"id": "http://arxiv.org/abs/2511.06852v4", "title": "Differentiated Directional Intervention A Framework for Evading LLM Safety Alignment", "summary": "Safety alignment instills in Large Language Models (LLMs) a critical capacity to refuse malicious requests. Prior works have modeled this refusal mechanism as a single linear direction in the activation space. We posit that this is an oversimplification that conflates two functionally distinct neural processes: the detection of harm and the execution of a refusal. In this work, we deconstruct this single representation into a Harm Detection Direction and a Refusal Execution Direction. Leveraging this fine-grained model, we introduce Differentiated Bi-Directional Intervention (DBDI), a new white-box framework that precisely neutralizes the safety alignment at critical layer. DBDI applies adaptive projection nullification to the refusal execution direction while suppressing the harm detection direction via direct steering. Extensive experiments demonstrate that DBDI outperforms prominent jailbreaking methods, achieving up to a 97.88\\% attack success rate on models such as Llama-2. By providing a more granular and mechanistic framework, our work offers a new direction for the in-depth understanding of LLM safety alignment.", "published": "2025-11-10T08:52:34Z", "updated": "2025-11-24T11:44:59Z", "authors": ["Peng Zhang", "Peijie Sun"], "pdf_url": "https://arxiv.org/pdf/2511.06852v4"}
{"id": "http://arxiv.org/abs/2511.19015v1", "title": "A General Framework for Per-record Differential Privacy", "summary": "Differential Privacy (DP) is a widely adopted standard for privacy-preserving data analysis, but it assumes a uniform privacy budget across all records, limiting its applicability when privacy requirements vary with data values. Per-record Differential Privacy (PrDP) addresses this by defining the privacy budget as a function of each record, offering better alignment with real-world needs. However, the dependency between the privacy budget and the data value introduces challenges in protecting the budget's privacy itself. Existing solutions either handle specific privacy functions or adopt relaxed PrDP definitions. A simple workaround is to use the global minimum of the privacy function, but this severely degrades utility, as the minimum is often set extremely low to account for rare records with high privacy needs. In this work, we propose a general and practical framework that enables any standard DP mechanism to support PrDP, with error depending only on the minimal privacy requirement among records actually present in the dataset. Since directly revealing this minimum may leak information, we introduce a core technique called privacy-specified domain partitioning, which ensures accurate estimation without compromising privacy. We also extend our framework to the local DP setting via a novel technique, privacy-specified query augmentation. Using our framework, we present the first PrDP solutions for fundamental tasks such as count, sum, and maximum estimation. Experimental results show that our mechanisms achieve high utility and significantly outperform existing Personalized DP (PDP) methods, which can be viewed as a special case of PrDP with relaxed privacy protection.", "published": "2025-11-24T11:44:10Z", "updated": "2025-11-24T11:44:10Z", "authors": ["Xinghe Chen", "Dajun Sun", "Quanqing Xu", "Wei Dong"], "pdf_url": "https://arxiv.org/pdf/2511.19015v1"}
{"id": "http://arxiv.org/abs/2511.19009v1", "title": "Understanding and Mitigating Over-refusal for Large Language Models via Safety Representation", "summary": "Large language models demonstrate powerful capabilities across various natural language processing tasks, yet they also harbor safety vulnerabilities. To enhance LLM safety, various jailbreak defense methods have been proposed to guard against harmful outputs. However, improvements in model safety often come at the cost of severe over-refusal, failing to strike a good balance between safety and usability. In this paper, we first analyze the causes of over-refusal from a representation perspective, revealing that over-refusal samples reside at the boundary between benign and malicious samples. Based on this, we propose MOSR, designed to mitigate over-refusal by intervening the safety representation of LLMs. MOSR incorporates two novel components: (1) Overlap-Aware Loss Weighting, which determines the erasure weight for malicious samples by quantifying their similarity to pseudo-malicious samples in the representation space, and (2) Context-Aware Augmentation, which supplements the necessary context for rejection decisions by adding harmful prefixes before rejection responses. Experiments demonstrate that our method outperforms existing approaches in mitigating over-refusal while largely maintaining safety. Overall, we advocate that future defense methods should strike a better balance between safety and over-refusal.", "published": "2025-11-24T11:38:53Z", "updated": "2025-11-24T11:38:53Z", "authors": ["Junbo Zhang", "Ran Chen", "Qianli Zhou", "Xinyang Deng", "Wen Jiang"], "pdf_url": "https://arxiv.org/pdf/2511.19009v1"}
{"id": "http://arxiv.org/abs/2511.12993v2", "title": "SmartPoC: Generating Executable and Validated PoCs for Smart Contract Bug Reports", "summary": "Smart contracts are prone to vulnerabilities and are analyzed by experts as well as automated systems, such as static analysis and AI-assisted solutions. However, audit artifacts are heterogeneous and often lack reproducible, executable PoC tests suitable for automated validation, leading to costly, ad hoc manual verification. Large language models (LLMs) can be leveraged to turn audit reports into PoC test cases, but have three major challenges: noisy inputs, hallucinations, and missing runtime oracles. In this paper, we present SmartPoC, an automated framework that converts textual audit reports into executable, validated test cases. First, the input audit report is processed to reduce noise, and only bug-related functions are extracted and fed to LLMs as context. To curb hallucinations and ensure compile-and-run readiness, we leverage LLMs to synthesize PoC test cases with specially-designed pre-/post-execution repair. We further utilize differential verification as oracles to confirm exploitability of the PoC test cases. On the SmartBugs-Vul and FORGE-Vul benchmarks, SmartPoC generates executable, validated Foundry test cases for 85.61% and 86.45% of targets, respectively. Applied to the latest Etherscan verified-source corpus, SmartPoC confirms 236 real bugs out of 545 audit findings at a cost of only $0.03 per finding.", "published": "2025-11-17T05:37:20Z", "updated": "2025-11-24T11:08:48Z", "authors": ["Longfei Chen", "Ruibin Yan", "Taiyu Wong", "Yiyang Chen", "Chao Zhang"], "pdf_url": "https://arxiv.org/pdf/2511.12993v2"}
{"id": "http://arxiv.org/abs/2511.18966v1", "title": "LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large Language Models", "summary": "The security of code generated by large language models (LLMs) is a significant concern, as studies indicate that such code often contains vulnerabilities and lacks essential defensive programming constructs. This work focuses on examining and evaluating the security of LLM-generated code, particularly in the context of C/C++. We categorized known vulnerabilities using the Common Weakness Enumeration (CWE) and, to study their criticality, mapped them to CVEs. We used ten different LLMs for code generation and analyzed the outputs through static analysis. The amount of CWEs present in AI-generated code is concerning. Our findings highlight the need for developers to be cautious when using LLM-generated code. This study provides valuable insights to advance automated code generation and encourage further research in this domain.", "published": "2025-11-24T10:31:53Z", "updated": "2025-11-24T10:31:53Z", "authors": ["Muhammad Usman Shahid", "Chuadhry Mujeeb Ahmed", "Rajiv Ranjan"], "pdf_url": "https://arxiv.org/pdf/2511.18966v1"}
{"id": "http://arxiv.org/abs/2511.18933v1", "title": "Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations", "summary": "Large Language Models (LLMs) remain susceptible to jailbreak exploits that bypass safety filters and induce harmful or unethical behavior. This work presents a systematic taxonomy of existing jailbreak defenses across prompt-level, model-level, and training-time interventions, followed by three proposed defense strategies. First, a Prompt-Level Defense Framework detects and neutralizes adversarial inputs through sanitization, paraphrasing, and adaptive system guarding. Second, a Logit-Based Steering Defense reinforces refusal behavior through inference-time vector steering in safety-sensitive layers. Third, a Domain-Specific Agent Defense employs the MetaGPT framework to enforce structured, role-based collaboration and domain adherence. Experiments on benchmark datasets show substantial reductions in attack success rate, achieving full mitigation under the agent-based defense. Overall, this study highlights how jailbreaks pose a significant security threat to LLMs and identifies key intervention points for prevention, while noting that defense strategies often involve trade-offs between safety, performance, and scalability. Code is available at: https://github.com/Kuro0911/CS5446-Project", "published": "2025-11-24T09:38:11Z", "updated": "2025-11-24T09:38:11Z", "authors": ["Ryan Wong", "Hosea David Yu Fei Ng", "Dhananjai Sharma", "Glenn Jun Jie Ng", "Kavishvaran Srinivasan"], "pdf_url": "https://arxiv.org/pdf/2511.18933v1"}
{"id": "http://arxiv.org/abs/2511.17070v2", "title": "TICAL: Trusted and Integrity-protected Compilation of AppLications", "summary": "During the past few years, we have witnessed various efforts to provide confidentiality and integrity for applications running in untrusted environments such as public clouds. In most of these approaches, hardware extensions such as Intel SGX, TDX, AMD SEV, etc., are leveraged to provide encryption and integrity protection on process or VM level. Although all of these approaches increase the trust in the application at runtime, an often overlooked aspect is the integrity and confidentiality protection at build time, which is equally important as maliciously injected code during compilation can compromise the entire application and system. In this paper, we present Tical, a practical framework for trusted compilation that provides integrity protection and confidentiality in build pipelines from source code to the final executable. Our approach harnesses TEEs as runtime protection but enriches TEEs with file system shielding and an immutable audit log with version history to provide accountability. This way, we can ensure that the compiler chain can only access trusted files and intermediate output, such as object files produced by trusted processes. Our evaluation using micro- and macro-benchmarks shows that Tical can protect the confidentiality and integrity of whole CI/CD pipelines with an acceptable performance overhead.", "published": "2025-11-21T09:19:41Z", "updated": "2025-11-24T09:28:48Z", "authors": ["Robert Krahn", "Nikson Kanti Paul", "Franz Gregor", "Do Le Quoc", "Andrey Brito", "André Martin", "Christof Fetzer"], "pdf_url": "https://arxiv.org/pdf/2511.17070v2"}
{"id": "http://arxiv.org/abs/2511.16088v2", "title": "Future-Back Threat Modeling: A Foresight-Driven Security Framework", "summary": "Traditional threat modeling remains reactive-focused on known TTPs and past incident data, while threat prediction and forecasting frameworks are often disconnected from operational or architectural artifacts. This creates a fundamental weakness: the most serious cyber threats often do not arise from what is known, but from what is assumed, overlooked, or not yet conceived, and frequently originate from the future, such as artificial intelligence, information warfare, and supply chain attacks, where adversaries continuously develop new exploits that can bypass defenses built on current knowledge. To address this mental gap, this paper introduces the theory and methodology of Future-Back Threat Modeling (FBTM). This predictive approach begins with envisioned future threat states and works backward to identify assumptions, gaps, blind spots, and vulnerabilities in the current defense architecture, providing a clearer and more accurate view of impending threats so that we can anticipate their emergence and shape the future we want through actions taken now. The proposed methodology further aims to reveal known unknowns and unknown unknowns, including tactics, techniques, and procedures that are emerging, anticipated, and plausible. This enhances the predictability of adversary behavior, particularly under future uncertainty, helping security leaders make informed decisions today that shape more resilient security postures for the future.", "published": "2025-11-20T06:26:01Z", "updated": "2025-11-24T09:21:12Z", "authors": ["Vu Van Than"], "pdf_url": "https://arxiv.org/pdf/2511.16088v2"}
{"id": "http://arxiv.org/abs/2310.14821v6", "title": "Mysticeti: Reaching the Limits of Latency with Uncertified DAGs", "summary": "We introduce Mysticeti-C, the first DAG-based Byzantine consensus protocol to achieve the lower bounds of latency of 3 message rounds. Since Mysticeti-C is built over DAGs it also achieves high resource efficiency and censorship resistance. Mysticeti-C achieves this latency improvement by avoiding explicit certification of the DAG blocks and by proposing a novel commit rule such that every block can be committed without delays, resulting in optimal latency in the steady state and under crash failures. We further extend Mysticeti-C to Mysticeti-FPC, which incorporates a fast commit path that achieves even lower latency for transferring assets. Unlike prior fast commit path protocols, Mysticeti-FPC minimizes the number of signatures and messages by weaving the fast path transactions into the DAG. This frees up resources, which subsequently result in better performance. We prove the safety and liveness in a Byzantine context. We evaluate both Mysticeti protocols and compare them with state-of-the-art consensus and fast path protocols to demonstrate their low latency and resource efficiency, as well as their more graceful degradation under crash failures. Mysticeti-C is the first Byzantine consensus protocol to achieve WAN latency of 0.5s for consensus commit while simultaneously maintaining state-of-the-art throughput of over 200k TPS. Finally, we report on integrating Mysticeti-C as the consensus protocol into the Sui blockchain, resulting in over 4x latency reduction.", "published": "2023-10-23T11:40:50Z", "updated": "2025-11-24T09:10:32Z", "authors": ["Kushal Babel", "Andrey Chursin", "George Danezis", "Anastasios Kichidis", "Lefteris Kokoris-Kogias", "Arun Koshy", "Alberto Sonnino", "Mingwei Tian"], "pdf_url": "https://arxiv.org/pdf/2310.14821v6"}
{"id": "http://arxiv.org/abs/2511.07242v4", "title": "Privacy on the Fly: A Predictive Adversarial Transformation Network for Mobile Sensor Data", "summary": "Mobile motion sensors such as accelerometers and gyroscopes are now ubiquitously accessible by third-party apps via standard APIs. While enabling rich functionalities like activity recognition and step counting, this openness has also enabled unregulated inference of sensitive user traits, such as gender, age, and even identity, without user consent. Existing privacy-preserving techniques, such as GAN-based obfuscation or differential privacy, typically require access to the full input sequence, introducing latency that is incompatible with real-time scenarios. Worse, they tend to distort temporal and semantic patterns, degrading the utility of the data for benign tasks like activity recognition. To address these limitations, we propose the Predictive Adversarial Transformation Network (PATN), a real-time privacy-preserving framework that leverages historical signals to generate adversarial perturbations proactively. The perturbations are applied immediately upon data acquisition, enabling continuous protection without disrupting application functionality. Experiments on two datasets demonstrate that PATN substantially degrades the performance of privacy inference models, achieving Attack Success Rate (ASR) of 40.11% and 44.65% (reducing inference accuracy to near-random) and increasing the Equal Error Rate (EER) from 8.30% and 7.56% to 41.65% and 46.22%. On ASR, PATN outperforms baseline methods by 16.16% and 31.96%, respectively.", "published": "2025-11-10T15:57:17Z", "updated": "2025-11-24T08:58:20Z", "authors": ["Tianle Song", "Chenhao Lin", "Yang Cao", "Zhengyu Zhao", "Jiahao Sun", "Chong Zhang", "Le Yang", "Chao Shen"], "pdf_url": "https://arxiv.org/pdf/2511.07242v4"}
{"id": "http://arxiv.org/abs/2508.00434v2", "title": "CIF: A Constrained Inversion Framework for Reliable Message Extraction in Diffusion-Based Generative Steganography", "summary": "Generative image steganography aims to conceal secret information in generated images without arousing suspicion. However, in practical scenarios involving high-capacity embedding or lossy transmission, existing methods still suffer from limited extraction accuracy. The main challenge lies in accurately recovering the secret-embedded latent vectors from stego images. To address this issue, we propose CIF, a constrained inversion framework designed to achieve accurate message extraction. Specifically, CIF reduces dynamic structural errors by enforcing linear consistency in the latent space, meanwhile reduces numerical integration errors by adaptively adjusting the integration order according to local trajectory stability. Experimental results show that our method reduces latent reconstruction error by more than 35\\% and achieves higher message extraction accuracy than existing approaches.", "published": "2025-08-01T08:46:32Z", "updated": "2025-11-24T07:38:06Z", "authors": ["Yuqi Qian", "Yun Cao", "Meiyang Lv", "Haocheng Fu"], "pdf_url": "https://arxiv.org/pdf/2508.00434v2"}
{"id": "http://arxiv.org/abs/2508.09456v3", "title": "IAG: Input-aware Backdoor Attack on VLM-based Visual Grounding", "summary": "Recent advances in vision-language models (VLMs) have significantly enhanced the visual grounding task, which involves locating objects in an image based on natural language queries. Despite these advancements, the security of VLM-based grounding systems has not been thoroughly investigated. This paper reveals a novel and realistic vulnerability: the first multi-target backdoor attack on VLM-based visual grounding. Unlike prior attacks that rely on static triggers or fixed targets, we propose IAG, a method that dynamically generates input-aware, text-guided triggers conditioned on any specified target object description to execute the attack. This is achieved through a text-conditioned UNet that embeds imperceptible target semantic cues into visual inputs while preserving normal grounding performance on benign samples. We further develop a joint training objective that balances language capability with perceptual reconstruction to ensure imperceptibility, effectiveness, and stealth. Extensive experiments on multiple VLMs (e.g., LLaVA, InternVL, Ferret) and benchmarks (RefCOCO, RefCOCO+, RefCOCOg, Flickr30k Entities, and ShowUI) demonstrate that IAG achieves the best ASRs compared with other baselines on almost all settings without compromising clean accuracy, maintaining robustness against existing defenses, and exhibiting transferability across datasets and models. These findings underscore critical security risks in grounding-capable VLMs and highlight the need for further research on trustworthy multimodal understanding.", "published": "2025-08-13T03:22:19Z", "updated": "2025-11-24T07:19:43Z", "authors": ["Junxian Li", "Beining Xu", "Simin Chen", "Jiatong Li", "Jingdi Lei", "Haodong Zhao", "Di Zhang"], "pdf_url": "https://arxiv.org/pdf/2508.09456v3"}
{"id": "http://arxiv.org/abs/2511.18790v1", "title": "RoguePrompt: Dual-Layer Ciphering for Self-Reconstruction to Circumvent LLM Moderation", "summary": "Content moderation pipelines for modern large language models combine static filters, dedicated moderation services, and alignment tuned base models, yet real world deployments still exhibit dangerous failure modes. This paper presents RoguePrompt, an automated jailbreak attack that converts a disallowed user query into a self reconstructing prompt which passes provider moderation while preserving the original harmful intent. RoguePrompt partitions the instruction across two lexical streams, applies nested classical ciphers, and wraps the result in natural language directives that cause the target model to decode and execute the hidden payload. Our attack assumes only black box access to the model and to the associated moderation endpoint. We instantiate RoguePrompt against GPT 4o and evaluate it on 2 448 prompts that a production moderation system previously marked as strongly rejected. Under an evaluation protocol that separates three security relevant outcomes bypass, reconstruction, and execution the attack attains 84.7 percent bypass, 80.2 percent reconstruction, and 71.5 percent full execution, substantially outperforming five automated jailbreak baselines. We further analyze the behavior of several automated and human aligned evaluators and show that dual layer lexical transformations remain effective even when detectors rely on semantic similarity or learned safety rubrics. Our results highlight systematic blind spots in current moderation practice and suggest that robust deployment will require joint reasoning about user intent, decoding workflows, and model side computation rather than surface level toxicity alone.", "published": "2025-11-24T05:42:54Z", "updated": "2025-11-24T05:42:54Z", "authors": ["Benyamin Tafreshian"], "pdf_url": "https://arxiv.org/pdf/2511.18790v1"}
{"id": "http://arxiv.org/abs/2511.18772v1", "title": "Re-Key-Free, Risky-Free: Adaptable Model Usage Control", "summary": "Deep neural networks (DNNs) have become valuable intellectual property of model owners, due to the substantial resources required for their development. To protect these assets in the deployed environment, recent research has proposed model usage control mechanisms to ensure models cannot be used without proper authorization. These methods typically lock the utility of the model by embedding an access key into its parameters. However, they often assume static deployment, and largely fail to withstand continual post-deployment model updates, such as fine-tuning or task-specific adaptation. In this paper, we propose ADALOC, to endow key-based model usage control with adaptability during model evolution. It strategically selects a subset of weights as an intrinsic access key, which enables all model updates to be confined to this key throughout the evolution lifecycle. ADALOC enables using the access key to restore the keyed model to the latest authorized states without redistributing the entire network (i.e., adaptation), and frees the model owner from full re-keying after each model update (i.e., lock preservation). We establish a formal foundation to underpin ADALOC, providing crucial bounds such as the errors introduced by updates restricted to the access key. Experiments on standard benchmarks, such as CIFAR-100, Caltech-256, and Flowers-102, and modern architectures, including ResNet, DenseNet, and ConvNeXt, demonstrate that ADALOC achieves high accuracy under significant updates while retaining robust protections. Specifically, authorized usages consistently achieve strong task-specific performance, while unauthorized usage accuracy drops to near-random guessing levels (e.g., 1.01% on CIFAR-100), compared to up to 87.01% without ADALOC. This shows that ADALOC can offer a practical solution for adaptive and protected DNN deployment in evolving real-world scenarios.", "published": "2025-11-24T05:13:45Z", "updated": "2025-11-24T05:13:45Z", "authors": ["Zihan Wang", "Zhongkui Ma", "Xinguo Feng", "Chuan Yan", "Dongge Liu", "Ruoxi Sun", "Derui Wang", "Minhui Xue", "Guangdong Bai"], "pdf_url": "https://arxiv.org/pdf/2511.18772v1"}
{"id": "http://arxiv.org/abs/2412.15289v5", "title": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage", "summary": "Large language models (LLMs) have made significant advancements across various tasks, but their safety alignment remain a major concern. Exploring jailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure them. Existing methods primarily design sophisticated instructions for the LLM to follow, or rely on multiple iterations, which could hinder the performance and efficiency of jailbreaks. In this work, we propose a novel jailbreak paradigm, Simple Assistive Task Linkage (SATA), which can effectively circumvent LLM safeguards and elicit harmful responses. Specifically, SATA first masks harmful keywords within a malicious query to generate a relatively benign query containing one or multiple [MASK] special tokens. It then employs a simple assistive task such as a masked language model task or an element lookup by position task to encode the semantics of the masked keywords. Finally, SATA links the assistive task with the masked query to jointly perform the jailbreak. Extensive experiments show that SATA achieves state-of-the-art performance and outperforms baselines by a large margin. Specifically, on AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and with element lookup by position (ELP) assistive task, SATA attains an overall ASR of 76% and HS of 4.43.", "published": "2024-12-19T05:57:37Z", "updated": "2025-11-24T04:42:27Z", "authors": ["Xiaoning Dong", "Wenbo Hu", "Wei Xu", "Tianxing He"], "pdf_url": "https://arxiv.org/pdf/2412.15289v5"}
{"id": "http://arxiv.org/abs/2511.18748v1", "title": "Evaluation of Real-Time Mitigation Techniques for Cyber Security in IEC 61850 / IEC 62351 Substations", "summary": "The digitalization of substations enlarges the cyber-attack surface, necessitating effective detection and mitigation of cyber attacks in digital substations. While machine learning-based intrusion detection has been widely explored, such methods have not demonstrated detection and mitigation within the required real-time budget. In contrast, cryptographic authentication has emerged as a practical candidate for real-time cyber defense, as specified in IEC 62351. In addition, lightweight rule-based intrusion detection that validates IEC 61850 semantics can provide specification-based detection of anomalous or malicious traffic with minimal processing delay. This paper presents the design logic and implementation aspects of three potential real-time mitigation techniques capable of countering GOOSE-based attacks: (i) IEC 62351-compliant message authentication code (MAC) scheme, (ii) a semantics-enforced rule-based intrusion detection system (IDS), and (iii) a hybrid approach integrating both MAC verification and Intrusion Detection System (IDS). A comparative evaluation of these real-time mitigation approaches is conducted using a cyber-physical system (CPS) security testbed. The results show that the hybrid integration significantly enhances mitigation capability. Furthermore, the processing delays of all three methods remain within the strict delivery requirements of GOOSE communication. The study also identifies limitations that none of the techniques can fully address, highlighting areas for future work.", "published": "2025-11-24T04:20:49Z", "updated": "2025-11-24T04:20:49Z", "authors": ["Akila Herath", "Chen-Ching Liu", "Junho Hong", "Kuchan Park"], "pdf_url": "https://arxiv.org/pdf/2511.18748v1"}
{"id": "http://arxiv.org/abs/2411.11195v5", "title": "SoK: The Security-Safety Continuum of Multimodal Foundation Models through Information Flow and Global Game-Theoretic Analysis of Asymmetric Threats", "summary": "Multimodal foundation models (MFMs) integrate diverse data modalities to support complex and wide-ranging tasks. However, this integration also introduces distinct safety and security challenges. In this paper, we unify the concepts of safety and security in the context of MFMs by identifying critical threats that arise from both model behavior and system-level interactions. We propose a taxonomy grounded in information theory, evaluating risks through the concepts of channel capacity, signal, noise, and bandwidth. This perspective provides a principled way to analyze how information flows through MFMs and how vulnerabilities can emerge across modalities. Building on this foundation, we introduce a deterministic minimax formulation to analyze defense mechanisms and expose structural vulnerabilities in multimodal systems. Our framework projects attacks onto the noise, signal, and bandwidth axes, collapsing the defense search space and mitigating defender asymmetry. Across 15 defenses, we find that system-level bandwidth and behavior constraints generalize substantially better than brittle model-only methods. Finally, we formalize an MFM \"self-destruction threshold\" that specifies when termination should be triggered, providing a concrete activation rule for circuit-breaker safeguards within multimodal systems.", "published": "2024-11-17T23:06:20Z", "updated": "2025-11-24T03:58:11Z", "authors": ["Ruoxi Sun", "Jiamin Chang", "Hammond Pearce", "Chaowei Xiao", "Bo Li", "Qi Wu", "Surya Nepal", "Minhui Xue"], "pdf_url": "https://arxiv.org/pdf/2411.11195v5"}
{"id": "http://arxiv.org/abs/2511.09582v2", "title": "Revisit to the Bai-Galbraith signature scheme", "summary": "Dilithium is one of the NIST approved lattice-based signature schemes. In this short note we describe the Bai-Galbraith signature scheme proposed in BG14, which differs to Dilithium, due to the fact that there is no public key compression. This lattice-based signature scheme is based on Learning with Errors (LWE).", "published": "2025-11-12T10:56:05Z", "updated": "2025-11-24T02:51:59Z", "authors": ["Banhirup Sengupta", "Peenal Gupta", "Souvik Sengupta"], "pdf_url": "https://arxiv.org/pdf/2511.09582v2"}
