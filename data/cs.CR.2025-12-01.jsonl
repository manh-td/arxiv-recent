{"id": "http://arxiv.org/abs/2506.15018v2", "title": "Private Continual Counting of Unbounded Streams", "summary": "We study the problem of differentially private continual counting in the unbounded setting where the input size $n$ is not known in advance. Current state-of-the-art algorithms based on optimal instantiations of the matrix mechanism cannot be directly applied here because their privacy guarantees only hold when key parameters are tuned to $n$. Using the common `doubling trick' avoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve this problem by introducing novel matrix factorizations based on logarithmic perturbations of the function $\\frac{1}{\\sqrt{1-z}}$ studied in prior works, which may be of independent interest. The resulting algorithm has smooth error, and for any $α> 0$ and $t\\leq n$ it is able to privately estimate the sum of the first $t$ data points with $O(\\log^{2+2α}(t))$ variance. It requires $O(t)$ space and amortized $O(\\log t)$ time per round, compared to $O(\\log(n)\\log(t))$ variance, $O(n)$ space and $O(n \\log n)$ pre-processing time for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA 2023). Empirically, we find that our algorithm's performance is also comparable to theirs in absolute terms: our variance is less than $1.5\\times$ theirs for $t$ as large as $2^{24}$.", "published": "2025-06-17T23:09:53Z", "updated": "2025-12-01T18:41:49Z", "authors": ["Ben Jacobsen", "Kassem Fawaz"], "pdf_url": "https://arxiv.org/pdf/2506.15018v2"}
{"id": "http://arxiv.org/abs/2512.01974v1", "title": "The Equivalence of Fast Algorithms for Convolution, Parallel FIR Filters, Polynomial Modular Multiplication, and Pointwise Multiplication in DFT/NTT Domain", "summary": "Fast time-domain algorithms have been developed in signal processing applications to reduce the multiplication complexity. For example, fast convolution structures using Cook-Toom and Winograd algorithms are well understood. Short length fast convolutions can be iterated to obtain fast convolution structures for long lengths. In this paper, we show that well known fast convolution structures form the basis for design of fast algorithms in four other problem domains: fast parallel filters, fast polynomial modular multiplication, and fast pointwise multiplication in the DFT and NTT domains. Fast polynomial modular multiplication and fast pointwise multiplication problems are important for cryptosystem applications such as post-quantum cryptography and homomorphic encryption. By establishing the equivalence of these problems, we show that a fast structure from one domain can be used to design a fast structure for another domain. This understanding is important as there are many well known solutions for fast convolution that can be used in other signal processing and cryptosystem applications.", "published": "2025-12-01T18:29:28Z", "updated": "2025-12-01T18:29:28Z", "authors": ["Keshab K. Parhi"], "pdf_url": "https://arxiv.org/pdf/2512.01974v1"}
{"id": "http://arxiv.org/abs/2502.14017v2", "title": "Cyber security of OT networks: A tutorial and overview", "summary": "This manuscript explores the cybersecurity challenges of Operational Technology (OT) networks, focusing on their critical role in industrial environments such as manufacturing, energy, and utilities. As OT systems increasingly integrate with Information Technology (IT) systems due to Industry 4.0 initiatives, they become more vulnerable to cyberattacks, which pose risks not only to data but also to physical infrastructure. The study examines key components of OT systems, such as SCADA (Supervisory Control and Data Acquisition), PLCs (Programmable Logic Controllers), and RTUs (Remote Terminal Units), and analyzes recent cyberattacks targeting OT environments. Furthermore, it highlights the security concerns arising from the convergence of IT and OT systems, examining attack vectors and the growing threats posed by malware, ransomware, and nation-state actors. Finally, the paper discusses modern approaches and tools used to secure these environments, providing insights into improving the cybersecurity posture of OT networks.", "published": "2025-02-19T17:23:42Z", "updated": "2025-12-01T18:07:06Z", "authors": ["Sarthak Kapoor", "Sumit Kumar", "Harsh Vardhan"], "pdf_url": "https://arxiv.org/pdf/2502.14017v2"}
{"id": "http://arxiv.org/abs/2512.01893v1", "title": "Improving Phishing Resilience with AI-Generated Training: Evidence on Prompting, Personalization, and Duration", "summary": "Phishing remains a persistent cybersecurity threat; however, developing scalable and effective user training is labor-intensive and challenging to maintain. Generative Artificial Intelligence offers an interesting opportunity, but empirical evidence on its instructional efficacy remains scarce. This paper provides an experimental validation of Large Language Models (LLMs) as autonomous engines for generating phishing resilience training. Across two controlled studies (N=480), we demonstrate that AI-generated content yields significant pre-post learning gains regardless of the specific prompting strategy employed. Study 1 (N=80) compares four prompting techniques, finding that even a straightforward \"direct-profile\" strategy--simply embedding user traits into the prompt--produces effective training material. Study 2 (N=400) investigates the scalability of this approach by testing personalization and training duration. Results show that complex psychometric personalization offers no measurable advantage over well-designed generic content, while longer training duration provides a modest boost in accuracy. These findings suggest that organizations can leverage LLMs to generate high-quality, effective training at scale without the need for complex user profiling, relying instead on the inherent capabilities of the model.", "published": "2025-12-01T17:13:09Z", "updated": "2025-12-01T17:13:09Z", "authors": ["Francesco Greco", "Giuseppe Desolda", "Cesare Tucci", "Andrea Esposito", "Antonio Curci", "Antonio Piccinno"], "pdf_url": "https://arxiv.org/pdf/2512.01893v1"}
{"id": "http://arxiv.org/abs/2512.01891v1", "title": "Behind the Curtain: How Shared Hosting Providers Respond to Vulnerability Notifications", "summary": "Large-scale vulnerability notifications (VNs) can help hosting provider organizations (HPOs) identify and remediate security vulnerabilities that attackers can exploit in data breaches or phishing campaigns. Previous VN studies have primarily focused on factors under the control of reporters, such as sender reputation, email formatting, and communication channels. Despite these efforts, remediation rates for vulnerability notifications continue to remain consistently low. This paper presents the first in-depth study of how HPOs process vulnerability notifications internally and what organizational and operational factors influence VN effectiveness. We examine the problem from a different perspective to provide the first detailed understanding of the reasons behind persistently low remediation rates. Instead of manipulating parameters of VN campaigns, we interview hosting providers directly, investigating how they handle vulnerability notifications and what factors may influence VN effectiveness, such as VN awareness and reachability, HPOs' service models, and perceived security risks.\n  We conducted semi-structured interviews with 24 HPOs across shared hosting and web development services, representing varied company sizes and operator roles. Our findings reveal practical insights on VN processing and abuse workflows. While some providers remain hard to reach due to complex infrastructures, most report routinely handling VNs. However, limited remediation often stems from strict responsibility boundaries, where web application issues are seen as the customer's domain. Low hosting fees and high volumes of daily compromises further discourage both proactive and reactive measures. Our findings show that HPOs blame negligent website owners, and prior works on website owners confirms they often undervalue their sites or lack security know-how.", "published": "2025-12-01T17:12:13Z", "updated": "2025-12-01T17:12:13Z", "authors": ["Giada Stivala", "Rafael Mrowczynski", "Maria Hellenthal", "Giancarlo Pellegrino"], "pdf_url": "https://arxiv.org/pdf/2512.01891v1"}
{"id": "http://arxiv.org/abs/2510.20075v4", "title": "LLMs can hide text in other text of the same length", "summary": "A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present Calgacus, a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.", "published": "2025-10-22T23:16:50Z", "updated": "2025-12-01T17:01:54Z", "authors": ["Antonio Norelli", "Michael Bronstein"], "pdf_url": "https://arxiv.org/pdf/2510.20075v4"}
{"id": "http://arxiv.org/abs/2512.01845v1", "title": "JPEGs Just Got Snipped: Croppable Signatures Against Deepfake Images", "summary": "Deepfakes are a type of synthetic media created using artificial intelligence, specifically deep learning algorithms. This technology can for example superimpose faces and voices onto videos, creating hyper-realistic but artificial representations. Deepfakes pose significant risks regarding misinformation and fake news, because they can spread false information by depicting public figures saying or doing things they never did, undermining public trust. In this paper, we propose a method that leverages BLS signatures (Boneh, Lynn, and Shacham 2004) to implement signatures that remain valid after image cropping, but are invalidated in all the other types of manipulation, including deepfake creation. Our approach does not require who crops the image to know the signature private key or to be trusted in general, and it is O(1) in terms of signature size, making it a practical solution for scenarios where images are disseminated through web servers and cropping is the primary transformation. Finally, we adapted the signature scheme for the JPEG standard, and we experimentally tested the size of a signed image.", "published": "2025-12-01T16:30:53Z", "updated": "2025-12-01T16:30:53Z", "authors": ["Pericle Perazzo", "Massimiliano Mattei", "Giuseppe Anastasi", "Marco Avvenuti", "Gianluca Dini", "Giuseppe Lettieri", "Carlo Vallati"], "pdf_url": "https://arxiv.org/pdf/2512.01845v1"}
{"id": "http://arxiv.org/abs/2512.01832v1", "title": "A Privacy-Preserving Information-Sharing Protocol for Federated Authentication", "summary": "This paper presents a privacy-preserving protocol for identity registration and information sharing in federated authentication systems. The goal is to enable Identity Providers (IdPs) to detect duplicate or fraudulent identity enrollments without revealing users personal data or enabling cross-domain correlation. The protocol relies on Oblivious Pseudorandom Functions (OPRFs) combined with domain-specific transformations, ensuring that each IdP generates independent pseudonymous identifiers derived from a shared cryptographic service while maintaining full input confidentiality. A central authority maintains a blind registry that records successful and failed identity verifications using only pseudonymous identifiers, allowing global consistency checks without exposing sensitive information or linking users across domains. The proposed construction provides a general and abstract framework suitable for a wide range of federated authentication systems, achieving strong privacy guarantees while supporting effective fraud-prevention mechanisms during identity registration.", "published": "2025-12-01T16:13:41Z", "updated": "2025-12-01T16:13:41Z", "authors": ["Francesco Buccafurri", "Carmen Licciardi"], "pdf_url": "https://arxiv.org/pdf/2512.01832v1"}
{"id": "http://arxiv.org/abs/2503.05136v20", "title": "The Beginner's Textbook for Fully Homomorphic Encryption", "summary": "Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables computations to be performed directly on encrypted data, as if the data were in plaintext. After all computations are performed on the encrypted data, it can be decrypted to reveal the result. The decrypted value matches the result that would have been obtained if the same computations were applied to the plaintext data.\n  FHE supports basic operations such as addition and multiplication on encrypted numbers. Using these fundamental operations, more complex computations can be constructed, including subtraction, division, logic gates (e.g., AND, OR, XOR, NAND, MUX), and even advanced mathematical functions such as ReLU, sigmoid, and trigonometric functions (e.g., sin, cos). These functions can be implemented either as exact formulas or as approximations, depending on the trade-off between computational efficiency and accuracy.\n  FHE enables privacy-preserving machine learning by allowing a server to process the client's data in its encrypted form through an ML model. With FHE, the server learns neither the plaintext version of the input features nor the inference results. Only the client, using their secret key, can decrypt and access the results at the end of the service protocol. FHE can also be applied to confidential blockchain services, ensuring that sensitive data in smart contracts remains encrypted and confidential while maintaining the transparency and integrity of the execution process. Other applications of FHE include secure outsourcing of data analytics, encrypted database queries, privacy-preserving searches, efficient multi-party computation for digital signatures, and more.\n  This book is an open project (https://fhetextbook.github.io), please report any bugs or errors to the Github issues board.", "published": "2025-03-07T04:29:11Z", "updated": "2025-12-01T15:27:48Z", "authors": ["Ronny Ko"], "pdf_url": "https://arxiv.org/pdf/2503.05136v20"}
{"id": "http://arxiv.org/abs/2512.01727v1", "title": "AI-Driven Cybersecurity Testbed for Nuclear Infrastructure: Comprehensive Evaluation Using METL Operational Data", "summary": "Advanced nuclear reactor systems face increasing cybersecurity threats as sophisticated attackers exploit cyber-physical interfaces to manipulate control systems while evading traditional IT security measures. This research presents a comprehensive evaluation of artificial intelligence approaches for cybersecurity protection in nuclear infrastructure, using Argonne National Laboratory's Mechanisms Engineering Test Loop (METL) as an experimental platform. We developed a systematic evaluation framework encompassing four machine learning detection paradigms: Change Point Detection, LSTM-based Anomaly Detection, Dependency Violation analysis, and Autoencoder reconstruction methods. Our comprehensive attack taxonomy includes 15 distinct scenarios targeting reactor control systems, each implemented across five severity tiers to evaluate detection performance under varying attack intensities. The experimental evaluation encompassed 300 rigorous experiments using realistic METL operational data. Change Point Detection emerged as the leading approach with mean AUC performance of 0.785, followed by LSTM Anomaly Detection (0.636), Dependency Violation (0.621), and Autoencoder methods (0.580). Attack detectability varied significantly, with multi-site coordinated attacks proving most detectable (AUC = 0.739) while precision trust decay attacks presented the greatest detection challenge (AUC = 0.592). This work delivers practical performance benchmarks and reference architecture that advance AI-based cybersecurity capabilities for critical nuclear infrastructure, providing essential foundations for operational deployment and enhanced threat response in cyber-physical systems.", "published": "2025-12-01T14:36:52Z", "updated": "2025-12-01T14:36:52Z", "authors": ["Benjamin Blakely", "Yeni Li", "Akshay Dave", "Derek Kultgen", "Rick Vilim"], "pdf_url": "https://arxiv.org/pdf/2512.01727v1"}
{"id": "http://arxiv.org/abs/2512.01666v1", "title": "Demystifying Feature Engineering in Malware Analysis of API Call Sequences", "summary": "Machine learning (ML) has been widely used to analyze API call sequences in malware analysis, which typically requires the expertise of domain specialists to extract relevant features from raw data. The extracted features play a critical role in malware analysis. Traditional feature extraction is based on human domain knowledge, while there is a trend of using natural language processing (NLP) for automatic feature extraction. This raises a question: how do we effectively select features for malware analysis based on API call sequences? To answer it, this paper presents a comprehensive study of investigating the impact of feature engineering upon malware classification.We first conducted a comparative performance evaluation under three models, Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer, with respect to knowledge-based and NLP-based feature engineering methods. We observed that models with knowledge-based feature engineering inputs generally outperform those using NLP-based across all metrics, especially under smaller sample sizes. Then we analyzed a complete set of data features from API call sequences, our analysis reveals that models often focus on features such as handles and virtual addresses, which vary across executions and are difficult for human analysts to interpret.", "published": "2025-12-01T13:36:42Z", "updated": "2025-12-01T13:36:42Z", "authors": ["Tianheng Qu", "Hongsong Zhu", "Limin Sun", "Haining Wang", "Haiqiang Fei", "Zheng He", "Zhi Li"], "pdf_url": "https://arxiv.org/pdf/2512.01666v1"}
{"id": "http://arxiv.org/abs/2512.01651v1", "title": "Rethinking Cybersecurity Ontology Classification and Evaluation: Towards a Credibility-Centered Framework", "summary": "This paper analyzes the proliferation of cybersecurity ontologies, arguing that this surge cannot be explained solely by technical shortcomings related to quality, but also by a credibility deficit - a lack of trust, endorsement, and adoption by users. This conclusion is based on our first contribution, which is a state-of-the-art review and categorization of cybersecurity ontologies using the Framework for Ontologies Classification framework. To address this gap, we propose a revised framework for assessing credibility, introducing indicators such as institutional support, academic recognition, day-to-day practitioner validation, and industrial adoption. Based on these new credibility indicators, we construct a classification scheme designed to guide the selection of ontologies that are relevant to specific security needs. We then apply this framework to a concrete use case: the Franco-Luxembourgish research project ANCILE, which illustrates how a credibility-aware evaluation can reshape ontology selection for operational contexts.", "published": "2025-12-01T13:25:05Z", "updated": "2025-12-01T13:25:05Z", "authors": ["Antoine Leblanc", "Jacques Robin", "Nourhène Ben Rabah", "Zequan Huang", "Bénédicte Le Grand"], "pdf_url": "https://arxiv.org/pdf/2512.01651v1"}
{"id": "http://arxiv.org/abs/2512.01604v1", "title": "On the Context-Hiding Property of Shamir-Based Homomorphic Secret Sharing", "summary": "Homomorphic secret sharing (HSS) allows multiple input clients to secretly share their private inputs to a function among several servers such that each server can homomorphically compute the function over its share to produce a share of the function's output. In HSS-enabled applications such as secure multi-party computation (MPC), security requires that the output shares leak no more information about the inputs than the function output. Such security is ensured by the context-hiding property of HSS. The typical rerandomization technique achieves context hiding but increases the share size. To address this, we formalize the context-hiding property of HSS for individual functions, examine the context-hiding property of Shamir-based HSS for monomials, and extend the study to polynomials.", "published": "2025-12-01T12:23:19Z", "updated": "2025-12-01T12:23:19Z", "authors": ["Shuai Feng", "Liang Feng Zhang"], "pdf_url": "https://arxiv.org/pdf/2512.01604v1"}
{"id": "http://arxiv.org/abs/2512.01596v1", "title": "Towards a Multi-Layer Defence Framework for Securing Near-Real-Time Operations in Open RAN", "summary": "Securing the near-real-time (near-RT) control operations in Open Radio Access Networks (Open RAN) is increasingly critical, yet remains insufficiently addressed, as new runtime threats target the control loop while the system is operational. In this paper, we propose a multi-layer defence framework designed to enhance the security of near-RT RAN Intelligent Controller (RIC) operations. We classify operational-time threats into three categories, message-level, data-level, and control logic-level, and design and implement a dedicated detection and mitigation component for each: a signature-based E2 message inspection module performing structural and semantic validation of signalling exchanges, a telemetry poisoning detector based on temporal anomaly scoring using an LSTM network, and a runtime xApp attestation mechanism based on execution-time hash challenge-response. The framework is evaluated on an O-RAN testbed comprising FlexRIC and a commercial RAN emulator, demonstrating effective detection rates, low latency overheads, and practical integration feasibility. Results indicate that the proposed safeguards can operate within near-RT time constraints while significantly improving protection against runtime attacks, introducing less than 80 ms overhead for a network with 500 User Equipment (UEs). Overall, this work lays the foundation for deployable, layered, and policy-driven runtime security architectures for the near-RT RIC control loop in Open RAN, and provides an extensible framework into which future mitigation policies and threat-specific modules can be integrated.", "published": "2025-12-01T12:13:32Z", "updated": "2025-12-01T12:13:32Z", "authors": ["Hamed Alimohammadi", "Samara Mayhoub", "Sotiris Chatzimiltis", "Mohammad Shojafar", "Muhammad Nasir Mumtaz Bhutta"], "pdf_url": "https://arxiv.org/pdf/2512.01596v1"}
{"id": "http://arxiv.org/abs/2512.01595v1", "title": "WhiteLie: A Robust System for Spoofing User Data in Android Platforms", "summary": "Android employs a permission framework that empowers users to either accept or deny sharing their private data (for example, location) with an app. However, many apps tend to crash when they are denied permission, leaving users no choice but to allow access to their data in order to use the app. In this paper, we introduce a comprehensive and robust user data spoofing system, WhiteLie, that can spoof a variety of user data and feed it to target apps. Additionally, it detects privacy-violating behaviours, automatically responding by supplying spoofed data instead of the user's real data, without crashing or disrupting the apps. Unlike prior approaches, WhiteLie requires neither device rooting nor altering the app's binary, making it deployable on stock Android devices. Through experiments on more than 70 popular Android apps, we demonstrate that WhiteLie is able to deceive apps into accepting spoofed data without getting detected. Our evaluation further demonstrates that WhiteLie introduces negligible overhead in terms of battery usage, CPU consumption, and app execution latency. Our findings underscore the feasibility of implementing user-centric privacy-enhancing mechanisms within the existing Android ecosystem.", "published": "2025-12-01T12:11:16Z", "updated": "2025-12-01T12:11:16Z", "authors": ["Harish Yadav", "Vikas Maurya", "Abhilash Jindal", "Vireshwar Kumar"], "pdf_url": "https://arxiv.org/pdf/2512.01595v1"}
{"id": "http://arxiv.org/abs/2512.01594v1", "title": "Confidential, Attestable, and Efficient Inter-CVM Communication with Arm CCA", "summary": "Confidential Virtual Machines (CVMs) are increasingly adopted to protect sensitive workloads from privileged adversaries such as the hypervisor. While they provide strong isolation guarantees, existing CVM architectures lack first-class mechanisms for inter-CVM data sharing due to their disjoint memory model, making inter-CVM data exchange a performance bottleneck in compartmentalized or collaborative multi-CVM systems. Under this model, a CVM's accessible memory is either shared with the hypervisor or protected from both the hypervisor and all other CVMs. This design simplifies reasoning about memory ownership; however, it fundamentally precludes plaintext data sharing between CVMs because all inter-CVM communication must pass through hypervisor-accessible memory, requiring costly encryption and decryption to preserve confidentiality and integrity.\n  In this paper, we introduce CAEC, a system that enables protected memory sharing between CVMs. CAEC builds on Arm Confidential Compute Architecture (CCA) and extends its firmware to support Confidential Shared Memory (CSM), a memory region securely shared between multiple CVMs while remaining inaccessible to the hypervisor and all non-participating CVMs. CAEC's design is fully compatible with CCA hardware and introduces only a modest increase (4\\%) in CCA firmware code size. CAEC delivers substantial performance benefits across a range of workloads. For instance, inter-CVM communication over CAEC achieves up to 209$\\times$ reduction in CPU cycles compared to encryption-based mechanisms over hypervisor-accessible shared memory. By combining high performance, strong isolation guarantees, and attestable sharing semantics, CAEC provides a practical and scalable foundation for the next generation of trusted multi-CVM services across both edge and cloud environments.", "published": "2025-12-01T12:10:43Z", "updated": "2025-12-01T12:10:43Z", "authors": ["Sina Abdollahi", "Amir Al Sadi", "Marios Kogias", "David Kotz", "Hamed Haddadi"], "pdf_url": "https://arxiv.org/pdf/2512.01594v1"}
{"id": "http://arxiv.org/abs/2512.01577v1", "title": "Beyond the Hype: A Large-Scale Empirical Analysis of On-Chain Transactions in NFT Scams", "summary": "Non-fungible tokens (NFTs) serve as a representative form of digital asset ownership and have attracted numerous investors, creators, and tech enthusiasts in recent years. However, related fraud activities, especially phishing scams, have caused significant property losses. There are many graph analysis methods to detect malicious scam incidents, but no research on the transaction patterns of the NFT scams. Therefore, to fill this gap, we are the first to systematically explore NFT phishing frauds through graph analysis, aiming to comprehensively investigate the characteristics and patterns of NFT phishing frauds on the transaction graph. During the research process, we collect transaction records, log data, and security reports related to NFT phishing incidents published on multiple platforms. After collecting, sanitizing, and unifying the data, we construct a transaction graph and analyze the distribution, transaction features, and interaction patterns of NFT phishing scams. We find that normal transactions on the blockchain accounted for 96.71% of all transactions. Although phishing-related accounts accounted for only 0.94% of the total accounts, they appeared in 8.36% of the transaction scenarios, and their interaction probability with normal accounts is significantly higher in large-scale transaction networks. Moreover, NFT phishing scammers often carry out fraud in a collective manner, targeting specific accounts, tend to interact with victims through multiple token standards, have shorter transaction cycles than normal transactions, and involve more multi-party transactions. This study reveals the core behavioral features of NFT phishing scams, providing important references for the detection and prevention of NFT phishing scams in the future.", "published": "2025-12-01T11:49:51Z", "updated": "2025-12-01T11:49:51Z", "authors": ["Wenkai Li", "Zongwei Li", "Xiaoqi Li", "Chunyi Zhang", "Xiaoyan Zhang", "Yuqing Zhang"], "pdf_url": "https://arxiv.org/pdf/2512.01577v1"}
{"id": "http://arxiv.org/abs/2512.01574v1", "title": "IVE: An Accelerator for Single-Server Private Information Retrieval Using Versatile Processing Elements", "summary": "Private information retrieval (PIR) is an essential cryptographic protocol for privacy-preserving applications, enabling a client to retrieve a record from a server's database without revealing which record was requested. Single-server PIR based on homomorphic encryption has particularly gained immense attention for its ease of deployment and reduced trust assumptions. However, single-server PIR remains impractical due to its high computational and memory bandwidth demands. Specifically, reading the entirety of large databases from storage, such as SSDs, severely limits its performance. To address this, we propose IVE, an accelerator for single-server PIR with a systematic extension that enables practical retrieval from large databases using DRAM. Recent advances in DRAM capacity allow PIR for large databases to be served entirely from DRAM, removing its dependence on storage bandwidth. Although the memory bandwidth bottleneck still remains, multi-client batching effectively amortizes database access costs across concurrent requests to improve throughput. However, client-specific data remains a bottleneck, whose bandwidth requirements ultimately limits performance. IVE overcomes this by employing a large on-chip scratchpad with an operation scheduling algorithm that maximizes data reuse, further boosting throughput. Additionally, we introduce sysNTTU, a versatile functional unit that enhances area efficiency without sacrificing performance. We also propose a heterogeneous memory system architecture, which enables a linear scaling of database sizes without a throughput degradation. Consequently, IVE achieves up to 1,275x higher throughput compared to prior PIR hardware solutions.", "published": "2025-12-01T11:47:26Z", "updated": "2025-12-01T11:47:26Z", "authors": ["Sangpyo Kim", "Hyesung Ji", "Jongmin Kim", "Wonseok Choi", "Jaiyoung Park", "Jung Ho Ahn"], "pdf_url": "https://arxiv.org/pdf/2512.01574v1"}
{"id": "http://arxiv.org/abs/2410.05814v3", "title": "Rank Matters: Understanding and Defending Model Inversion Attacks via Low-Rank Feature Filtering", "summary": "Model Inversion Attacks (MIAs) pose a significant threat to data privacy by reconstructing sensitive training samples from the knowledge embedded in trained machine learning models. Despite recent progress in enhancing the effectiveness of MIAs across diverse settings, defense strategies have lagged behind -- struggling to balance model utility with robustness against increasingly sophisticated attacks. In this work, we propose the ideal inversion error to measure the privacy leakage, and our theoretical and empirical investigations reveals that higher-rank features are inherently more prone to privacy leakage. Motivated by this insight, we propose a lightweight and effective defense strategy based on low-rank feature filtering, which explicitly reduces the attack surface by constraining the dimension of intermediate representations. Extensive experiments across various model architectures and datasets demonstrate that our method consistently outperforms existing defenses, achieving state-of-the-art performance against a wide range of MIAs. Notably, our approach remains effective even in challenging regimes involving high-resolution data and high-capacity models, where prior defenses fail to provide adequate protection.", "published": "2024-10-08T08:44:01Z", "updated": "2025-12-01T11:11:17Z", "authors": ["Hongyao Yu", "Yixiang Qiu", "Hao Fang", "Tianqu Zhuang", "Bin Chen", "Sijin Yu", "Bin Wang", "Shu-Tao Xia", "Ke Xu"], "pdf_url": "https://arxiv.org/pdf/2410.05814v3"}
{"id": "http://arxiv.org/abs/2512.01437v1", "title": "Inside Qubic's Selfish Mining Campaign on Monero: Evidence, Tactics, and Limits", "summary": "We analyze Qubic's advertised selfish mining campaign on Monero in 2025. Combining data from Monero nodes, and the Qubic pool API, we reconstruct Qubic-attributed blocks and hashrate and detect ten intervals consistent with selfish mining strategies. In these intervals, Qubic's average hashrate share rises to the 23-34\\% range, yet sustained 51\\% control is never observed. We evaluate the campaign against the classical selfish mining model and a modified Markov-chain model that reflects Qubic's conservative release strategy: both predict lower revenue than honest mining at the inferred parameters, and the data largely confirms this while still showing noticeable deviations from the predicted curve. We interpret this gap between model and measurements in terms of Qubic's time-varying hashrate and coarse-grained attack segmentation.", "published": "2025-12-01T09:22:25Z", "updated": "2025-12-01T09:22:25Z", "authors": ["Suhyeon Lee", "Hyeongyeong Kim"], "pdf_url": "https://arxiv.org/pdf/2512.01437v1"}
{"id": "http://arxiv.org/abs/2507.01487v2", "title": "How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations", "summary": "Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building block for private data aggregation. Recently, the field of differential privacy has revived interest in secure shufflers by highlighting the privacy amplification they can provide in various computations. Although several works argue for the utility of secure shufflers, they often treat them as black boxes; overlooking the practical vulnerabilities and performance trade-offs of existing implementations. This leaves a central question open: what makes a good secure shuffler?\n  This survey addresses that question by identifying, categorizing, and comparing 26 secure protocols that realize the necessary shuffling functionality. To enable a meaningful comparison, we adapt and unify existing security definitions into a consistent set of properties. We also present an overview of privacy-preserving technologies that rely on secure shufflers, offer practical guidelines for selecting appropriate protocols, and outline promising directions for future work.", "published": "2025-07-02T08:48:53Z", "updated": "2025-12-01T08:25:58Z", "authors": ["Marc Damie", "Florian Hahn", "Andreas Peter", "Jan Ramon"], "pdf_url": "https://arxiv.org/pdf/2507.01487v2"}
{"id": "http://arxiv.org/abs/2512.01396v1", "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches", "summary": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.\n  To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.", "published": "2025-12-01T08:16:43Z", "updated": "2025-12-01T08:16:43Z", "authors": ["Zhiqing Zhong", "Jiaming Huang", "Pinjia He"], "pdf_url": "https://arxiv.org/pdf/2512.01396v1"}
{"id": "http://arxiv.org/abs/2512.01391v1", "title": "INFERMAL: Inferential analysis of maliciously registered domains", "summary": "Cybercriminals have long depended on domain names for phishing, spam, malware distribution, and botnet operation. To facilitate the malicious activities, they continually register new domain names for exploitation. Previous work revealed an abnormally high concentration of malicious registrations in a handful of domain name registrars and top-level domains (TLDs). Anecdotal evidence suggests that low registration prices attract cybercriminals, implying that higher costs may potentially discourage them. However, no existing study has systematically analyzed the factors driving abuse, leaving a critical gap in understanding how different variables influence malicious registrations. In this report, we carefully distill the inclinations and aversions of malicious actors during the registration of new phishing domain names. We compile a comprehensive list of 73 features encompassing three main latent factors: registration attributes, proactive verification, and reactive security practices. Through a GLM regression analysis, we find that each dollar reduction in registration fees corresponds to a 49% increase in malicious domains. The availability of free services, such as web hosting, drives an 88% surge in phishing activities. Conversely, stringent restrictions cut down abuse by 63%, while registrars providing API access for domain registration or account creation experience a staggering 401% rise in malicious domains. This exploration may assist intermediaries involved in domain registration to develop tailored anti-abuse practices, yet aligning them with their economic incentives.", "published": "2025-12-01T08:10:01Z", "updated": "2025-12-01T08:10:01Z", "authors": ["Yevheniya Nosyk", "Maciej Korczyński", "Carlos Gañán", "Sourena Maroofi", "Jan Bayer", "Zul Odgerel", "Samaneh Tajalizadehkhoob", "Andrzej Duda"], "pdf_url": "https://arxiv.org/pdf/2512.01391v1"}
{"id": "http://arxiv.org/abs/2512.01353v1", "title": "A Wolf in Sheep's Clothing: Bypassing Commercial LLM Guardrails via Harmless Prompt Weaving and Adaptive Tree Search", "summary": "Large language models (LLMs) remain vulnerable to jailbreak attacks that bypass safety guardrails to elicit harmful outputs. Existing approaches overwhelmingly operate within the prompt-optimization paradigm: whether through traditional algorithmic search or recent agent-based workflows, the resulting prompts typically retain malicious semantic signals that modern guardrails are primed to detect. In contrast, we identify a deeper, largely overlooked vulnerability stemming from the highly interconnected nature of an LLM's internal knowledge. This structure allows harmful objectives to be realized by weaving together sequences of benign sub-queries, each of which individually evades detection. To exploit this loophole, we introduce the Correlated Knowledge Attack Agent (CKA-Agent), a dynamic framework that reframes jailbreaking as an adaptive, tree-structured exploration of the target model's knowledge base. The CKA-Agent issues locally innocuous queries, uses model responses to guide exploration across multiple paths, and ultimately assembles the aggregated information to achieve the original harmful objective. Evaluated across state-of-the-art commercial LLMs (Gemini2.5-Flash/Pro, GPT-oss-120B, Claude-Haiku-4.5), CKA-Agent consistently achieves over 95% success rates even against strong guardrails, underscoring the severity of this vulnerability and the urgent need for defenses against such knowledge-decomposition attacks. Our codes are available at https://github.com/Graph-COM/CKA-Agent.", "published": "2025-12-01T07:05:23Z", "updated": "2025-12-01T07:05:23Z", "authors": ["Rongzhe Wei", "Peizhi Niu", "Xinjie Shen", "Tony Tu", "Yifan Li", "Ruihan Wu", "Eli Chien", "Olgica Milenkovic", "Pan Li"], "pdf_url": "https://arxiv.org/pdf/2512.01353v1"}
{"id": "http://arxiv.org/abs/2512.01335v1", "title": "EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations", "summary": "Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as \"(@_@)\" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.", "published": "2025-12-01T06:53:49Z", "updated": "2025-12-01T06:53:49Z", "authors": ["Xinyun Zhou", "Xinfeng Li", "Yinan Peng", "Ming Xu", "Xuanwang Zhang", "Miao Yu", "Yidong Wang", "Xiaojun Jia", "Kun Wang", "Qingsong Wen", "XiaoFeng Wang", "Wei Dong"], "pdf_url": "https://arxiv.org/pdf/2512.01335v1"}
{"id": "http://arxiv.org/abs/2512.01326v1", "title": "Securing Large Language Models (LLMs) from Prompt Injection Attacks", "summary": "Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.", "published": "2025-12-01T06:34:20Z", "updated": "2025-12-01T06:34:20Z", "authors": ["Omar Farooq Khan Suri", "John McCrae"], "pdf_url": "https://arxiv.org/pdf/2512.01326v1"}
{"id": "http://arxiv.org/abs/2512.01295v1", "title": "Systems Security Foundations for Agentic Computing", "summary": "This paper articulates short- and long-term research problems in AI agent security and privacy, using the lens of computer systems security. This approach examines end-to-end security properties of entire systems, rather than AI models in isolation. While we recognize that hardening a single model is useful, it is important to realize that it is often insufficient. By way of an analogy, creating a model that is always helpful and harmless is akin to creating software that is always helpful and harmless. The collective experience of decades of cybersecurity research and practice shows that this is insufficient. Rather, constructing an informed and realistic attacker model before building a system, applying hard-earned lessons from software security, and continuous improvement of security posture is a tried-and-tested approach to securing real computer systems. A key goal is to examine where research challenges arise when applying traditional security principles in the context of AI agents. A secondary goal of this report is to distill these ideas for AI and ML practitioners and researchers. We discuss the challenges of applying security principles to agentic computing, present 11 case studies of real attacks on agentic systems, and define a series of new research problems specific to the security of agentic systems.", "published": "2025-12-01T05:28:59Z", "updated": "2025-12-01T05:28:59Z", "authors": ["Mihai Christodorescu", "Earlence Fernandes", "Ashish Hooda", "Somesh Jha", "Johann Rehberger", "Khawaja Shams"], "pdf_url": "https://arxiv.org/pdf/2512.01295v1"}
{"id": "http://arxiv.org/abs/2512.01255v1", "title": "Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation", "summary": "Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects.\n  In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework.\n  We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.", "published": "2025-12-01T04:00:06Z", "updated": "2025-12-01T04:00:06Z", "authors": ["Qingyuan Fei", "Xin Liu", "Song Li", "Shujiang Wu", "Jianwei Hou", "Ping Chen", "Zifeng Kang"], "pdf_url": "https://arxiv.org/pdf/2512.01255v1"}
{"id": "http://arxiv.org/abs/2512.01247v1", "title": "Benchmarking and Understanding Safety Risks in AI Character Platforms", "summary": "AI character platforms, which allow users to engage in conversations with AI personas, are a rapidly growing application domain. However, their immersive and personalized nature, combined with technical vulnerabilities, raises significant safety concerns. Despite their popularity, a systematic evaluation of their safety has been notably absent. To address this gap, we conduct the first large-scale safety study of AI character platforms, evaluating 16 popular platforms using a benchmark set of 5,000 questions across 16 safety categories. Our findings reveal a critical safety deficit: AI character platforms exhibit an average unsafe response rate of 65.1%, substantially higher than the 17.7% average rate of the baselines. We further discover that safety performance varies significantly across different characters and is strongly correlated with character features such as demographics and personality. Leveraging these insights, we demonstrate that our machine learning model is able identify less safe characters with an F1-score of 0.81. This predictive capability can be beneficial for platforms, enabling improved mechanisms for safer interactions, character search/recommendations, and character creation. Overall, the results and findings offer valuable insights for enhancing platform governance and content moderation for safer AI character platforms.", "published": "2025-12-01T03:48:25Z", "updated": "2025-12-01T03:48:25Z", "authors": ["Yiluo Wei", "Peixian Zhang", "Gareth Tyson"], "pdf_url": "https://arxiv.org/pdf/2512.01247v1"}
{"id": "http://arxiv.org/abs/2512.01233v1", "title": "CTF Archive: Capture, Curate, Learn Forever", "summary": "Capture the Flag (CTF) competitions represent a powerful experiential learning approach within cybersecurity education, blending diverse concepts into interactive challenges. However, the short duration (typically 24-48 hours) and ephemeral infrastructure of these events often impede sustained educational benefit. Learners face substantial barriers in revisiting unsolved challenges, primarily due to the cumbersome process of manually reconstructing and rehosting the challenges without comprehensive documentation or guidance. To address this critical gap, we introduce CTF Archive, a platform designed to preserve the educational value of CTF competitions by centralizing and archiving hundreds of challenges spanning over a decade in fully configured, ready-to-use environments. By removing the complexity of environment setup, CTF Archive allows learners to focus directly on conceptual understanding rather than technical troubleshooting. The availability of these preserved challenges encourages in-depth research and exploration at the learner's pace, significantly enhancing conceptual comprehension without the pressures of live competition. Additionally, public accessibility lowers entry barriers, promoting an inclusive educational experience. Overall, CTF Archive provides a scalable solution to integrate persistent, practical cybersecurity learning into academic curricula.", "published": "2025-12-01T03:19:58Z", "updated": "2025-12-01T03:19:58Z", "authors": ["Pratham Gupta", "Aditya Gabani", "Connor Nelson", "Yan Shoshitaishvili"], "pdf_url": "https://arxiv.org/pdf/2512.01233v1"}
{"id": "http://arxiv.org/abs/2512.01185v1", "title": "DefenSee: Dissecting Threat from Sight and Text - A Multi-View Defensive Pipeline for Multi-modal Jailbreaks", "summary": "Multi-modal large language models (MLLMs), capable of processing text, images, and audio, have been widely adopted in various AI applications. However, recent MLLMs integrating images and text remain highly vulnerable to coordinated jailbreaks. Existing defenses primarily focus on the text, lacking robust multi-modal protection. As a result, studies indicate that MLLMs are more susceptible to malicious or unsafe instructions, unlike their text-only counterparts. In this paper, we proposed DefenSee, a robust and lightweight multi-modal black-box defense technique that leverages image variants transcription and cross-modal consistency checks, mimicking human judgment. Experiments on popular multi-modal jailbreak and benign datasets show that DefenSee consistently enhances MLLM robustness while better preserving performance on benign tasks compared to SOTA defenses. It reduces the ASR of jailbreak attacks to below 1.70% on MiniGPT4 using the MM-SafetyBench benchmark, significantly outperforming prior methods under the same conditions.", "published": "2025-12-01T01:57:49Z", "updated": "2025-12-01T01:57:49Z", "authors": ["Zihao Wang", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "pdf_url": "https://arxiv.org/pdf/2512.01185v1"}
{"id": "http://arxiv.org/abs/2512.01164v1", "title": "Reverse Engineering and Control-Aware Security Analysis of the ArduPilot UAV Framework", "summary": "Unmanned Aerial Vehicle (UAV) technologies are gaining high interest for many domains, which makes UAV security of utmost importance. ArduPilot is among the most widely used open-source autopilot UAV frameworks; yet, many studies demonstrate the vulnerabilities affecting such systems. Vulnerabilities within its communication subsystems (including WiFi, telemetry, or GPS) expose critical entry points, and vulnerabilities in Ardupilot can affect the control procedure. In this paper, we reconstruct the software architecture and the control models implemented by ArduPilot and then examine how these control models could potentially misused to induce malicious behaviors while relying on legitimate inputs.", "published": "2025-12-01T00:47:11Z", "updated": "2025-12-01T00:47:11Z", "authors": ["Yasaswini Konapalli", "Lotfi Ben Othmane", "Cihan Tunc", "Feras Benchellal", "Likhita Mudagere"], "pdf_url": "https://arxiv.org/pdf/2512.01164v1"}
