{"id": "http://arxiv.org/abs/2602.14135v1", "title": "ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI", "summary": "Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the \"ForesightSafety Bench\" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.", "published": "2026-02-15T13:12:44Z", "updated": "2026-02-15T13:12:44Z", "authors": ["Haibo Tong", "Feifei Zhao", "Linghao Feng", "Ruoyu Wu", "Ruolin Chen", "Lu Jia", "Zhou Zhao", "Jindong Li", "Tenglong Li", "Erliang Lin", "Shuai Yang", "Enmeng Lu", "Yinqian Sun", "Qian Zhang", "Zizhe Ruan", "Zeyang Yue", "Ping Wu", "Huangrui Li", "Chengyi Sun", "Yi Zeng"], "pdf_url": "https://arxiv.org/pdf/2602.14135v1"}
{"id": "http://arxiv.org/abs/2602.14116v1", "title": "Toward a Military Smart Cyber Situational Awareness (CSA)", "summary": "The development of technology across multiple sectors and the growing importance of cyber warfare make the development of Cyber Situational Awareness (CSA) a fundamental component of any cyber defense strategy. CSA, as a practice, enables understanding of the current landscape within an organization or critical infrastructure, anticipating potential threats, and responding appropriately to cyber risks. With CSA, we are not simply seeking a passive point of view, but rather informed decision-making that allows us to improve response times and monitor the consequences and effects an attack has on one of our elements and how it will affect other elements it interacts with. In this paper, we review 5 CSA platforms, seeking differentiating characteristics between each proposal and outlining 6 proposed criteria that can be applied when creating a military smart CSA platform. To this end, we have validated the proposed criteria in CRUSOE, an open-source CSA platform developed by CSIRT-MU. After applying some modifications and experiments, it turned out to be applicable to this field.", "published": "2026-02-15T12:33:04Z", "updated": "2026-02-15T12:33:04Z", "authors": ["Anthony Feijó-Añazco", "Antonio López Martínez", "Daniel Díaz-López", "Angel Luis Perales Gómez", "Pantaleone Nespoli", "Gregorio Martínez Pérez"], "pdf_url": "https://arxiv.org/pdf/2602.14116v1"}
{"id": "http://arxiv.org/abs/2602.14106v1", "title": "Anticipating Adversary Behavior in DevSecOps Scenarios through Large Language Models", "summary": "The most valuable asset of any cloud-based organization is data, which is increasingly exposed to sophisticated cyberattacks. Until recently, the implementation of security measures in DevOps environments was often considered optional by many government entities and critical national services operating in the cloud. This includes systems managing sensitive information, such as electoral processes or military operations, which have historically been valuable targets for cybercriminals. Resistance to security implementation is often driven by concerns over losing agility in software development, increasing the risk of accumulated vulnerabilities. Nowadays, patching software is no longer enough; adopting a proactive cyber defense strategy, supported by Artificial Intelligence (AI), is crucial to anticipating and mitigating threats. Thus, this work proposes integrating the Security Chaos Engineering (SCE) methodology with a new LLM-based flow to automate the creation of attack defense trees that represent adversary behavior and facilitate the construction of SCE experiments based on these graphical models, enabling teams to stay one step ahead of attackers and implement previously unconsidered defenses. Further detailed information about the experiment performed, along with the steps to replicate it, can be found in the following repository: https://github.com/mariomc14/devsecops-adversary-llm.git.", "published": "2026-02-15T11:43:04Z", "updated": "2026-02-15T11:43:04Z", "authors": ["Mario Marín Caballero", "Miguel Betancourt Alonso", "Daniel Díaz-López", "Angel Luis Perales Gómez", "Pantaleone Nespoli", "Gregorio Martínez Pérez"], "pdf_url": "https://arxiv.org/pdf/2602.14106v1"}
{"id": "http://arxiv.org/abs/2602.14095v1", "title": "NEST: Nascent Encoded Steganographic Thoughts", "summary": "Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.", "published": "2026-02-15T11:05:18Z", "updated": "2026-02-15T11:05:18Z", "authors": ["Artem Karpov"], "pdf_url": "https://arxiv.org/pdf/2602.14095v1"}
{"id": "http://arxiv.org/abs/2504.20532v2", "title": "TriniMark: A Robust Generative Speech Watermarking Method for Trinity-Level Traceability", "summary": "Diffusion-based speech generation has achieved remarkable fidelity, increasing the risk of misuse and unauthorized redistribution. However, most existing generative speech watermarking methods are developed for GAN-based pipelines, and watermarking for diffusion-based speech generation remains comparatively underexplored. In addition, prior work often focuses on content-level provenance, while support for model-level and user-level attribution is less mature. We propose \\textbf{TriniMark}, a diffusion-based generative speech watermarking framework that targets trinity-level traceability, i.e., the ability to associate a generated speech sample with (i) the embedded watermark message (content-level provenance), (ii) the source generative model (model-level attribution), and (iii) the end user who requested generation (user-level traceability). TriniMark uses a lightweight encoder to embed watermark bits into time-domain speech features and reconstruct the waveform, and a temporal-aware gated convolutional decoder for reliable bit recovery. We further introduce a waveform-guided fine-tuning strategy to transfer watermarking capability into a diffusion model. Finally, we incorporate variable-watermark training so that a single trained model can embed different watermark messages at inference time, enabling scalable user-level traceability. Experiments on speech datasets indicate that TriniMark maintains speech quality while improving robustness to common single and compound signal-processing attacks, and it supports high-capacity watermarking for large-scale traceability.", "published": "2025-04-29T08:23:28Z", "updated": "2026-02-15T10:54:16Z", "authors": ["Yue Li", "Weizhi Liu", "Kaiqing Lin", "Dongdong Lin", "Kassem Kallas"], "pdf_url": "https://arxiv.org/pdf/2504.20532v2"}
{"id": "http://arxiv.org/abs/2508.03882v2", "title": "Simulating Cyberattacks through a Breach Attack Simulation (BAS) Platform empowered by Security Chaos Engineering (SCE)", "summary": "In today digital landscape, organizations face constantly evolving cyber threats, making it essential to discover slippery attack vectors through novel techniques like Security Chaos Engineering (SCE), which allows teams to test defenses and identify vulnerabilities effectively. This paper proposes to integrate SCE into Breach Attack Simulation (BAS) platforms, leveraging adversary profiles and abilities from existing threat intelligence databases. This innovative proposal for cyberattack simulation employs a structured architecture composed of three layers: SCE Orchestrator, Connector, and BAS layers. Utilizing MITRE Caldera in the BAS layer, our proposal executes automated attack sequences, creating inferred attack trees from adversary profiles. Our proposal evaluation illustrates how integrating SCE with BAS can enhance the effectiveness of attack simulations beyond traditional scenarios, and be a useful component of a cyber defense strategy.", "published": "2025-08-05T19:52:57Z", "updated": "2026-02-15T10:49:13Z", "authors": ["Arturo Sánchez-Matas", "Pablo Escribano Ruiz", "Daniel Díaz-López", "Angel Luis Perales Gómez", "Pantaleone Nespoli", "Gregorio Martínez Pérez"], "pdf_url": "https://arxiv.org/pdf/2508.03882v2"}
{"id": "http://arxiv.org/abs/2506.05402v3", "title": "Lorica: A Synergistic Fine-Tuning Framework for Advancing Personalized Adversarial Robustness", "summary": "The growing use of large pre-trained models in edge computing has made model inference on mobile clients both feasible and popular. Yet these devices remain vulnerable to adversarial attacks, threatening model robustness and security. Federated adversarial training (FAT) offers a promising solution by enhancing robustness while preserving client privacy. However, FAT often yields a generalized global model that struggles with heterogeneous client data, leading to limited personalization and significant communication overhead. In this paper, we propose \\textit{Lorica}, a personalized synergistic adversarial training framework that delivers customized defense models through a two-phase process. In Phase 1, \\textit{Lorica} applies LoRA-FA for local adversarial fine-tuning, enabling personalized robustness while reducing communication by uploading only LoRA-FA parameters. In Phase 2, a forward-gating selection strategy improves benign accuracy, further refining the personalized model. This yields tailored defense models that effectively balance robustness and accuracy. Extensive experiments on benchmark datasets demonstrate that \\textit{Lorica} can achieve up to 68$\\times$ improvements in communication efficiency compared to state-of-the-art algorithms, while achieving up to 29.9\\% and 52.2\\% enhancements in adversarial robustness and benign accuracy, respectively.", "published": "2025-06-04T03:31:32Z", "updated": "2026-02-15T10:29:09Z", "authors": ["Tianyu Qi", "Lei Xue", "Yufeng Zhan", "Xiaobo Ma"], "pdf_url": "https://arxiv.org/pdf/2506.05402v3"}
{"id": "http://arxiv.org/abs/2602.14055v1", "title": "The Inevitability of Side-Channel Leakage in Encrypted Traffic", "summary": "The widespread adoption of TLS 1.3 and QUIC has rendered payload content invisible, shifting traffic analysis toward side-channel features. However, rigorous justification for why side-channel leakage is inevitable in encrypted communications has been lacking. This paper establishes a strict foundation from information theory by constructing a formal model \\(Σ=(Γ,Ω)\\), where \\(Γ=(A,Π,Φ,N)\\) describes the causal chain of application generation, protocol encapsulation, encryption transformation, and network transmission, while \\(Ω\\) characterizes observation capabilities. Based on composite channel structure, data processing inequality, and Lipschitz statistics propagation, we propose and prove the Side-Channel Existence Theorem: for distinguishable semantic pairs, under conditions including mapping non-degeneracy (\\(\\mathbb{E}[d(z_P,z_N)\\mid X]\\le C\\)), protocol-layer distinguishability (expectation difference \\(\\ge\\barΔ\\)), Lipschitz continuity, observation non-degeneracy (\\(ρ>0\\)), and propagation condition (\\(C<\\barΔ/2L_\\varphi\\)), the mutual information \\(I(X;Y)\\) is strictly positive with explicit lower bound. The corollary shows that in efficiency-prioritized systems, leakage is inevitable when at least one application pair is distinguishable. Three factors determine the boundary: non-degeneracy constant \\(C\\) constrained by efficiency, distinguishability \\(\\barΔ\\) from application diversity, and \\(ρ\\) from analyst capabilities. This establishes the first rigorous information-theoretic foundation for encrypted traffic side channels, providing verifiable predictions for attack feasibility, quantifiable benchmarks for defenses, and mathematical basis for efficiency-privacy tradeoffs.", "published": "2026-02-15T08:55:19Z", "updated": "2026-02-15T08:55:19Z", "authors": ["Guangjie Liu", "Guang Chen", "Weiwei Liu"], "pdf_url": "https://arxiv.org/pdf/2602.14055v1"}
{"id": "http://arxiv.org/abs/2602.10892v2", "title": "Resilient Alerting Protocols for Blockchains", "summary": "Smart contracts are stateful programs deployed on blockchains; they secure over a trillion dollars in transaction value per year. High-stakes smart contracts often rely on timely alerts about external events, but prior work has not analyzed their resilience to an attacker suppressing alerts via bribery. We formalize this challenge in a cryptoeconomic setting as the \\emph{alerting problem}, giving rise to a game between a bribing adversary and~$n$ rational participants, who pay a penalty if they are caught deviating from the protocol. We establish a quadratic, i.e.,~$O(n^2)$, upper bound, whereas a straightforward alerting protocol only achieves~$O(n)$ bribery cost.\n  We present a \\emph{simultaneous game} that asymptotically achieves the quadratic upper bound and thus asymptotically-optimal bribery resistance. We then present two protocols that implement our simultaneous game: The first leverages a strong network synchrony assumption. The second relaxes this strong assumption and instead takes advantage of trusted hardware and blockchain proof-of-publication to establish a timed commitment scheme. These two protocols are constant-time but incur a linear storage overhead on the blockchain. We analyze a third, \\emph{sequential alerting} protocol that optimistically incurs no on-chain storage overhead, at the expense of~$O(n)$ worst-case execution time. All three protocols achieve asymptotically-optimal bribery costs, but with different resource and performance tradeoffs. Together, they illuminate a rich design space for practical solutions to the alerting problem.", "published": "2026-02-11T14:23:15Z", "updated": "2026-02-15T08:31:28Z", "authors": ["Marwa Moullem", "Lorenz Breidenbach", "Ittay Eyal", "Ari Juels"], "pdf_url": "https://arxiv.org/pdf/2602.10892v2"}
{"id": "http://arxiv.org/abs/2602.14030v1", "title": "MC$^2$Mark: Distortion-Free Multi-Bit Watermarking for Long Messages", "summary": "Large language models now produce text indistinguishable from human writing, which increases the need for reliable provenance tracing. Multi-bit watermarking can embed identifiers into generated text, but existing methods struggle to keep both text quality and watermark strength while carrying long messages. We propose MC$^2$Mark, a distortion-free multi-bit watermarking framework designed for reliable embedding and decoding of long messages. Our key technical idea is Multi-Channel Colored Reweighting, which encodes bits through structured token reweighting while keeping the token distribution unbiased, together with Multi-Layer Sequential Reweighting to strengthen the watermark signal and an evidence-accumulation detector for message recovery. Experiments show that MC$^2$Mark improves detectability and robustness over prior multi-bit watermarking methods while preserving generation quality, achieving near-perfect accuracy for short messages and exceeding the second-best method by nearly 30% for long messages.", "published": "2026-02-15T07:29:06Z", "updated": "2026-02-15T07:29:06Z", "authors": ["Xuehao Cui", "Ruibo Chen", "Yihan Wu", "Heng Huang"], "pdf_url": "https://arxiv.org/pdf/2602.14030v1"}
{"id": "http://arxiv.org/abs/2410.10481v5", "title": "Model-based Large Language Model Customization as Service", "summary": "Prominent Large Language Model (LLM) services from providers like OpenAI and Google excel at general tasks but often underperform on domain-specific applications. Current customization services for these LLMs typically require users to upload data for fine-tuning, posing significant privacy risks. While differentially private (DP) data synthesis presents a potential alternative, its application commonly results in low effectiveness due to the introduction of excessive noise on data for DP. To overcome this, we introduce Llamdex, a novel framework that facilitates LLM customization as a service, where the client uploads pre-trained domain-specific models rather than data. This client-uploaded model, optionally protected by DP with much lower noise, is inserted into the base LLM via connection modules. Significantly, these connecting modules are trained without requiring sensitive domain data, enabling clients to customize LLM services while preserving data privacy. Experiments demonstrate that Llamdex improves domain-specific accuracy by up to 26% over state-of-the-art private data synthesis methods under identical privacy constraints and, by obviating the need for users to provide domain context within queries, maintains inference efficiency comparable to the original LLM service.", "published": "2024-10-14T13:18:20Z", "updated": "2026-02-15T06:34:28Z", "authors": ["Zhaomin Wu", "Jizhou Guo", "Junyi Hou", "Bingsheng He", "Lixin Fan", "Qiang Yang"], "pdf_url": "https://arxiv.org/pdf/2410.10481v5"}
{"id": "http://arxiv.org/abs/2602.14012v1", "title": "From SFT to RL: Demystifying the Post-Training Pipeline for LLM-based Vulnerability Detection", "summary": "The integration of LLMs into vulnerability detection (VD) has shifted the field toward interpretable and context-aware analysis. While post-training methods have shown promise in general coding tasks, their systematic application to VD remains underexplored. In this paper, we present the first comprehensive investigation into the post-training pipeline for LLM-based VD, spanning from cold-start SFT to off-policy preference optimization and on-policy RL, uncovering how data curation, stage interactions, reward mechanisms, and evaluation protocols collectively dictate the efficacy of model training and assessment. Our study identifies practical guidelines and insights: (1) SFT based on rejection sampling greatly outperforms rationalization-based supervision, which can introduce hallucinations due to ground-truth leakage. (2) While increased SFT epochs constantly benefit preference optimization, excessive SFT inhibits self-exploration during RL, ultimately limiting performance gains. (3) Coarse-grained reward signals often mislead RL, whereas fine-grained root-cause judgments ensure reliable credit assignment. Specification-based rewards offer further benefits but incur significant effort in specification generation. (4) Although filtering extremely hard-to-detect vulnerability samples improves RL training efficiency, the cost of performance loss should be considered in practical applications. (5) Models trained under GRPO significantly outperform those using SFT and preference optimization (i.e., DPO and ORPO), as well as a series of zero-shot SOTA LLMs, underscoring the significant potential of on-policy RL for LLM-based VD. (6) In contrast to binary matching that tends to overestimate performance, LLM-as-a-Judge based on root-cause analysis provides a more robust evaluation protocol, although its accuracy varies across judge models with different levels of security expertise.", "published": "2026-02-15T06:33:25Z", "updated": "2026-02-15T06:33:25Z", "authors": ["Youpeng Li", "Fuxun Yu", "Xinda Wang"], "pdf_url": "https://arxiv.org/pdf/2602.14012v1"}
{"id": "http://arxiv.org/abs/2408.09935v2", "title": "Privacy Technologies for Financial Intelligence", "summary": "Financial crimes like money laundering and terrorism financing can have significant impacts on society, including loss of trust in the integrity of the financial system, misuse and mismanagement of public funds, increase in societal problems like drug trafficking and illicit gambling, and loss of innocent lives due to terrorism activities. Effective detection of complex financial crimes remains a formidable challenge for regulators and financial institutions because the critical data needed to establish patterns and criminality are often dispersed across multiple organisations and cannot be linked due to privacy constraints around large-scale data matching. Recent advances in privacy and confidential computing technologies, which enable private and secure data analysis across organisations, offer a promising opportunity for regulators and the financial industry to come together to enhance their collaborative risk detection while maintaining privacy standards. This paper, through a survey of the financial intelligence ecosystem, seeks to identify opportunities for the utilisation of privacy technologies to improve the state-of-the-art in financial-crime detection.", "published": "2024-08-19T12:13:53Z", "updated": "2026-02-15T04:10:15Z", "authors": ["Yang Li", "Thilina Ranbaduge", "Kee Siong Ng"], "pdf_url": "https://arxiv.org/pdf/2408.09935v2"}
{"id": "http://arxiv.org/abs/2510.02356v3", "title": "Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark", "summary": "The deployment of Large Language Models (LLMs) in embodied agents creates an urgent need to measure their privacy awareness in the physical world. Existing evaluation methods, however, are confined to natural language based scenarios. To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation benchmark designed to quantify the physical-world privacy awareness of LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across four tiers to test an agent's ability to handle sensitive objects, adapt to changing environments, balance task execution with privacy constraints, and resolve conflicts with social norms. Our measurements reveal a critical deficit in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\\% accuracy in scenarios involving changing physical environments. Furthermore, when a task was accompanied by a privacy request, models prioritized completion over the constraint in up to 86\\% of cases. In high-stakes situations pitting privacy against critical social norms, leading models like GPT-4o and Claude-3.5-haiku disregarded the social norm over 15\\% of the time. These findings, demonstrated by our benchmark, underscore a fundamental misalignment in LLMs regarding physically grounded privacy and establish the need for more robust, physically-aware alignment. Codes and datasets will be available at https://github.com/Graph-COM/EAPrivacy.", "published": "2025-09-27T23:39:56Z", "updated": "2026-02-15T00:49:48Z", "authors": ["Xinjie Shen", "Mufei Li", "Pan Li"], "pdf_url": "https://arxiv.org/pdf/2510.02356v3"}
