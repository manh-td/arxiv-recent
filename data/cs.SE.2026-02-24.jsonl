{"id": "http://arxiv.org/abs/2602.21074v1", "title": "Validation of an analyzability model for quantum software: a family of experiments", "summary": "The analyzability of hybrid software, which integrates both classical and quantum components, is a key factor in ensuring its maintainability and industrial adoption. This article presents the empirical validation, through a family of experiments, of the quantum component of a previously proposed hybrid software analyzability model based on the ISO/IEC 25010 standard. The experimental series consists of four studies involving participants with diverse profiles in both academic and professional settings. In these experiments, the model's ability to effectively measure the analyzability of quantum algorithms is assessed, and the relationship between the analyzability levels computed by the model and the participant's perceptions of the complexity of these algorithms is examined. The results indicate that the proposed model effectively distinguishes between quantum software components with varying levels of analyzability and aligns with human perception, reinforcing its validity in quantum computing.", "published": "2026-02-24T16:35:20Z", "updated": "2026-02-24T16:35:20Z", "authors": ["Ana Díaz-Muñoz", "José A. Cruz-Lemus", "Moisés Rodríguez", "Maria Teresa Baldassarre", "Mario Piattini"], "pdf_url": "https://arxiv.org/pdf/2602.21074v1"}
{"id": "http://arxiv.org/abs/2602.21037v1", "title": "Automated Detection and Mitigation of Dependability Failures in Healthcare Scenarios through Digital Twins", "summary": "Medical Cyber-Physical Systems (CPSs) integrating Patients, Devices, and healthcare personnel (Physicians) form safety-critical PDP triads whose dependability is challenged by system heterogeneity and uncertainty in human and physiological behavior. While existing clinical decision support systems support clinical practice, there remains a need for proactive, reliability-oriented methodologies capable of identifying and mitigating failure scenarios before patient safety is compromised. This paper presents M-GENGAR, a methodology based on a closed-loop Digital Twin (DT) paradigm for dependability assurance of medical CPSs. The approach combines Stochastic Hybrid Automata modeling, data-driven learning of patient dynamics, and Statistical Model Checking with an offline critical scenario detection phase that integrates model-space exploration and diversity analysis to systematically identify and classify scenarios violating expert-defined dependability requirements. M-GENGAR also supports the automated synthesis of mitigation strategies, enabling runtime feedback and control within the DT loop. We evaluate M-GENGAR on a representative use case study involving a pulmonary ventilator. Results show that, in 87.5% of the evaluated scenarios, strategies synthesized through formal game-theoretic analysis stabilize patient vital metrics at least as effectively as human decision-making, while maintaining relevant metrics 20% closer to nominal healthy values on average.", "published": "2026-02-24T15:56:20Z", "updated": "2026-02-24T15:56:20Z", "authors": ["Bruno Guindani", "Matteo Camilli", "Livia Lestingi", "Marcello M. Bersani"], "pdf_url": "https://arxiv.org/pdf/2602.21037v1"}
{"id": "http://arxiv.org/abs/2602.21033v1", "title": "MIP Candy: A Modular PyTorch Framework for Medical Image Processing", "summary": "Medical image processing demands specialized software that handles high-dimensional volumetric data, heterogeneous file formats, and domain-specific training procedures. Existing frameworks either provide low-level components that require substantial integration effort or impose rigid, monolithic pipelines that resist modification. We present MIP Candy (MIPCandy), a freely available, PyTorch-based framework designed specifically for medical image processing. MIPCandy provides a complete, modular pipeline spanning data loading, training, inference, and evaluation, allowing researchers to obtain a fully functional process workflow by implementing a single method, $\\texttt{build_network}$, while retaining fine-grained control over every component. Central to the design is $\\texttt{LayerT}$, a deferred configuration mechanism that enables runtime substitution of convolution, normalization, and activation modules without subclassing. The framework further offers built-in $k$-fold cross-validation, dataset inspection with automatic region-of-interest detection, deep supervision, exponential moving average, multi-frontend experiment tracking (Weights & Biases, Notion, MLflow), training state recovery, and validation score prediction via quotient regression. An extensible bundle ecosystem provides pre-built model implementations that follow a consistent trainer--predictor pattern and integrate with the core framework without modification. MIPCandy is open-source under the Apache-2.0 license and requires Python~3.12 or later. Source code and documentation are available at https://github.com/ProjectNeura/MIPCandy.", "published": "2026-02-24T15:55:04Z", "updated": "2026-02-24T15:55:04Z", "authors": ["Tianhao Fu", "Yucheng Chen"], "pdf_url": "https://arxiv.org/pdf/2602.21033v1"}
{"id": "http://arxiv.org/abs/2602.21026v1", "title": "A Modular Multi-Document Framework for Scientific Visualization and Simulation in Java", "summary": "This paper presents the design and implementation of a modular multi-document interface (MDI) framework for scientific visualization and simulation in the Java Virtual Machine (JVM) ecosystem. The framework emphasizes architectural separation between visualization layers, simulation engines, and optional hardware-accelerated 3D rendering. 3D functionality is isolated into a separate module to prevent unnecessary dependency coupling in 2D-only applications. We describe the core abstractions, threading model, simulation integration strategy, and dependency isolation approach. A case study involving a real-time 3D gas expansion simulation integrated with synchronized 2D entropy plotting demonstrates architectural cohesion. The framework is publicly available via Maven Central and targets long-lived scientific and engineering desktop applications.", "published": "2026-02-24T15:48:39Z", "updated": "2026-02-24T15:48:39Z", "authors": ["David Heddle"], "pdf_url": "https://arxiv.org/pdf/2602.21026v1"}
{"id": "http://arxiv.org/abs/2602.20979v1", "title": "Toward an Agentic Infused Software Ecosystem", "summary": "Fully leveraging the capabilities of AI agents in software development requires a rethinking of the software ecosystem itself. To this end, this paper outlines the creation of an Agentic Infused Software Ecosystem (AISE), that rests on three pillars. The first, of course, is the AI agents themselves, which in the past 5 years have moved from simple code completion and toward sophisticated independent development tasks, a trend which will only continue. The second pillar is the programming language and APIs (or tools) that these agents use to accomplish tasks, and increasingly, serve as the communication substrate that humans and AI agents interact and collaborate through. The final pillar is the runtime environment and ecosystem that agents operate within, and which provide the capabilities that programmatic agents use to interface with (and effect actions in) the external world. To realize the vision of AISE, all three pillars must be advanced in a holistic manner, and critically, in a manner that is synergistic for AI agents as they exist today, those that will exist in the future, and for the human developers that work alongside them.", "published": "2026-02-24T15:01:29Z", "updated": "2026-02-24T15:01:29Z", "authors": ["Mark Marron"], "pdf_url": "https://arxiv.org/pdf/2602.20979v1"}
{"id": "http://arxiv.org/abs/2601.13134v2", "title": "Earth Embeddings as Products: Taxonomy, Ecosystem, and Standardized Access", "summary": "Geospatial Foundation Models (GFMs) provide powerful representations, but high compute costs hinder their widespread use. Pre-computed embedding data products offer a practical \"frozen\" alternative, yet they currently exist in a fragmented ecosystem of incompatible formats and resolutions. This lack of standardization creates an engineering bottleneck that prevents meaningful model comparison and reproducibility. We formalize this landscape through a three-layer taxonomy: Data, Tools, and Value. We survey existing products to identify interoperability barriers. To bridge this gap, we extend TorchGeo with a unified API that standardizes the loading and querying of diverse embedding products. By treating embeddings as first-class geospatial datasets, we decouple downstream analysis from model-specific engineering, providing a roadmap for more transparent and accessible Earth observation workflows.", "published": "2026-01-19T15:20:18Z", "updated": "2026-02-24T14:32:15Z", "authors": ["Heng Fang", "Adam J. Stewart", "Isaac Corley", "Xiao Xiang Zhu", "Hossein Azizpour"], "pdf_url": "https://arxiv.org/pdf/2601.13134v2"}
{"id": "http://arxiv.org/abs/2602.20924v1", "title": "Airavat: An Agentic Framework for Internet Measurement", "summary": "Internet measurement faces twin challenges: complex analyses require expert-level orchestration of tools, yet even syntactically correct implementations can have methodological flaws and can be difficult to verify. Democratizing measurement capabilities thus demands automating both workflow generation and verification against methodological standards established through decades of research.\n  We present Airavat, the first agentic framework for Internet measurement workflow generation with systematic verification and validation. Airavat coordinates a set of agents mirroring expert reasoning: three agents handle problem decomposition, solution design, and code implementation, with assistance from a registry of existing tools. Two specialized engines ensure methodological correctness: a Verification Engine evaluates workflows against a knowledge graph encoding five decades of measurement research, while a Validation Engine identifies appropriate validation techniques grounded in established methodologies. Through four Internet measurement case studies, we demonstrate that Airavat (i) generates workflows matching expert-level solutions, (ii) makes sound architectural decisions, (iii) addresses novel problems without ground truth, and (iv) identifies methodological flaws missed by standard execution-based testing.", "published": "2026-02-24T14:04:18Z", "updated": "2026-02-24T14:04:18Z", "authors": ["Alagappan Ramanathan", "Eunju Kang", "Dongsu Han", "Sangeetha Abdu Jyothi"], "pdf_url": "https://arxiv.org/pdf/2602.20924v1"}
{"id": "http://arxiv.org/abs/2602.20799v1", "title": "Unseen-Codebases-Domain Data Synthesis and Training Based on Code Graphs", "summary": "In the context of newly release software frameworks, large language models (LLMs) often exhibit poor performance and a high rate of hallucination, as they are not exposed to such environments during training. Although inference-time augmentation techniques such as retrieval-augmented generation (RAG) can partially mitigate hallucinations, knowledge injection through prompting alone is insufficient to enable models to fully understand the intrinsic relationships among different components of a codebase, or to reason about the correct compositions and apply. Although explicit knowledge injection can be achieved through post-training, compared with public code domains, unseen codebases typically provide only source code and lack large volumes of high-quality, usage-oriented code that can be directly leveraged as training data. Consequently, existing data synthesis approaches are insufficient to adequately capture unseen codebases usage scenarios when restricted to source code alone. To address these challenges, we propose UCD-Training, a two-stage training framework for reasoning-aware data synthesis grounded in a code graph constructed from unseen codebases. UCD-Training first parses the source code to build a code graph, then conducts dependency-preserving continued pretraining (CPT) using file-level dependency data, followed by graph-grounded supervised fine-tuning (SFT) on three types of synthesized data augmented with explicit reasoning traces: (1) single-hop relation reasoning data, (2) compositional API reasoning data, and (3) codebase utilization data. We further introduce a new benchmark, UnseenCodeBench, for code generation on unseen codebases and conduct comprehensive experiments across multiple codebases.", "published": "2026-02-24T11:36:34Z", "updated": "2026-02-24T11:36:34Z", "authors": ["Guangsheng Ou", "Qiming Zhang", "Sirong Chen", "Anji Li", "Dong Xu", "Tiancheng Luo", "Dekun Dai", "Cuiyun Gao", "Long Wang", "Jun Zhou", "Mingwei Liu", "Zibin Zheng"], "pdf_url": "https://arxiv.org/pdf/2602.20799v1"}
{"id": "http://arxiv.org/abs/2506.06251v2", "title": "DesignBench: A Comprehensive Benchmark for MLLM-based Front-end Code Generation", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in automated front-end engineering, e.g., generating UI code from visual designs. However, existing front-end UI code generation benchmarks have the following limitations: (1) While framework-based development becomes predominant in modern front-end programming, current benchmarks fail to incorporate mainstream development frameworks. (2) Existing evaluations focus solely on the UI code generation task, whereas practical UI development involves several iterations, including refining editing, and repairing issues. (3) Current benchmarks employ unidimensional evaluation, lacking investigation into influencing factors like task difficulty, input context variations, and in-depth code-level analysis. To bridge these gaps, we introduce DesignBench, a multi-framework, multi-task evaluation benchmark for assessing MLLMs' capabilities in automated front-end engineering. DesignBench encompasses three widely-used UI frameworks (React, Vue, and Angular) alongside vanilla HTML/CSS, and evaluates on three essential front-end tasks (generation, edit, and repair) in real-world development workflows. DesignBench contains 900 webpage samples spanning over 11 topics, 9 edit types, and 6 issue categories, enabling detailed analysis of MLLM performance across multiple dimensions. Our systematic evaluation reveals critical insights into MLLMs' framework-specific limitations, task-related bottlenecks, and performance variations under different conditions, providing guidance for future research in automated front-end development. Our code and data are available at https://github.com/WebPAI/DesignBench.", "published": "2025-06-06T17:21:21Z", "updated": "2026-02-24T10:32:26Z", "authors": ["Jingyu Xiao", "Man Ho Lam", "Ming Wang", "Yuxuan Wan", "Junliang Liu", "Yintong Huo", "Michael R. Lyu"], "pdf_url": "https://arxiv.org/pdf/2506.06251v2"}
{"id": "http://arxiv.org/abs/2602.18579v2", "title": "Refactoring for Novices in Java: An Eye Tracking Study on the Extract vs. Inline Methods", "summary": "Developers often extract methods to improve readability, understanding, and reuse, while inlining keeps logic in one block. Prior work based on static metrics has not shown clear differences between these practices, and the human side of comprehension and navigation remains underexplored. We investigate Inline Method vs. Extract Method refactorings using a dynamic approach: eye tracking while participants read and solve tasks. We analyze key code areas and compare visual effort and reading behavior (fixation duration and count, regressions, revisits), alongside time and attempts. We ran a controlled experiment with 32 Java novices, followed by short interviews. Each participant solved eight simple tasks across four programs presented in an inlined version and four in an extracted version. We also surveyed 58 additional novices for complementary quantitative and qualitative data. Results show that effects depend on task difficulty. In two tasks, method extraction improved performance and reduced visual effort, with time decreasing by up to 78.8% and regressions by 84.6%. For simpler tasks (e.g., square area), extraction hurt performance: time increased by up to 166.9% and regressions by 200%. Even with meaningful method names, novices often switched back and forth between call sites and extracted methods, increasing navigation and cognitive load. Preferences frequently favored extraction for readability and reuse, but did not always match measured performance. These findings suggest educators should be cautious about premature modularization for novices and highlight eye tracking as a useful complement to static metrics.", "published": "2026-02-20T19:37:14Z", "updated": "2026-02-24T09:44:17Z", "authors": ["José Aldo Silva da Costa", "Rohit Gheyi", "José Júnior Silva da Costa", "Márcio Ribeiro", "Rodrigo Bonifácio", "Hyggo Almeida", "Ana Carla Bibiano", "Alessandro Garcia"], "pdf_url": "https://arxiv.org/pdf/2602.18579v2"}
{"id": "http://arxiv.org/abs/2602.20717v1", "title": "PackMonitor: Enabling Zero Package Hallucinations Through Decoding-Time Monitoring", "summary": "As Large Language Models (LLMs) are increasingly integrated into software development workflows, their trustworthiness has become a critical concern. However, in dependency recommendation scenarios, the reliability of LLMs is undermined by widespread package hallucinations, where models often recommend hallucinated packages. Recent studies have proposed a range of approaches to mitigate this issue. Nevertheless, existing approaches typically merely reduce hallucination rates rather than eliminate them, leaving persistent software security risks.\n  In this work, we argue that package hallucinations are theoretically preventable based on the key insight that package validity is decidable through finite and enumerable authoritative package lists. Building on this, we propose PackMonitor, the first approach capable of fundamentally eliminating package hallucinations by continuously monitoring the model's decoding process and intervening when necessary. To implement this in practice, PackMonitor addresses three key challenges: (1) determining when to trigger intervention via a Context-Aware Parser that continuously monitors model outputs and selectively activates intervening only during installation command generation; (2) resolving how to intervene by employing a Package-Name Intervenor that strictly limits the decoding space to an authoritative package list; and (3) ensuring monitoring efficiency through a DFA-Caching Mechanism that enables scalability to millions of packages with negligible overhead. Extensive experiments on five widely used LLMs demonstrate that PackMonitor is a training-free, plug-and-play solution that consistently reduces package hallucination rates to zero while maintaining low-latency inference and preserving original model capabilities.", "published": "2026-02-24T09:26:11Z", "updated": "2026-02-24T09:26:11Z", "authors": ["Xiting Liu", "Yuetong Liu", "Yitong Zhang", "Jia Li", "Shi-Min Hu"], "pdf_url": "https://arxiv.org/pdf/2602.20717v1"}
{"id": "http://arxiv.org/abs/2602.20684v1", "title": "Agile V: A Compliance-Ready Framework for AI-Augmented Engineering -- From Concept to Audit-Ready Delivery", "summary": "Current AI-assisted engineering workflows lack a built-in mechanism to maintain task-level verification and regulatory traceability at machine-speed delivery. Agile V addresses this gap by embedding independent verification and audit artifact generation into each task cycle. The framework merges Agile iteration with V-Model verification into a continuous Infinity Loop, deploying specialized AI agents for requirements, design, build, test, and compliance, governed by mandatory human approval gates. We evaluate three hypotheses: (H1) audit-ready artifacts emerge as a by-product of development, (H2) 100% requirement-level verification is achievable with independent test generation, and (H3) verified increments can be delivered with single-digit human interactions per cycle. A feasibility case study on a Hardware-in-the-Loop system (about 500 LOC, 8 requirements, 54 tests) supports all three hypotheses: audit-ready documentation was generated automatically (H1), 100% requirement-level pass rate was achieved (H2), and only 6 prompts per cycle were required (H3), yielding an estimated 10-50x cost reduction versus a COCOMO II baseline (sensitivity range from pessimistic to optimistic assumptions). We invite independent replication to validate generalizability.", "published": "2026-02-24T08:41:05Z", "updated": "2026-02-24T08:41:05Z", "authors": ["Christopher Koch", "Joshua Andreas Wellbrock"], "pdf_url": "https://arxiv.org/pdf/2602.20684v1"}
{"id": "http://arxiv.org/abs/2602.20644v1", "title": "An LLM-driven Scenario Generation Pipeline Using an Extended Scenic DSL for Autonomous Driving Safety Validation", "summary": "Real-world crash reports, which combine textual summaries and sketches, are valuable for scenario-based testing of autonomous driving systems (ADS). However, current methods cannot effectively translate this multimodal data into precise, executable simulation scenarios, hindering the scalability of ADS safety validation. In this work, we propose a scalable and verifiable pipeline that uses a large language model (GPT-4o mini) and a probabilistic intermediate representation (an Extended Scenic domain-specific language) to automatically extract semantic scenario configurations from crash reports and generate corresponding simulation-ready scenarios. Unlike earlier approaches such as ScenicNL and LCTGen (which generate scenarios directly from text) or TARGET (which uses deterministic mappings from traffic rules), our method introduces an intermediate Scenic DSL layer to separate high-level semantic understanding from low-level scenario rendering, reducing errors and capturing real-world variability. We evaluated the pipeline on cases from the NHTSA CIREN database. The results show high accuracy in knowledge extraction: 100% correctness for environmental and road network attributes, and 97% and 98% for oracle and actor trajectories, respectively, compared to human-derived ground truth. We executed the generated scenarios in the CARLA simulator using the Autoware driving stack, and they consistently triggered the intended traffic-rule violations (such as opposite-lane crossing and red-light running) across 2,000 scenario variations. These findings demonstrate that the proposed pipeline provides a legally grounded, scalable, and verifiable approach to ADS safety validation.", "published": "2026-02-24T07:44:26Z", "updated": "2026-02-24T07:44:26Z", "authors": ["Fida Khandaker Safa", "Yupeng Jiang", "Xi Zheng"], "pdf_url": "https://arxiv.org/pdf/2602.20644v1"}
{"id": "http://arxiv.org/abs/2602.20610v1", "title": "SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference", "summary": "Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions.", "published": "2026-02-24T07:01:17Z", "updated": "2026-02-24T07:01:17Z", "authors": ["Cuong Chi Le", "Minh V. T Pham", "Tung Vu Duy", "Cuong Duc Van", "Huy N. Phan", "Hoang N. Phan", "Tien N. Nguyen"], "pdf_url": "https://arxiv.org/pdf/2602.20610v1"}
{"id": "http://arxiv.org/abs/2602.20598v1", "title": "A Case Study on Runtime Verification of a Continuous Deployment Process", "summary": "We report our experience in applying runtime monitoring to a FluxCD-based continuous deployment (CD) process. Our target system consists of GitHub Actions, GitHub Container Registry (GHCR), FluxCD, and an application running on Kubernetes. We monitored its logs using SyMon. In our setting, we regard a deployment update as detected when FluxCD's polling log resolves the latest image tag. Through the case study, we found that FluxCD did not always detect a new image within five minutes after it was pushed to GHCR, whereas it always did so within ten minutes in the collected logs. Moreover, our results show that SyMon is fast enough for near-real-time monitoring in our setting.", "published": "2026-02-24T06:39:48Z", "updated": "2026-02-24T06:39:48Z", "authors": ["Shoma Ansai", "Masaki Waga"], "pdf_url": "https://arxiv.org/pdf/2602.20598v1"}
{"id": "http://arxiv.org/abs/2406.11935v3", "title": "A Problem-Oriented Perspective and Anchor Verification for Code Optimization", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in solving various programming tasks, such as code generation. However, their potential for code optimization, particularly in performance enhancement, remains largely unexplored. This paper investigates the capabilities of LLMs in optimizing code for minimal execution time, addressing a critical gap in current research. The recently proposed code optimization methods construct program optimization pairs based on iterative submissions from the same programmer for the same problem. However, this approach confines LLMs to local performance improvements, neglecting global algorithmic innovation. To overcome this limitation, we adopt a completely different perspective by reconstructing the optimization pairs into a problem-oriented approach. This allows for the integration of various ideas from multiple programmers tackling the same problem. Furthermore, we observe that code optimization presents greater challenges compared to code generation, often accompanied by \"optimization tax\". Recognizing the inherent trade-offs in correctness and efficiency, we introduce a novel anchor verification framework to mitigate this \"optimization tax\". Ultimately, the problem oriented perspective combined with the anchor verification framework significantly enhances both the correct optimization ratio and speedup to new levels.", "published": "2024-06-17T16:10:10Z", "updated": "2026-02-24T02:18:19Z", "authors": ["Tong Ye", "Tengfei Ma", "Xuhong Zhang", "Hang Yu", "Jianwei Yin", "Wenhai Wang"], "pdf_url": "https://arxiv.org/pdf/2406.11935v3"}
{"id": "http://arxiv.org/abs/2602.20478v1", "title": "Codified Context: Infrastructure for AI Agents in a Complex Codebase", "summary": "LLM-based agentic coding assistants lack persistent memory: they lose coherence across sessions, forget project conventions, and repeat known mistakes. Recent studies characterize how developers configure agents through manifest files, but an open challenge remains how to scale such configurations for large, multi-agent projects. This paper presents a three-component codified context infrastructure developed during construction of a 108,000-line C# distributed system: (1) a hot-memory constitution encoding conventions, retrieval hooks, and orchestration protocols; (2) 19 specialized domain-expert agents; and (3) a cold-memory knowledge base of 34 on-demand specification documents. Quantitative metrics on infrastructure growth and interaction patterns across 283 development sessions are reported alongside four observational case studies illustrating how codified context propagates across sessions to prevent failures and maintain consistency. The framework is published as an open-source companion repository.", "published": "2026-02-24T02:11:26Z", "updated": "2026-02-24T02:11:26Z", "authors": ["Aristidis Vasilopoulos"], "pdf_url": "https://arxiv.org/pdf/2602.20478v1"}
