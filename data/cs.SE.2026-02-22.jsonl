{"id": "http://arxiv.org/abs/2602.19338v1", "title": "Complex Event Processing in the Edge: A Combined Optimization Approach for Data and Code Placement", "summary": "The increasing variety of input data and complexity of tasks that are handled by the devices of internet of things (IoT) environments require solutions that consider the limited hardware and computation power of the edge devices. Complex event processing (CEP), can be given as an example, which involves reading and aggregating data from multiple sources to infer triggering of important events. In this study, we balance the execution costs between different paths of the CEP task graph with a constrained programming optimization approach and improve critical path performance. The proposed approach is implemented as a Python library, allowing small-scale IoT devices to adaptively optimize code and I/O assignments and improve overall latency and throughput. The implemented library abstracts away the communication details and allows virtualization of a shared memory between IoT devices. The results show that optimizing critical path performance increases throughput and reduces delay across multiple devices during CEP operations.", "published": "2026-02-22T21:01:58Z", "updated": "2026-02-22T21:01:58Z", "authors": ["Halit Uyanık", "Tolga Ovatman"], "pdf_url": "https://arxiv.org/pdf/2602.19338v1"}
{"id": "http://arxiv.org/abs/2306.07400v2", "title": "Neural Embeddings for Web Testing", "summary": "Web test automation techniques often rely on crawlers to infer models of web applications for automated test generation. However, current crawlers rely on state equivalence algorithms that struggle to distinguish near-duplicate pages, often leading to redundant test cases and incomplete coverage of application functionality. In this paper, we present a model-based test generation approach that employs transformer-based Siamese neural networks (SNNs) to infer web application models more accurately. By learning similarity-based representations, SNNs capture structural and textual relationships among web pages, improving near-duplicate detection during crawling and enhancing the quality of inferred models, and thus, the effectiveness of generated test suites. Our evaluation across nine web apps shows that SNNs outperform state-of-the-art techniques in near-duplicate detection, resulting in superior web app models with an average F-1 score improvement of 56%. These enhanced models enable the generation of more effective test suites that achieve higher code coverage, with improvements ranging from 6% to 21% and averaging at 12%.", "published": "2023-06-12T19:59:36Z", "updated": "2026-02-22T20:29:49Z", "authors": ["Kasun Kanaththage", "Luigi Libero Lucio Starace", "Matteo Biagiola", "Paolo Tonella", "Andrea Stocco"], "pdf_url": "https://arxiv.org/pdf/2306.07400v2"}
{"id": "http://arxiv.org/abs/2510.07070v2", "title": "Building an Open AIBOM Standard in the Wild", "summary": "Modern software engineering increasingly relies on open, community-driven standards, yet how such standards are created in fast-evolving domains like AI-powered systems remains underexplored. This paper presents a detailed experience report on the development of the AI Bill of Materials AIBOM specification, an extension of the ISO/IEC 5962:2021 Software Package Data Exchange (SPDX) software bill of materials (SBOM) standard, which captures AI components such as datasets and iterative training artifacts. Framed through the lens of Action Research (AR), we document a global, multi-stakeholder effort involving over 90 contributors and structured AR cycles. The resulting specification was validated through four complementary approaches: alignment with major regulations and ethical standards (e.g., EU AI Act and IEEE 7000 standards), systematic mapping to six industry use cases, semi-structured practitioner interviews, and an industrial case study. Beyond delivering a validated artefact, our paper documents the process of building the AIBOM specification in the wild, and reflects on how it aligns with the AR cycle, and distills lessons that can inform future standardization efforts in the software engineering community.", "published": "2025-10-08T14:32:31Z", "updated": "2026-02-22T19:46:23Z", "authors": ["Gopi Krishnan Rajbahadur", "Keheliya Gallaba", "Elyas Rashno", "Arthit Suriyawongkul", "Karen Bennet", "Kate Stewart", "Ahmed E. Hassan"], "pdf_url": "https://arxiv.org/pdf/2510.07070v2"}
{"id": "http://arxiv.org/abs/2602.19294v1", "title": "Towards Automated Page Object Generation for Web Testing using Large Language Models", "summary": "Page Objects (POs) are a widely adopted design pattern for improving the maintainability and scalability of automated end-to-end web tests. However, creating and maintaining POs is still largely a manual, labor-intensive activity, while automated solutions have seen limited practical adoption. In this context, the potential of Large Language Models (LLMs) for these tasks has remained largely unexplored. This paper presents an empirical study on the feasibility of using LLMs, specifically GPT-4o and DeepSeek Coder, to automatically generate POs for web testing. We evaluate the generated artifacts on an existing benchmark of five web applications for which manually written POs are available (the ground truth), focusing on accuracy (i.e., the proportion of ground truth elements correctly identified) and element recognition rate (i.e., the proportion of ground truth elements correctly identified or marked for modification). Our results show that LLMs can generate syntactically correct and functionally useful POs with accuracy values ranging from 32.6% to 54.0% and element recognition rate exceeding 70% in most cases. Our study contributes the first systematic evaluation of LLMs strengths and open challenges for automated PO generation, and provides directions for further research on integrating LLMs into practical testing workflows.", "published": "2026-02-22T18:06:57Z", "updated": "2026-02-22T18:06:57Z", "authors": ["Betül Karagöz", "Filippo Ricca", "Matteo Biagiola", "Andrea Stocco"], "pdf_url": "https://arxiv.org/pdf/2602.19294v1"}
{"id": "http://arxiv.org/abs/2602.19276v1", "title": "ComUICoder: Component-based Reusable UI Code Generation for Complex Websites via Semantic Segmentation and Element-wise Feedback", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated strong performance on the UI-to-code task, which aims to generate UI code from design mock-ups. However, when applied to long and complex websites, they often struggle with fragmented segmentation, redundant code generation for repetitive components, and frequent UI inconsistencies. To systematically investigate and address these challenges, we introduce ComUIBench, a new multi-page complex webpage benchmark with component annotations, designed to evaluate MLLMs' ability to generate reusable UI code in realistic website scenarios. Building upon this benchmark, we propose ComUICoder, a component-based UI code generation framework that emphasizes semantic-aware segmentation, code reuse, and fine-grained refinement. Specifically, ComUICoder incorporates (1) Hybrid Semantic-aware Block Segmentation for accurate UI semantic coherent block detection, (2) Visual-aware Graph-based Block Merge to consolidate structurally similar components within and across webpages for reusable implementation, and (3) Priority-based Element-wise Feedback to refine generated code and reduce element-level inconsistencies. Extensive experiments demonstrate that ComUICoder significantly improves overall generation quality and code reusability on complex multipage websites. Our datasets and code are publicly available at https://github.com/WebPAI/ComUICoder.", "published": "2026-02-22T17:17:16Z", "updated": "2026-02-22T17:17:16Z", "authors": ["Jingyu Xiao", "Jiantong Qin", "Shuoqi Li", "Man Ho Lam", "Yuxuan Wan", "Jen-tse Huang", "Yintong Huo", "Michael R. Lyu"], "pdf_url": "https://arxiv.org/pdf/2602.19276v1"}
{"id": "http://arxiv.org/abs/2602.19274v1", "title": "DD-CAM: Minimal Sufficient Explanations for Vision Models Using Delta Debugging", "summary": "We introduce a gradient-free framework for identifying minimal, sufficient, and decision-preserving explanations in vision models by isolating the smallest subset of representational units whose joint activation preserves predictions. Unlike existing approaches that aggregate all units, often leading to cluttered saliency maps, our approach, DD-CAM, identifies a 1-minimal subset whose joint activation suffices to preserve the prediction (i.e., removing any unit from the subset alters the prediction). To efficiently isolate minimal sufficient subsets, we adapt delta debugging, a systematic reduction strategy from software debugging, and configure its search strategy based on unit interactions in the classifier head: testing individual units for models with non-interacting units and testing unit combinations for models in which unit interactions exist. We then generate minimal, prediction-preserving saliency maps that highlight only the most essential features. Our experimental evaluation demonstrates that our approach can produce more faithful explanations and achieve higher localization accuracy than the state-of-the-art CAM-based approaches.", "published": "2026-02-22T17:12:31Z", "updated": "2026-02-22T17:12:31Z", "authors": ["Krishna Khadka", "Yu Lei", "Raghu N. Kacker", "D. Richard Kuhn"], "pdf_url": "https://arxiv.org/pdf/2602.19274v1"}
{"id": "http://arxiv.org/abs/2602.19218v1", "title": "Gecko: A Simulation Environment with Stateful Feedback for Refining Agent Tool Calls", "summary": "The ability to use tools is fundamental for large language model (LLM) agents. Given a task, existing systems use LLMs to plan and generate tool calls, which are executed by real-world tools to complete the task. However, tool calls are prone to errors because they are derived merely from LLM intrinsic capabilities. What is more, while it is useful to let LLMs iteratively refine the tool-call sequence using execution results from real tools, this process can be expensive and lead to unsafe results. To improve LLM tool calls and address issues caused by using real tools for refinement, we introduce Gecko, a comprehensive environment that simulates tool responses using a combination of rules and LLMs. Specifically, Gecko checks the validity of tool calls including input arguments and tool names, synthesizes reasonable responses that adhere to the output schema, and assesses whether all task objectives have been achieved. These three types of feedback provided by Gecko allow LLMs to refine their tool calls, forming a simple yet effective test-time scaling method named GATS. On BFCLv3 and $τ^2$-bench, GATS consistently improves the tool calling performance of various LLMs including GPT-4o, GPT-5, and Gemini-3.0-pro. We further discuss working mechanisms of our method and share future possibilities.", "published": "2026-02-22T15:02:00Z", "updated": "2026-02-22T15:02:00Z", "authors": ["Zeyu Zhang", "Guohao Li", "Zhenchang Xing", "Alexandros Apostolopoulos", "Yu Lin Lee", "Liang Zheng"], "pdf_url": "https://arxiv.org/pdf/2602.19218v1"}
{"id": "http://arxiv.org/abs/2602.09467v3", "title": "Toward Linking Declined Proposals and Source Code: An Exploratory Study on the Go Repository", "summary": "Traceability links are key information sources for software developers, connecting software artifacts. Such links play an important role, particularly between contribution artifacts and their corresponding source code. Through these links, developers can trace the discussions in contributions and uncover design rationales, constraints, and security concerns. Previous studies have mainly examined accepted contributions, while those declined after discussion have been overlooked. Declined-contribution discussions capture valuable design rationale and implicit decision criteria, revealing why features are accepted or rejected. Our prior work also shows developers often revisit and resubmit declined contributions, making traceability to them useful. In this study, we present the first attempt to establish traceability links between declined contributions and related source code. We propose a linking approach and conduct an empirical analysis of the generated links to discuss the factors that affect link generation. As our dataset, we use proposals from the official Go repository, which are GitHub issues used to propose new features or language changes. To link declined proposals to source code, we design an LLM-driven pipeline. Our results show that the pipeline selected the correct granularity for each declined proposal with an accuracy of 0.836, and generated correct links at that granularity with a mean precision of 0.643. To clarify the challenges of linking declined proposals, we conduct a failure analysis of instances where the pipeline failed to generate links. In these cases, discussions were often redundant and lacked concrete information (e.g., details on how the feature should be implemented).", "published": "2026-02-10T07:01:13Z", "updated": "2026-02-22T14:11:21Z", "authors": ["Sota Nakashima", "Masanari Kondo", "Mahmoud Alfadel", "Aly Ahmad", "Toshihiro Nakae", "Hidenori Matsuzaki", "Yasutaka Kamei"], "pdf_url": "https://arxiv.org/pdf/2602.09467v3"}
{"id": "http://arxiv.org/abs/2512.01356v2", "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM", "summary": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.", "published": "2025-12-01T07:10:23Z", "updated": "2026-02-22T13:09:58Z", "authors": ["Yuxin Zhang", "Yuxia Zhang", "Zeyu Sun", "Yanjie Jiang", "Hui Liu"], "pdf_url": "https://arxiv.org/pdf/2512.01356v2"}
{"id": "http://arxiv.org/abs/2602.19098v1", "title": "A Systematic Evaluation of Environmental Flakiness in JavaScript Tests", "summary": "Test flakiness is a significant issue in industry, affecting test efficiency and product quality. While extensive research has examined the impact of flaky tests, many root causes remain unexplored, particularly in the context of dynamic languages such as JavaScript. In this paper, we conduct a systematic evaluation of the impact of environmental factors on test flakiness in JavaScript. We first executed test suites across multiple environmental configurations to determine whether changes in the environment could lead to flaky behavior. We selected three environmental factors to manipulate: the operating system, the Node.js version, and the browser. We identified a total of 65 environmental flaky projects, with 28 related to operating system issues, five to Node.js version compatibility, 16 to a combination of operating system and Node.js issues, and 17 related to browser compatibility. To address environmental flakiness, we developed a lightweight mitigation approach, js-env-sanitizer, that can sanitize environmental-related flaky tests by skipping and reporting them (rather than failing), allowing CI builds to continue/succeed without rerunning entire test suites. The tool achieves high accuracy with minimal performance or configuration overhead, and currently supports three popular JavaScript testing frameworks (Jest, Mocha, and Vitest)", "published": "2026-02-22T08:59:27Z", "updated": "2026-02-22T08:59:27Z", "authors": ["Negar Hashemi", "Amjed Tahir", "August Shi", "Shawn Rasheed", "Rachel Blagojevic"], "pdf_url": "https://arxiv.org/pdf/2602.19098v1"}
{"id": "http://arxiv.org/abs/2602.10471v2", "title": "TestExplora: Benchmarking LLMs for Proactive Bug Discovery via Repository-Level Test Generation", "summary": "Given that Large Language Models (LLMs) are increasingly applied to automate software development, comprehensive software assurance spans three distinct goals: regression prevention, reactive reproduction, and proactive discovery. Current evaluations systematically overlook the third goal. Specifically, they either treat existing code as ground truth (a compliance trap) for regression prevention, or depend on post-failure artifacts (e.g., issue reports) for bug reproduction-so they rarely surface defects before failures. To bridge this gap, we present TestExplora, a benchmark designed to evaluate LLMs as proactive testers within full-scale, realistic repository environments. TestExplora contains 2,389 tasks from 482 repositories and hides all defect-related signals. Models must proactively find bugs by comparing implementations against documentation-derived intent, using documentation as the oracle. Furthermore, to keep evaluation sustainable and reduce leakage, we propose continuous, time-aware data collection. Our evaluation reveals a significant capability gap: state-of-the-art models achieve a maximum Fail-to-Pass (F2P) rate of only 16.06%. Further analysis indicates that navigating complex cross-module interactions and leveraging agentic exploration are critical to advancing LLMs toward autonomous software quality assurance. Consistent with this, SWEAgent instantiated with GPT-5-mini achieves an F2P of 17.27% and an F2P@5 of 29.7%, highlighting the effectiveness and promise of agentic exploration in proactive bug discovery tasks.", "published": "2026-02-11T03:22:51Z", "updated": "2026-02-22T05:13:04Z", "authors": ["Steven Liu", "Jane Luo", "Xin Zhang", "Aofan Liu", "Hao Liu", "Jie Wu", "Ziyang Huang", "Yangyu Huang", "Yu Kang", "Scarlett Li"], "pdf_url": "https://arxiv.org/pdf/2602.10471v2"}
{"id": "http://arxiv.org/abs/2510.00920v3", "title": "Can Emulating Semantic Translation Help LLMs with Code Translation? A Study Based on Pseudocode", "summary": "Although large language models (LLMs) show promising potential in code translation, they still struggle to generate accurate translations using the commonly adopted direct code-to-code translation approach, which converts an original program into the target programming language (PL) in a single step. Inspired by the success of incorporating intermediate steps to guide LLMs in resolving challenging tasks, in this study, we explore pseudocode-based code translation. This approach emulates human semantic translation by first interpreting the original program's intent and logic into pseudocode and then implementing it in the target PL. To understand the effectiveness of this underexplored approach, we present a systematic empirical study on pseudocode-based code translation, aiming to investigate its helpfulness in enhancing the direct translation approach, illuminate its effective usage, and identify its limitations. By comparing direct and pseudocode-based translation on 9,690 translation tasks across six PLs with five popular LLMs, we found that pseudocode-based translation can effectively complement direct translation, particularly when translating from flexible to rigid PLs and handling a low-training-resource PL. Based on the findings, we suggest combining the translation results of both approaches for test-based selection to leverage their complementary strengths. We also reveal the advantages of pseudocode-based translation in decoupling the code understanding and generation burden on complicated programs and mitigating distractions from PL-specific implementations in original programs, as well as its limitations due to incorrect, incomplete, or ambiguous pseudocode. Our study sheds light on the effective use of pseudocode-based translation and provides evidence to help enhance LLMs in code translation.", "published": "2025-10-01T13:58:19Z", "updated": "2026-02-22T02:45:05Z", "authors": ["Songqiang Chen", "Congying Xu", "Jingyi Chen", "Jialun Cao", "Jiarong Wu", "Shing-Chi Cheung"], "pdf_url": "https://arxiv.org/pdf/2510.00920v3"}
