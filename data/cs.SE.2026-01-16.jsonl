{"id": "http://arxiv.org/abs/2512.18852v2", "title": "What Drives Issue Resolution Speed? An Empirical Study of Scientific Workflow Systems on GitHub", "summary": "Scientific Workflow Systems (SWSs) play a vital role in enabling reproducible, scalable, and automated scientific analysis. Like other open-source software, these systems depend on active maintenance and community engagement to remain reliable and sustainable. However, despite the importance of timely issue resolution for software quality and community trust, little is known about what drives issue resolution speed within SWSs. This paper presents an empirical study of issue management and resolution across a collection of GitHub-hosted SWS projects. We analyze 21,116 issues to investigate how project characteristics, issue metadata, and contributor interactions affect time-to-close. Specifically, we address two research questions: (1) how issues are managed and addressed in SWSs, and (2) how issue and contributor features relate to issue resolution speed. We find that 68.91% of issues are closed, with half of them resolved within 18.09 days. Our results show that although SWS projects follow structured issue management practices, the issue resolution speed varies considerably across systems. Factors such as labeling and assigning issues are associated with faster issue resolution. Based on our findings, we make recommendations for developers to better manage SWS repository issues and improve their quality.", "published": "2025-12-21T18:41:27Z", "updated": "2026-01-16T18:47:55Z", "authors": ["Khairul Alam", "Banani Roy"], "pdf_url": "https://arxiv.org/pdf/2512.18852v2"}
{"id": "http://arxiv.org/abs/2601.11510v1", "title": "Applying Formal Methods Tools to an Electronic Warfare Codebase (Experience report)", "summary": "While using formal methods offers advantages over unit testing, their steep learning curve can be daunting to developers and can be a major impediment to widespread adoption. To support integration into an industrial software engineering workflow, a tool must provide useful information and must be usable with relatively minimal user effort. In this paper, we discuss our experiences associated with identifying and applying formal methods tools on an electronic warfare (EW) system with stringent safety requirements and present perspectives on formal methods tools from EW software engineers who are proficient in development yet lack formal methods training. In addition to a difference in mindset between formal methods and unit testing approaches, some formal methods tools use terminology or annotations that differ from their target programming language, creating another barrier to adoption. Input/output contracts, objects in memory affected by a function, and loop invariants can be difficult to grasp and use. In addition to usability, our findings include a comparison of vulnerabilities detected by different tools. Finally, we present suggestions for improving formal methods usability including better documentation of capabilities, decreased manual effort, and improved handling of library code.", "published": "2026-01-16T18:46:19Z", "updated": "2026-01-16T18:46:19Z", "authors": ["Letitia W. Li", "Denley Lam", "Vu Le", "Daniel Mitchell", "Mark J. Gerken", "Robert B. Ross"], "pdf_url": "https://arxiv.org/pdf/2601.11510v1"}
{"id": "http://arxiv.org/abs/2512.02795v2", "title": "Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior", "summary": "Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse", "published": "2025-12-02T14:12:36Z", "updated": "2026-01-16T16:58:19Z", "authors": ["Marcus Kessel"], "pdf_url": "https://arxiv.org/pdf/2512.02795v2"}
{"id": "http://arxiv.org/abs/2601.11430v1", "title": "A Practical Guide to Establishing Technical Debt Management", "summary": "This white paper provides an overview of the topic of \"technical debt\" and presents an approach for managing technical debt in teams. The white paper is based on the results of my dissertation, which aimed to translate scientific findings into practical guidance. To this end, I collaborated with other researchers to support three teams from different companies in adapting and establishing a technical debt management system tailored to their specific needs. Research findings were supplemented with details or additional approaches. Research results that were less practical were discarded. The result is a guide on establishing technical debt management within a team. The guide is intended to provide orientation and not be a rigid framework. We distinguish between \"best practices\" and \"nice-to-haves.\" \"Best practices\" are understood to be all approaches that were adopted by all three teams. \"Nice-to-haves\" were used by at least one team. In many places, it is explicitly mentioned that the team should decide together how to design the process. This also applies, of course, to all areas where this was not explicitly mentioned. This white paper explicitly does not cover the establishment of technical debt management across the entire company, but provides suggestions for this at the end.", "published": "2026-01-16T16:52:18Z", "updated": "2026-01-16T16:52:18Z", "authors": ["Marion Wiese"], "pdf_url": "https://arxiv.org/pdf/2601.11430v1"}
{"id": "http://arxiv.org/abs/2601.11362v1", "title": "RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback", "summary": "Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.", "published": "2026-01-16T15:18:33Z", "updated": "2026-01-16T15:18:33Z", "authors": ["Manjeshwar Aniruddh Mallya", "Alessio Ferrari", "Mohammad Amin Zadenoori", "Jacek Dąbrowski"], "pdf_url": "https://arxiv.org/pdf/2601.11362v1"}
{"id": "http://arxiv.org/abs/2511.09122v2", "title": "Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation", "summary": "Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.", "published": "2025-11-12T08:56:11Z", "updated": "2026-01-16T14:53:55Z", "authors": ["Joschka Kersting", "Michael Rummel", "Gesa Benndorf"], "pdf_url": "https://arxiv.org/pdf/2511.09122v2"}
{"id": "http://arxiv.org/abs/2511.15817v5", "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code", "summary": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.\n  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.", "published": "2025-11-19T19:18:28Z", "updated": "2026-01-16T14:32:35Z", "authors": ["Alejandro Velasco", "Daniel Rodriguez-Cardenas", "Dipin Khati", "David N. Palacio", "Luftar Rahman Alif", "Denys Poshyvanyk"], "pdf_url": "https://arxiv.org/pdf/2511.15817v5"}
{"id": "http://arxiv.org/abs/2601.11299v1", "title": "Automation and Reuse Practices in GitHub Actions Workflows: A Practitioner's Perspective", "summary": "GitHub natively supports workflow automation through GitHub Actions. Yet, workflow maintenance is often considered a burden for software developers, who frequently face difficulties in writing, testing, debugging, and maintaining workflows. Little knowledge exists concerning the automation and reuse practices favoured by workflow practitioners. We therefore surveyed 419 practitioners to elucidate good and bad workflow development practices and to identify opportunities for supporting workflow maintenance. Specifically, we investigate the tasks that practitioners tend to automate using GitHub Actions, their preferred workflow creation mechanisms, and the non-functional characteristics they prioritise. We also examine the practices and challenges associated with GitHub's workflow reuse mechanisms. We observe a tendency to focus automation efforts on core CI/CD tasks, with less emphasis on crucial areas like security analysis and performance monitoring. Practitioners strongly rely on reusable Actions, but reusable workflows see less frequent adoption. Furthermore, we observed challenges with Action versioning and maintenance. Copy-pasting remains a common practice to have more control and avoid the complexity of depending on reusable components. These insights suggest the need for improved tooling, enhanced support for a wide range of automation tasks, and better mechanisms for discovering, managing, and trusting reusable workflow components.", "published": "2026-01-16T13:54:54Z", "updated": "2026-01-16T13:54:54Z", "authors": ["Hassan Onsori Delicheh", "Guillaume Cardoen", "Alexandre Decan", "Tom Mens"], "pdf_url": "https://arxiv.org/pdf/2601.11299v1"}
{"id": "http://arxiv.org/abs/2504.00013v2", "title": "Towards Industrial-scale Product Configuration", "summary": "We address the challenge of product configuration in the context of increasing customer demand for diverse and complex products. We propose a solution through a curated selection of product model benchmarks formulated in the COOM language, divided into three fragments of increasing complexity. Each fragment is accompanied by a corresponding bike model example, and additional scalable product models are included in the COOM suite, along with relevant resources. We outline an ASP-based workflow for solving COOM-based configuration problems, highlighting its adaptability to different paradigms and alternative ASP solutions. The COOM Suite aims to provide a comprehensive, accessible, and representative set of examples that can serve as a common ground for stakeholders in the field of product configuration.", "published": "2025-03-26T15:12:30Z", "updated": "2026-01-16T12:24:21Z", "authors": ["Joachim Baumeister", "Susana Hahn", "Konstantin Herud", "Max Ostrowski", "Jochen Reutelshöfer", "Nicolas Rühling", "Torsten Schaub", "Philipp Wanko"], "pdf_url": "https://arxiv.org/pdf/2504.00013v2"}
{"id": "http://arxiv.org/abs/2508.05192v2", "title": "AI-assisted JSON Schema Creation and Mapping", "summary": "Model-Driven Engineering (MDE) places models at the core of system and data engineering processes. In the context of research data, these models are typically expressed as schemas that define the structure and semantics of datasets. However, many domains still lack standardized models, and creating them remains a significant barrier, especially for non-experts. We present a hybrid approach that combines large language models (LLMs) with deterministic techniques to enable JSON Schema creation, modification, and schema mapping based on natural language inputs by the user. These capabilities are integrated into the open-source tool MetaConfigurator, which already provides visual model editing, validation, code generation, and form generation from models. For data integration, we generate schema mappings from heterogeneous JSON, CSV, XML, and YAML data using LLMs, while ensuring scalability and reliability through deterministic execution of generated mapping rules. The applicability of our work is demonstrated in an application example in the field of chemistry. By combining natural language interaction with deterministic safeguards, this work significantly lowers the barrier to structured data modeling and data integration for non-experts.", "published": "2025-08-07T09:27:10Z", "updated": "2026-01-16T12:16:49Z", "authors": ["Felix Neubauer", "Jürgen Pleiss", "Benjamin Uekermann"], "pdf_url": "https://arxiv.org/pdf/2508.05192v2"}
{"id": "http://arxiv.org/abs/2601.11138v1", "title": "Patterns of Bot Participation and Emotional Influence in Open-Source Development", "summary": "We study how bots contribute to open-source discussions in the Ethereum ecosystem and whether they influence developers' emotional tone. Our dataset covers 36,875 accounts across ten repositories with 105 validated bots (0.28%). Human participation follows a U-shaped pattern, while bots engage in uniform (pull requests) or late-stage (issues) activity. Bots respond faster than humans in pull requests but play slower maintenance roles in issues. Using a model trained on 27 emotion categories, we find bots are more neutral, yet their interventions are followed by reduced neutrality in human comments, with shifts toward gratitude, admiration, and optimism and away from confusion. These findings indicate that even a small number of bots are associated with changes in both timing and emotional dynamics of developer communication.", "published": "2026-01-16T09:58:28Z", "updated": "2026-01-16T09:58:28Z", "authors": ["Matteo Vaccargiu", "Riccardo Lai", "Maria Ilaria Lunesu", "Andrea Pinna", "Giuseppe Destefanis"], "pdf_url": "https://arxiv.org/pdf/2601.11138v1"}
{"id": "http://arxiv.org/abs/2601.11077v1", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "published": "2026-01-16T08:23:52Z", "updated": "2026-01-16T08:23:52Z", "authors": ["Jie Yang", "Honglin Guo", "Li Ji", "Jiazheng Zhou", "Rui Zheng", "Zhikai Lei", "Shuo Zhang", "Zhiheng Xi", "Shichun Liu", "Yuxin Wang", "Bo Wang", "Yining Zheng", "Tao Gui", "Xipeng Qiu"], "pdf_url": "https://arxiv.org/pdf/2601.11077v1"}
{"id": "http://arxiv.org/abs/2504.18776v2", "title": "ThinkFL: Self-Refining Failure Localization for Microservice Systems via Reinforcement Fine-Tuning", "summary": "As modern microservice systems grow increasingly popular and complex-often consisting of hundreds or even thousands of fine-grained, interdependent components-they are becoming more susceptible to frequent and subtle failures. Ensuring system reliability therefore hinges on accurate and efficient failure localization. Traditional failure localization approaches based on small models lack the flexibility to adapt to diverse failure scenarios, while recent LLM-based methods suffer from two major limitations: they often rely on rigid invocation workflows that constrain the model's ability to dynamically explore optimal localization paths, and they require resource-intensive inference, making them cost-prohibitive for real-world deployment. To address these challenges, we explore the use of reinforcement fine-tuning to equip lightweight LLMs with reasoning and self-refinement capabilities, significantly improving the cost-effectiveness and adaptability of LLM-based failure localization. We begin with an empirical study to identify three key capabilities essential for accurate localization. Building on these insights, we propose a progressive multi-stage GRPO fine-tuning framework, which integrates a multi-factor failure localization grader and a recursion-of-thought actor module. The resulting model, ThinkFL, not only outperforms existing state-of-the-art LLMs and baseline methods in localization accuracy but also reduces end-to-end localization latency from minutes to seconds, demonstrating strong potential for real-world applications.", "published": "2025-04-26T03:08:30Z", "updated": "2026-01-16T02:40:53Z", "authors": ["Lingzhe Zhang", "Yunpeng Zhai", "Tong Jia", "Chiming Duan", "Siyu Yu", "Jinyang Gao", "Bolin Ding", "Zhonghai Wu", "Ying Li"], "pdf_url": "https://arxiv.org/pdf/2504.18776v2"}
{"id": "http://arxiv.org/abs/2601.10942v1", "title": "Change And Cover: Last-Mile, Pull Request-Based Regression Test Augmentation", "summary": "Software is in constant evolution, with developers frequently submitting pull requests (PRs) to introduce new features or fix bugs. Testing PRs is critical to maintaining software quality. Yet, even in projects with extensive test suites, some PR-modified lines remain untested, leaving a \"last-mile\" regression test gap. Existing test generators typically aim to improve overall coverage, but do not specifically target the uncovered lines in PRs. We present Change And Cover (ChaCo), an LLM-based test augmentation technique that addresses this gap. It makes three contributions: (i) ChaCo considers the PR-specific patch coverage, offering developers augmented tests for code just when it is on the developers' mind. (ii) We identify providing suitable test context as a crucial challenge for an LLM to generate useful tests, and present two techniques to extract relevant test content, such as existing test functions, fixtures, and data generators. (iii) To make augmented tests acceptable for developers, ChaCo carefully integrates them into the existing test suite, e.g., by matching the test's structure and style with the existing tests, and generates a summary of the test addition for developer review. We evaluate ChaCo on 145 PRs from three popular and complex open-source projects - SciPy, Qiskit, and Pandas. The approach successfully helps 30% of PRs achieve full patch coverage, at the cost of $0.11, showing its effectiveness and practicality. Human reviewers find the tests to be worth adding (4.53/5.0), well integrated (4.2/5.0), and relevant to the PR (4.7/5.0). Ablations show test context is crucial for context-aware test generation, leading to 2x coverage. We submitted 12 tests, of which 8 have already been merged, and two previously unknown bugs were exposed and fixed. We envision our approach to be integrated into CI workflows, automating the last mile of regression test augmentation.", "published": "2026-01-16T02:08:16Z", "updated": "2026-01-16T02:08:16Z", "authors": ["Zitong Zhou", "Matteo Paltenghi", "Miryung Kim", "Michael Pradel"], "pdf_url": "https://arxiv.org/pdf/2601.10942v1"}
{"id": "http://arxiv.org/abs/2504.05509v2", "title": "Enforcing Control Flow Integrity on DeFi Smart Contracts", "summary": "Smart contracts power decentralized financial (DeFi) services but are vulnerable to security exploits that can lead to significant financial losses. Existing security measures often fail to adequately protect these contracts due to the composability of DeFi protocols and the increasing sophistication of attacks. Through a large-scale empirical study of historical transactions from the 37 hacked DeFi protocols, we discovered that while benign transactions typically exhibit a limited number of unique control flows, in stark contrast, attack transactions consistently introduce novel, previously unobserved control flows. Building on these insights, we developed CrossGuard, a novel framework that enforces control flow integrity onchain to secure smart contracts. Crucially, CrossGuard does not require prior knowledge of specific hacks. Instead, configured only once at deployment, it enforces control flow whitelisting policies and applies simplification heuristics at runtime. This approach monitors and prevents potential attacks by reverting all transactions that do not adhere to the established control flow whitelisting rules. Our evaluation demonstrates that CrossGuard effectively blocks 35 of the 37 analyzed attacks when configured only once at contract deployment, maintaining a low false positive rate of 0.26% and minimal additional gas costs. These results underscore the efficacy of applying control flow integrity to smart contracts, significantly enhancing security beyond traditional methods and addressing the evolving threat landscape in the DeFi ecosystem.", "published": "2025-04-07T21:08:16Z", "updated": "2026-01-16T00:40:36Z", "authors": ["Zhiyang Chen", "Sidi Mohamed Beillahi", "Pasha Barahimi", "Cyrus Minwalla", "Han Du", "Andreas Veneris", "Fan Long"], "pdf_url": "https://arxiv.org/pdf/2504.05509v2"}
{"id": "http://arxiv.org/abs/2508.03846v2", "title": "Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices", "summary": "Empathy is a powerful yet often overlooked element in software engineering (SE), supporting better teamwork, smoother communication, and effective decision-making.This paper introduces 17 actionable empathy guidelines designed to support practitioners, teams, and organisations. We also explore how these guidelines can be implemented in practice by examining real-world applications, challenges, and strategies to overcome them shared by software practitioners. To support adoption, we present a visual prioritisation framework that categorises the guidelines based on perceived importance, ease of implementation, and willingness to adopt. The findings offer practical and flexible suggestions for integrating empathy into everyday SE work, helping teams move from principles to sustainable action.", "published": "2025-08-05T18:44:12Z", "updated": "2026-01-16T00:05:09Z", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "pdf_url": "https://arxiv.org/pdf/2508.03846v2"}
