{"id": "http://arxiv.org/abs/2602.23262v1", "title": "Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling", "summary": "Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks.", "published": "2026-02-26T17:36:48Z", "updated": "2026-02-26T17:36:48Z", "authors": ["Jasmine Bayrooti", "Weiwei Kong", "Natalia Ponomareva", "Carlos Esteves", "Ameesh Makadia", "Amanda Prorok"], "pdf_url": "https://arxiv.org/pdf/2602.23262v1"}
{"id": "http://arxiv.org/abs/2602.23261v1", "title": "Strengthening security and noise resistance in one-way quantum key distribution protocols through hypercube-based quantum walks", "summary": "Quantum Key Distribution (QKD) is a foundational cryptographic protocol that ensures information-theoretic security. However, classical protocols such as BB84, though favored for their simplicity, offer limited resistance to eavesdropping, and perform poorly under realistic noise conditions. Recent research has explored the use of discrete-time Quantum Walks (QWs) to enhance QKD schemes. In this work, we specifically focus on a one-way QKD protocol, where security depends exclusively on the underlying Quantum Walk (QW) topology, rather than the details of the protocol itself. Our paper introduces a novel protocol based on QWs over a hypercube topology and demonstrates that, under identical parameters, it provides significantly enhanced security and noise resistance compared to the circular topology (i.e., state-of-the-art), thereby strengthening protection against eavesdropping. Furthermore, we introduce an efficient and extensible simulation framework for one-way QKD protocols based on QWs, supporting both circular and hypercube topologies. Implemented with IBM's software development kit for quantum computing (i.e., Qiskit), our toolkit enables noise-aware analysis under realistic noise models. To support reproducibility and future developments, we release our entire simulation framework as open-source. This contribution establishes a foundation for the design of topology-aware QKD protocols that combine enhanced noise tolerance with topologically driven security.", "published": "2026-02-26T17:35:53Z", "updated": "2026-02-26T17:35:53Z", "authors": ["David Polzoni", "Tommaso Bianchi", "Mauro Conti"], "pdf_url": "https://arxiv.org/pdf/2602.23261v1"}
{"id": "http://arxiv.org/abs/2507.00522v2", "title": "Cyber Attacks Detection, Prevention, and Source Localization in Digital Substation Communication using Hybrid Statistical-Deep Learning", "summary": "The digital transformation of power systems is accelerating the adoption of IEC 61850 standard. However, its communication protocols, including Sampled Values (SV), lack built-in security features such as authentication and encryption, making them vulnerable to malicious packet injection. Such cyber attacks can delay fault clearance or trigger unintended circuit breaker operations. While most existing research focuses on detecting cyber attacks in digital substations, intrusion prevention systems have been disregarded because of the risk of potential communication network disruptions. This paper proposes a novel method using hybrid statistical-deep learning for the detection, prevention, and source localization of IEC 61850 SV injection attacks. The method uses exponentially modified Gaussian distributions to model communication network latency and long short-term memory and Elman recurrent neural network to detect anomalous variations in the estimated probability distributions. It effectively discards malicious SV frames with minimal processing overhead and latency, maintains robustness against communication network latency variation and time-synchronization issues, and guarantees a near-zero false positive rate in non-attack scenarios. Comprehensive validation is conducted on three testbeds involving industrial-grade devices, hardware-in-the-loop simulations, virtualized intelligent electronic devices and merging units, and high-fidelity emulated communication networks. Results demonstrate the method's suitability for practical deployment in IEC 61850-compliant digital substations.", "published": "2025-07-01T07:38:22Z", "updated": "2026-02-26T17:03:49Z", "authors": ["Nicola Cibin", "Bas Mulder", "Herman Carstens", "Peter Palensky", "Alexandru Ştefanov"], "pdf_url": "https://arxiv.org/pdf/2507.00522v2"}
{"id": "http://arxiv.org/abs/2602.23167v1", "title": "SettleFL: Trustless and Scalable Reward Settlement Protocol for Federated Learning on Permissionless Blockchains (Extended version)", "summary": "In open Federated Learning (FL) environments where no central authority exists, ensuring collaboration fairness relies on decentralized reward settlement, yet the prohibitive cost of permissionless blockchains directly clashes with the high-frequency, iterative nature of model training. Existing solutions either compromise decentralization or suffer from scalability bottlenecks due to linear on-chain costs. To address this, we present SettleFL, a trustless and scalable reward settlement protocol designed to minimize total economic friction by offering a family of two interoperable protocols. Leveraging a shared domain-specific circuit architecture, SettleFL offers two interoperable strategies: (1) a Commit-and-Challenge variant that minimizes on-chain costs via optimistic execution and dispute-driven arbitration, and (2) a Commit-with-Proof variant that guarantees instant finality through per-round validity proofs. This design allows the protocol to flexibly adapt to varying latency and cost constraints while enforcing rational robustness without trusted coordination. We conduct extensive experiments combining real FL workloads and controlled simulations. Results show that SettleFL remains practical when scaling to 800 participants, achieving substantially lower gas cost.", "published": "2026-02-26T16:31:23Z", "updated": "2026-02-26T16:31:23Z", "authors": ["Shuang Liang", "Yang Hua", "Linshan Jiang", "Peishen Yan", "Tao Song", "Bin Yao", "Haibing Guan"], "pdf_url": "https://arxiv.org/pdf/2602.23167v1"}
{"id": "http://arxiv.org/abs/2602.23163v1", "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring", "summary": "Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \\textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \\textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.", "published": "2026-02-26T16:27:24Z", "updated": "2026-02-26T16:27:24Z", "authors": ["Usman Anwar", "Julianna Piskorz", "David D. Baek", "David Africa", "Jim Weatherall", "Max Tegmark", "Christian Schroeder de Witt", "Mihaela van der Schaar", "David Krueger"], "pdf_url": "https://arxiv.org/pdf/2602.23163v1"}
{"id": "http://arxiv.org/abs/2602.23121v1", "title": "Automated Vulnerability Detection in Source Code Using Deep Representation Learning", "summary": "Each year, software vulnerabilities are discovered, which pose significant risks of exploitation and system compromise. We present a convolutional neural network model that can successfully identify bugs in C code. We trained our model using two complementary datasets: a machine-labeled dataset created by Draper Labs using three static analyzers and the NIST SATE Juliet human-labeled dataset designed for testing static analyzers. In contrast with the work of Russell et al. on these datasets, we focus on C programs, enabling us to specialize and optimize our detection techniques for this language. After removing duplicates from the dataset, we tokenize the input into 91 token categories. The category values are converted to a binary vector to save memory. Our first convolution layer is chosen so that the entire encoding of the token is presented to the filter. We use two convolution and pooling layers followed by two fully connected layers to classify programs into either a common weakness enumeration category or as ``clean.'' We obtain higher recall than prior work by Russell et al. on this dataset when requiring high precision. We also demonstrate on a custom Linux kernel dataset that we are able to find real vulnerabilities in complex code with a low false-positive rate.", "published": "2026-02-26T15:35:17Z", "updated": "2026-02-26T15:35:17Z", "authors": ["C. Seas", "G. Fitzpatrick", "J. A. Hamilton", "M. C. Carlisle"], "pdf_url": "https://arxiv.org/pdf/2602.23121v1"}
{"id": "http://arxiv.org/abs/2602.23079v1", "title": "Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent", "summary": "The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.", "published": "2026-02-26T15:05:13Z", "updated": "2026-02-26T15:05:13Z", "authors": ["Boyang Zhang", "Yang Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.23079v1"}
{"id": "http://arxiv.org/abs/2602.23067v1", "title": "A High-Throughput AES-GCM Implementation on GPUs for Secure, Policy-Based Access to Massive Astronomical Catalogs", "summary": "The era of large astronomical surveys generates massive image catalogs requiring efficient and secure access, particularly during pre-publication periods where data confidentiality and integrity are paramount. While Findable, Accessible, Interoperable, and Reusable (FAIR) principles guide the eventual public dissemination of data, traditional security methods for restricted phases often lack granularity or incur prohibitive performance penalties. To address this, we present a framework that integrates a flexible policy engine for fine-grained access control with a novel GPU-accelerated implementation of the AES-GCM authenticated encryption protocol.\n  The novelty of this work lies in the adaptation and optimization of a parallel tree-reduction strategy to overcome the main performance bottleneck in authenticated encryption on GPUs: the inherently sequential Galois/Counter Mode (GCM) authentication hash (GHASH). We present both the algorithmic adaptation and its efficient execution on GPU architectures. Although similar parallelization techniques have been explored in cryptographic research, this is, to our knowledge, the first demonstration of their integration into a high-throughput encryption framework specifically designed for large-scale astronomical data. Our implementation transforms the sequential GHASH computation into a highly parallelizable, logarithmic-time process, achieving authenticated encryption throughput suitable for petabyte-scale image analysis.\n  Our solution provides a robust mechanism for data providers to enforce access policies, ensuring both confidentiality and integrity without hindering research workflows, thereby facilitating a secure and managed transition of data to public, FAIR archives.", "published": "2026-02-26T14:54:24Z", "updated": "2026-02-26T14:54:24Z", "authors": ["Samuel Lemes-Perera", "Miguel R. Alarcon", "Pino Caballero-Gil", "Miquel Serra-Ricart"], "pdf_url": "https://arxiv.org/pdf/2602.23067v1"}
{"id": "http://arxiv.org/abs/2510.07452v2", "title": "PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing", "summary": "Language models (LMs) may memorize personally identifiable information (PII) from training data, enabling adversaries to extract it during inference. Existing defense mechanisms such as differential privacy (DP) reduce this leakage, but incur large drops in utility. Based on a comprehensive study using circuit discovery to identify the computational circuits responsible PII leakage in LMs, we hypothesize that specific PII leakage circuits in LMs should be responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware Targeted Circuit PatcHing), a novel approach that first identifies and subsequently directly edits PII circuits to reduce leakage. PATCH achieves better privacy-utility trade-off than existing defenses, e.g., reducing recall of PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to reduce recall of residual leakage of an LM to as low as 0.01%. Our analysis shows that PII leakage circuits persist even after the application of existing defense mechanisms. In contrast, PATCH can effectively mitigate their impact.", "published": "2025-10-08T18:58:41Z", "updated": "2026-02-26T13:37:58Z", "authors": ["Anthony Hughes", "Vasisht Duddu", "N. Asokan", "Nikolaos Aletras", "Ning Ma"], "pdf_url": "https://arxiv.org/pdf/2510.07452v2"}
{"id": "http://arxiv.org/abs/2602.22983v1", "title": "Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search", "summary": "As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.", "published": "2026-02-26T13:25:35Z", "updated": "2026-02-26T13:25:35Z", "authors": ["Xun Huang", "Simeng Qin", "Xiaoshuang Jia", "Ranjie Duan", "Huanqian Yan", "Zhitao Zeng", "Fei Yang", "Yang Liu", "Xiaojun Jia"], "pdf_url": "https://arxiv.org/pdf/2602.22983v1"}
{"id": "http://arxiv.org/abs/2506.12108v2", "title": "A Lightweight IDS for Early APT Detection Using a Novel Feature Selection Method", "summary": "An Advanced Persistent Threat (APT) is a multistage, highly sophisticated, and covert form of cyber threat that gains unauthorized access to networks to either steal valuable data or disrupt the targeted network. These threats often remain undetected for extended periods, emphasizing the critical need for early detection in networks to mitigate potential APT consequences. In this work, we propose a feature selection method for developing a lightweight intrusion detection system capable of effectively identifying APTs at the initial compromise stage. Our approach leverages the XGBoost algorithm and Explainable Artificial Intelligence (XAI), specifically utilizing the SHAP (SHapley Additive exPlanations) method for identifying the most relevant features of the initial compromise stage. The results of our proposed method showed the ability to reduce the selected features of the SCVIC-APT-2021 dataset from 77 to just four while maintaining consistent evaluation metrics for the suggested system. The estimated metrics values are 97% precision, 100% recall, and a 98% F1 score. The proposed method not only aids in preventing successful APT consequences but also enhances understanding of APT behavior at early stages.", "published": "2025-06-13T09:07:56Z", "updated": "2026-02-26T12:57:51Z", "authors": ["Bassam Noori Shaker", "Bahaa Al-Musawi", "Mohammed Falih Hassan"], "pdf_url": "https://arxiv.org/pdf/2506.12108v2"}
{"id": "http://arxiv.org/abs/2509.04403v2", "title": "Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios", "summary": "Multimodal large language models (MLLMs) are rapidly evolving, presenting increasingly complex safety challenges. However, current dataset construction methods, which are risk-oriented, fail to cover the growing complexity of real-world multimodal safety scenarios (RMS). And due to the lack of a unified evaluation metric, their overall effectiveness remains unproven. This paper introduces a novel image-oriented self-adaptive dataset construction method for RMS, which starts with images and end constructing paired text and guidance responses. Using the image-oriented method, we automatically generate an RMS dataset comprising 35k image-text pairs with guidance responses. Additionally, we introduce a standardized safety dataset evaluation metric: fine-tuning a safety judge model and evaluating its capabilities on other safety datasets.Extensive experiments on various tasks demonstrate the effectiveness of the proposed image-oriented pipeline. The results confirm the scalability and effectiveness of the image-oriented approach, offering a new perspective for the construction of real-world multimodal safety datasets. The dataset is presented at https://huggingface.co/datasets/NewCityLetter/RMS2/tree/main.", "published": "2025-09-04T17:13:59Z", "updated": "2026-02-26T11:09:21Z", "authors": ["Jingen Qu", "Lijun Li", "Bo Zhang", "Yichen Yan", "Jing Shao"], "pdf_url": "https://arxiv.org/pdf/2509.04403v2"}
{"id": "http://arxiv.org/abs/2512.06660v2", "title": "Towards Small Language Models for Security Query Generation in SOC Workflows", "summary": "Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.", "published": "2025-12-07T05:18:27Z", "updated": "2026-02-26T10:44:56Z", "authors": ["Saleha Muzammil", "Rahul Reddy", "Vishal Kamalakrishnan", "Hadi Ahmadi", "Wajih Ul Hassan"], "pdf_url": "https://arxiv.org/pdf/2512.06660v2"}
{"id": "http://arxiv.org/abs/2410.10922v3", "title": "Towards Privacy-Guaranteed Label Unlearning in Vertical Federated Learning: Few-Shot Forgetting without Disclosure", "summary": "This paper addresses the critical challenge of unlearning in Vertical Federated Learning (VFL), a setting that has received far less attention than its horizontal counterpart. Specifically, we propose the first method tailored to \\textit{label unlearning} in VFL, where labels play a dual role as both essential inputs and sensitive information. To this end, we employ a representation-level manifold mixup mechanism to generate synthetic embeddings for both unlearned and retained samples. This is to provide richer signals for the subsequent gradient-based label forgetting and recovery steps. These augmented embeddings are then subjected to gradient-based label forgetting, effectively removing the associated label information from the model. To recover performance on the retained data, we introduce a recovery-phase optimization step that refines the remaining embeddings. This design achieves effective label unlearning while maintaining computational efficiency. We validate our method through extensive experiments on diverse datasets, including MNIST, CIFAR-10, CIFAR-100, ModelNet, Brain Tumor MRI, COVID-19 Radiography, and Yahoo Answers demonstrate strong efficacy and scalability. Overall, this work establishes a new direction for unlearning in VFL, showing that re-imagining mixup as an efficient mechanism can unlock practical and utility-preserving unlearning. The code is publicly available at \\href{https://github.com/bryanhx/Towards-Privacy-Guaranteed-Label-Unlearning-in-Vertical-Federated-Learning}{https://github.com/bryanhx/Towards-Privacy-Guaranteed-Label-Unlearning-in-Vertical-Federated-Learning}", "published": "2024-10-14T12:08:12Z", "updated": "2026-02-26T10:05:51Z", "authors": ["Hanlin Gu", "Hong Xi Tae", "Chee Seng Chan", "Lixin Fan"], "pdf_url": "https://arxiv.org/pdf/2410.10922v3"}
{"id": "http://arxiv.org/abs/2506.09950v4", "title": "Oracle-Based Multistep Strategy for Solving Polynomial Systems Over Finite Fields and Algebraic Cryptanalysis of the Aradi Cipher", "summary": "The multistep solving strategy consists in a divide-and-conquer approach: when a multivariate polynomial system is computationally infeasible to solve directly, one variable is assigned over the elements of the base finite field, and the procedure is recursively applied to the resulting simplified systems. In a previous work by the same authors (among others), this approach proved effective in the algebraic cryptanalysis of the Trivium cipher. In this paper, we present a new formulation of the corresponding algorithm based on a Depth-First Search strategy, along with a novel complexity analysis leveraging tree structures. We also introduce the notion of an ``oracle function'', which is intended to determine whether evaluating a new variable is required to simplify the current polynomial system. This notion allows us to unify all previously proposed variants of the multistep strategy, including the classical hybrid approach, by appropriately selecting the oracle function. Finally, we employ the multistep solving strategy in the cryptanalysis of the NSA's recently introduced low-latency block cipher Aradi, achieving a first full-round algebraic attack that exposes structural features in its symbolic model.", "published": "2025-06-11T17:18:25Z", "updated": "2026-02-26T09:04:56Z", "authors": ["Roberto La Scala", "Sharwan Kumar Tiwari"], "pdf_url": "https://arxiv.org/pdf/2506.09950v4"}
{"id": "http://arxiv.org/abs/2602.22729v1", "title": "RandSet: Randomized Corpus Reduction for Fuzzing Seed Scheduling", "summary": "Seed explosion is a fundamental problem in fuzzing seed scheduling, where a fuzzer maintains a huge corpus and fails to choose promising seeds. Existing works focus on seed prioritization but still suffer from seed explosion since corpus size remains huge. We tackle this from a new perspective: corpus reduction, i.e., computing a seed corpus subset. However, corpus reduction could lead to poor seed diversity and large runtime overhead. Prior techniques like cull_queue, AFL-Cmin, and MinSet suffer from poor diversity or prohibitive overhead, making them unsuitable for high-frequency seed scheduling.\n  We propose RandSet, a novel randomized corpus reduction technique that reduces corpus size and yields diverse seed selection simultaneously with minimal overhead. Our key insight is introducing randomness into corpus reduction to enjoy two benefits of a randomized algorithm: randomized output (diverse seed selection) and low runtime cost. Specifically, we formulate corpus reduction as a set cover problem and compute a randomized subset covering all features of the entire corpus. We then schedule seeds from this small, randomized subset rather than the entire corpus, effectively mitigating seed explosion.\n  We implement RandSet on three popular fuzzers: AFL++, LibAFL, and Centipede, and evaluate it on standalone programs, FuzzBench, and Magma. Results show RandSet achieves significantly more diverse seed selection than other reduction techniques, with average subset ratios of 4.03% and 5.99% on standalone and FuzzBench programs. RandSet achieves a 16.58% coverage gain on standalone programs and up to 3.57% on FuzzBench in AFL++, triggers up to 7 more ground-truth bugs than the state-of-the-art on Magma, while introducing only 1.17%-3.93% overhead.", "published": "2026-02-26T08:11:38Z", "updated": "2026-02-26T08:11:38Z", "authors": ["Yuchong Xie", "Kaikai Zhang", "Yu Liu", "Rundong Yang", "Ping Chen", "Shuai Wang", "Dongdong She"], "pdf_url": "https://arxiv.org/pdf/2602.22729v1"}
{"id": "http://arxiv.org/abs/2602.22724v1", "title": "AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification", "summary": "Large language model (LLM) agents increasingly rely on external tools and retrieval systems to autonomously complete complex tasks. However, this design exposes agents to indirect prompt injection (IPI), where attacker-controlled context embedded in tool outputs or retrieved content silently steers agent actions away from user intent. Unlike prompt-based attacks, IPI unfolds over multi-turn trajectories, making malicious control difficult to disentangle from legitimate task execution. Existing inference-time defenses primarily rely on heuristic detection and conservative blocking of high-risk actions, which can prematurely terminate workflows or broadly suppress tool usage under ambiguous multi-turn scenarios. We propose AgentSentry, a novel inference-time detection and mitigation framework for tool-augmented LLM agents. To the best of our knowledge, AgentSentry is the first inference-time defense to model multi-turn IPI as a temporal causal takeover. It localizes takeover points via controlled counterfactual re-executions at tool-return boundaries and enables safe continuation through causally guided context purification that removes attack-induced deviations while preserving task-relevant evidence. We evaluate AgentSentry on the \\textsc{AgentDojo} benchmark across four task suites, three IPI attack families, and multiple black-box LLMs. AgentSentry eliminates successful attacks and maintains strong utility under attack, achieving an average Utility Under Attack (UA) of 74.55 %, improving UA by 20.8 to 33.6 percentage points over the strongest baselines without degrading benign performance.", "published": "2026-02-26T07:59:10Z", "updated": "2026-02-26T07:59:10Z", "authors": ["Tian Zhang", "Yiwei Xu", "Juan Wang", "Keyan Guo", "Xiaoyang Xu", "Bowen Xiao", "Quanlong Guan", "Jinlin Fan", "Jiawei Liu", "Zhiquan Liu", "Hongxin Hu"], "pdf_url": "https://arxiv.org/pdf/2602.22724v1"}
{"id": "http://arxiv.org/abs/2602.22700v1", "title": "IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation", "summary": "Commercial large language models are typically deployed as black-box API services, requiring users to trust providers to execute inference correctly and report token usage honestly. We present IMMACULATE, a practical auditing framework that detects economically motivated deviations-such as model substitution, quantization abuse, and token overbilling-without trusted hardware or access to model internals. IMMACULATE selectively audits a small fraction of requests using verifiable computation, achieving strong detection guarantees while amortizing cryptographic overhead. Experiments on dense and MoE models show that IMMACULATE reliably distinguishes benign and malicious executions with under 1% throughput overhead. Our code is published at https://github.com/guo-yanpei/Immaculate.", "published": "2026-02-26T07:21:02Z", "updated": "2026-02-26T07:21:02Z", "authors": ["Yanpei Guo", "Wenjie Qu", "Linyu Wu", "Shengfang Zhai", "Lionel Z. Wang", "Ming Xu", "Yue Liu", "Binhang Yuan", "Dawn Song", "Jiaheng Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.22700v1"}
{"id": "http://arxiv.org/abs/2602.22699v1", "title": "DPSQL+: A Differentially Private SQL Library with a Minimum Frequency Rule", "summary": "SQL is the de facto interface for exploratory data analysis; however, releasing exact query results can expose sensitive information through membership or attribute inference attacks. Differential privacy (DP) provides rigorous privacy guarantees, but in practice, DP alone may not satisfy governance requirements such as the \\emph{minimum frequency rule}, which requires each released group (cell) to include contributions from at least $k$ distinct individuals. In this paper, we present \\textbf{DPSQL+}, a privacy-preserving SQL library that simultaneously enforces user-level $(\\varepsilon,δ)$-DP and the minimum frequency rule. DPSQL+ adopts a modular architecture consisting of: (i) a \\emph{Validator} that statically restricts queries to a DP-safe subset of SQL; (ii) an \\emph{Accountant} that consistently tracks cumulative privacy loss across multiple queries; and (iii) a \\emph{Backend} that interfaces with various database engines, ensuring portability and extensibility. Experiments on the TPC-H benchmark demonstrate that DPSQL+ achieves practical accuracy across a wide range of analytical workloads -- from basic aggregates to quadratic statistics and join operations -- and allows substantially more queries under a fixed global privacy budget than prior libraries in our evaluation.", "published": "2026-02-26T07:21:00Z", "updated": "2026-02-26T07:21:00Z", "authors": ["Tomoya Matsumoto", "Shokichi Takakura", "Shun Takagi", "Satoshi Hasegawa"], "pdf_url": "https://arxiv.org/pdf/2602.22699v1"}
{"id": "http://arxiv.org/abs/2602.22689v1", "title": "No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings", "summary": "Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training. However, existing approaches assume access to ground-truth captions. This assumption fails in realistic scenarios where only images are available and their textual annotations remain undisclosed, rendering prior methods ineffective when substituted with vision-language model (VLM) captions. In this work, we propose MoFit, a caption-free MIA framework that constructs synthetic conditioning inputs that are explicitly overfitted to the target model's generative manifold. Given a query image, MoFit proceeds in two stages: (i) model-fitted surrogate optimization, where a perturbation applied to the image is optimized to construct a surrogate in regions of the model's unconditional prior learned from member samples, and (ii) surrogate-driven embedding extraction, where a model-fitted embedding is derived from the surrogate and then used as a mismatched condition for the query image. This embedding amplifies conditional loss responses for member samples while leaving hold-outs relatively less affected, thereby enhancing separability in the absence of ground-truth captions. Our comprehensive experiments across multiple datasets and diffusion models demonstrate that MoFit consistently outperforms prior VLM-conditioned baselines and achieves performance competitive with caption-dependent methods.", "published": "2026-02-26T07:07:11Z", "updated": "2026-02-26T07:07:11Z", "authors": ["Joonsung Jeon", "Woo Jae Kim", "Suhyeon Ha", "Sooel Son", "Sung-Eui Yoon"], "pdf_url": "https://arxiv.org/pdf/2602.22689v1"}
{"id": "http://arxiv.org/abs/2502.04758v2", "title": "Differential Privacy of Quantum and Quantum-Inspired Classical Recommendation Algorithms", "summary": "We study the differential privacy (DP) of the quantum recommendation algorithm of Kerenidis--Prakash and its quantum-inspired classical counterpart. Under standard low-rank and incoherence assumptions on the preference matrix, we show that the randomness already present in the algorithms' measurement/$\\ell_2$-sampling steps can act as a privacy-curating mechanism, yielding $(\\varepsilon,δ)$-DP without injecting additional DP noise through the interface. Concretely, for a system with $m$ users and $n$ items and rank parameter $k$, we prove $\\varepsilon=\\mathcal O(\\sqrt{k/n})$ and $δ= \\mathcal O\\big(k^2/\\min^2\\{m,n\\}\\big)$; in the typical regime $k=\\mathrm{polylog}(m,n)$ this simplifies to $\\varepsilon=\\tilde{\\mathcal O}(1/\\sqrt n)$ and $δ=\\tilde{\\mathcal O}\\big(1/\\min^2\\{m,n\\}\\big)$. Our analysis introduces a perturbation technique for truncated SVD under a single-entry update, which tracks the induced change in the low-rank reconstruction while avoiding unstable singular-vector comparisons. Finally, we validate the scaling on real-world rating datasets and compare against classical DP recommender baselines.", "published": "2025-02-07T08:45:00Z", "updated": "2026-02-26T07:00:42Z", "authors": ["Chenjian Li", "Mingsheng Ying", "Ji Guan"], "pdf_url": "https://arxiv.org/pdf/2502.04758v2"}
{"id": "http://arxiv.org/abs/2505.05712v4", "title": "LLM-Text Watermarking based on Lagrange Interpolation", "summary": "The rapid advancement of LLMs (Large Language Models) has established them as a foundational technology for many AI and ML-powered human computer interactions. A critical challenge in this context is the attribution of LLM-generated text -- either to the specific language model that produced it or to the individual user who embedded their identity via a so-called multi-bit watermark. This capability is essential for combating misinformation, fake news, misinterpretation, and plagiarism. One of the key techniques for addressing this challenge is digital watermarking.\n  This work presents a watermarking scheme for LLM-generated text based on Lagrange interpolation, enabling the recovery of a multi-bit author identity even when the text has been heavily redacted by an adversary. The core idea is to embed a continuous sequence of points $(x, f(x))$ that lie on a single straight line. The $x$-coordinates are computed pseudorandomly using a cryptographic hash function $H$ applied to the concatenation of the previous token's identity and a secret key $s_k$. Crucially, the $x$-coordinates do not need to be embedded into the text -- only the corresponding $f(x)$ values are embedded. During extraction, the algorithm recovers the original points along with many spurious ones, forming an instance of the Maximum Collinear Points (MCP) problem, which can be solved efficiently. Experimental results demonstrate that the proposed method is highly effective, allowing the recovery of the author identity even when as few as three genuine points remain after adversarial manipulation.", "published": "2025-05-09T01:19:01Z", "updated": "2026-02-26T03:23:21Z", "authors": ["Jarosław Janas", "Paweł Morawiecki", "Josef Pieprzyk"], "pdf_url": "https://arxiv.org/pdf/2505.05712v4"}
{"id": "http://arxiv.org/abs/2602.22562v1", "title": "Layer-Targeted Multilingual Knowledge Erasure in Large Language Models", "summary": "Recent work has demonstrated that machine unlearning in Large Language Models (LLMs) fails to generalize across languages: knowledge erased in one language frequently remains accessible through others. However, the underlying cause of this failure and a principled solution remain open. In this work, we identify intervention depth as the key factor determining multilingual generalization. Through systematic layer-wise experiments, we characterize two distinct failure modes: shallow-layer interventions achieve erasure but collapse multilingual capabilities in held-out languages, while deep-layer interventions preserve utility but fail to erase target knowledge even in source languages. These findings reveal that the choice of intervention layer is not a free parameter; it fundamentally determines whether multilingual unlearning succeeds. We propose MUTE (Multilingual Unlearning via Targeted Erasure), a framework that uses Centered Kernel Alignment (CKA) and Linguistic Regions Development Score (LRDS) to identify intermediate, language-agnostic layers where cross-lingual representations converge. By restricting unlearning updates to these layers, MUTE achieves robust multilingual knowledge erasure while optimizing on only a small set of source languages. Extensive experiments across three LLM architectures and three unlearning algorithms validate our approach, with mechanistic analysis via Logit Lens probing confirming genuine knowledge removal rather than output-level suppression.", "published": "2026-02-26T03:00:07Z", "updated": "2026-02-26T03:00:07Z", "authors": ["Taoran Li", "Varun Chandrasekaran", "Zhiyuan Yu"], "pdf_url": "https://arxiv.org/pdf/2602.22562v1"}
{"id": "http://arxiv.org/abs/2602.21394v2", "title": "MemoPhishAgent: Memory-Augmented Multi-Modal LLM Agent for Phishing URL Detection", "summary": "Traditional phishing website detection relies on static heuristics or reference lists, which lag behind rapidly evolving attacks. While recent systems incorporate large language models (LLMs), they are still prompt-based, deterministic pipelines that underutilize reasoning capability. We present MemoPhishAgent (MPA), a memory-augmented multi-modal LLM agent that dynamically orchestrates phishing-specific tools and leverages episodic memories of past reasoning trajectories to guide decisions on recurring and novel threats. On two public datasets, MPA outperforms three state-of-the-art (SOTA) baselines, improving recall by 13.6%. To better reflect realistic, user-facing phishing detection performance, we further evaluate MPA on a benchmark of real-world suspicious URLs actively crawled from five social media platforms, where it improves recall by 20%. Detailed analysis shows episodic memory contributes up to 27% recall gain without introducing additional computational overhead. The ablation study confirms the necessity of the agent-based approach compared to prompt-based baselines and validates the effectiveness of our tool design. Finally, MPA is deployed in production, processing 60K targeted high-risk URLs weekly, and achieving 91.44% recall, providing proactive protection for millions of customers. Together, our results show that combining multi-modal reasoning with episodic memory yields robust phishing detection in realistic user-exposure settings.", "published": "2026-02-24T21:50:46Z", "updated": "2026-02-26T02:32:50Z", "authors": ["Xuan Chen", "Hao Liu", "Tao Yuan", "Mehran Kafai", "Piotr Habas", "Xiangyu Zhang"], "pdf_url": "https://arxiv.org/pdf/2602.21394v2"}
{"id": "http://arxiv.org/abs/2602.22525v1", "title": "Systems-Level Attack Surface of Edge Agent Deployments on IoT", "summary": "Edge deployment of LLM agents on IoT hardware introduces attack surfaces absent from cloud-hosted orchestration. We present an empirical security analysis of three architectures (cloud-hosted, edge-local swarm, and hybrid) using a multi-device home-automation testbed with local MQTT messaging and an Android smartphone as an edge inference node. We identify five systems-level attack surfaces, including two emergent failures observed during live testbed operation: coordination-state divergence and induced trust erosion. We frame core security properties as measurable systems metrics: data egress volume, failover window exposure, sovereignty boundary integrity, and provenance chain completeness. Our measurements show that edge-local deployments eliminate routine cloud data exposure but silently degrade sovereignty when fallback mechanisms trigger, with boundary crossings invisible at the application layer. Provenance chains remain complete under cooperative operation yet are trivially bypassed without cryptographic enforcement. Failover windows create transient blind spots exploitable for unauthorised actuation. These results demonstrate that deployment architecture, not just model or prompt design, is a primary determinant of security risk in agent-controlled IoT systems.", "published": "2026-02-26T01:48:46Z", "updated": "2026-02-26T01:48:46Z", "authors": ["Zhonghao Zhan", "Krinos Li", "Yefan Zhang", "Hamed Haddadi"], "pdf_url": "https://arxiv.org/pdf/2602.22525v1"}
