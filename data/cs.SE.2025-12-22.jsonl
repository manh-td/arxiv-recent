{"id": "http://arxiv.org/abs/2306.00029v2", "title": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLMs", "summary": "Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners.", "published": "2023-05-31T05:24:48Z", "updated": "2025-12-22T18:29:22Z", "authors": ["Nghi D. Q. Bui", "Hung Le", "Yue Wang", "Junnan Li", "Akhilesh Deepak Gotmare", "Steven C. H. Hoi"], "pdf_url": "https://arxiv.org/pdf/2306.00029v2"}
{"id": "http://arxiv.org/abs/2512.19644v1", "title": "More code, less validation: Risk factors for over-reliance on AI coding tools among scientists", "summary": "Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.", "published": "2025-12-22T18:17:54Z", "updated": "2025-12-22T18:17:54Z", "authors": ["Gabrielle O'Brien", "Alexis Parker", "Nasir Eisty", "Jeffrey Carver"], "pdf_url": "https://arxiv.org/pdf/2512.19644v1"}
{"id": "http://arxiv.org/abs/2512.10218v2", "title": "Does SWE-Bench-Verified Test Agent Ability or Model Memory?", "summary": "SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.", "published": "2025-12-11T02:11:06Z", "updated": "2025-12-22T16:30:01Z", "authors": ["Thanosan Prathifkumar", "Noble Saji Mathews", "Meiyappan Nagappan"], "pdf_url": "https://arxiv.org/pdf/2512.10218v2"}
{"id": "http://arxiv.org/abs/2512.19509v1", "title": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models", "summary": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.", "published": "2025-12-22T16:04:56Z", "updated": "2025-12-22T16:04:56Z", "authors": ["Shangbo Yun", "Xiaodong Gu", "Jianghong Huang", "Beijun Shen"], "pdf_url": "https://arxiv.org/pdf/2512.19509v1"}
{"id": "http://arxiv.org/abs/2512.19481v1", "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis", "summary": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.", "published": "2025-12-22T15:32:45Z", "updated": "2025-12-22T15:32:45Z", "authors": ["Katharina Stengg", "Christian Macho", "Martin Pinzger"], "pdf_url": "https://arxiv.org/pdf/2512.19481v1"}
{"id": "http://arxiv.org/abs/2512.10713v2", "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code", "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.", "published": "2025-12-11T14:49:56Z", "updated": "2025-12-22T12:47:11Z", "authors": ["Itay Dreyfuss", "Antonio Abu Nassar", "Samuel Ackerman", "Axel Ben David", "Eitan Farchi", "Rami Katan", "Orna Raz", "Marcel Zalmanovici"], "pdf_url": "https://arxiv.org/pdf/2512.10713v2"}
{"id": "http://arxiv.org/abs/2503.04521v2", "title": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market", "summary": "The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.", "published": "2025-03-06T15:08:31Z", "updated": "2025-12-22T11:48:52Z", "authors": ["Songyuan Li", "Jia Hu", "Geyong Min", "Haojun Huang", "Jiwei Huang"], "pdf_url": "https://arxiv.org/pdf/2503.04521v2"}
{"id": "http://arxiv.org/abs/2509.06716v2", "title": "Efficiently Ranking Software Variants with Minimal Benchmarks", "summary": "Benchmarking is a common practice in software engineering to assess the qualities and performance of software variants, coming from multiple competing systems or from configurations of the same system. Benchmarks are used notably to compare and understand variant performance, fine-tune software, detect regressions, or design new software systems. The execution of benchmarks to get a complete picture of software variants is highly costly in terms of computational resources and time. In this paper, we propose a novel approach for reducing benchmarks while maintaining stable rankings, using test suite optimization techniques. That is, we remove instances from the benchmarks while trying to keep the same rankings of the variants on all tests. Our method, BISection Sampling, BISS, strategically retains the most critical tests and applies a novel divide-and-conquer approach to efficiently sample among relevant remaining tests. We experiment with datasets and use cases from LLM leaderboards, SAT competitions, and configurable systems for performance modeling. Our results show that our method outperforms baselines even when operating on a subset of variants. Using BISS, we reduce the computational cost of the benchmarks on average to 44% and on more than half the benchmarks by up to 99% without loss in ranking stability.", "published": "2025-09-08T14:11:35Z", "updated": "2025-12-22T10:17:59Z", "authors": ["Théo Matricon", "Mathieu Acher", "Helge Spieker", "Arnaud Gotlieb"], "pdf_url": "https://arxiv.org/pdf/2509.06716v2"}
{"id": "http://arxiv.org/abs/2512.19215v1", "title": "Semantically-Equivalent Transformations-Based Backdoor Attacks against Neural Code Models: Characterization and Mitigation", "summary": "Neural code models have been increasingly incorporated into software development processes. However, their susceptibility to backdoor attacks presents a significant security risk. The state-of-the-art understanding focuses on injection-based attacks, which insert anomalous patterns into software code. These attacks can be neutralized by standard sanitization techniques. This status quo may lead to a false sense of security regarding backdoor attacks. In this paper, we introduce a new kind of backdoor attacks, dubbed Semantically-Equivalent Transformation (SET)-based backdoor attacks, which use semantics-preserving low-prevalence code transformations to generate stealthy triggers. We propose a framework to guide the generation of such triggers. Our experiments across five tasks, six languages, and models like CodeBERT, CodeT5, and StarCoder show that SET-based attacks achieve high success rates (often >90%) while preserving model utility. The attack proves highly stealthy, evading state-of-the-art defenses with detection rates on average over 25.13% lower than injection-based counterparts. We evaluate normalization-based countermeasures and find they offer only partial mitigation, confirming the attack's robustness. These results motivate further investigation into scalable defenses tailored to SET-based attacks.", "published": "2025-12-22T09:54:52Z", "updated": "2025-12-22T09:54:52Z", "authors": ["Junyao Ye", "Zhen Li", "Xi Tang", "Shouhuai Xu", "Deqing Zou", "Zhongsheng Yuan"], "pdf_url": "https://arxiv.org/pdf/2512.19215v1"}
{"id": "http://arxiv.org/abs/2512.17280v2", "title": "Sensor Management System (SMS): Open-source software for FAIR sensor metadata management in Earth system sciences", "summary": "Deriving reliable conclusions and insights from environmental observational data urgently requires the enrichment with consistent and comprehensive metadata, including time-resolved context such as changing deployments, configurations, and maintenance actions. We have therefore developed the Sensor Management System (SMS), which provides a user-friendly and feature-rich platform for modeling even the most complex sensor systems and managing all sensor-related information across their life cycle. Each entity is described via well-defined terms like Devices, Platforms and Configurations, as well as Sites that are further enhanced with attributes for, e.g., instrument manufacturers, contact information or measured quantities and complemented by a continuous history of system-related actions. By further linking the SMS to sub-sequent systems and services like PID-registration or controlled vocabularies and establishing a community of end-users, the SMS provides the central element of a digital ecosystem, that fosters a more consistent, sustainable and FAIR provision of sensor-related metadata.", "published": "2025-12-19T06:55:44Z", "updated": "2025-12-22T09:29:13Z", "authors": ["Christof Lorenz", "Nils Brinckmann", "Jan Bumberger", "Marc Hanisch", "Tobias Kuhnert", "Ulrich Loup", "Rubankumar Moorthy", "Florian Obsersteiner", "David Schäfer", "Thomas Schnicke"], "pdf_url": "https://arxiv.org/pdf/2512.17280v2"}
{"id": "http://arxiv.org/abs/2512.19153v1", "title": "University Rents Enabling Corporate Innovation: Mapping Academic Researcher Coding and Discursive Labour in the R Language Ecosystem", "summary": "This article explores the role of unrecognised labour in corporate innovation systems via an analysis of researcher coding and discursive contributions to R, one of the largest statistical software ecosystems. Studies of online platforms typically focus on how platform affordances constrain participants' actions, and profit from their labour. We innovate by connecting the labour performed inside digital platforms to the professional employment of participants. Our case study analyses 8,924 R package repositories on GitHub, examining commits and communications. Our quantitative findings show that researchers, alongside non-affiliated contributors, are the most frequent owners of R package repositories and their most active contributors. Researchers are more likely to hold official roles compared to the average, and to engage in collaborative problem-solving and support work during package development. This means there is, underneath the 'recognised' category of star researchers who transition between academia and industry and secure generous funding, an 'unrecognised' category of researchers who not only create and maintain key statistical infrastructure, but also provide support to industry employees, for no remuneration. Our qualitative findings show how this unrecognised labour affects practitioners. Finally, our analysis of the ideology and practice of free, libre and open source software (FLOSS) shows how this ideology and practice legitimate the use of 'university rents' by Big Tech.", "published": "2025-12-22T08:50:21Z", "updated": "2025-12-22T08:50:21Z", "authors": ["Xiaolan Cai", "Mathieu O'Neil", "Stefano Zacchiroli"], "pdf_url": "https://arxiv.org/pdf/2512.19153v1"}
{"id": "http://arxiv.org/abs/2512.14806v4", "title": "Let the Barbarians In: How AI Can Accelerate Systems Performance Research", "summary": "Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight.\n  Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.", "published": "2025-12-16T18:51:23Z", "updated": "2025-12-22T08:18:12Z", "authors": ["Audrey Cheng", "Shu Liu", "Melissa Pan", "Zhifei Li", "Shubham Agarwal", "Mert Cemri", "Bowen Wang", "Alexander Krentsel", "Tian Xia", "Jongseok Park", "Shuo Yang", "Jeff Chen", "Lakshya Agrawal", "Ashwin Naren", "Shulu Li", "Ruiying Ma", "Aditya Desai", "Jiarong Xing", "Koushik Sen", "Matei Zaharia", "Ion Stoica"], "pdf_url": "https://arxiv.org/pdf/2512.14806v4"}
{"id": "http://arxiv.org/abs/2512.19122v1", "title": "BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation", "summary": "Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.", "published": "2025-12-22T07:53:16Z", "updated": "2025-12-22T07:53:16Z", "authors": ["Mahir Labib Dihan", "Sadif Ahmed", "Md Nafiu Rahman"], "pdf_url": "https://arxiv.org/pdf/2512.19122v1"}
{"id": "http://arxiv.org/abs/2512.19018v1", "title": "PEAK: A Performance Engineering AI-Assistant for GPU Kernels Powered by Natural Language Transformations", "summary": "Advancements in large language models (LLMs) are showing promising impact in software development and programming assistance. However, these models struggle when operating on low-level backend code. This challenge is exacerbated in the domain of GPU kernels, where performance-critical details are coupled to rapidly evolving hardware characteristics and available code examples are sparse.\n  In this work, we introduce PEAK, a Performance Engineering AI-Assistant for GPU Kernels powered by natural language transformations. PEAK utilizes the key insight that iterative code transformations (optimizations) can straightforwardly be written in natural language, and then carried out by LLMs. Thus, these transformations can be rapidly developed, encoding general portable optimizations, but also easily specialized to specific GPU devices and even kernels. These natural transformations are supported by a modular and extensible infrastructure that additionally performs validation and performance evaluation. We demonstrate the flexibility of PEAK by instantiating it for three backends, CUDA, HIP, and HLSL, and create 16 natural transformations for optimizing matrix multiplication kernels. We show that our resulting implementations are competitive with vendor libraries when available, and for HLSL (without a library) our implementations match the hardware documented FLOPS. PEAK allows the fine-grained exploration of several research questions around how LLMs behave in this domain, including characterizing transformations and their errors; and how performance evolves along optimization sequences. PEAK provides an interface that can either be utilized by performance engineers to improve productivity, or driven completely autonomously (e.g., by an AI agent), providing a forward-compatible design that can continue to improve with advances in AI capabilities.", "published": "2025-12-22T04:15:24Z", "updated": "2025-12-22T04:15:24Z", "authors": ["Muhammad Usman Tariq", "Abhinav Jangda", "Angelica Moreira", "Madan Musuvathi", "Tyler Sorensen"], "pdf_url": "https://arxiv.org/pdf/2512.19018v1"}
{"id": "http://arxiv.org/abs/2512.18996v1", "title": "Modular Layout Synthesis (MLS): Front-end Code via Structure Normalization and Constrained Generation", "summary": "Automated front-end engineering drastically reduces development cycles and minimizes manual coding overhead. While Generative AI has shown promise in translating designs to code, current solutions often produce monolithic scripts, failing to natively support modern ecosystems like React, Vue, or Angular. Furthermore, the generated code frequently suffers from poor modularity, making it difficult to maintain. To bridge this gap, we introduce Modular Layout Synthesis (MLS), a hierarchical framework that merges visual understanding with structural normalization. Initially, a visual-semantic encoder maps the screen capture into a serialized tree topology, capturing the essential layout hierarchy. Instead of simple parsing, we apply heuristic deduplication and pattern recognition to isolate reusable blocks, creating a framework-agnostic schema. Finally, a constraint-based generation protocol guides the LLM to synthesize production-ready code with strict typing and component props. Evaluations show that MLS significantly outperforms existing baselines, ensuring superior code reusability and structural integrity across multiple frameworks", "published": "2025-12-22T03:24:11Z", "updated": "2025-12-22T03:24:11Z", "authors": ["Chong Liu", "Ming Zhang", "Fei Li", "Hao Zhou", "Xiaoshuang Chen", "Ye Yuan"], "pdf_url": "https://arxiv.org/pdf/2512.18996v1"}
{"id": "http://arxiv.org/abs/2403.15676v5", "title": "AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs", "summary": "Zero-knowledge proof (ZKP) systems have surged attention and held a fundamental role in contemporary cryptography. Zero-knowledge succinct non-interactive argument of knowledge (zk-SNARK) protocols dominate the ZKP usage, implemented through arithmetic circuit programming paradigm. However, underconstrained or overconstrained circuits may lead to bugs. The former refers to circuits that lack the necessary constraints, resulting in unexpected solutions and causing the verifier to accept a bogus witness, and the latter refers to circuits that are constrained excessively, resulting in lacking necessary solutions and causing the verifier to accept no witness. This paper introduces a novel approach for pinpointing two distinct types of bugs in ZKP circuits. The method involves encoding the arithmetic circuit constraints to polynomial equation systems and solving them over finite fields by the computer algebra system. The classification of verification results is refined, greatly enhancing the expressive power of the system. A tool, AC4, is proposed to represent the implementation of the method. Experiments show that AC4 demonstrates a increase in the solved rate, showing a 29% improvement over Picus and CIVER, and a slight improvement over halo2-analyzer, a checker for halo2 circuits. Within a solvable range, the checking time has also exhibited noticeable improvement, demonstrating a magnitude increase compared to previous efforts.", "published": "2024-03-23T01:44:57Z", "updated": "2025-12-22T02:52:50Z", "authors": ["Qizhe Yang", "Boxuan Liang", "Hao Chen", "Guoqiang Li"], "pdf_url": "https://arxiv.org/pdf/2403.15676v5"}
{"id": "http://arxiv.org/abs/2512.18966v1", "title": "Scrum Sprint Planning: LLM-based and algorithmic solutions", "summary": "Planning for an upcoming project iteration (sprint) is one of the key activities in Scrum planning. In this paper, we present our work in progress on exploring the applicability of Large Language Models (LLMs) for solving this problem. We conducted case studies with manually created data sets to investigate the applicability of OpenAI models for supporting the sprint planning activities. In our experiments, we applied three models provided OpenAI: GPT-3.5 Turbo, GPT-4.0 Turbo, and Val. The experiments demonstrated that the results produced by the models aren't of acceptable quality for direct use in Scrum projects.", "published": "2025-12-22T02:26:11Z", "updated": "2025-12-22T02:26:11Z", "authors": ["Yuwon Yoon", "Kevin Iwan", "Madeleine Zwart", "Xiaohan Qin", "Hina Lee", "Maria Spichkova"], "pdf_url": "https://arxiv.org/pdf/2512.18966v1"}
{"id": "http://arxiv.org/abs/2405.04994v3", "title": "SPVR: syntax-to-prompt vulnerability repair based on large language models", "summary": "Purpose: In the field of vulnerability repair, previous research has leveraged pretrained models and LLM-based prompt engineering, among which LLM-based approaches show better generalizability and achieve the best performance. However, the LLM-based approaches generally regard vulnerability repair as a sequence-to-sequence task, and do not explicitly capture the syntax patterns for different vulnerability types, leading to limited accuracy. We aim to create a method that ensures the specificity of prompts targeting vulnerable code while also leveraging the generative capabilities of Large Language Models. Methods: We propose SPVR (Syntax-to-Prompt Vulnerability Repair), a novel framework that collects information from syntax trees, and generates corresponding prompts. Our method consists of three steps: rule design, prompt generation, and patch generation. In the rule design step, our method parses code patches and designs rules to extract relevant contextual information. These rules aid in identifying vulnerability-related issues. In the prompt generation step, our method extracts information from vulnerable code with pre-defined rules, automatically converting them into prompts. We also incorporate the description of CWE (Common Weakness Enumeration) as known information into the prompts. Finally, in the patch generation step, this prompt will serve as input to any conversational LLM to obtain code patches. Results: Extensive experiments validate that our method achieves excellent results in assisting LLMs to fix vulnerabilities accurately. We utilize multiple Large Language Models to validate the effectiveness of our work, repairing 143 of 547 vulnerable code using ChatGPT-4. We conducted a comparison of our approach against several existing vulnerability repair approaches (including fine-tuning-based and prompt-based), across multiple metrics.", "published": "2024-05-08T11:58:55Z", "updated": "2025-12-22T01:23:57Z", "authors": ["Ruoke Wang", "Zongjie Li", "Cuiyun Gao", "Chaozheng Wang", "Yang Xiao", "Xuan Wang"], "pdf_url": "https://arxiv.org/pdf/2405.04994v3"}
{"id": "http://arxiv.org/abs/2512.18940v1", "title": "FASTRIC: Prompt Specification Language for Verifiable LLM Interactions", "summary": "Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.", "published": "2025-12-22T01:19:50Z", "updated": "2025-12-22T01:19:50Z", "authors": ["Wen-Long Jin"], "pdf_url": "https://arxiv.org/pdf/2512.18940v1"}
{"id": "http://arxiv.org/abs/2409.01382v2", "title": "Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary Models Across Function and Class Granularities", "summary": "The adoption of Large Language Models (LLMs) for code generation risks incorporating vulnerable code into software systems. Existing detectors face two critical limitations: a lack of systematic cross-model validation and opaque \"black box\" operation. We address this through a comparative study of code generated by four distinct LLMs: GPT-3.5, Claude 3 Haiku, Claude Haiku 4.5, and GPT-OSS.\n  Analyzing 14,485 Python functions and 11,913 classes from the CodeSearchNet dataset, we generated corresponding code with all four LLMs. Using interpretable software metrics, we trained CatBoost classifiers for each configuration. Our analysis reveals that granularity effects dominate model differences by a factor of 8.6, with negligible feature overlap, indicating that function-level and class-level detection rely on fundamentally disjoint structural signatures.\n  We discover critical granularity-dependent inversions: while modern models (Claude, GPT-OSS) are more detectable at the class level, GPT-3.5 is an anomaly that uniquely excels at the function level. SHAP analysis identifies the Comment-to-Code Ratio as the sole universal discriminator. However, its predictive magnitude varies drastically across models, explaining why detectors trained on specific LLMs fail to generalize.\n  Our findings demonstrate that GPT-3.5's exceptional detectability (AUC-ROC 0.96) is unrepresentative of contemporary models (AUC-ROC approximately between 0.68 and 0.80). Robust detection requires moving beyond single-model studies to account for substantial diversity in structural fingerprints across architectures and granularities.", "published": "2024-09-02T17:25:15Z", "updated": "2025-12-22T01:18:44Z", "authors": ["Musfiqur Rahman", "SayedHassan Khatoonabadi", "Ahmad Abdellatif", "Emad Shihab"], "pdf_url": "https://arxiv.org/pdf/2409.01382v2"}
